{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка матричного равенства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверяем истинное равенство A*B и C:\n",
      "True\n",
      "Проверяем заведомо неверное равенство:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def freivalds_check(A, B, C, k=2):\n",
    "    \"\"\"\n",
    "    Проверяет равенство A @ B = C\n",
    "    с помощью алгоритма Фрейвалдса k раз.\n",
    "    Возвращает True (скорее всего) или False (точно).\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    for _ in range(k):\n",
    "        # Случайный вектор (элементы -1 или 1)\n",
    "        r = np.random.randint(0, 2, size=(n,)) * 2 - 1\n",
    "        \n",
    "        Br = B @ r      # O(n^2)\n",
    "        C_r = C @ r     # O(n^2)\n",
    "        ABr = A @ Br    # O(n^2)\n",
    "        \n",
    "        if not np.allclose(ABr, C_r, atol=1e-8):\n",
    "            return False  # Точно не равны\n",
    "    return True           # Скорее всего равны\n",
    "\n",
    "# Демонстрация\n",
    "n = 500\n",
    "A = np.random.randn(n, n)\n",
    "B = np.random.randn(n, n)\n",
    "C = A @ B  # Истинное произведение\n",
    "\n",
    "print(\"Проверяем истинное равенство A*B и C:\")\n",
    "print(freivalds_check(A, B, C, k=3))\n",
    "\n",
    "# Немного подпортим C\n",
    "C_perturbed = C.copy()\n",
    "C_perturbed[0,0] += 1e-1\n",
    "\n",
    "print(\"Проверяем заведомо неверное равенство:\")\n",
    "print(freivalds_check(A, B, C_perturbed, k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рандомизированное матричное умножение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительная ошибка Фробениуса: 0.0377\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randomized_matrix_multiplication(A, B, k):\n",
    "    \"\"\"Приближенное матричное умножение AB с сэмплированием.\"\"\"\n",
    "    m, n = A.shape\n",
    "    n_b, p = B.shape\n",
    "    if n != n_b:\n",
    "        raise ValueError(\"Несовместимые размеры матриц\")\n",
    "\n",
    "    # Вычисляем нормы для вероятностей (можно упростить до p_i = 1/n)\n",
    "    col_norms_A = np.linalg.norm(A, axis=0)\n",
    "    row_norms_B = np.linalg.norm(B, axis=1)\n",
    "    probs = col_norms_A * row_norms_B\n",
    "    sum_probs = np.sum(probs)\n",
    "    if sum_probs == 0: # Если одна из матриц нулевая\n",
    "         return np.zeros((m, p))\n",
    "    probs /= sum_probs\n",
    "\n",
    "    # Можно использовать равномерное распределение p_i = 1/n\n",
    "    # probs = np.ones(n) / n\n",
    "\n",
    "    C_approx = np.zeros((m, p))\n",
    "    indices = np.random.choice(n, size=k, p=probs)\n",
    "    # Если используем равномерное распределение:\n",
    "    # indices = np.random.choice(n, size=k)\n",
    "    # probs_selected = np.ones(k) / n\n",
    "\n",
    "    probs_selected = probs[indices]\n",
    "\n",
    "    for t in range(k):\n",
    "        idx = indices[t]\n",
    "        p_i = probs_selected[t]\n",
    "        if p_i > 1e-9: # Избегаем деления на ноль\n",
    "            A_col = A[:, idx:idx+1] # A^{(i_t)} как столбец (m x 1)\n",
    "            B_row = B[idx:idx+1, :] # B_{(i_t)} как строка (1 x p)\n",
    "            C_approx += (1.0 / (k * p_i)) * (A_col @ B_row)\n",
    "\n",
    "    return C_approx\n",
    "\n",
    "# Пример использования\n",
    "m, n, p = 50, 100, 60\n",
    "A = np.random.rand(m, n)\n",
    "B = np.random.rand(n, p)\n",
    "k = 500 # Количество сэмплов\n",
    "\n",
    "C_exact = A @ B\n",
    "C_approx = randomized_matrix_multiplication(A, B, k)\n",
    "\n",
    "error = np.linalg.norm(C_exact - C_approx, 'fro') / np.linalg.norm(C_exact, 'fro')\n",
    "print(f\"Относительная ошибка Фробениуса: {error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительная ошибка (F-норма): 0.0902\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randomized_matmul(A, B, k):\n",
    "    \"\"\"\n",
    "    Приближённое умножение матриц A (m x p) и B (p x n).\n",
    "    Возвращает матрицу C ~ A @ B.\n",
    "    \"\"\"\n",
    "    m, p = A.shape\n",
    "    p2, n = B.shape\n",
    "    assert p == p2\n",
    "    \n",
    "    # Вероятности берём пропорционально нормам столбцов A * строк B\n",
    "    col_norms = np.linalg.norm(A, axis=0)  # размер p\n",
    "    row_norms = np.linalg.norm(B, axis=1)  # размер p\n",
    "    weights = col_norms * row_norms\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Сэмплим k индексов без возвращения\n",
    "    idx = np.random.choice(p, size=k, replace=False, p=weights)\n",
    "    \n",
    "    # Формируем сумму\n",
    "    C_approx = np.zeros((m, n))\n",
    "    for i in idx:\n",
    "        # A^{(i)} -- i-ый столбец A, B_{(i)} -- i-ая строка B\n",
    "        # Учитываем масштабирование 1/(k * p_i)\n",
    "        C_approx += (1.0 / (k * weights[i])) * np.outer(A[:, i], B[i, :])\n",
    "    \n",
    "    return C_approx\n",
    "\n",
    "# Тест\n",
    "m, p, n = 100, 500, 100\n",
    "A = np.random.randn(m, p)\n",
    "B = np.random.randn(p, n)\n",
    "C_true = A @ B\n",
    "\n",
    "# k намного меньше p\n",
    "k = 500\n",
    "C_approx = randomized_matmul(A, B, k)\n",
    "\n",
    "error_rel = np.linalg.norm(C_true - C_approx, 'fro') / np.linalg.norm(C_true, 'fro')\n",
    "print(f\"Относительная ошибка (F-норма): {error_rel:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка следа матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точный след: 10055.8264\n",
      "Оценка Хатчинсона (k=100): 10048.4577, Ошибка: 7.3687\n",
      "Оценка Жирарда (k=100): 10072.2824, Ошибка: 16.4561\n",
      "Внутренняя размерность (intdim): 99.7131 (макс. 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hutchinson_trace_estimator(A, k):\n",
    "    \"\"\"Оценка следа Tr(A) методом Хатчинсона.\"\"\"\n",
    "    n = A.shape[0]\n",
    "    trace_estimates = []\n",
    "    for _ in range(k):\n",
    "        w = np.random.choice([-1.0, 1.0], size=(n, 1))\n",
    "        trace_estimate = (w.T @ A @ w).item() # w^T A w\n",
    "        trace_estimates.append(trace_estimate)\n",
    "    return np.mean(trace_estimates)\n",
    "\n",
    "def girard_trace_estimator(A, k):\n",
    "    \"\"\"Оценка следа Tr(A) методом Жирарда.\"\"\"\n",
    "    n = A.shape[0]\n",
    "    trace_estimates = []\n",
    "    for _ in range(k):\n",
    "        w = np.random.randn(n, 1) # w ~ N(0, I)\n",
    "        trace_estimate = (w.T @ A @ w).item() # w^T A w\n",
    "        trace_estimates.append(trace_estimate)\n",
    "    return np.mean(trace_estimates)\n",
    "\n",
    "# Пример использования\n",
    "n = 100\n",
    "# Создаем симметричную положительно определенную матрицу\n",
    "A_sym = np.random.rand(n, n)\n",
    "A_sym = 0.5 * (A_sym + A_sym.T)\n",
    "A_sym = A_sym + n * np.eye(n) # Гарантируем положительную определенность\n",
    "\n",
    "exact_trace = np.trace(A_sym)\n",
    "k = 100 # Количество векторов для сэмплирования\n",
    "\n",
    "hutchinson_est = hutchinson_trace_estimator(A_sym, k)\n",
    "girard_est = girard_trace_estimator(A_sym, k)\n",
    "\n",
    "print(f\"Точный след: {exact_trace:.4f}\")\n",
    "print(f\"Оценка Хатчинсона (k={k}): {hutchinson_est:.4f}, Ошибка: {abs(hutchinson_est - exact_trace):.4f}\")\n",
    "print(f\"Оценка Жирарда (k={k}): {girard_est:.4f}, Ошибка: {abs(girard_est - exact_trace):.4f}\")\n",
    "\n",
    "# Оценка intdim\n",
    "norm_f_sq = np.linalg.norm(A_sym, 'fro')**2\n",
    "intdim = (exact_trace**2) / norm_f_sq\n",
    "print(f\"Внутренняя размерность (intdim): {intdim:.4f} (макс. {n})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Истинный след:      -9.50\n",
      "Оценка Хатчинсона: -9.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hutchinson_trace_estimator(A_mv, n, k=10):\n",
    "    \"\"\"\n",
    "    Оценивает Tr(A), не зная A явно, \n",
    "    но имея функцию A_mv(x), вычисляющую A@x.\n",
    "    \n",
    "    Параметры:\n",
    "    -----------\n",
    "    A_mv : function\n",
    "        Функция, принимающая на вход x и возвращающая A@x.\n",
    "    n : int\n",
    "        Размерность матрицы A (n x n).\n",
    "    k : int\n",
    "        Число случайных векторов для усреднения.\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    for _ in range(k):\n",
    "        # Случайный радемахеровский вектор\n",
    "        r = np.random.randint(0, 2, size=n)*2 - 1\n",
    "        Ar = A_mv(r)\n",
    "        estimates.append(r @ Ar)  # скаляр w^T (A w)\n",
    "    return np.mean(estimates)\n",
    "\n",
    "# Пример: матрица A и явный A_mv\n",
    "n = 30\n",
    "A = np.random.randn(n, n)\n",
    "A = 0.5*(A + A.T)  # Сделаем её симметричной для наглядности\n",
    "true_trace = np.trace(A)\n",
    "\n",
    "# Создаём функцию A_mv\n",
    "def A_mv(x):\n",
    "    return A @ x\n",
    "\n",
    "k = 8000\n",
    "trace_hat = hutchinson_trace_estimator(A_mv, n, k)\n",
    "print(f\"Истинный след:      {true_trace:.2f}\")\n",
    "print(f\"Оценка Хатчинсона: {trace_hat:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Истинный след:          39814.18\n",
      "Оценка Жирара (k=30):  40171.33\n",
      "Intrinsic dimension:    9.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def girard_trace_estimator(A_mv, n, k=10):\n",
    "    \"\"\"\n",
    "    Оценивает Tr(A), используя гауссовские случайные векторы.\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    for _ in range(k):\n",
    "        w = np.random.randn(n)  # гауссовский вектор\n",
    "        Aw = A_mv(w)\n",
    "        estimates.append(w @ Aw)\n",
    "    return np.mean(estimates)\n",
    "\n",
    "def intrinsic_dimension(A):\n",
    "    # Предполагаем, что A задана явно, чтобы просто продемонстрировать\n",
    "    trace_val = np.trace(A)\n",
    "    fro_norm = np.linalg.norm(A, 'fro')\n",
    "    return trace_val / fro_norm\n",
    "\n",
    "# Демонстрация\n",
    "n = 200\n",
    "A = np.random.randn(n, n)\n",
    "A = A @ A.T  # делаем A = A^T A, чтобы получилась ПД матрица\n",
    "true_trace = np.trace(A)\n",
    "\n",
    "def A_mv(x):\n",
    "    return A @ x\n",
    "\n",
    "k = 30\n",
    "trace_est_girard = girard_trace_estimator(A_mv, n, k)\n",
    "idim = intrinsic_dimension(A)\n",
    "\n",
    "print(f\"Истинный след:          {true_trace:.2f}\")\n",
    "print(f\"Оценка Жирара (k={k}):  {trace_est_girard:.2f}\")\n",
    "print(f\"Intrinsic dimension:    {idim:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рандомизированный SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранг приближения: 10\n",
      "Ошибка рандомизированного SVD: 1.4632e-02\n",
      "Ошибка точного SVD ранга k:     1.4126e-02\n",
      "Первые 5 синг. чисел (рандом): [88663.64610265   107.31285841   104.0445584    102.02981015\n",
      "   101.02648135]\n",
      "Первые 5 синг. чисел (точно):  [88663.91400357   138.4687148    135.01731642   134.03101164\n",
      "   132.74498632]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randomized_svd(A, rank, oversampling=10):\n",
    "    \"\"\"Вычисляет приближенное SVD ранга 'rank'.\"\"\"\n",
    "    m, n = A.shape\n",
    "    ell = rank + oversampling # Целевой ранг + доп. сэмплирование\n",
    "\n",
    "    # Шаг 1: Генерируем случайную матрицу\n",
    "    Omega = np.random.randn(n, ell)\n",
    "\n",
    "    # Шаг 2: Вычисляем Y = A * Omega\n",
    "    Y = A @ Omega\n",
    "\n",
    "    # Шаг 3: Находим ортонормированный базис Q для Y (QR разложение)\n",
    "    Q, _ = np.linalg.qr(Y) # Q имеет размер (m x ell)\n",
    "\n",
    "    # Шаг 4: Проецируем A на базис Q: B = Q^T * A\n",
    "    B = Q.T @ A # B имеет размер (ell x n)\n",
    "\n",
    "    # Шаг 5: Вычисляем SVD для маленькой матрицы B\n",
    "    U_tilde, s, Vh = np.linalg.svd(B, full_matrices=False) # U_tilde (ell x ell), s (ell,), Vh (ell x n)\n",
    "\n",
    "    # Шаг 6: Формируем U для исходной матрицы A\n",
    "    U = Q @ U_tilde # U имеет размер (m x ell)\n",
    "\n",
    "    # Шаг 7: Обрезаем до нужного ранга k\n",
    "    U_k = U[:, :rank]\n",
    "    s_k = s[:rank]\n",
    "    Vh_k = Vh[:rank, :]\n",
    "\n",
    "    return U_k, s_k, Vh_k\n",
    "\n",
    "# Пример использования\n",
    "m, n = 1000, 500\n",
    "A = np.random.rand(m, n) @ np.random.rand(n, n) # Создаем матрицу с падающими сингулярными числами\n",
    "\n",
    "target_rank = 10\n",
    "\n",
    "U_r, s_r, Vh_r = randomized_svd(A, target_rank)\n",
    "A_approx_r = U_r @ np.diag(s_r) @ Vh_r\n",
    "\n",
    "# Для сравнения - точное SVD\n",
    "U_exact, s_exact, Vh_exact = np.linalg.svd(A, full_matrices=False)\n",
    "A_approx_exact_k = U_exact[:, :target_rank] @ np.diag(s_exact[:target_rank]) @ Vh_exact[:target_rank, :]\n",
    "\n",
    "error_rand = np.linalg.norm(A - A_approx_r, 'fro') / np.linalg.norm(A, 'fro')\n",
    "error_exact_k = np.linalg.norm(A - A_approx_exact_k, 'fro') / np.linalg.norm(A, 'fro')\n",
    "\n",
    "print(f\"Ранг приближения: {target_rank}\")\n",
    "print(f\"Ошибка рандомизированного SVD: {error_rand:.4e}\")\n",
    "print(f\"Ошибка точного SVD ранга k:     {error_exact_k:.4e}\")\n",
    "print(f\"Первые 5 синг. чисел (рандом): {s_r[:5]}\")\n",
    "print(f\"Первые 5 синг. чисел (точно):  {s_exact[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительная ошибка (F-регион): 2.0224e-03\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randomized_svd(A, k, p=5):\n",
    "    \"\"\"\n",
    "    Возвращает приближение SVD матрицы A: A ~ U @ np.diag(S) @ V.\n",
    "    k - желаемый ранг, p - оверсэмплинг.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    # 1. Случайная матрица G\n",
    "    G = np.random.randn(n, k + p)\n",
    "    # 2. Y = A G\n",
    "    Y = A @ G  # размер (m x (k+p))\n",
    "    # 3. QR-разложение\n",
    "    Q, _ = np.linalg.qr(Y, mode='reduced')  # Q: (m x (k+p))\n",
    "    # 4. B = Q^T A\n",
    "    B = Q.T @ A  # размер ((k+p) x n)\n",
    "    # 5. Точное SVD матрицы B\n",
    "    u_hat, s, v = np.linalg.svd(B, full_matrices=False)\n",
    "    # 6. U = Q u_hat\n",
    "    U = Q @ u_hat  # размер (m x (k+p))\n",
    "    # Обрежем до ранга k\n",
    "    return U[:, :k], s[:k], v[:k, :]\n",
    "\n",
    "# Демонстрация\n",
    "m, n, k = 1000, 300, 50\n",
    "A = np.random.randn(m, k) @ np.random.randn(k, n)  # матрица ранга k (точно)\n",
    "A += 0.01*np.random.randn(m, n)  # чуть портим\n",
    "\n",
    "U_approx, S_approx, V_approx = randomized_svd(A, k=50, p=30)\n",
    "A_approx = U_approx @ np.diag(S_approx) @ V_approx\n",
    "\n",
    "error_fro = np.linalg.norm(A - A_approx, 'fro') / np.linalg.norm(A, 'fro')\n",
    "print(f\"Относительная ошибка (F-регион): {error_fro:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaczmarz algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительная ошибка в решении: 7.3028e-02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randomized_kaczmarz(A, b, iters=1000):\n",
    "    \"\"\"\n",
    "    Решает систему A x = b методом Kaczmarz.\n",
    "    Возвращает приблизительное решение x.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    x = np.zeros(n)\n",
    "    row_norms = np.sum(A*A, axis=1)\n",
    "    row_norms_sum = np.sum(row_norms)\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        # Выбираем строку i случайно, пропорц. ее норме\n",
    "        i = np.random.choice(m, p=row_norms/row_norms_sum)\n",
    "        a_i = A[i, :]\n",
    "        residual = a_i.dot(x) - b[i]\n",
    "        x = x - (residual / (a_i.dot(a_i))) * a_i\n",
    "    return x\n",
    "\n",
    "# Демонстрация\n",
    "m, n = 500, 300\n",
    "A = np.random.randn(m, n)\n",
    "x_true = np.random.randn(n)\n",
    "b = A @ x_true\n",
    "\n",
    "x_est = randomized_kaczmarz(A, b, iters=5000)\n",
    "error = np.linalg.norm(x_est - x_true)/np.linalg.norm(x_true)\n",
    "print(f\"Относительная ошибка в решении: {error:.4e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmarx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
