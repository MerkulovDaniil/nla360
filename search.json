[
  {
    "objectID": "lectures/lecture-14/lecture-14.html",
    "href": "lectures/lecture-14/lecture-14.html",
    "title": "Questions?",
    "section": "",
    "text": "Arnoldi orthogonalization of Krylov subspaces\nLanczos for the symmetric case\nEnergy functional and conjugate gradient method\nConvergence analysis\nNon-symmetric case: idea of GMRES"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#previous-part",
    "href": "lectures/lecture-14/lecture-14.html#previous-part",
    "title": "Questions?",
    "section": "",
    "text": "Arnoldi orthogonalization of Krylov subspaces\nLanczos for the symmetric case\nEnergy functional and conjugate gradient method\nConvergence analysis\nNon-symmetric case: idea of GMRES"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#now-we-will-cover-the-following-topics",
    "href": "lectures/lecture-14/lecture-14.html#now-we-will-cover-the-following-topics",
    "title": "Questions?",
    "section": "Now we will cover the following topics",
    "text": "Now we will cover the following topics\n\nMore iterative methods: MINRES, BiCG and BiCGStab\nThe concept of preconditioners"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#what-methods-to-use",
    "href": "lectures/lecture-14/lecture-14.html#what-methods-to-use",
    "title": "Questions?",
    "section": "What methods to use",
    "text": "What methods to use\n\nIf a matrix is symmetric (Hermitian) positive definite, use CG method.\nIf a matrix is symmetric but indefinite, we can use MINRES method (GMRES applied to a symmetric system)\nIf a matrix is non-symmetric and not very big, use GMRES\nIf a matrix is non-symmetric and we can store limited amount of vectors, use either: GMRES with restarts, or BiCGStab (the latter of the product with A^{\\top} is also available).\n\n\nMore detailed flowchart from this book"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#minres",
    "href": "lectures/lecture-14/lecture-14.html#minres",
    "title": "Questions?",
    "section": "MINRES",
    "text": "MINRES\nThe MINRES method is GMRES applied to a symmetric system.\n\nWe search the solution in the subspace spanned by columns of Q_j\n\n x_j = x_0 + Q_j c_j \n\nWe minimize the residual norm\n\n\\Vert Ax_j - f\\Vert_2 = \\Vert Ax_0 + AQ_j c_j - f\\Vert_2 =  \\Vert A Q_j c_j - r_0 \\Vert_2 = \\Vert Q_j H_j c_j + h_{j, j-1} q_j c_j - r_0 \\Vert_2 = \\Vert Q_{j+1} \\widetilde{H}_{j+1}  c_j - r_0 \\Vert_2 \\rightarrow \\min_{c_j}\nwhich is equivalent to a linear least squares with an almost tridiagonal matrix\n\\Vert \\widetilde{H}_{j+1} c_{j} - \\gamma e_0 \\Vert_2 \\rightarrow \\min_{c_{j}}.\n\nIn a similar fashion, we can derive short-term recurrences.\nA careful implementation of MINRES requires at most 5 vectors to be stored."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#difference-between-minres-and-cg",
    "href": "lectures/lecture-14/lecture-14.html#difference-between-minres-and-cg",
    "title": "Questions?",
    "section": "Difference between MINRES and CG",
    "text": "Difference between MINRES and CG\n\nMINRES minimizes \\Vert Ax_k - f \\Vert_2 over the Krylov subspace\nCG minimize (Ax, x) - 2(f, x) over the Krylov subspace\nMINRES works for indefinite (i.e., non-positive definite) problems.\n\nCG stores less vectors (3 instead of 5).\nNow, let us talk about non-symmetric systems."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#non-symmetric-systems",
    "href": "lectures/lecture-14/lecture-14.html#non-symmetric-systems",
    "title": "Questions?",
    "section": "Non-symmetric systems",
    "text": "Non-symmetric systems\n\nThe main disadvantage of GMRES: we have to store all the vectors, so the memory cost grows with each step.\nWe can do restarts (i.e. get a new residual and a new Krylov subspace): we find some approximate solution x and now solve the linear system for the correction:\n\nA(x + e) = f, \\quad Ae = f - Ax,\nand generate the new Krylov subspace from the residual vector. This spoils the convergence, as we will see from the demo.\n\nimport scipy.sparse.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rc(\"text\", usetex=True)\nimport numpy as np\nimport scipy as sp\n\nn = 300\nex = np.ones(n);\nA = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.random.randn(n)\n\nres_gmres_rst = []\nres_gmres = []\ndef gmres_rst_cl(r):\n    res_gmres_rst.append(np.linalg.norm(r))\n    \ndef gmres_rst(r):\n    res_gmres.append(np.linalg.norm(r))\n\nsol = scipy.sparse.linalg.gmres(A, rhs, restart=20, callback=gmres_rst_cl)\nsol = scipy.sparse.linalg.gmres(A, rhs, restart=n, callback=gmres_rst)\n\nlim = 300\nplt.semilogy(res_gmres_rst[:lim], marker='.',color='k', label='GMRES, restart=20')\nplt.semilogy(res_gmres[:lim], marker='x',color='r', label='GMRES, no restart')\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Residual norm', fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.legend(fontsize=20)\n\n\n\n\n\n\n\n\n\nHow to avoid such spoiling of convergence?\n\nBiConjugate Gradient method (named BiCG, proposed by Fletcher, original paper) avoids that using “short recurrences” like in the CG method."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#idea-of-biconjugate-gradient",
    "href": "lectures/lecture-14/lecture-14.html#idea-of-biconjugate-gradient",
    "title": "Questions?",
    "section": "Idea of biconjugate gradient",
    "text": "Idea of biconjugate gradient\nIdea of BiCG method is to use the normal equations:\nA^* A x = A^* f,\nand apply the CG method to it.\n\nThe condition number has squared, thus we need stabilization.\nThe stabilization idea proposed by Van der Vorst et al. improves the stability (later in the lecture)\n\nLet us do some demo for a simple non-symmetric matrix to demonstrate instability of BiCG method.\n\nres_all_bicg = []\ndef bicg_cl(x):\n    res_all_bicg.append(np.linalg.norm(A.dot(x) - rhs))\n    \nsol = scipy.sparse.linalg.bicg(A, rhs, x0=np.zeros(n), callback=bicg_cl)\nplt.semilogy(res_all_bicg, label='BiCG')\nplt.semilogy(res_gmres_rst[:n], label='GMRES, restart=20')\nplt.semilogy(res_gmres, label='GMRES, no restart')\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Residual norm', fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.legend(fontsize=20)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#biconjugate-gradients",
    "href": "lectures/lecture-14/lecture-14.html#biconjugate-gradients",
    "title": "Questions?",
    "section": "BiConjugate Gradients",
    "text": "BiConjugate Gradients\nThere are two options:\n\nUse \\mathcal{K}(A^* A, A^* f) to generate the subspace. That leads to square of condition number\nInstead, use two Krylov subspaces \\mathcal{K}(A) and \\mathcal{K}(A^*) to generate two basises that are biorthogonal (so-called biorthogonal Lanczos).\n\nThe goal is to compute the Petrov-Galerkin projection\nW^* A V \\widehat{x} = W^* f\nwith columns W from the Krylov subspace of A^*, V from A (cf. with CG case).\nThat may lead to instabilities if we try to recompute the solutions in the efficient way. It is related to the pivoting (which we did not use in CG), and it is not naturally implemented here."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#notes-about-bicg",
    "href": "lectures/lecture-14/lecture-14.html#notes-about-bicg",
    "title": "Questions?",
    "section": "Notes about BiCG",
    "text": "Notes about BiCG\nA practical implementation of BiCG uses two-sided Lanczos process: generating Krylov subspace for A and A^{\\top}\nIn partucular\n\n\\alpha_j = \\frac{(r_j, \\hat{r}_j)}{(Ap_j, \\hat{p}_j)}\n$x_{j+1} = x_j + _j p_j $\nr_{j+1} = r_j - \\alpha_j Ap_j\n\\hat{r}_{j+1} = \\hat{r}_j - \\alpha_j A^{\\top}\\hat{p}_j\n\\beta_j = \\frac{(r_{j+1}, \\hat{r}_{j+1})}{(r_j, \\hat{r}_j)}\np_{j+1} = r_{j+1} + \\beta_j p_j\n\\hat{p}_{j+1} = \\hat{r}_{j+1} - \\beta_j \\hat{p}_j\n\nNow we move to the stable version of the BiCG method"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#bicgstab",
    "href": "lectures/lecture-14/lecture-14.html#bicgstab",
    "title": "Questions?",
    "section": "BiCGStab",
    "text": "BiCGStab\n\nBiCGStab is frequently used, and represent a stabilized version of BiCG. It has faster and smoother convergence than original BiCG method.\nThe formulas can be found, for example, here\nIt is a combination of BiCG step followed by GMRES(1) step in order to smooth the convergence.\nFor more details, please consult the book “Iterative Krylov Methods for Large Linear Systems” by H. Van-der Vorst.\n\nA short demo to compare “Stabilized” vs “Non-stabilized” versions.\n\nimport scipy.sparse.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\nn = 600\n\nex = np.ones(n);\nA = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr') \nrhs = np.random.randn(n)\n\n# ee = sp.sparse.eye(n)\n# A = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr')\n# A = sp.sparse.kron(A, ee) + sp.sparse.kron(ee, A)\n# rhs = np.ones(n * n)\n\nprint(\"Dimension of the linear system = {}\".format(A.shape[0]))\n\nres_all_bicg = []\nres_all_bicgstab = []\ndef bicg_cl(x):\n    res_all_bicg.append(np.linalg.norm(A.dot(x) - rhs))\n\ndef bicgstab_cl(x):\n    res_all_bicgstab.append(np.linalg.norm(A.dot(x) - rhs))\n\nres_gmres_rst = []\nres_gmres = []\ndef gmres_rst_cl(r):\n    res_gmres_rst.append(np.linalg.norm(r))\n    \ndef gmres_rst(r):\n    res_gmres.append(np.linalg.norm(r))\n\nsol2 = scipy.sparse.linalg.gmres(A, rhs, restart=20, callback=gmres_rst_cl)\nsol2 = scipy.sparse.linalg.gmres(A, rhs, restart=n, callback=gmres_rst)\n\n    \nsol2 = scipy.sparse.linalg.bicg(A, rhs, x0=np.zeros(A.shape[0]), callback=bicg_cl)\nsol2 = scipy.sparse.linalg.bicgstab(A, rhs, x0=np.zeros(A.shape[0]), callback=bicgstab_cl)\nres_all_bicg = np.array(res_all_bicg)/res_all_bicg[0]\nres_all_bicgstab = np.array(res_all_bicgstab)/res_all_bicgstab[0]\nres_gmres_rst = np.array(res_gmres_rst)/res_gmres_rst[0]\nres_gmres = np.array(res_gmres)/res_gmres[0]\n\nlim = 500\nplt.semilogy(res_all_bicgstab[:lim], marker='.',color='k', label='BiCGStab')\nplt.semilogy(res_all_bicg[:lim], marker='x',color='r', label='BiCG')\nplt.semilogy(res_gmres_rst[:lim], marker='.',color='y', label='GMRES res(n)')\nplt.semilogy(res_gmres[:lim], marker='x',color='g', label='GMRES')\n\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Retative residual norm', fontsize=20)\nplt.legend(loc='best', fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\nDimension of the linear system = 600"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#nonlinear-gmres-or-anderson-acceleration",
    "href": "lectures/lecture-14/lecture-14.html#nonlinear-gmres-or-anderson-acceleration",
    "title": "Questions?",
    "section": "“Nonlinear GMRES” or Anderson acceleration",
    "text": "“Nonlinear GMRES” or Anderson acceleration\nWe can apply the GMRES-like idea to speed up the convergence of a given fixed-point iteration\nx_{k+1} = \\Phi(x_k).\nThis was actually older than the GMRES, and known as an Direct Inversion in Iterated Subspaces in Quantum Chemistry, or Anderson Acceleration.\nIdea: use history for the update\nx_{k+1} = \\Phi(x_k) + \\sum_{s=1}^m \\alpha_s (x_{k - s} - \\Phi(x_{k - s})), \nand the parameters \\alpha_s are selected to minimize the norm of the residual\n \\min_{\\alpha} \\left \\| \\sum_{s=1}^m \\alpha_s (x_{k - s} - \\Phi(x_{k - s})) \\right\\|_2, \\quad \\sum_{s=1}^m \\alpha_s = 1\nMore details see in the original paper"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#battling-the-condition-number",
    "href": "lectures/lecture-14/lecture-14.html#battling-the-condition-number",
    "title": "Questions?",
    "section": "Battling the condition number",
    "text": "Battling the condition number\n\nThe condition number problem is un-avoidable if only the matrix-by-vector product is used.\nThus we need an army of preconditioners to solve it.\nThere are several general purpose preconditioners that we can use, but often for a particular problem a special design is needed."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#preconditioner-general-concept",
    "href": "lectures/lecture-14/lecture-14.html#preconditioner-general-concept",
    "title": "Questions?",
    "section": "Preconditioner: general concept",
    "text": "Preconditioner: general concept\nThe general concept of the preconditioner is simple:\nGiven a linear system\nA x = f,\nwe want to find the matrix P_R and/or P_L such that\n\nCondition number of AP_R^{-1} (right preconditioner) or P^{-1}_LA (right preconditioner) or P^{-1}_L A P_R^{-1} is better than for A\nWe can easily solve P_Ly = g or P_Ry = g for any g (otherwise we could choose e.g. P_L = A)\n\nThen we solve for (right preconditioner)\n AP_R^{-1} y = f \\quad \\Rightarrow \\quad P_R x = y\nor (left preconditioner)\n P_L^{-1} A x = P_L^{-1}f, or even both  P_L^{-1} A P_R^{-1} y = P_L^{-1}f \\quad \\Rightarrow \\quad P_R x = y.\nThe best choice is of course P = A, but this does not make life easier.\nOne of the ideas is to use other iterative methods (beside Krylov) as preconditioners."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#other-iterative-methods-as-preconditioners",
    "href": "lectures/lecture-14/lecture-14.html#other-iterative-methods-as-preconditioners",
    "title": "Questions?",
    "section": "Other iterative methods as preconditioners",
    "text": "Other iterative methods as preconditioners\nThere are other iterative methods that we have not mentioned.\n\nJacobi method\nGauss-Seidel\nSOR(\\omega) (Successive over-relaxation) and its symmetric modification SSOR(\\omega)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#jacobi-method-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#jacobi-method-as-preconditioner",
    "title": "Questions?",
    "section": "Jacobi method (as preconditioner)",
    "text": "Jacobi method (as preconditioner)\nConsider again the matrix with non-zero diagonal. To get the Jacobi method you express the diagonal element:\na_{ii} x_i = -\\sum_{i \\ne j} a_{ij} x_j + f_i\nand use this to iteratively update x_i:\n x_i^{(k+1)} = -\\frac{1}{a_{ii}}\\left( \\sum_{i \\ne j} a_{ij} x_j^{(k)} + f_i \\right),\nor in the matrix form\n\nx^{(k+1)} = D^{-1}\\left((D-A)x^{(k)} + f\\right)\n\nwhere D = \\mathrm{diag}(A) and finally\n\nx^{(k+1)} = x^{(k)} - D^{-1}(Ax^{(k)} - f).\n\nSo, Jacobi method is nothing, but simple Richardson iteration with \\tau=1 and left preconditioner P = D - diagonal of a matrix. Therefore we will refer to P = \\mathrm{diag}(A) as Jacobi preconditioner. Note that it can be used for any other method like Chebyshev or Krylov-type methods."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#properties-of-the-jacobi-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#properties-of-the-jacobi-preconditioner",
    "title": "Questions?",
    "section": "Properties of the Jacobi preconditioner",
    "text": "Properties of the Jacobi preconditioner\nJacobi preconditioner:\n\nVery easy to compute and apply\nWorks well for diagonally dominant matrices (remember the Gershgorin circle theorem!)\nUseless if all diagonal entries are the same (proportional to the identity matrix)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-as-preconditioner",
    "title": "Questions?",
    "section": "Gauss-Seidel (as preconditioner)",
    "text": "Gauss-Seidel (as preconditioner)\nAnother well-known method is Gauss-Seidel method.\nIts canonical form is very similar to the Jacobi method, with a small difference. When we update x_i as\nx_i^{(k+1)} := -\\frac{1}{a_{ii}}\\left( \\sum_{j =1}^{i-1} a_{ij} x_j^{(k+1)} +\\sum_{j = i+1}^n a_{ij} x_j^{(k)} - f_i \\right)\nwe use it in the later updates. In the Jacobi method we use the full vector from the previous iteration.\nIts matrix form is more complicated."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-matrix-version",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-matrix-version",
    "title": "Questions?",
    "section": "Gauss-Seidel: matrix version",
    "text": "Gauss-Seidel: matrix version\nGiven A = A^{*} &gt; 0 we have\nA = L + D + L^{*},\nwhere D is the diagonal of A, L is lower-triangular part with zero on the diagonal.\nOne iteration of the GS method reads\n\nx^{(k+1)} = x^{(k)} - (L + D)^{-1}(Ax^{(k)} - f).\n\nand we refer to the preconditioner P = L+D as Gauss-Seidel preconditioner.\nGood news: $(I - (L+D)^{-1} A) &lt; 1, $ where \\rho is the spectral radius,\ni.e. for a positive definite matrix GS-method always converges."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-and-coordinate-descent",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-and-coordinate-descent",
    "title": "Questions?",
    "section": "Gauss-Seidel and coordinate descent",
    "text": "Gauss-Seidel and coordinate descent\nGS-method can be viewed as a coordinate descent method, applied to the energy functional\nF(x) = (Ax, x) - 2(f, x)\nwith the iteration\nx_i := \\arg \\min_z F(x_1, \\ldots, x_{i-1}, z, x_{i+1}, \\ldots, x_d).\nMoreover, the order in which we eliminate variables, is really important!"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#side-note-nonlinear-gauss-seidel-a.k.a-coordinate-descent",
    "href": "lectures/lecture-14/lecture-14.html#side-note-nonlinear-gauss-seidel-a.k.a-coordinate-descent",
    "title": "Questions?",
    "section": "Side note: Nonlinear Gauss-Seidel (a.k.a coordinate descent)",
    "text": "Side note: Nonlinear Gauss-Seidel (a.k.a coordinate descent)\nIf F is given, and we optimize one coordinate at a time, we have\nx_i := \\arg \\min_z F(x_1, \\ldots, x_{i-1}, z, x_{i+1}, \\ldots, x_d).\nNote the convergence result for block coordinate descent for the case of a general functional F:\nit converges locally with the speed of the GS-method applied to the Hessian\nH = \\nabla^2 F\nof the functional.\nThus, if F is twice differentiable and x_* is the local minimum, then H &gt; 0 can Gauss-Seidel converges."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#successive-overrelaxation-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#successive-overrelaxation-as-preconditioner",
    "title": "Questions?",
    "section": "Successive overrelaxation (as preconditioner)",
    "text": "Successive overrelaxation (as preconditioner)\nWe can even introduce a parameter \\omega into the GS-method preconditioner, giving a successive over-relaxation (SOR(\\omega)) method:\n\nx^{(k+1)} = x^{(k)} - \\omega (D + \\omega L)^{-1}(Ax^{(k)} - f).\n\nP = \\frac{1}{\\omega}(D+\\omega L).\n\nConverges for 0&lt;\\omega &lt; 2.\nOptimal selection of \\omega is not trivial. If the Jacobi method converges, then \\omega^* = \\frac{2}{1 + \\sqrt{1 - \\rho_J^2}}, where \\rho_J is spectral radius of Jacobi iterations\nNote that \\omega = 1 gives us a Gauss-Seidel preconditioner."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#preconditioners-for-sparse-matrices",
    "href": "lectures/lecture-14/lecture-14.html#preconditioners-for-sparse-matrices",
    "title": "Questions?",
    "section": "Preconditioners for sparse matrices",
    "text": "Preconditioners for sparse matrices\n\nIf A is sparse, one iteration of Jacobi, GS and SOR method is cheap (what complexity?).\nFor GS, we need to solve linear system with a sparse triangular matrix L, which costs \\mathcal{O}(nnz).\nFor sparse matrices, however, there are more complicated algorithms, based on the idea of approximate LU-decomposition.\nRemember the motivation for CG: possibility of the early stopping, how to do approximate LU-decomposition for a sparse matrix?"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#remember-the-gaussian-elimination",
    "href": "lectures/lecture-14/lecture-14.html#remember-the-gaussian-elimination",
    "title": "Questions?",
    "section": "Remember the Gaussian elimination",
    "text": "Remember the Gaussian elimination\n\nDecompose the matrix A in the form\n\nA = P_1 L U P^{\\top}_2, \nwhere P_1 and P_2 are certain permutation matrices (which do the pivoting).\n\nThe most natural idea is to use sparse L and U.\nIt is not possible without fill-in growth for example for matrices, coming from 2D/3D Partial Differential equations (PDEs).\nWhat to do?"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#incomplete-lu",
    "href": "lectures/lecture-14/lecture-14.html#incomplete-lu",
    "title": "Questions?",
    "section": "Incomplete LU",
    "text": "Incomplete LU\n\nSuppose you want to eliminate a variable x_1, and the equations have the form\n\n5 x_1 + x_4 + x_{10} = 1, \\quad 3 x_1 + x_4 + x_8 = 0, \\ldots,\nand in all other equations x_1 are not present.\n\nAfter the elimination, only x_{10} will enter additionally to the second equation (new fill-in).\n\nx_4 + x_8 + 3(1 - x_4 - x_{10})/5 = 0\n\nIn the Incomplete LU case (actually, ILU(0)) we just throw away the new fill-in."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#incomplete-lu-formal-definition",
    "href": "lectures/lecture-14/lecture-14.html#incomplete-lu-formal-definition",
    "title": "Questions?",
    "section": "Incomplete-LU: formal definition",
    "text": "Incomplete-LU: formal definition\nWe run the usual LU-decomposition cycle, but avoid inserting non-zeros other than the initial non-zero pattern.\n    L = np.zeros((n, n))\n    U = np.zeros((n, n))\n    for k in range(n): #Eliminate one row   \n        L[k, k] = 1\n        for i in range(k+1, n):\n            L[i, k] = a[i, k] / a[k, k]\n            for j in range(k+1, n):\n                a[i, j] = a[i, j] - L[i, k] * a[k, j]  #New fill-ins appear here\n        for j in range(k, n):\n            U[k, j] = a[k, j]"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#iluk",
    "href": "lectures/lecture-14/lecture-14.html#iluk",
    "title": "Questions?",
    "section": "ILU(k)",
    "text": "ILU(k)\n\nYousef Saad (who is the author of GMRES) also had a seminal paper on the Incomplete LU decomposition\nA good book on the topic is Iterative methods for sparse linear systems by Y. Saad, 2003\nAnd he proposed ILU(k) method, which has a nice interpretation in terms of graphs."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#iluk-idea",
    "href": "lectures/lecture-14/lecture-14.html#iluk-idea",
    "title": "Questions?",
    "section": "ILU(k): idea",
    "text": "ILU(k): idea\n\nThe idea of ILU(k) is very instructive and is based on the connection between sparse matrices and graphs.\nSuppose you have an n \\times n matrix A and a corresponding adjacency graph.\nThen we eliminate one variable (vertex) and get a smaller system of size (n-1) \\times (n-1).\nNew edges (=fill-in) appears between high-order neighbors."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#lu-graphs",
    "href": "lectures/lecture-14/lecture-14.html#lu-graphs",
    "title": "Questions?",
    "section": "LU & graphs",
    "text": "LU & graphs\n\nThe new edge can appear only between vertices that had common neighbour: it means, that they are second-order neigbours.\n\nThis is also a sparsity pattern of the matrix A^2.\nThe ILU(k) idea is to leave only the elements in L and U that are k-order neighbours in the original graph.\nThe ILU(2) is very efficient, but for some reason completely abandoned (i.e. there is no implementation in MATLAB and SciPy).\nThere is an original Sparsekit software by Saad, which works quite well."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#ilu-thresholded-ilut",
    "href": "lectures/lecture-14/lecture-14.html#ilu-thresholded-ilut",
    "title": "Questions?",
    "section": "ILU Thresholded (ILUT)",
    "text": "ILU Thresholded (ILUT)\nA much more popular approach is based on the so-called thresholded LU.\nYou do the standard Gaussian elimination with fill-ins, but either:\n\nThrow away elements that are smaller than threshold, and/or control the amount of non-zeros you are allowed to store.\nThe smaller is the threshold, the better is the preconditioner, but more memory it takes.\n\nIt is denoted ILUT(\\tau)."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#symmetric-positive-definite-case",
    "href": "lectures/lecture-14/lecture-14.html#symmetric-positive-definite-case",
    "title": "Questions?",
    "section": "Symmetric positive definite case",
    "text": "Symmetric positive definite case\n\nIn the SPD case, instead of incomplete LU you should use Incomplete Cholesky, which is twice faster and consumes twice less memory.\nBoth ILUT and Ichol are implemented in SciPy and you can try (nothing quite fancy, but it works)."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#second-order-lu-preconditioners",
    "href": "lectures/lecture-14/lecture-14.html#second-order-lu-preconditioners",
    "title": "Questions?",
    "section": "Second-order LU preconditioners",
    "text": "Second-order LU preconditioners\n\nThere is a more efficient (but much less popular due to the limit of open-source implementations) second-order LU factorization proposed by I. Kaporin\nThe idea (for symmetric matrices) is to approximate the matrix in the form\n\nA \\approx U_2 U^{\\top}_2 + U^{\\top}_2 R_2 + R^{\\top}_2 U_2,\nwhich is just the expansion of the UU^{\\top} with respect to the perturbation of U.\n\nU_1 and U_2 are upper-triangular and sparse, whereare R_2 is small with respect to the drop tolerance parameter."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#summary-of-this-part",
    "href": "lectures/lecture-14/lecture-14.html#summary-of-this-part",
    "title": "Questions?",
    "section": "Summary of this part",
    "text": "Summary of this part\n\nJacobi, Gauss-Seidel, SSOR (as preconditioners)\nIncomplete LU, three flavours: ILU(k), ILUT, ILU2"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#next-lecture",
    "href": "lectures/lecture-14/lecture-14.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nTensors\nTucker and CP decompositions\nTensor train decomposition\nApplications"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html",
    "href": "lectures/lecture-12/lecture-12.html",
    "title": "The main topics for today",
    "section": "",
    "text": "Direct methods for LU\nGraph separators"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#previous-lecture",
    "href": "lectures/lecture-12/lecture-12.html#previous-lecture",
    "title": "The main topics for today",
    "section": "",
    "text": "Direct methods for LU\nGraph separators"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#matrix-as-a-black-box",
    "href": "lectures/lecture-12/lecture-12.html#matrix-as-a-black-box",
    "title": "The main topics for today",
    "section": "Matrix as a black box",
    "text": "Matrix as a black box\n\nWe have now an absolutely different view on a matrix: matrix is now a linear operator, that acts on a vector,\nand this action can be computed in \\mathcal{O}(N) operations.\nThis is the only information we know about the matrix: the  matrix-by-vector product (matvec) \nCan we solve linear systems using only matvecs?\nOf course, we can multiply by the colums of the identity matrix, and recover the full matrix, but it is not what we need."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#richardson-iteration",
    "href": "lectures/lecture-12/lecture-12.html#richardson-iteration",
    "title": "The main topics for today",
    "section": "Richardson iteration",
    "text": "Richardson iteration\nThe simplest idea is the “simple iteration method” or Richardson iteration.\nAx = f, \\tau  (Ax - f) = 0, x - \\tau (Ax - f) = x, x_{k+1} = x_k - \\tau (Ax_k - f),\nwhere \\tau is the iteration parameter, which can be always chosen such that the method converges."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#connection-to-odes",
    "href": "lectures/lecture-12/lecture-12.html#connection-to-odes",
    "title": "The main topics for today",
    "section": "Connection to ODEs",
    "text": "Connection to ODEs\n\nThe Richardson iteration has a deep connection to the Ordinary Differential Equations (ODE).\nConsider a time-dependent problem\n\n\\frac{dy}{dt} + A y = f, \\quad y(0) = y_0.\n\nThen y(t) \\rightarrow A^{-1} f as t \\rightarrow \\infty, and the Euler scheme reads\n\n\\frac{y_{k+1} - y_k}{\\tau} = -A y_k + f.\nwhich leads to the Richardson iteration\n\n    y_{k+1} = y_k - \\tau(Ay_k -f)"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#convergence-of-the-richardson-method",
    "href": "lectures/lecture-12/lecture-12.html#convergence-of-the-richardson-method",
    "title": "The main topics for today",
    "section": "Convergence of the Richardson method",
    "text": "Convergence of the Richardson method\n\nLet x_* be the solution; introduce an error e_k = x_{k} - x_*, then\n\n\n     e_{k+1} = (I - \\tau A) e_k,\n\ntherefore if \\Vert I - \\tau A \\Vert &lt; 1 in any norm, the iteration converges.\n\nFor symmetric positive definite case it is always possible to select \\tau such that the method converges.\nWhat about the non-symmetric case? Below demo will be presented…"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#optimal-parameter-choice",
    "href": "lectures/lecture-12/lecture-12.html#optimal-parameter-choice",
    "title": "The main topics for today",
    "section": "Optimal parameter choice",
    "text": "Optimal parameter choice\n\nThe choise of \\tau that minimizes \\|I - \\tau A\\|_2 for A = A^* &gt; 0 is (prove it!)\n\n\n  \\tau_\\mathrm{opt} = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}.\n\nwhere \\lambda_{\\min} is the minimal eigenvalue, and \\lambda_{\\max} is the maximal eigenvalue of the matrix A.\n\nSo, to find optimal parameter, we need to know the bounds of the spectrum of the matrix A, and we can compute it by using power method."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#condition-number-and-convergence-speed",
    "href": "lectures/lecture-12/lecture-12.html#condition-number-and-convergence-speed",
    "title": "The main topics for today",
    "section": "Condition number and convergence speed",
    "text": "Condition number and convergence speed\nEven with the optimal parameter choice, the error at the next step satisfies\n\\|e_{k+1}\\|_2 \\leq q \\|e_k\\|_2 , \\quad\\rightarrow \\quad \\|e_k\\|_2 \\leq q^{k} \\|e_0\\|_2,\nwhere\n\n   q = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{\\mathrm{cond}(A) - 1}{\\mathrm{cond}(A)+1},\n\n\\mathrm{cond}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \\quad \\text{for} \\quad A=A^*&gt;0\nis the condition number of A.\nLet us do some demo…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rc(\"text\", usetex=True)\nimport scipy as sp\nimport scipy.sparse\nimport scipy.sparse.linalg as spla\nimport scipy\nfrom scipy.sparse import csc_matrix\nn = 500\nex = np.ones(n);\nA = sp.sparse.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\nev1, vec = spla.eigsh(A, k=2, which='LA')\nev2, vec = spla.eigsh(A, k=2, which='SA')\nlam_max = ev1[0]\nlam_min = ev2[0]\n\ntau_opt = 2.0/(lam_max + lam_min)\n\nfig, ax = plt.subplots()\nplt.close(fig)\n\nniters = 1000\nx = np.zeros(n)\nres_richardson = []\nfor i in range(niters):\n    rr = A.dot(x) - rhs\n    x = x - tau_opt * rr\n    res_richardson.append(np.linalg.norm(rr))\n#Convergence of an ordinary Richardson (with optimal parameter)\nplt.semilogy(res_richardson)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nprint(\"Maximum eigenvalue = {}, minimum eigenvalue = {}\".format(lam_max, lam_min))\ncond_number = lam_max.real / lam_min.real\nprint(\"Condition number = {}\".format(cond_number))\n#print(np.array(res_richardson)[1:] / np.array(res_richardson)[:-1])\nprint(\"Theoretical factor: {}\".format((cond_number - 1) / (cond_number + 1)))\n\nMaximum eigenvalue = 3.99984271815512, minimum eigenvalue = 3.932084756989659e-05\nCondition number = 101723.207035276\nTheoretical factor: 0.9999803389964071\n\n\n\n\n\n\n\n\n\n\nThus, for ill-conditioned matrices the error of the simple iteration method decays very slowly.\nThis is another reason why condition number is so important:\n\nBesides the bound on the error in the solution, it also gives an estimate of the number of iterations for the iterative methods.\n\nMain questions for the iterative method is how to make the matrix better conditioned.\nThe answer is  use preconditioners. Preconditioners will be discussed in further lectures.\n\n\nConsider non-hermitian matrix A\nPossible cases of Richardson iteration behaviour: - convergence - divergence - almost stable trajectory\nQ: how can we identify our case before running iterative method?\n\n# B = np.random.randn(2, 2)\nB = np.array([[1, 2], [-1, 0]])\n# B = np.array([[0, 1], [-1, 0]])\nx_true = np.zeros(2)\nf = B.dot(x_true)\neigvals = np.linalg.eigvals(B)\nprint(\"Spectrum of the matrix = {}\".format(eigvals))\n\n# Run Richardson iteration\nx = np.array([0, -1])\ntau = 1e-2\nconv_x = [x]\nr = B.dot(x) - f\nconv_r = [np.linalg.norm(r)]\nnum_iter = 1000\nfor i in range(num_iter):\n    x = x - tau * r\n    conv_x.append(x)\n    r = B.dot(x) - f\n    conv_r.append(np.linalg.norm(r))\n\nSpectrum of the matrix = [0.5+1.32287566j 0.5-1.32287566j]\n\n\n\nplt.semilogy(conv_r)\nplt.xlabel(\"Number of iteration, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm\", fontsize=20)\n\nText(0, 0.5, 'Residual norm')\n\n\n\n\n\n\n\n\n\n\nplt.scatter([x[0] for x in conv_x], [x[1] for x in conv_x])\nplt.xlabel(\"$x$\", fontsize=20)\nplt.ylabel(\"$y$\", fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.title(\"$x_0 = (0, -1)$\", fontsize=20)\n\nText(0.5, 1.0, '$x_0 = (0, -1)$')"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#better-iterative-methods",
    "href": "lectures/lecture-12/lecture-12.html#better-iterative-methods",
    "title": "The main topics for today",
    "section": "Better iterative methods",
    "text": "Better iterative methods\nBut before preconditioners, we can use better iterative methods.\nThere is a whole zoo of iterative methods, but we need to know just few of them."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#attempt-1-the-steepest-descent-method",
    "href": "lectures/lecture-12/lecture-12.html#attempt-1-the-steepest-descent-method",
    "title": "The main topics for today",
    "section": "Attempt 1: The steepest descent method",
    "text": "Attempt 1: The steepest descent method\n\nSuppose we change \\tau every step, i.e. \n\n\n   x_{k+1} = x_k - \\tau_k (A x_k - f).\n\n\nA possible choice of \\tau_k is such that it minimizes norm of the current residual\n\n \\tau_k = \\arg\\min_{\\tau} \\|A(x_k - \\tau_k (A x_k - f)) - f\\|_2^2.\n\nThis problem can be solved analytically (derive this solution!)\n\n \\tau_k = \\frac{r_k^{\\top}r_k}{r_k^{\\top}Ar_k}, \\quad r_k = Ax_k - f \n\nThis method is called the steepest descent.\nHowever, it still converges similarly to the Richardson iteration."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#attempt-2-chebyshev-iteration",
    "href": "lectures/lecture-12/lecture-12.html#attempt-2-chebyshev-iteration",
    "title": "The main topics for today",
    "section": "Attempt 2: Chebyshev iteration",
    "text": "Attempt 2: Chebyshev iteration\nAnother way to find \\tau_k is to consider\ne_{k+1} = (I - \\tau_k A) e_k = (I - \\tau_k A) (I - \\tau_{k-1} A)  e_{k-1} = \\ldots = p(A) e_0, \nwhere p(A) is a matrix polynomial (simplest matrix function)\n\n   p(A) = (I - \\tau_k A) \\ldots (I - \\tau_0 A),\n\nand p(0) = 1."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#optimal-choice-of-time-steps",
    "href": "lectures/lecture-12/lecture-12.html#optimal-choice-of-time-steps",
    "title": "The main topics for today",
    "section": "Optimal choice of time steps",
    "text": "Optimal choice of time steps\nThe error is written as\ne_{k+1} = p(A) e_0, \nand hence\n\\|e_{k+1}\\| \\leq \\|p(A)\\| \\|e_0\\|, \nwhere p(0) = 1 and p(A) is a matrix polynomial.\nTo get better error reduction, we need to minimize\n\\Vert p(A) \\Vert\nover all possible polynomials p(x) of degree k+1 such that p(0)=1. We will use \\|\\cdot\\|_2."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros",
    "href": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros",
    "title": "The main topics for today",
    "section": "Polynomials least deviating from zeros",
    "text": "Polynomials least deviating from zeros\nImportant special case: A = A^* &gt; 0.\nThen A = U \\Lambda U^*,\nand\n\\Vert p(A) \\Vert_2 = \\Vert U p(\\Lambda) U^* \\Vert_2 = \\Vert p(\\Lambda) \\Vert_2 = \\max_i |p(\\lambda_i)| \\overset{!}{\\leq}\n\\max_{\\lambda_\\min \\leq \\lambda {\\leq} \\lambda_\\max} |p(\\lambda)|.\nThe latter inequality is the only approximation. Here we make a  crucial assumption  that we do not want to benefit from distribution of spectra between \\lambda_\\min and \\lambda_\\max.\nThus, we need to find a polynomial such that p(0) = 1, that has the least possible deviation from 0 on [\\lambda_\\min, \\lambda_\\max]."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros-2",
    "href": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros-2",
    "title": "The main topics for today",
    "section": "Polynomials least deviating from zeros (2)",
    "text": "Polynomials least deviating from zeros (2)\nWe can do the affine transformation of the interval [\\lambda_\\min, \\lambda_\\max] to the interval [-1, 1]:\n\n\\xi = \\frac{{\\lambda_\\max + \\lambda_\\min - (\\lambda_\\min-\\lambda_\\max)x}}{2}, \\quad x\\in [-1, 1].\n\nThe problem is then reduced to the problem of finding the polynomial least deviating from zero on an interval [-1, 1]."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#exact-solution-chebyshev-polynomials",
    "href": "lectures/lecture-12/lecture-12.html#exact-solution-chebyshev-polynomials",
    "title": "The main topics for today",
    "section": "Exact solution: Chebyshev polynomials",
    "text": "Exact solution: Chebyshev polynomials\nThe exact solution to this problem is given by the famous Chebyshev polynomials of the form\nT_n(x) =  \\cos (n \\arccos x)"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#what-do-you-need-to-know-about-chebyshev-polynomials",
    "href": "lectures/lecture-12/lecture-12.html#what-do-you-need-to-know-about-chebyshev-polynomials",
    "title": "The main topics for today",
    "section": "What do you need to know about Chebyshev polynomials",
    "text": "What do you need to know about Chebyshev polynomials\n\nThis is a polynomial!\nWe can express T_n from T_{n-1} and T_{n-2}:\n\nT_n(x) = 2x T_{n-1}(x) - T_{n-2}(x), \\quad T_0(x)=1, \\quad T_1(x)=x\n\n|T_n(x)| \\leq 1 on x \\in [-1, 1].\nIt has (n+1) alternation points, where the maximal absolute value is achieved (this is the sufficient and necessary condition for the optimality) (Chebyshev alternance theorem, no proof here).\nThe roots are just\n\nn \\arccos x_k = \\frac{\\pi}{2} + \\pi k, \\quad \\rightarrow\\quad x_k = \\cos \\frac{\\pi(2k + 1)}{2n}, \\; k = 0, \\ldots,n-1\nWe can plot them…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nx1 = np.linspace(-1, 1, 128)\nx2 = np.linspace(-1.1, 1.1, 128)\np = np.polynomial.Chebyshev((0, 0, 0, 0, 0, 0, 0, 0, 0, 1), (-1, 1)) #These are Chebyshev series, a proto of \"chebfun system\" in MATLAB\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(x1, p(x1))\nax1.set_title('Interval $x\\in[-1, 1]$')\nax2.plot(x2, p(x2))\nax2.set_title('Interval $x\\in[-1.1, 1.1]$')\n\nText(0.5, 1.0, 'Interval $x\\\\in[-1.1, 1.1]$')"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#convergence-of-the-chebyshev-accelerated-richardson-iteration",
    "href": "lectures/lecture-12/lecture-12.html#convergence-of-the-chebyshev-accelerated-richardson-iteration",
    "title": "The main topics for today",
    "section": "Convergence of the Chebyshev-accelerated Richardson iteration",
    "text": "Convergence of the Chebyshev-accelerated Richardson iteration\nNote that p(x) = (1-\\tau_n x)\\dots (1-\\tau_0 x), hence roots of p(x) are 1/\\tau_i and that we additionally need to map back from [-1,1] to [\\lambda_\\min, \\lambda_\\max]. This results into\n\\tau_i = \\frac{2}{\\lambda_\\max + \\lambda_\\min - (\\lambda_\\max - \\lambda_\\min)x_i}, \\quad x_i = \\cos \\frac{\\pi(2i + 1)}{2n}\\quad i=0,\\dots,n-1\nThe convergence (we only give the result without the proof) is now given by\n\n   e_{k+1} \\leq C q^k e_0, \\quad q = \\frac{\\sqrt{\\mathrm{cond}(A)}-1}{\\sqrt{\\mathrm{cond}(A)}+1},\n\nwhich is better than in the Richardson iteration.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 64\nex = np.ones(n);\n\nA = sp.sparse.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\nev1, vec = spla.eigsh(A, k=2, which='LA')\nev2, vec = spla.eigsh(A, k=2, which='SA')\nlam_max = ev1[0]\nlam_min = ev2[0]\n\nniters = 64\nroots = [np.cos((np.pi * (2 * i + 1)) / (2 * niters)) for i in range(niters)]\ntaus = [(lam_max + lam_min - (lam_min - lam_max) * r) / 2 for r in roots]\nx = np.zeros(n)\nr = A.dot(x) - rhs\nres_cheb = [np.linalg.norm(r)]\n\nprint(1/np.array(taus))\n\nfor i in range(niters):\n    x = x - 1.0 / taus[i] * r\n    r = A.dot(x) - rhs\n    res_cheb.append(np.linalg.norm(r))\n    \nplt.semilogy(res_richardson, label=\"Richardson\")\nplt.semilogy(res_cheb, label=\"Chebyshev\")\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n[2.50622630e-01 2.50924658e-01 2.51530169e-01 2.52442095e-01\n 2.53664865e-01 2.55204461e-01 2.57068471e-01 2.59266178e-01\n 2.61808653e-01 2.64708878e-01 2.67981890e-01 2.71644951e-01\n 2.75717745e-01 2.80222614e-01 2.85184826e-01 2.90632893e-01\n 2.96598938e-01 3.03119118e-01 3.10234126e-01 3.17989766e-01\n 3.26437629e-01 3.35635888e-01 3.45650225e-01 3.56554925e-01\n 3.68434169e-01 3.81383570e-01 3.95511991e-01 4.10943730e-01\n 4.27821135e-01 4.46307759e-01 4.66592186e-01 4.88892691e-01\n 5.13462953e-01 5.40599097e-01 5.70648426e-01 6.04020340e-01\n 6.41200067e-01 6.82766079e-01 7.29412363e-01 7.81977158e-01\n 8.41480389e-01 9.09172947e-01 9.86602291e-01 1.07570085e+00\n 1.17890673e+00 1.29933105e+00 1.44099335e+00 1.60915915e+00\n 1.81083297e+00 2.05549470e+00 2.35622630e+00 2.73148428e+00\n 3.20797673e+00 3.82550533e+00 4.64546321e+00 5.76650218e+00\n 7.35516474e+00 9.71018736e+00 1.34098577e+01 1.96891165e+01\n 3.15513749e+01 5.76947930e+01 1.29218671e+02 3.40581915e+02]\n\n\n\n\n\n\n\n\n\n\nWhat happened with great Chebyshev iterations?\n\nniters = 64\nroots = [np.cos((np.pi * (2 * i + 1)) / (2 * niters)) for i in range(niters)]\ntaus = [(lam_max + lam_min - (lam_min - lam_max) * r) / 2 for r in roots]\nx = np.zeros(n)\nr = A.dot(x) - rhs\nres_cheb_even = [np.linalg.norm(r)]\n#print(taus)\n\n# Implementation may be non-optimal if number of iterations is not power of two\ndef leb_shuffle_2n(n):\n    if n == 1:\n        return np.array([0,], dtype=int)\n    else:\n        prev = leb_shuffle_2n(n // 2)\n        ans = np.zeros(n, dtype=int)\n        ans[::2] = prev\n        ans[1::2] = n - 1 - prev\n        return ans\n\ngood_perm_even = leb_shuffle_2n(niters)\nprint(good_perm_even, len(good_perm_even))\n# good_perm_even = np.random.permutation([i for i in range(niters)])\nts = np.array(taus)[good_perm_even]\nplt.figure()\nplt.plot(1/ts)\nfor i in range(niters):\n    x = x - 1.0/taus[good_perm_even[i]] * r\n    r = A.dot(x) - rhs\n    res_cheb_even.append(np.linalg.norm(r))\nplt.figure()\n    \nplt.semilogy(res_richardson, label=\"Richardson\")\nplt.semilogy(res_cheb_even, label=\"Chebyshev\")\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n[ 0 63 31 32 15 48 16 47  7 56 24 39  8 55 23 40  3 60 28 35 12 51 19 44\n  4 59 27 36 11 52 20 43  1 62 30 33 14 49 17 46  6 57 25 38  9 54 22 41\n  2 61 29 34 13 50 18 45  5 58 26 37 10 53 21 42] 64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPermutation of roots of Chebyshev polynomial has crucial effect on convergence\nOn the optimal permutation you can read in paper (V. Lebedev, S. Finogenov 1971) (ru, en)\n\n\n\nChebfun project\n\nOpensource project for numerical computing (Python and Matlab interfaces)\nIt is based on numerical algorithms working with piecewise polynomial interpolants and Chebyshev polynomials\nThis project was initiated by Nick Trefethen and his student Zachary Battles in 2002, see paper on chebfun project in SISC\nChebfun toolbox focuses mostly on the following problems\n\nApproximation\nQuadrature\nODE\nPDE\nRootfinding\n1D global optimization"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#beyond-chebyshev",
    "href": "lectures/lecture-12/lecture-12.html#beyond-chebyshev",
    "title": "The main topics for today",
    "section": "Beyond Chebyshev",
    "text": "Beyond Chebyshev\n\nWe have made an important assumption about the spectrum: it is contained within an interval over the real line (and we need to know the bounds)\nIf the spectrum is contained within two intervals, and we know the bounds, we can also put the optimization problem for the optimal polynomial."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#spectrum-of-the-matrix-contained-in-multiple-segments",
    "href": "lectures/lecture-12/lecture-12.html#spectrum-of-the-matrix-contained-in-multiple-segments",
    "title": "The main topics for today",
    "section": "Spectrum of the matrix contained in multiple segments",
    "text": "Spectrum of the matrix contained in multiple segments\n\nFor the case of two segments the best polynomial is given by Zolotarev polynomials (expressed in terms of elliptic functions). Original paper was published in 1877, see details here\nFor the case of more than two segments the best polynomial can be expressed in terms of hyperelliptic functions"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#how-can-we-make-it-better",
    "href": "lectures/lecture-12/lecture-12.html#how-can-we-make-it-better",
    "title": "The main topics for today",
    "section": "How can we make it better",
    "text": "How can we make it better\n\nThe implementation of the Chebyshev acceleration requires the knowledge of the spectrum.\nIt only stores the previous vector x_k and computes the new correction vector\n\nr_k = A x_k - f.\n\nIt belongs to the class of two-term iterative methods, i.e. it approximates x_{k+1} using 2 vectors: x_k and r_k.\nIt appears that if we store more vectors, then we can go without the spectrum estimation (and better convergence in practice)!"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#crucial-point-krylov-subspace",
    "href": "lectures/lecture-12/lecture-12.html#crucial-point-krylov-subspace",
    "title": "The main topics for today",
    "section": "Crucial point: Krylov subspace",
    "text": "Crucial point: Krylov subspace\nThe Chebyshev method produces the approximation of the form\nx_{k+1} = x_0 + p(A) r_0,\ni.e. it lies in the Krylov subspace of the matrix which is defined as\n\n   \\mathcal{K}_k(A, r_0) = \\mathrm{Span}(r_0, Ar_0, A^2 r_0, \\ldots, A^{k-1}r_0 )\n\nThe most natural approach then is to find the vector in this linear subspace that minimizes certain norm of the error"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#idea-of-krylov-methods",
    "href": "lectures/lecture-12/lecture-12.html#idea-of-krylov-methods",
    "title": "The main topics for today",
    "section": "Idea of Krylov methods",
    "text": "Idea of Krylov methods\nThe idea is to minimize given functional: - Energy norm of error for systems with hermitian positive-definite matrices (CG method). - Residual norm for systems with general matrices (MINRES and GMRES methods). - Rayleigh quotient for eigenvalue problems (Lanczos method).\nTo make methods practical one has to 1. Orthogonalize vectors A^i r_0 of the Krylov subspace for stability (Lanczos process). 2. Derive recurrent formulas to decrease complexity.\nWe will consider these methods in details on the next lecture."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#take-home-message",
    "href": "lectures/lecture-12/lecture-12.html#take-home-message",
    "title": "The main topics for today",
    "section": "Take home message",
    "text": "Take home message\n\nMain idea of iterative methods\nRichardson iteration: hermitian and non-hermitian case\nChebyshev acceleration\nDefinition of Krylov subspace\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html",
    "href": "lectures/lecture-4/lecture-4.html",
    "title": "Eckart-Young theorem",
    "section": "",
    "text": "Peak performance of algorithm\nComplexity of matrix multiplication algorithms\nIdea of blocking (why it is good?)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#previous-lecture",
    "href": "lectures/lecture-4/lecture-4.html#previous-lecture",
    "title": "Eckart-Young theorem",
    "section": "",
    "text": "Peak performance of algorithm\nComplexity of matrix multiplication algorithms\nIdea of blocking (why it is good?)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#todays-lecture",
    "href": "lectures/lecture-4/lecture-4.html#todays-lecture",
    "title": "Eckart-Young theorem",
    "section": "Todays lecture",
    "text": "Todays lecture\n\nMatrix rank\nSkeleton decomposition\nLow-rank approximation\nSingular Value Decomposition (SVD)\nApplications of SVD"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#matrix-and-linear-spaces",
    "href": "lectures/lecture-4/lecture-4.html#matrix-and-linear-spaces",
    "title": "Eckart-Young theorem",
    "section": "Matrix and linear spaces",
    "text": "Matrix and linear spaces\n\nA matrix can be considered as a sequence of vectors that are columns of a matrix:\n\n A = [a_1, \\ldots, a_m], \nwhere a_m \\in \\mathbb{C}^{n\\times 1}.\n\nA matrix-by-vector product is equivalent to taking a linear combination of those columns\n\n y =  Ax \\quad \\Longleftrightarrow \\quad y = a_1 x_1 + a_2 x_2 + \\ldots +a_m x_m. \n\nThis is a special case of block matrix notation (columns are also blocks) that we have already seen (blocking to fit cache memory, Strassen algorithm)."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#linear-dependence",
    "href": "lectures/lecture-4/lecture-4.html#linear-dependence",
    "title": "Eckart-Young theorem",
    "section": "Linear dependence",
    "text": "Linear dependence\nDefinition. Vectors a_i are called linearly dependent, if there exist simultaneously non-zero coefficients x_i such that\n\\sum_i a_i x_i = 0,\nor in the matrix form\n Ax = 0, \\quad \\Vert x \\Vert \\ne 0. \nIn this case, we say that the matrix A has a non-trivial nullspace (or kernel) denoted by N(A) (or \\text{ker}(A)).\nVectors that are not linearly dependent are called linearly independent."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#linear-vector-space",
    "href": "lectures/lecture-4/lecture-4.html#linear-vector-space",
    "title": "Eckart-Young theorem",
    "section": "Linear (vector) space",
    "text": "Linear (vector) space\nA linear space spanned by vectors \\{a_1, \\ldots, a_m\\} is defined as all possible vectors of the form\n \\mathcal{L}(a_1, \\ldots, a_m) = \\left\\{y: y = \\sum_{i=1}^m a_i x_i, \\, \\forall x_i, \\, i=1,\\dots, n \\right\\}, \nIn the matrix form, the linear space is a set of all y such that\ny = A x.\nThis set is also called the range (or image) of the matrix, denoted by \\text{range}(A) (or \\text{im}(A)) respectively."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#dimension-of-a-linear-space",
    "href": "lectures/lecture-4/lecture-4.html#dimension-of-a-linear-space",
    "title": "Eckart-Young theorem",
    "section": "Dimension of a linear space",
    "text": "Dimension of a linear space\n\nThe dimension of a linear space \\text{im}(A) denoted by \\text{dim}\\, \\text{im} (A) is the minimal number of vectors required to represent each vector from \\text{im} (A).\nThe dimension of \\text{im}(A) has a direct connection to the matrix rank."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#matrix-rank",
    "href": "lectures/lecture-4/lecture-4.html#matrix-rank",
    "title": "Eckart-Young theorem",
    "section": "Matrix rank",
    "text": "Matrix rank\n\nRank of a matrix A is a maximal number of linearly independent columns in a matrix A, or the dimension of its column space = \\text{dim} \\, \\text{im}(A).\nYou can also use linear combination of rows to define the rank, i.e. formally there are two ranks: column rank and row rank of a matrix.\n\nTheorem\nThe dimension of the column space of the matrix is equal to the dimension of its row space.\nProof\n\nIn the matrix form this fact can be written as \\mathrm{dim}\\ \\mathrm{im} (A) = \\mathrm{dim}\\ \\mathrm{im} (A^\\top).\nThus, there is a single rank!"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#full-rank-matrix",
    "href": "lectures/lecture-4/lecture-4.html#full-rank-matrix",
    "title": "Eckart-Young theorem",
    "section": "Full-rank matrix",
    "text": "Full-rank matrix\n\nA matrix A \\in \\mathbb{R}^{m \\times n} is called of full-rank, if \\mathrm{rank}(A) = \\min(m, n).\n\nSuppose, we have a linear space, spanned by n vectors. Let these vectors be random with elements from standard normal distribution \\mathcal{N}(0, 1).\nQ: What is the probability of the fact that this subspace has dimension m &lt; n?\nA: Random matrix has full rank with probability 1."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#dimensionality-reduction",
    "href": "lectures/lecture-4/lecture-4.html#dimensionality-reduction",
    "title": "Eckart-Young theorem",
    "section": "Dimensionality reduction",
    "text": "Dimensionality reduction\n\nA lot of data from real-world applications are high dimensional, for instance images (e.g. 512\\times 512 pixels), texts, graphs.\nHowever, working with high-dimensional data is not an easy task.\nIs it possible to reduce the dimensionality, preserving important relations between objects such as distance?\n\nJohnson–Lindenstrauss lemma\nLet N\\gg 1. Given 0 &lt; \\epsilon &lt; 1, a set of m points in \\mathbb{R}^N and n &gt; \\frac{8 \\log m}{\\epsilon^2} (we want n\\ll N).\nThen there exists linear map f from \\mathbb{R}^N \\rightarrow \\mathbb{R}^n such that the following inequality holds:\n(1 - \\epsilon) \\Vert u - v \\Vert^2 \\leq \\Vert f(u) - f(v) \\Vert^2 \\leq (1 + \\epsilon) \\Vert u - v \\Vert^2.\n\nThis theorem states that there exists a map from high- to a low-dimensional space so that distances between points in these spaces are almost the same.\nIt is not very practical due to the dependence on \\epsilon.\nThis lemma does not give a recipe how to construct f, but guarantees that f exists."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#skeleton-decomposition",
    "href": "lectures/lecture-4/lecture-4.html#skeleton-decomposition",
    "title": "Eckart-Young theorem",
    "section": "Skeleton decomposition",
    "text": "Skeleton decomposition\nA very useful representation for computation of the matrix rank is the skeleton decomposition and is closely related to the rank. This decompositions explains, why and how matrices of low rank can be compressed.\nIt can be graphically represented as follows:\n or in the matrix form\n A = C \\widehat{A}^{-1} R, \nwhere C are some k=\\mathrm{rank}(A) columns of A, R are some k rows of A and \\widehat{A} is the nonsingular submatrix on the intersection.\n\nRemark\nWe have not yet formally defined the inverse, so just a reminder:\n\nAn inverse of the matrix P is the matrix Q = P^{-1} such that $ P Q = QP = I$.\n\nIf the matrix is square and has full rank then the inverse exists.\n\n\n\nProof for the skeleton decomposition\n\nLet C\\in \\mathbb{C}^{n\\times k} be the k columns based on the nonsingular submatrix \\widehat{A}. Therefore they are linearly independent.\nTake any other column a_i of A. Then a_i can be represented as a linear combination of the columns of C, i.e. a_i = C x_i, where x_i is a vector of coefficients.\na_i = C x_i are n equations. We take k equations of those corresponding to the rows that contain \\widehat{A} and get the equation\n\n\\widehat{r}_i = \\widehat{A} x_i \\quad \\Longrightarrow \\quad x_i = \\widehat{A}^{-1} \\widehat r_i\nThus, a_i = C\\widehat{A}^{-1} \\widehat r_i for every i and\nA = [a_1,\\dots, a_m] = C\\widehat{A}^{-1} R.\n\n\nA closer look on the skeleton decomposition\n\nAny rank-r matrix can be written in the form\n\nA = C \\widehat{A}^{-1} R,\nwhere C is n \\times r, R is r \\times m and \\widehat{A} is r \\times r, or\n A = UV, \nwhere U and V are not unique, e.g. U = C \\widehat{A}^{-1}, V=R.\n\nThe form A = U V is standard for skeleton decomposition.\nThus, every rank-r matrix can be written as a product of a “skinny” (“tall”) matrix U by a “fat” (“short”) matrix V.\n\nIn the index form, it is\n a_{ij} = \\sum_{\\alpha=1}^r u_{i \\alpha} v_{\\alpha j}. \nFor rank 1, we have\n a_{ij} = u_i v_j, \ni.e. it is a separation of indices and rank-r is a sum of rank-1 matrices!\n\n\nStorage\nIt is interesting to note, that for the rank-r matrix\nA = U V\nonly U and V can be stored, which gives us (n+m) r parameters, so it can be used for compression. We can also compute matrix-by-vector Ax product much faster:\n\nMultiplication y = Vx costs \\mathcal{O}(mr) flops.\nMultiplication z = Uy costs \\mathcal{O}(nr) flops.\n\nThe same works for addition, elementwise multiplication, etc. For addition:\n    A_1 + A_2 = U_1 V_1 + U_2 V_2 = [U_1|U_2] [V_1^\\top|V_2^\\top]^\\top \n\n#A fast matrix-by-vector product demo\nimport jax\nimport jax.numpy as jnp\nn = 4096\nr = 10\nu = jax.random.normal(jax.random.PRNGKey(0), (n, r))\nv = jax.random.normal(jax.random.PRNGKey(10), (n, r))\na = u @ v.T\nx = jax.random.normal(jax.random.PRNGKey(1), (n,))\nprint(n*n/(2*n*r))\n%timeit (a @ x).block_until_ready()\n%timeit (u @ (v.T @ x)).block_until_ready()\n\nPlatform 'METAL' is experimental and not all JAX functionality may be correctly supported!\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1730792144.773664 9725347 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\nI0000 00:00:1730792144.809984 9725347 service.cc:145] XLA service 0x10c42af30 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1730792144.810137 9725347 service.cc:153]   StreamExecutor device (0): Metal, &lt;undefined&gt;\nI0000 00:00:1730792144.813529 9725347 mps_client.cc:406] Using Simple allocator.\nI0000 00:00:1730792144.813538 9725347 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator.\n\n\nMetal device set to: Apple M2 Pro\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\n204.8\n612 μs ± 4.04 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n400 μs ± 32.9 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#computing-matrix-rank",
    "href": "lectures/lecture-4/lecture-4.html#computing-matrix-rank",
    "title": "Eckart-Young theorem",
    "section": "Computing matrix rank",
    "text": "Computing matrix rank\nWe can also try to compute the matrix rank using the built-in jnp.linalg.matrix_rank function\n\n#Computing matrix rank\nimport numpy as np\nn = 50 \na = np.ones((n, n))\nprint('Rank of the matrix:', np.linalg.matrix_rank(a))\nb = a + 1e-8 * np.random.randn(*a.shape)\nprint('Rank of the matrix:', np.linalg.matrix_rank(b, tol=1e-7))\n\nRank of the matrix: 1\nRank of the matrix: 9\n\n\n So, small perturbations might crucially affect the rank! \n\nInstability of the matrix rank\nFor any rank-r matrix A with r &lt; \\min(m, n) there is a matrix B such that its rank is equal to \\min(m, n) and\n \\Vert A - B \\Vert = \\epsilon. \nQ: So, does this mean that numerically matrix rank has no meaning? (I.e., small perturbations lead to full rank!)\nA: No. We should find a matrix B such that \\|A-B\\| = \\epsilon and B has minimal rank. So we can only compute rank with given accuracy \\epsilon. One of the approaches to compute matrix rank r is SVD."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#low-rank-approximation",
    "href": "lectures/lecture-4/lecture-4.html#low-rank-approximation",
    "title": "Eckart-Young theorem",
    "section": "Low rank approximation",
    "text": "Low rank approximation\nThe important problem in many applications is to find low-rank approximation of the given matrix with given accurcacy \\epsilon or rank r.  Examples: * principal component analysis * recommender systems * least squares * neural network compression\nThese problems can be solved by SVD."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#singular-value-decomposition",
    "href": "lectures/lecture-4/lecture-4.html#singular-value-decomposition",
    "title": "Eckart-Young theorem",
    "section": "Singular value decomposition",
    "text": "Singular value decomposition\nTo compute low-rank approximation, we need to compute singular value decomposition (SVD).\nTheorem Any matrix A\\in \\mathbb{C}^{n\\times m} can be written as a product of three matrices:\n A = U \\Sigma V^*, \nwhere - U is an n \\times K unitary matrix, - V is an m \\times K unitary matrix, K = \\min(m, n), - \\Sigma is a diagonal matrix with non-negative elements \\sigma_1 \\geq  \\ldots, \\geq \\sigma_K on the diagonal. - Moreover, if \\text{rank}(A) = r, then \\sigma_{r+1} = \\dots = \\sigma_K = 0.\n\nProof\n\nMatrix A^*A is Hermitian, hence diagonalizable in unitary basis (will be discussed further in the course).\nA^*A\\geq0 (non-negative definite), so eigenvalues are non-negative. Therefore, there exists unitary matrix V = [v_1, \\dots, v_n] such that\n\n V^* A^* A V = \\text{diag}(\\sigma_1^2,\\dots, \\sigma_n^2), \\quad \\sigma_1\\geq \\sigma_2\\geq \\dots \\geq \\sigma_n. \nLet \\sigma_i = 0 for i&gt;r, where r is some integer.  Let V_r= [v_1, \\dots, v_r], \\Sigma_r = \\text{diag}(\\sigma_1, \\dots,\\sigma_r). Hence\n V^*_r A^* A V_r = \\Sigma_r^2 \\quad \\Longrightarrow \\quad (\\Sigma_r^{-1} V_r^* A^*) (A V_r\\Sigma_r^{-1} ) = I. \nAs a result, matrix U_r = A V_r\\Sigma_r^{-1} satisfies U_r^* U_r = I and hence has orthogonal columns.  Let us add to U_r any orthogonal columns that are orthogonal to columns in U_r and denote this matrix as U. Then\n AV = U \\begin{bmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}\\quad \\Longrightarrow \\quad U^* A V = \\begin{bmatrix}\\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}.\n\nSince multiplication by non-singular matrices does not change rank of A, we have r = \\text{rank}(A).\nCorollary 1: A = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_\\alpha v_\\alpha^* or elementwise a_{ij} = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_{i\\alpha} \\overline{v}_{j\\alpha}\nCorollary 2: \\text{ker}(A) = \\mathcal{L}\\{v_{r+1},\\dots,v_n\\}\n\\text{im}(A) = \\mathcal{L}\\{u_{1},\\dots,u_r\\}\n\\text{ker}(A^*) = \\mathcal{L}\\{u_{r+1},\\dots,u_n\\}\n\\text{im}(A^*) = \\mathcal{L}\\{v_{1},\\dots,v_r\\}"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#proof-1",
    "href": "lectures/lecture-4/lecture-4.html#proof-1",
    "title": "Eckart-Young theorem",
    "section": "Proof",
    "text": "Proof\n\nSince \\text{rank} (B) = r, it holds \\text{dim}~\\text{ker}~B = n-r.\nHence there exists z\\not=0 such that z\\in \\text{ker}(B) \\cap \\mathcal{L}(v_1,\\dots,v_{r+1}) (as \\text{dim}\\{v_1,\\dots,v_{r+1}\\} = r+1).\nFix \\|z\\| = 1. Therefore,\n\n \\|A-B\\|_2^2 \\geq \\|(A-B)z\\|_2^2 = \\|Az\\|_2^2 = \\| U\\Sigma V^* z\\|^2_2= \\|\\Sigma V^* z\\|^2_2 = \\sum_{i=1}^{n} \\sigma_i^2 (v_i^*z)^2 =\\sum_{i=1}^{r+1} \\sigma_i^2 (v_i^*z)^2 \\geq \\sigma_{r+1}^2\\sum_{i=1}^{r+1} (v_i^*z)^2 = \\sigma_{r+1}^2 \nas \\sigma_1\\geq \\dots \\geq \\sigma_{r+1} and \\sum_{i=1}^{r+1} (v_i^*z)^2 = \\|V^*z\\|_2^2 = \\|z\\|_2^2 = 1."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#main-result-on-low-rank-approximation",
    "href": "lectures/lecture-4/lecture-4.html#main-result-on-low-rank-approximation",
    "title": "Eckart-Young theorem",
    "section": "Main result on low-rank approximation",
    "text": "Main result on low-rank approximation\nCorollary: computation of the best rank-r approximation is equivalent to setting \\sigma_{r+1}= 0, \\ldots, \\sigma_K = 0. The error\n \\min_{A_r} \\Vert A - A_r \\Vert_2 = \\sigma_{r+1}, \\quad \\min_{A_r} \\Vert A - A_r \\Vert_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{K}^2} \nthat is why it is important to look at the decay of the singular values."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#computing-svd",
    "href": "lectures/lecture-4/lecture-4.html#computing-svd",
    "title": "Eckart-Young theorem",
    "section": "Computing SVD",
    "text": "Computing SVD\n\nAlgorithms for the computation of the SVD are tricky and will be discussed later.\nBut for numerics, we can use NumPy or JAX or PyTorch already!\n\nLet us go back to the previous example\n\n#Computing matrix rank\nimport numpy as np\nn = 50 \na = jnp.ones((n, n))\nprint('Rank of the matrix:', np.linalg.matrix_rank(a))\nb = a + 1e-5 * np.random.randn(*a.shape)\nprint('Rank of the matrix:', np.linalg.matrix_rank(b, tol=1e-7))\n\nRank of the matrix: 1\nRank of the matrix: 49\n\n\n\nu, s, v = np.linalg.svd(b) #b = u@jnp.diag(s)@v \nprint(s/s[0])\nprint(s[1]/s[0])\nr = 1\nu1 = u[:, :r]\ns1 = s[:r]\nv1 = v[:r, :]\na1 = u1.dot(np.diag(s1).dot(v1))\nprint(np.linalg.norm(b - a1, 2)/s[0])\n\n[1.00000000e+00 2.74448832e-09 2.44654918e-09 2.42773562e-09\n 2.31264167e-09 2.27689153e-09 2.19991302e-09 2.14396536e-09\n 2.11550251e-09 1.97278176e-09 1.94284870e-09 1.89051773e-09\n 1.81330587e-09 1.72565526e-09 1.67993428e-09 1.59113590e-09\n 1.58090460e-09 1.54788430e-09 1.51371766e-09 1.35601427e-09\n 1.29745155e-09 1.28204694e-09 1.24122734e-09 1.16997512e-09\n 1.16408528e-09 1.07099869e-09 1.02269543e-09 9.77647860e-10\n 9.55021103e-10 8.96594708e-10 8.66479269e-10 8.33647595e-10\n 8.09757684e-10 7.82520013e-10 7.40035132e-10 6.93012504e-10\n 6.22256571e-10 5.69581513e-10 5.40911549e-10 4.96101659e-10\n 4.29921664e-10 3.88697820e-10 3.55032936e-10 2.93775499e-10\n 2.44412221e-10 2.10276245e-10 1.34695842e-10 1.19487903e-10\n 7.45166628e-11 4.16866804e-11]\n2.744488324665868e-09\n2.7444883257824676e-09"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#separation-of-variables-for-2d-functions",
    "href": "lectures/lecture-4/lecture-4.html#separation-of-variables-for-2d-functions",
    "title": "Eckart-Young theorem",
    "section": "Separation of variables for 2D functions",
    "text": "Separation of variables for 2D functions\nWe can use SVD to compute approximations of function-related matrices, i.e. the matrices of the form\na_{ij} = f(x_i, y_j),\nwhere f is a certain function, and x_i, \\quad i = 1, \\ldots, n and y_j, \\quad j = 1, \\ldots, m are some one-dimensional grids.\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rc(\"text\", usetex=False)\n\nn = 100\na = [[1.0/(i-j+0.5) for i in range(n)] for j in range(n)] #Hilbert matrix \n#a = jnp.ones((n, n)) + 1e-3*jax.random.normal(jax.random.PRNGKey(67575), (n, n))\na = np.array(a)\nu, s, v = np.linalg.svd(a)\nprint(s[50] - np.pi)\nplt.plot(s[:30], 'x')\ns\n#plt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\n#plt.xlabel(r\"Singular value index, $i$\", fontsize=24)\n# plt.grid(True)\n# plt.xticks(fontsize=26)\n# plt.yticks(fontsize=26)\n# #We have very good low-rank approximation of it!\n\n4.440892098500626e-16\n\n\narray([3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159265, 3.14159265,\n       3.14159265, 3.14159265, 3.14159265, 3.14159263, 3.14159247,\n       3.14159137, 3.14158397, 3.14153694, 3.14125447, 3.13966004,\n       3.13127783, 3.09092653, 2.91974503, 2.33329   , 0.9473473 ])\n\n\n\n\n\n\n\n\n\n\nFunction approximation\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nn = 128\nt = np.linspace(0, 5, n)\nx, y = np.meshgrid(t, t)\nf = 1.0 / (x + y + 0.01) # test your own function. Check 1.0 / (x - y + 0.5)\nu, s, v = np.linalg.svd(f, full_matrices=False)\nr = 10\nu = u[:, :r]\ns = s[:r]\nv = v[:r, :] # Mind the transpose here!\nfappr = (u * s[None, :]) @ v\ner = np.linalg.norm(fappr - f, 'fro') / np.linalg.norm(f, 'fro')\nprint(er)\nplt.semilogy(s/s[0])\n#plt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\n#plt.xlabel(r\"Singular value index, $i$\", fontsize=24)\n#plt.grid(True)\n#plt.xticks(fontsize=26)\n#plt.yticks(fontsize=26)\n\n1.800487035360937e-07"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#and-3d-plots",
    "href": "lectures/lecture-4/lecture-4.html#and-3d-plots",
    "title": "Eckart-Young theorem",
    "section": "And 3d plots…",
    "text": "And 3d plots…\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n# plt.xkcd()\nfig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(x, y, f)\nax.set_title('Original function')\nax = fig.add_subplot(122, projection='3d')\nax.plot_surface(x, y, fappr - f)\nax.set_title('Approximation error with rank=%d, err=%3.1e' % (r, er))\nfig.subplots_adjust()\nfig.tight_layout()"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#singular-values-of-a-random-gaussian-matrix",
    "href": "lectures/lecture-4/lecture-4.html#singular-values-of-a-random-gaussian-matrix",
    "title": "Eckart-Young theorem",
    "section": "Singular values of a random Gaussian matrix",
    "text": "Singular values of a random Gaussian matrix\nWhat is the singular value decay of a random matrix?\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nn = 1000\na = jax.random.normal(jax.random.PRNGKey(244747), (n, n))\nu, s, v = jnp.linalg.svd(a)\nplt.semilogy(s/s[0])\nplt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\nplt.xlabel(r\"Singular value index, $i$\", fontsize=24)\nplt.grid(True)\nplt.xticks(fontsize=26)\nplt.yticks(fontsize=26)\n\n(array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n        1.e+02]),\n [Text(0, 1e-06, '$\\\\mathdefault{10^{-6}}$'),\n  Text(0, 1e-05, '$\\\\mathdefault{10^{-5}}$'),\n  Text(0, 0.0001, '$\\\\mathdefault{10^{-4}}$'),\n  Text(0, 0.001, '$\\\\mathdefault{10^{-3}}$'),\n  Text(0, 0.01, '$\\\\mathdefault{10^{-2}}$'),\n  Text(0, 0.1, '$\\\\mathdefault{10^{-1}}$'),\n  Text(0, 1.0, '$\\\\mathdefault{10^{0}}$'),\n  Text(0, 10.0, '$\\\\mathdefault{10^{1}}$'),\n  Text(0, 100.0, '$\\\\mathdefault{10^{2}}$')])"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#linear-factor-analysis-low-rank",
    "href": "lectures/lecture-4/lecture-4.html#linear-factor-analysis-low-rank",
    "title": "Eckart-Young theorem",
    "section": "Linear factor analysis & low-rank",
    "text": "Linear factor analysis & low-rank\nConsider a linear factor model,\ny = Ax, \nwhere y is a vector of length n, and x is a vector of length r.\nThe data is organized as samples: we observe vectors\ny_1, \\ldots, y_T,\nbut do not know matrix A, then the factor model can be written as\n\n  Y = AX,\n\nwhere Y is n \\times T, A is n \\times r and X is r \\times T.\n\nThis is exactly a rank-r model: it tells us that the vectors lie in a small subspace.\nWe also can use SVD to recover this subspace (but not the independent components).\nPrincipal component analysis can be done by SVD, checkout the implementation in sklearn package."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#applications-of-svd",
    "href": "lectures/lecture-4/lecture-4.html#applications-of-svd",
    "title": "Eckart-Young theorem",
    "section": "Applications of SVD",
    "text": "Applications of SVD\n\nSVD is extremely important in computational science and engineering.\nIt has many names: Principal component analysis, Proper Orthogonal Decomposition, Empirical Orthogonal Functions\nNow we will consider compression of dense matrix and active subspaces method"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#dense-matrix-compression",
    "href": "lectures/lecture-4/lecture-4.html#dense-matrix-compression",
    "title": "Eckart-Young theorem",
    "section": "Dense matrix compression",
    "text": "Dense matrix compression\nDense matrices typically require N^2 elements to be stored. A rank-r approximation can reduces this number to \\mathcal{O}(Nr)\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nn = 256\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\n#u, s, v = np.linalg.svd(a)\nu, s, v = jnp.linalg.svd(a[n//2:, :n//2])\nplt.semilogy(s/s[0])\nplt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\nplt.xlabel(r\"Singular value index, $i$\", fontsize=24)\nplt.grid(True)\nplt.xticks(fontsize=26)\nplt.yticks(fontsize=26)\n#s[0] - jnp.pi\n#u, s, v = jnp.linalg.svd(a[:128:, :128])\n#print(s[0]-jnp.pi)\n\nDeviceArray(-0.3372376, dtype=float32)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#compression-of-parameters-in-fully-connected-neural-networs",
    "href": "lectures/lecture-4/lecture-4.html#compression-of-parameters-in-fully-connected-neural-networs",
    "title": "Eckart-Young theorem",
    "section": "Compression of parameters in fully-connected neural networs",
    "text": "Compression of parameters in fully-connected neural networs\n\nOne of the main building blocks of the modern deep neural networks is fully-connected layer a.k.a. linear layer\nThis layer implements the action of a linear function to an input vector: f(x) = Wx + b, where W is a trainable matrix and b is a trainable bias vector\nBoth W and b are updated during training of the network according to some optimization method, i.e. SGD, Adam, etc…\nHowever, the storing of the trained optimal parameters (W and b) can be prohibitive if you want to port your trained network to the device, where memory is limited\nAs a possible recipe, you can compress matrices W_i from the i-th linear layer with the truncated SVD based on the singular values!\nWhat do you get after such apprioximation of W?\n\nmemory efficient storage\nfaster inference\nmoderate degradation of the accuracy in solving the target task, i.e. image classification"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#active-subspaces",
    "href": "lectures/lecture-4/lecture-4.html#active-subspaces",
    "title": "Eckart-Young theorem",
    "section": "Active Subspaces",
    "text": "Active Subspaces\n\nSuppose, we are given a function f(x), \\ x \\in \\mathcal{X} \\subseteq \\mathbb{R}^{n} and want find its low-dimensional parametrization. Here \\mathcal{X} is the domain of f(x).\nInformally, we are searching for the directions in which a f(x) changes a lot on average and for the directions in which f(x) is almost constant.\nFormally, we assume that there is a matrix W \\in \\mathbb{R}^{r \\times n} and a function g: \\mathbb{R^r} \\to \\mathbb{R}, such that for every x \\in \\mathcal{X}\n\n\nf(x)\n\\approx\ng(W x).\n\n\nHow to discover Active Subspaces:\nUsing SVD: 1. Choose m, the number of estimations. This hyperparameter stands for the number of Monte Carlo estimations. The larger m, the more accurate the result is. 2. Draw samples \\lbrace x_i \\rbrace_{i = 1}^{m} from \\mathcal{X} (according to some prior probability density function) 3. For each x_i compute \\nabla f(x_i) 4. Compute the SVD of the matrix\n\nG\n    :=\n\\dfrac{1}{\\sqrt{m}}\n\\begin{bmatrix}\n\\nabla f(x_1) & \\nabla f(x_2) & \\ldots & \\nabla f(x_m)\n\\end{bmatrix}\n    \\approx\nU \\Sigma V^\\top.\n\n\nEstimate the rank of G \\approx U_r \\Sigma_r V_r^\\top. The rank r of the matrix G is the dimensionality of the active subspace.\nLow-dimensional vectors are estimated as x_{\\text{AS}} = U_r^\\top x.\n\nFor further details, look into the book „Active Subspaces: Emerging Ideas in Dimension Reduction for Parameter Studies“ (2015) by Paul Constantine."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#take-home-message",
    "href": "lectures/lecture-4/lecture-4.html#take-home-message",
    "title": "Eckart-Young theorem",
    "section": "Take home message",
    "text": "Take home message\n\nMatrix rank definition\nSkeleton approximation and dyadic representation of a rank-r matrix\nSingular value decomposition and Eckart-Young theorem\nThree applications of SVD (linear factor analysis, dense matrix compression, active subspaces)."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#next-lecture",
    "href": "lectures/lecture-4/lecture-4.html#next-lecture",
    "title": "Eckart-Young theorem",
    "section": "Next lecture",
    "text": "Next lecture\n\nLinear systems\nInverse matrix\nCondition number\nLinear least squares\nPseudoinverse\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html",
    "href": "lectures/lecture-2/lecture-2.html",
    "title": "",
    "section": "",
    "text": "Floating point (double, single, half precisions, number of bytes), rounding error\nVector norms are measures of smallness, used to compute the distance and accuracy\nForward/backward error (and stability of algorithms)"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#recap-of-the-previous-lecture",
    "href": "lectures/lecture-2/lecture-2.html#recap-of-the-previous-lecture",
    "title": "",
    "section": "",
    "text": "Floating point (double, single, half precisions, number of bytes), rounding error\nVector norms are measures of smallness, used to compute the distance and accuracy\nForward/backward error (and stability of algorithms)"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#notations",
    "href": "lectures/lecture-2/lecture-2.html#notations",
    "title": "",
    "section": "Notations",
    "text": "Notations\nWe use notation\nA= \\begin{bmatrix} a_{11} & \\dots & a_{1m} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n1} & \\dots & a_{nm}  \\end{bmatrix} \\equiv \\{a_{ij}\\}_{i,j=1}^{n,m}\\in \\mathbb{C}^{n\\times m}.\nA^*\\stackrel{\\mathrm{def}}{=}\\overline{A^\\top}."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#matrices-and-norms",
    "href": "lectures/lecture-2/lecture-2.html#matrices-and-norms",
    "title": "",
    "section": "Matrices and norms",
    "text": "Matrices and norms\n\nRecall vector norms that allow to evaluate distance between two vectors or how large are elements of a vector.\nHow to generalize this concept to matrices?\nA trivial answer is that there is no big difference between matrices and vectors, and here comes the simplest matrix norm –– Frobenius norm:\n\n \\Vert A \\Vert_F \\stackrel{\\mathrm{def}}{=} \\Big(\\sum_{i=1}^n \\sum_{j=1}^m |a_{ij}|^2\\Big)^{1/2}"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#matrix-norms",
    "href": "lectures/lecture-2/lecture-2.html#matrix-norms",
    "title": "",
    "section": "Matrix norms",
    "text": "Matrix norms\n\\Vert \\cdot \\Vert is called a matrix norm if it is a vector norm on the vector space of n \\times m matrices: 1. \\|A\\| \\geq 0 and if \\|A\\| = 0 then A = O 3. \\|\\alpha A\\| = |\\alpha| \\|A\\| 4. \\|A+B\\| \\leq \\|A\\| + \\|B\\| (triangle inequality)\nAdditionally some norms can satisfy the submultiplicative property\n\n \\Vert A B \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert \nThese norms are called submultiplicative norms.\nThe submultiplicative property is needed in many places, for example in the estimates for the error of solution of linear systems (we will cover this topic later).\nExample of a non-submultiplicative norm is Chebyshev norm\n\n \\|A\\|_C = \\displaystyle{\\max_{i,j}}\\, |a_{ij}|"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#operator-norms",
    "href": "lectures/lecture-2/lecture-2.html#operator-norms",
    "title": "",
    "section": "Operator norms",
    "text": "Operator norms\n\nThe most important class of the matrix norms is the class of operator norms. They are defined as\n\n \\Vert A \\Vert_{*,**} = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_*}{\\Vert x \\Vert_{**}}, \nwhere \\Vert \\cdot \\Vert_* and \\| \\cdot \\|_{**} are vector norms.\n\nIt is easy to check that operator norms are submultiplicative if \\|\\cdot\\|_* = \\|\\cdot\\|_{**}. Otherwise, it can be non-submultiplicative, think about example.\nFrobenius norm is a matrix norm, but not an operator norm, i.e. you can not find \\Vert \\cdot \\Vert_* and \\| \\cdot \\|_{**} that induce it.\nThis is a nontrivial fact and the general criterion for matrix norm to be an operator norm can be found in Theorem 6 and Corollary 4. For \\Vert \\cdot \\Vert_* = \\| \\cdot \\|_{**} let us check on the blackboard!"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#matrix-p-norms",
    "href": "lectures/lecture-2/lecture-2.html#matrix-p-norms",
    "title": "",
    "section": "Matrix p-norms",
    "text": "Matrix p-norms\nImportant case of operator norms are matrix p-norms, which are defined for \\|\\cdot\\|_* = \\|\\cdot\\|_{**} = \\|\\cdot\\|_p. \nAmong all p-norms three norms are the most common ones:\n\np = 1, \\quad \\Vert A \\Vert_{1} = \\displaystyle{\\max_j \\sum_{i=1}^n} |a_{ij}|.\np = 2, \\quad spectral norm, denoted by \\Vert A \\Vert_2.\np = \\infty, \\quad \\Vert A \\Vert_{\\infty} = \\displaystyle{\\max_i \\sum_{j=1}^m} |a_{ij}|.\n\nLet us check it for p=\\infty on a blackboard."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#spectral-norm",
    "href": "lectures/lecture-2/lecture-2.html#spectral-norm",
    "title": "",
    "section": "Spectral norm",
    "text": "Spectral norm\n\nSpectral norm, \\Vert A \\Vert_2 is one of the most used matrix norms (along with the Frobenius norm).\nIt can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it.\n\nIt is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\n \\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^*A)} \nwhere \\sigma_1(A) is the largest singular value of the matrix A and ^* is a conjugate transpose of the matrix.\n\nWe will soon learn all about the SVD. Meanwhile, we can already compute the norm in Python.\n\n\nimport jax.numpy as jnp\nimport numpy as np\nimport jax\nn = 100\nm = 2000\na = np.random.randn(n, m) #Random n x m matrix\ns1 = np.linalg.norm(a, 2) #Spectral\ns2 = np.linalg.norm(a, 'fro') #Frobenius\ns3 = np.linalg.norm(a, 1) #1-norm\ns4 = np.linalg.norm(a, jnp.inf) \nprint('Spectral: {0:} \\nFrobenius: {1:} \\n1-norm: {2:} \\ninfinity: {3:}'.format(s1, s2, s3, s4))\n\nSpectral: 54.23724766209146 \nFrobenius: 447.08449637895757 \n1-norm: 102.09472515184673 \ninfinity: 1690.7456169067727"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#examples",
    "href": "lectures/lecture-2/lecture-2.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\nSeveral examples of optimization problems where matrix norms arise: *  \\displaystyle{\\min_{\\mathrm{rank}(A_r)=r}}\\| A - A_r\\|  –– finding best rank-r approximation. SVD helps to solve this problem for \\|\\cdot\\|_2 and \\|\\cdot\\|_F.\n\n\\displaystyle{\\min_B}\\| P_\\Omega \\odot(A - B)\\| + \\mathrm{rank}(B) –– matrix completion.\n\n (P_\\Omega)_{ij} = \\begin{cases} 1 & i,j\\in\\Omega \\\\ 0 & \\text{otherwise}, \\end{cases} \nwhere \\odot denotes Hadamard product (elementwise)\n\n\\displaystyle{\\min_{B,C\\geq 0}} \\|A - BC\\|_F –– nonnegative matrix factorization. Symbol B\\geq0 here means that all elements of B are nonnegative."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#scalar-product",
    "href": "lectures/lecture-2/lecture-2.html#scalar-product",
    "title": "",
    "section": "Scalar product",
    "text": "Scalar product\nWhile norm is a measure of distance, the scalar product takes angle into account.\nIt is defined as\n\nFor vectors:  (x, y) =  x^* y = \\sum_{i=1}^n \\overline{x}_i y_i,  where \\overline{x} denotes the complex conjugate of x. The Euclidean norm is then\n\n \\Vert x \\Vert_2 = \\sqrt{(x, x)}, \nor it is said the norm is induced by the scalar product.\n\nFor matrices (Frobenius scalar product):\n\n (A, B)_F = \\displaystyle{\\sum_{i=1}^{n}\\sum_{j=1}^{m}} \\overline{a}_{ij} b_{ij} \\equiv \\mathrm{trace}(A^* B), \nwhere \\mathrm{trace}(A) denotes the sum of diagonal elements of A. One can check that \\|A\\|_F = \\sqrt{(A, A)_F}.\nRemark. The angle between two vectors is defined as\n \\cos \\phi = \\frac{(x, y)}{\\Vert x \\Vert_2 \\Vert y \\Vert_2}. \nSimilar expression holds for matrices.\n\nAn important property of the scalar product is the Cauchy-Schwarz-Bunyakovski inequality:\n\n|(x, y)| \\leq \\Vert x \\Vert_2 \\Vert y \\Vert_2,\nand thus the angle between two vectors is defined properly."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#matrices-preserving-the-norm",
    "href": "lectures/lecture-2/lecture-2.html#matrices-preserving-the-norm",
    "title": "",
    "section": "Matrices preserving the norm",
    "text": "Matrices preserving the norm\n\nFor stability it is really important that the error does not grow after we apply some transformations.\nSuppose you are given \\widehat{x} –– the approximation of x such that,\n\n \\frac{\\Vert x - \\widehat{x} \\Vert}{\\Vert x \\Vert} \\leq \\varepsilon. \n\nLet us calculate a linear transformation of x and \\widehat{x}:\n\n y = U x, \\quad \\widehat{y} = U \\widehat{x}. \n\nWhen building new algorithms, we want to use transformations that do not increase (or even preserve) the error:\n\n \\frac{\\Vert y - \\widehat{y} \\Vert}{\\Vert y \\Vert } = \\frac{\\Vert U ( x - \\widehat{x}) \\Vert}{\\Vert U  x\\Vert}  \\leq \\varepsilon. \n\nThe question is for which kind of matrices the norm of the vector will not change, so that\n\n \\frac{\\Vert U ( x - \\widehat{x}) \\Vert}{\\Vert U  x\\Vert} = \\frac{ \\|x - \\widehat{x}\\|}{\\|x\\|}. \n\nFor the euclidean norm \\|\\cdot\\|_2 the answer is unitary (or orthogonal in the real case) matrices."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#unitary-orthogonal-matrices",
    "href": "lectures/lecture-2/lecture-2.html#unitary-orthogonal-matrices",
    "title": "",
    "section": "Unitary (orthogonal) matrices",
    "text": "Unitary (orthogonal) matrices\n\nLet U be complex n \\times n matrix, and \\Vert U z \\Vert_2 = \\Vert z \\Vert_2 for all z.\nThis can happen if and only if (can be abbreviated as iff)\n\n U^* U = I_n, \nwhere I_n is an identity matrix n\\times n.\n\nComplex n\\times n square matrix is called unitary if\n\n U^*U = UU^* = I_n, \nwhich means that columns and rows of unitary matrices both form orthonormal basis in \\mathbb{C}^{n}.\n\nFor rectangular matrices of size m\\times n (n\\not= m) only one of the equalities can hold\n\n$ U^*U = I_n$ –– left unitary for m&gt;n\n$ UU^* = I_m$ –– right unitary for m&lt;n\n\nIn the case of real matrices U^* = U^T and matrices such that\n\n U^TU = UU^T = I \nare called orthogonal."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#unitary-matrices",
    "href": "lectures/lecture-2/lecture-2.html#unitary-matrices",
    "title": "",
    "section": "Unitary matrices",
    "text": "Unitary matrices\nImportant property: a product of two unitary matrices is a unitary matrix:\n(UV)^* UV = V^* (U^* U) V = V^* V = I,\n\nLater we will show that there are types of matrices (Householder reflections and Givens rotations) composition of which is able to produce arbitrary unitary matrix\nThis idea is a core of some algorithms, e.g. QR decomposition"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#unitary-invariance-of-cdot_2-and-cdot_f-norms",
    "href": "lectures/lecture-2/lecture-2.html#unitary-invariance-of-cdot_2-and-cdot_f-norms",
    "title": "",
    "section": "Unitary invariance of \\|\\cdot\\|_2 and \\|\\cdot\\|_F norms",
    "text": "Unitary invariance of \\|\\cdot\\|_2 and \\|\\cdot\\|_F norms\n\nFor vector 2-norm we have already seen that \\Vert U z \\Vert_2 = \\Vert z \\Vert_2 for any unitary U.\nOne can show that unitary matrices also do not change matrix norms \\|\\cdot\\|_2 and \\|\\cdot\\|_F, i.e. for any square A and unitary U,V:\n\n \\| UAV\\|_2 = \\| A \\|_2 \\qquad \\| UAV\\|_F = \\| A \\|_F.\n\nFor \\|\\cdot\\|_2 it follows from the definition of an operator norm and the fact that vector 2-norm is unitary invariant.\nFor \\|\\cdot\\|_F it follows from \\|A\\|_F^2 = \\mathrm{trace}(A^*A) and the fact that \\mathrm{trace}(BC) = \\mathrm{trace}(CB)."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#examples-of-unitary-matrices",
    "href": "lectures/lecture-2/lecture-2.html#examples-of-unitary-matrices",
    "title": "",
    "section": "Examples of unitary matrices",
    "text": "Examples of unitary matrices\n\nThere are two important classes of unitary matrices, using composition of which we can construct any unitary matrix:\n\nHouseholder matrices\nGivens (Jacobi) matrices\n\nOther important examples are\n\nPermutation matrix P whose rows (columns) are permutation of rows (columns) of the identity matrix.\nFourier matrix F_n = \\frac{1}{\\sqrt{n}} \\left\\{ e^{-i\\frac{2\\pi kl}{n}}\\right\\}_{k,l=0}^{n-1}"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#householder-matrices",
    "href": "lectures/lecture-2/lecture-2.html#householder-matrices",
    "title": "",
    "section": "Householder matrices",
    "text": "Householder matrices\n\nHouseholder matrix is the matrix of the form\n\nH \\equiv H(v) = I - 2 vv^*,\nwhere v is an n \\times 1 column and v^* v = 1. - Can you show that H is unitary and Hermitian (H^* = H)?\n- It is also a reflection:\n Hx = x - 2(v^* x) v"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#important-property-of-householder-reflection",
    "href": "lectures/lecture-2/lecture-2.html#important-property-of-householder-reflection",
    "title": "",
    "section": "Important property of Householder reflection",
    "text": "Important property of Householder reflection\n\nA nice property of Householder transformation is that it can zero all elements of a vector except for the first one:\n\n H \\begin{bmatrix} \\times \\\\ \\times \\\\ \\times \\\\ \\times  \\end{bmatrix} =  \\begin{bmatrix} \\times \\\\ 0 \\\\ 0 \\\\ 0  \\end{bmatrix}. \nProof (for real case). Let e_1 = (1,0,\\dots, 0)^T, then we want to find v such that\n H x = x - 2(v^* x) v = \\alpha e_1, \nwhere \\alpha is an unknown constant. Since \\|\\cdot\\|_2 is unitary invariant we get\n\\|x\\|_2 = \\|Hx\\|_2 = \\|\\alpha e_1\\|_2 = |\\alpha|.\nand \\alpha = \\pm \\|x\\|_2\nAlso, we can express v from x - 2(v^* x) v = \\alpha e_1:\nv = \\dfrac{x-\\alpha e_1}{2 v^* x}\nMultiplying the latter expression by x^* we get\nx^* x - 2 (v^* x) x^* v = \\alpha x_1; \nor\n \\|x\\|_2^2 - 2 (v^* x)^2 = \\alpha x_1. \nTherefore,\n (v^* x)^2 = \\frac{\\|x\\|_2^2 - \\alpha x_1}{2}. \nSo, v exists and equals\n v = \\dfrac{x \\mp \\|x\\|_2 e_1}{2v^* x} = \\dfrac{x \\mp \\|x\\|_2 e_1}{\\pm\\sqrt{2(\\|x\\|_2^2 \\mp \\|x\\|_2 x_1)}}."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#householder-algorithm-for-qr-decomposition",
    "href": "lectures/lecture-2/lecture-2.html#householder-algorithm-for-qr-decomposition",
    "title": "",
    "section": "Householder algorithm for QR decomposition",
    "text": "Householder algorithm for QR decomposition\nUsing the obtained property we can make arbitrary matrix A lower triangular:\n\nH_2 H_1 A = \\begin{bmatrix} \\times & \\times & \\times & \\times \\\\ 0 & \\times & \\times & \\times  \\\\  0 & 0 & \\boldsymbol{\\times} & \\times\\\\ 0 &0 & \\boldsymbol{\\times} & \\times  \\\\ 0 &0 & \\boldsymbol{\\times} & \\times \\end{bmatrix} \nthen finding H_3=\\begin{bmatrix}I_2 & \\\\ & {\\widetilde H}_3 \\end{bmatrix} such that\n {\\widetilde H}_3 \\begin{bmatrix} \\boldsymbol{\\times}\\\\ \\boldsymbol{\\times} \\\\ \\boldsymbol{\\times}  \\end{bmatrix} = \\begin{bmatrix} \\times \\\\ 0 \\\\ 0  \\end{bmatrix}. \nwe get\n H_3 H_2 H_1 A =  \\begin{bmatrix} \\times & \\times & \\times & \\times \\\\  0 & \\times & \\times & \\times  \\\\  0 & 0 & {\\times} & \\times\\\\  0 &0 & 0 & \\times  \\\\  0 &0 & 0 & \\times  \\end{bmatrix} \nFinding H_4 by analogy we arrive at upper-triangular matrix.\nSince product and inverse of unitary matrices is a unitary matrix we get:\nCorollary: (QR decomposition) Every A\\in \\mathbb{C}^{n\\times m} can be represented as\n A = QR, \nwhere Q is unitary and R is upper triangular.\nSee poster, what are the sizes of Q and R for n&gt;m and n&lt;m."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#givens-jacobi-matrix",
    "href": "lectures/lecture-2/lecture-2.html#givens-jacobi-matrix",
    "title": "",
    "section": "Givens (Jacobi) matrix",
    "text": "Givens (Jacobi) matrix\n\nA Givens matrix is a matrix\n\n G = \\begin{bmatrix} \\cos \\alpha & -\\sin \\alpha \\\\ \\sin \\alpha & \\cos \\alpha \\end{bmatrix},\nwhich is a rotation.\n\nFor a general case, we select two (i, j) planes and rotate vector x\n\n x' = G x, \nonly in the i-th and j-th positions:\n x'_i =  x_i\\cos \\alpha - x_j\\sin \\alpha , \\quad x'_j = x_i \\sin \\alpha  +  x_j\\cos\\alpha, \nwith all other x_i remain unchanged. - Therefore, we can make elements in the j-th position zero by choosing \\alpha such that\n \\cos \\alpha = \\frac{x_i}{\\sqrt{x_i^2 + x_j^2}}, \\quad \\sin \\alpha = -\\frac{x_j}{\\sqrt{x_i^2 + x_j^2}} \n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nalpha = -3*jnp.pi / 4\nG = jnp.array([\n    [jnp.cos(alpha), -jnp.sin(alpha)],\n    [jnp.sin(alpha), jnp.cos(alpha)]\n])\nx = jnp.array([-1./jnp.sqrt(2), 1./jnp.sqrt(2)])\ny = G @ x\n\nplt.quiver([0, 0], [0, 0], [x[0], y[0]], [x[1], y[1]], angles='xy', scale_units='xy', scale=1)\nplt.xlim(-1., 1.)\nplt.ylim(-1., 1.)"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#qr-via-givens-rotations",
    "href": "lectures/lecture-2/lecture-2.html#qr-via-givens-rotations",
    "title": "",
    "section": "QR via Givens rotations",
    "text": "QR via Givens rotations\nSimilarly we can make matrix upper-triangular using Givens rotations:\n\\begin{bmatrix} \\times & \\times & \\times \\\\ \\bf{*} & \\times & \\times \\\\ \\bf{*} & \\times & \\times \\end{bmatrix} \\to \\begin{bmatrix} * & \\times & \\times \\\\ * & \\times & \\times \\\\ 0 & \\times & \\times \\end{bmatrix} \\to \\begin{bmatrix} \\times & \\times & \\times \\\\ 0 & * & \\times \\\\ 0 & * & \\times \\end{bmatrix} \\to \\begin{bmatrix} \\times & \\times & \\times \\\\ 0 & \\times & \\times \\\\ 0 & 0 & \\times \\end{bmatrix}"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#givens-vs.-householder-transformations",
    "href": "lectures/lecture-2/lecture-2.html#givens-vs.-householder-transformations",
    "title": "",
    "section": "Givens vs. Householder transformations",
    "text": "Givens vs. Householder transformations\n\nHouseholder reflections are useful for dense matrices (complexity is \\approx twice smaller than for Jacobi) and we need to zero large number of elements.\nGivens rotations are more suitable for sparse matrice or parallel machines as they act locally on elements."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#singular-value-decomposition",
    "href": "lectures/lecture-2/lecture-2.html#singular-value-decomposition",
    "title": "",
    "section": "Singular Value Decomposition",
    "text": "Singular Value Decomposition\nSVD will be considered later in more details.\nTheorem. Any matrix A\\in \\mathbb{C}^{n\\times m} can be written as a product of three matrices:\n A = U \\Sigma V^*, \nwhere - U is an n \\times n unitary matrix - V is an m \\times m unitary matrix - \\Sigma is a diagonal matrix with non-negative elements \\sigma_1 \\geq  \\ldots, \\geq \\sigma_{\\min (m,n)} on the diagonal.\nMoreover, if \\text{rank}(A) = r, then \\sigma_{r+1} = \\dots = \\sigma_{\\min (m,n)} = 0.\nSee poster for the visualization.\n\n\nImportant note: if one truncates (replace by 0) all singular values except for r first, then the resulting matrix yields best rank-r approximation both in \\|\\cdot\\|_2 and \\|\\cdot\\|_F.\nThis is called Eckart-Young theorem and will be proved later in our course."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#summary",
    "href": "lectures/lecture-2/lecture-2.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nMost important matrix norms: Frobenius and spectral\nUnitary matrices preserve these norms\nThere are two “basic” classes of unitary matrices: Householder and Givens matrices"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#questions",
    "href": "lectures/lecture-2/lecture-2.html#questions",
    "title": "",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "lectures/general_info/general_info.html",
    "href": "lectures/general_info/general_info.html",
    "title": "",
    "section": "",
    "text": "The course is about solving problems arising in computing\nThree main problems:\n\nsolving linear systems\nfinding eigenvalues and eigenvectors\ncomputing matrix functions (matrix exponential especially)\n\nThe methods are different for small-scale and large-scale problems:\n\nusing matrix decompositions\niterative methods"
  },
  {
    "objectID": "lectures/general_info/general_info.html#about-the-course",
    "href": "lectures/general_info/general_info.html#about-the-course",
    "title": "",
    "section": "",
    "text": "The course is about solving problems arising in computing\nThree main problems:\n\nsolving linear systems\nfinding eigenvalues and eigenvectors\ncomputing matrix functions (matrix exponential especially)\n\nThe methods are different for small-scale and large-scale problems:\n\nusing matrix decompositions\niterative methods"
  },
  {
    "objectID": "lectures/general_info/general_info.html#learning-outcomes",
    "href": "lectures/general_info/general_info.html#learning-outcomes",
    "title": "",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nSolve medium-scale numerical linear algebra problems (solve linear systems, compute eigenvalues and eigenvectors, solve linear least squares) using matrix factorizations\nIterative methods for sparse/structured systems\nFind which methods are the most appropriate for the particular problem\nFind appropriate software"
  },
  {
    "objectID": "lectures/general_info/general_info.html#approximate-syllabus",
    "href": "lectures/general_info/general_info.html#approximate-syllabus",
    "title": "",
    "section": "(Approximate) Syllabus",
    "text": "(Approximate) Syllabus\n\nWeek 1: Intro and floating points arithmetics, matrices, vectors, norms, ranks.\nWeek 2: Matrix decompositions 1: SVD and its applications\nWeek 3: Matrix decompositions 2: Linear systems and LU, eigendecomposition\nWeek 4: Matrix decompositions 3: QR and Schur + project proposal deadline\nWeek 5: Sparse and structured matrices, iterative methods (part 1)\nWeek 6: Iterative methods (part 2), matrix functions and advanced topics\nWeek 7: Oral exam (two days)\nWeek 8: Application period and project presentations"
  },
  {
    "objectID": "lectures/general_info/general_info.html#lecture-access",
    "href": "lectures/general_info/general_info.html#lecture-access",
    "title": "",
    "section": "Lecture access",
    "text": "Lecture access\n\nLectures can be downloaded and viewed on GitHub: (link distributed today)\nTelegram chat for the course (link distributed today)"
  },
  {
    "objectID": "lectures/general_info/general_info.html#team",
    "href": "lectures/general_info/general_info.html#team",
    "title": "",
    "section": "Team",
    "text": "Team\nCourse instructor: Ivan Oseledets\nTAs: Anastasia Batsheva, Alexander Zubrey, Artem Basharin, Alexander The Great"
  },
  {
    "objectID": "lectures/general_info/general_info.html#how-do-we-grade",
    "href": "lectures/general_info/general_info.html#how-do-we-grade",
    "title": "",
    "section": "How do we grade",
    "text": "How do we grade\n\n40% homework - includes 2 problem sets with coding in Python and theoretical problems\n20% final exam - the format and rules will be announced later\n20% midterm test - written test with simple problems\n20% term project - more details and policies about this activity will be presented later\n10% bonus - solve bonus tasks to get extra points\n\nTotal maximum is 110%."
  },
  {
    "objectID": "lectures/general_info/general_info.html#problem-sets",
    "href": "lectures/general_info/general_info.html#problem-sets",
    "title": "",
    "section": "Problem sets",
    "text": "Problem sets\n\nHomework is distributed in Jupyter notebooks\nProblem sets contain both theoretical and programming tasks\nNo hand-written solutions are accepted, only Markdown and \\LaTeX text in the single Jupyter Notebook file are Ok."
  },
  {
    "objectID": "lectures/general_info/general_info.html#problem-set-rules",
    "href": "lectures/general_info/general_info.html#problem-set-rules",
    "title": "",
    "section": "Problem set rules",
    "text": "Problem set rules\n\nSolutions must be submitted on Canvas before the deadline\nDeadlines are strict. After the deadline Canvas submission is closed. Only the last submission will be graded.\nDeadline for every problem set will be announced at the moment of publishing\nProblem sets will be checked for plagiarism. If noticed, the score will be divided by a number of similar works"
  },
  {
    "objectID": "lectures/general_info/general_info.html#attendance",
    "href": "lectures/general_info/general_info.html#attendance",
    "title": "",
    "section": "Attendance",
    "text": "Attendance\nAttendance is not strict, but do not disappoint us."
  },
  {
    "objectID": "lectures/general_info/general_info.html#exam",
    "href": "lectures/general_info/general_info.html#exam",
    "title": "",
    "section": "Exam",
    "text": "Exam\n\nClassical oral exam with a list of questions (questions from 2020)\nIf you fail exam, you get zero for the course\nThe list of questions such that if you can not answer any question from this list during the final exam, you will automatically get F for this course. It will be probably updated by the end of November."
  },
  {
    "objectID": "lectures/general_info/general_info.html#projects",
    "href": "lectures/general_info/general_info.html#projects",
    "title": "",
    "section": "Projects",
    "text": "Projects\n\nWe give you time (\\approx one week) and points (20% of your final grade) to test studied methods in real-world problems\nWe will provide some examples in the second part of this class\nFor inspiration you can checkout recent conference papers (e.g. here)\n2–5 students per team\nStart thinking about your project topics as early as possible!"
  },
  {
    "objectID": "lectures/general_info/general_info.html#grades",
    "href": "lectures/general_info/general_info.html#grades",
    "title": "",
    "section": "Grades",
    "text": "Grades\n\nA: 86 - 100 %\nB: 76 - 85 %\nC: 66 - 75 %\nD: 56 - 65 %\nE: 46 - 55 %\nF: 0 - 45 %\n\nBut they can be slightly adjusted."
  },
  {
    "objectID": "lectures/general_info/general_info.html#python",
    "href": "lectures/general_info/general_info.html#python",
    "title": "",
    "section": "Python",
    "text": "Python\n\nWe will use Python ecosystem for programming.\nPlease, follow the rules that will be given in the problem sets"
  },
  {
    "objectID": "lectures/general_info/general_info.html#materials",
    "href": "lectures/general_info/general_info.html#materials",
    "title": "",
    "section": "Materials",
    "text": "Materials\nOur materials: - Lecture notes are avaliable online - Matrix decomposition cheat sheet\nIf you have difficulties with basic linear algebra: - Cheat sheet with basics - Gilbert Strang book “Introduction to Linear Algebra” - Gilbert Strang has recorded lectures on YouTube\nComprehensive NLA books: - Gene H. Golub, Charles. F. Van Loan, “Matrix computations” (4th edition) - Lloyd N. Trefethen and David Bau III, “Numerical Linear Algebra” - Eugene. E. Tyrtyshnikov, “Brief introduction to numerical analysis” - James W. Demmel, “Numerical Linear Algebra”\n\nMany applications of linear algebra you can find in Introduction to Applied Linear Algebra by S. Boyd and L. Vandenberghe"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html",
    "href": "lectures/lecture-11/lecture-11.html",
    "title": "Questions?",
    "section": "",
    "text": "Distributed memory for huge dense matrices\nSparse matrix formats (COO, CSR, CSC)\nMatrix-by-vector product\nInefficiency of sparse matrix processing\nApproaches to reduce cache misses"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#recap-of-the-previous-part",
    "href": "lectures/lecture-11/lecture-11.html#recap-of-the-previous-part",
    "title": "Questions?",
    "section": "",
    "text": "Distributed memory for huge dense matrices\nSparse matrix formats (COO, CSR, CSC)\nMatrix-by-vector product\nInefficiency of sparse matrix processing\nApproaches to reduce cache misses"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#plan-for-this-week-how-to-solve-large-sparse-linear-systems",
    "href": "lectures/lecture-11/lecture-11.html#plan-for-this-week-how-to-solve-large-sparse-linear-systems",
    "title": "Questions?",
    "section": "Plan for this week: how to solve large sparse linear systems",
    "text": "Plan for this week: how to solve large sparse linear systems\n\nDirect methods (start today and continue in the next lecture)\n\nLU decomposition\nNumber of reordering techniques to minimize fill-in\n\nKrylov methods"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#plan-for-today",
    "href": "lectures/lecture-11/lecture-11.html#plan-for-today",
    "title": "Questions?",
    "section": "Plan for today",
    "text": "Plan for today\nSparse direct solvers:\n\nLU decomposition of sparse matrices\nfill-in of L and U factors\nnested dissection\nspectral clustering in details"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#direct-methods-for-sparse-matrix-lu-decomposition",
    "href": "lectures/lecture-11/lecture-11.html#direct-methods-for-sparse-matrix-lu-decomposition",
    "title": "Questions?",
    "section": "Direct methods for sparse matrix: LU decomposition",
    "text": "Direct methods for sparse matrix: LU decomposition\n\nWhy sparse linear systems can be solved faster, what is the technique?\nIn the LU factorization of the matrix A the factors L and U can be also sparse:\n\nA = L U\n\nAnd solving linear systems with sparse triangular matrices is very easy.\n\n Note that the inverse matrix to sparse matrix is not sparse! \n\nimport numpy as np\nimport scipy.sparse as spsp\nn = 7\nex = np.ones(n);\na = spsp.spdiags(np.vstack((ex,  np.random.rand(n), np.random.rand(n))), [-1, 0, 1], n, n, 'csr'); \nb = np.array(np.linalg.inv(a.toarray()))\nprint(a.toarray())\nprint(b)\nnp.linalg.svd(b[:3, 4:])[1]\n\n[[0.81628124 0.46963379 0.         0.         0.         0.\n  0.        ]\n [1.         0.4010937  0.52590463 0.         0.         0.\n  0.        ]\n [0.         1.         0.31569952 0.05086576 0.         0.\n  0.        ]\n [0.         0.         1.         0.92964368 0.53328924 0.\n  0.        ]\n [0.         0.         0.         1.         0.03959506 0.01950792\n  0.        ]\n [0.         0.         0.         0.         1.         0.19991324\n  0.40202748]\n [0.         0.         0.         0.         0.         1.\n  0.83648376]]\n[[ 8.27969626e-01  3.24143929e-01 -5.18854601e-01 -6.66664548e-03\n   3.25895391e-02  2.26486550e-03 -1.08853060e-03]\n [ 6.90205720e-01 -5.63401979e-01  9.01833054e-01  1.15874490e-02\n  -5.66446236e-02 -3.93661455e-03  1.89199994e-03]\n [-2.10077402e+00  1.71482242e+00  2.98790004e-01  3.83908521e-03\n  -1.87671623e-02 -1.30425589e-03  6.26846250e-04]\n [-5.30658931e-01  4.33166929e-01  7.54748405e-02 -2.51631866e-01\n   1.23008889e+00  8.54871214e-02 -4.10864784e-02]\n [ 4.86433539e+00 -3.97066571e+00 -6.91847278e-01  2.30660735e+00\n  -2.10913161e+00 -1.46577692e-01  7.04475839e-02]\n [ 1.73291328e+01 -1.41454460e+01 -2.46469711e+00  8.21725928e+00\n  -7.51375447e+00 -4.08466841e+00  1.96315700e+00]\n [-2.07166398e+01  1.69106044e+01  2.94649725e+00 -9.82357295e+00\n   8.98254670e+00  4.88314130e+00 -1.15143537e+00]]\n\n\narray([6.81937164e-02, 3.86855873e-17, 2.42041921e-19])"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#and-the-factors",
    "href": "lectures/lecture-11/lecture-11.html#and-the-factors",
    "title": "Questions?",
    "section": "And the factors…",
    "text": "And the factors…\nL and U are typically sparse. In the tridiagonal case they are even bidiagonal!\n\nfrom scipy.sparse.linalg import splu\nT = splu(a.tocsc(), permc_spec=\"NATURAL\")\nprint(T.L.toarray())\n\n[[ 1.          0.          0.          0.          0.          0.\n   0.        ]\n [-0.5         1.          0.          0.          0.          0.\n   0.        ]\n [ 0.         -0.66666667  1.          0.          0.          0.\n   0.        ]\n [ 0.          0.         -0.75        1.          0.          0.\n   0.        ]\n [ 0.          0.          0.         -0.8         1.          0.\n   0.        ]\n [ 0.          0.          0.          0.         -0.83333333  1.\n   0.        ]\n [ 0.          0.          0.          0.          0.         -0.85714286\n   1.        ]]\n\n\nInteresting to note that splu without permc_spec will produce permutations which will not yield the bidiagonal factor:\n\nfrom scipy.sparse.linalg import splu\nT = splu(a.tocsc())\nprint(T.L.todense())\nprint(T.perm_c)\n\n[[ 1.          0.          0.          0.          0.          0.\n   0.        ]\n [ 0.          1.          0.          0.          0.          0.\n   0.        ]\n [ 0.          0.          1.          0.          0.          0.\n   0.        ]\n [ 0.          0.          0.          1.          0.          0.\n   0.        ]\n [ 0.          0.          0.          0.          1.          0.\n   0.        ]\n [ 0.          0.          0.          0.          0.19991324  1.\n   0.        ]\n [ 0.81628124  0.14222853 -0.47418756  0.43359091 -0.00845846  0.23571107\n   1.        ]]\n[0 1 2 3 5 4 6]"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#d-case",
    "href": "lectures/lecture-11/lecture-11.html#d-case",
    "title": "Questions?",
    "section": "2D-case",
    "text": "2D-case\nFrom a matrix that comes as a discretization of a two-dimensional problem everything is much worse:\n\nimport scipy as sp\nimport scipy.sparse as spsp\nimport scipy.sparse.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 128\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = spsp.csc_matrix(A)\nT = spsp.linalg.splu(A, permc_spec=\"NATURAL\")\nplt.spy(T.L, marker='.', color='k', markersize=8)\nT.L\n#plt.spy(A, marker='.', color='k', markersize=8)\n\n&lt;16384x16384 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2097279 stored elements in Compressed Sparse Column format&gt;\n\n\n\n\n\n\n\n\n\nFor correct permutation in 2D case the number of nonzeros in L factor grows as \\mathcal{O}(N \\log N). But complexity is \\mathcal{O}(N^{3/2})."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#main-challenge-how-to-make-factors-l-and-u-as-sparse-as-possible",
    "href": "lectures/lecture-11/lecture-11.html#main-challenge-how-to-make-factors-l-and-u-as-sparse-as-possible",
    "title": "Questions?",
    "section": "Main challenge: how to make factors L and U as sparse as possible?",
    "text": "Main challenge: how to make factors L and U as sparse as possible?"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#main-tool-to-analyze-factors-sparsity-graph-theory",
    "href": "lectures/lecture-11/lecture-11.html#main-tool-to-analyze-factors-sparsity-graph-theory",
    "title": "Questions?",
    "section": "Main tool to analyze factors sparsity: graph theory",
    "text": "Main tool to analyze factors sparsity: graph theory\n\nThe number of nonzeros in LU decomposition has a deep connection to the graph theory.\nnetworkx package can be used to visualize graphs, given only the adjacency matrix.\nIt may even recover to some extend the graph structure.\n\n\nimport networkx as nx\nimport scipy as sp\nn = 3\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(sp.sparse.kron(lp1, e), e) + sp.sparse.kron(sp.sparse.kron(e, lp1), e) + sp.sparse.kron(sp.sparse.kron(e, e), lp1)\nA = spsp.csc_matrix(A)\nG = nx.Graph(A)\nnx.draw(G, pos=nx.spectral_layout(G))#, node_size=1)"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#what-is-fill-in",
    "href": "lectures/lecture-11/lecture-11.html#what-is-fill-in",
    "title": "Questions?",
    "section": "What is fill-in?",
    "text": "What is fill-in?\n\nThe fill-in of a matrix are those entries which change from an initial zero to a nonzero value during the execution of an algorithm.\nThe fill-in is different for different permutations. So, before factorization we need to find reordering which produces the smallest fill-in.\n\nExample\nA = \\begin{bmatrix} * & * & * & * & *\\\\ * & * & 0 & 0 & 0 \\\\ * & 0  & * & 0 & 0 \\\\ * & 0 & 0& * & 0 \\\\ * & 0 & 0& 0 & * \\end{bmatrix} \nIf we eliminate elements from the top to the bottom, then we will obtain dense matrix. However, we could maintain sparsity if elimination was done from the bottom to the top."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#example-of-dense-factors-after-lu",
    "href": "lectures/lecture-11/lecture-11.html#example-of-dense-factors-after-lu",
    "title": "Questions?",
    "section": "Example of dense factors after LU",
    "text": "Example of dense factors after LU\nGiven matrix A=A^*&gt;0 we calculate its Cholesky decomposition A = LL^*.\nFactor L can be dense even if A is sparse:\n\n\\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} =\n\\begin{bmatrix} * &  &  &  \\\\ * & * &  &  \\\\ * & * & * &  \\\\ * & * & * & * \\end{bmatrix}\n\\begin{bmatrix} * & * & * & * \\\\  & * & * & * \\\\  &  & * & * \\\\  &  &  & * \\end{bmatrix}\n\nHow to make factors sparse, i.e. to minimize the fill-in?"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#why-permutations-can-reduce-fill-in-here-the-example",
    "href": "lectures/lecture-11/lecture-11.html#why-permutations-can-reduce-fill-in-here-the-example",
    "title": "Questions?",
    "section": "Why permutations can reduce fill-in? Here the example…",
    "text": "Why permutations can reduce fill-in? Here the example…\nWe need to find a permutation of indices so that factors are sparse, i.e. we build Cholesky factorisation of PAP^\\top, where P is a permutation matrix.\nFor the example from the previous slide\n\nP \\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} P^\\top =\n\\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\ * & * & * & * \\end{bmatrix} =\n\\begin{bmatrix} * &  &  &  \\\\  & * &  &  \\\\  &  & * &  \\\\ * & * & * & * \\end{bmatrix}\n\\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\  & &  & * \\end{bmatrix}\n\nwhere\n\nP = \\begin{bmatrix}  &  &  & 1 \\\\  &  & 1 &  \\\\  & 1 &  &  \\\\ 1 &  &  &  \\end{bmatrix}\n\n\nArrowhead form of the matrix gives sparse factors in LU decomposition\n\n\nimport numpy as np\nimport scipy.sparse as spsp\nimport scipy.sparse.linalg as spsplin\nimport scipy.linalg as splin\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nA = spsp.coo_matrix((np.random.randn(10), ([0, 0, 0, 0, 1, 1, 2, 2, 3, 3], \n                                           [0, 1, 2, 3, 0, 1, 0, 2, 0, 3])))\nprint(\"Original matrix\")\nplt.spy(A)\nplt.show()\nlu = spsplin.splu(A.tocsc(), permc_spec=\"NATURAL\")\nprint(\"L factor\")\nplt.spy(lu.L)\nplt.show()\nprint(\"U factor\")\nplt.spy(lu.U)\nplt.show()\nprint(\"Column permutation:\", lu.perm_c)\nprint(\"Row permutation:\", lu.perm_r)\n\nOriginal matrix\n\n\n\n\n\n\n\n\n\nL factor\n\n\n\n\n\n\n\n\n\nU factor\n\n\n\n\n\n\n\n\n\nColumn permutation: [0 1 2 3]\nRow permutation: [1 3 2 0]"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#block-version-of-appropriate-sparsity-pattern-arrowhead-structure",
    "href": "lectures/lecture-11/lecture-11.html#block-version-of-appropriate-sparsity-pattern-arrowhead-structure",
    "title": "Questions?",
    "section": "Block version of appropriate sparsity pattern (arrowhead structure)",
    "text": "Block version of appropriate sparsity pattern (arrowhead structure)\n\nPAP^\\top = \\begin{bmatrix} A_{11} &  & A_{13} \\\\  & A_{22} & A_{23} \\\\ A_{31} & A_{32} & A_{33}\\end{bmatrix}\n\nthen\n\nPAP^\\top = \\begin{bmatrix} A_{11} & 0 & 0 \\\\ 0 & A_{22} & 0 \\\\ A_{31} & A_{32} & A_{33} - A_{31}A_{11}^{-1} A_{13} - A_{32}A_{22}^{-1}A_{23} \\end{bmatrix} \\begin{bmatrix}  I & 0 & A_{11}^{-1}A_{13} \\\\ 0 & I & A_{22}^{-1}A_{23} \\\\ 0 & 0 & I\\end{bmatrix}\n\n\nBlock $ A_{33} - A_{31}A_{11}^{-1} A_{13} - A_{32}A_{22}^{-1}A_{23}$ is Schur complement for block diagonal matrix \\begin{bmatrix} A_{11} & 0 \\\\ 0 & A_{22} \\end{bmatrix}\nWe reduce problem to solving smaller linear systems with A_{11} and A_{22}"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#what-we-can-do-to-minimize-fill-in",
    "href": "lectures/lecture-11/lecture-11.html#what-we-can-do-to-minimize-fill-in",
    "title": "Questions?",
    "section": "What we can do to minimize fill-in?",
    "text": "What we can do to minimize fill-in?\n\nReordering the rows and the columns of the sparse matrix in order to reduce the number of nonzeros in L and U factors is called fill-in minimization.\nUnfortunately, this paper by Rose and Tarjan in 1975 proves that fill-in minimization problem is NP-complete.\nBut, many heuristics exist:\n\nMarkowitz pivoting - order by the product of nonzeros in column and row and stability constraint\nMinimum degree ordering - order by the degree of the vertex\nCuthill–McKee algorithm (and reverse Cuthill-McKee) - reorder to minimize the bandwidth (does not exploit graph representation).\nNested dissection: split the graph into two with minimal number of vertices on the separator (set of vertices removed after we separate the graph into two distinct connected graphs).  Complexity of the algorithm depends on the size of the graph separator. For 1D Laplacian separator contains only 1 vertex, in 2D - \\sqrt{N} vertices.\n\n\n\nHow can we find permutation?\n\nKey idea comes from graph theory\nSparse matrix can be treated as an adjacency matrix of a certain graph: the vertices (i, j) are connected, if the corresponding matrix element is non-zero.\n\n\n\nExample\nGraphs of \\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} and \\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\ * & * & * & * \\end{bmatrix} have the following form:\n and \n\nWhy the second ordering is better than the first one?"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#fill-in-upper-bound-minimization-markowitz-pivoting",
    "href": "lectures/lecture-11/lecture-11.html#fill-in-upper-bound-minimization-markowitz-pivoting",
    "title": "Questions?",
    "section": "Fill-in upper bound minimization: Markowitz pivoting",
    "text": "Fill-in upper bound minimization: Markowitz pivoting\n\nGeneral purpose approach to ordering the elements of sparse matrix that will be eliminated\nThe Markowitz merit for every non-zero element with indices (i, j) is computed as (r_i - 1)(c_j - 1), where r_i is number of nonzeros elements in the i-th row and c_j is a number of non-zero elements in the j-th column\nThis value is an upper abound on the fill-in after eliminating the (i, j) element. Why?\nWe can order elements with respect to these values, select the one with minimum value, eliminate it and update matrix. What about stability?\nThen re-compute these values and repeat the procedure\nThis method gives us the permutations of rows and columns and sparse factors\nMain drawback is efficient supporting number of nnz in every row and column after matrix update without complete re-calculating\nMore details can be found in book Direct Methods for Sparse Matrices by I. S. Duff, A. M. Erisman, and J. K. Reid\nDe facto standard approach in solving Linear programming (LP) problems and their MILP modifications"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#what-if-consider-only-neighbours-we-get-minimal-degree-ordering",
    "href": "lectures/lecture-11/lecture-11.html#what-if-consider-only-neighbours-we-get-minimal-degree-ordering",
    "title": "Questions?",
    "section": "What if consider only neighbours? We get minimal degree ordering!",
    "text": "What if consider only neighbours? We get minimal degree ordering!\n\nThe idea is to eliminate rows and/or columns with fewer non-zeros, update fill-in and then repeat. How it relates to Markowitz pivoting?\nEfficient implementation is an issue (adding/removing elements).\nCurrent champion is “approximate minimal degree” by Amestoy, Davis, Duff.\nIt is suboptimal even for 2D PDE problems\nSciPy sparse package uses minimal ordering approach for different matrices (A^{\\top}A, A + A^{\\top})"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#but-in-these-methods-we-ignore-the-knowledge-of-good-structure-for-sparse-lu-lets-exploit-it-explicitly-in-the-method",
    "href": "lectures/lecture-11/lecture-11.html#but-in-these-methods-we-ignore-the-knowledge-of-good-structure-for-sparse-lu-lets-exploit-it-explicitly-in-the-method",
    "title": "Questions?",
    "section": "But in these methods we ignore the knowledge of good structure for sparse LU! Let’s exploit it explicitly in the method!",
    "text": "But in these methods we ignore the knowledge of good structure for sparse LU! Let’s exploit it explicitly in the method!"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#how-to-formalize-reduction-to-block-arrowhead-form",
    "href": "lectures/lecture-11/lecture-11.html#how-to-formalize-reduction-to-block-arrowhead-form",
    "title": "Questions?",
    "section": "How to formalize reduction to block arrowhead form?",
    "text": "How to formalize reduction to block arrowhead form?\nDefinition. A separator in a graph G is a set S of vertices whose removal leaves at least two connected components.\nSeparator S gives the following ordering for an N-vertex graph G: - Find a separator S, whose removal leaves connected components T_1, T_2, \\ldots, T_k - Number the vertices of S from N − |S| + 1 to N - Recursively, number the vertices of each component: T_1 from 1 to |T_1|, T_2 from |T_1| + 1 to |T_1| + |T_2|, etc - If a component is small enough, enumeration in this component is arbitrarily\n\nSeparator and block arrowhead structure: example\nSeparator for the 2D Laplacian matrix\n\nA_{2D} = I \\otimes A_{1D} + A_{1D} \\otimes I, \\quad A_{1D} = \\mathrm{tridiag}(-1, 2, -1),\n\nis as follows\n \nOnce we have enumerated first indices in \\alpha, then in \\beta and separators indices in \\sigma we get the following matrix\n\nPAP^\\top = \\begin{bmatrix} A_{\\alpha\\alpha} &  & A_{\\alpha\\sigma} \\\\  & A_{\\beta\\beta} & A_{\\beta\\sigma} \\\\ A_{\\sigma\\alpha} & A_{\\sigma\\beta} & A_{\\sigma\\sigma}\\end{bmatrix}\n\nwhich has arrowhrad structure.\n\nThus, the problem of finding permutation was reduced to the problem of finding graph separator!"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#the-method-of-recursive-reduction-to-block-arrowhead-structure-nested-dissection",
    "href": "lectures/lecture-11/lecture-11.html#the-method-of-recursive-reduction-to-block-arrowhead-structure-nested-dissection",
    "title": "Questions?",
    "section": "The method of recursive reduction to block arrowhead structure – Nested dissection",
    "text": "The method of recursive reduction to block arrowhead structure – Nested dissection\n\nFor blocks A_{\\alpha\\alpha}, A_{\\beta\\beta} we continue splitting recursively.\nWhen the recursion is done, we need to eliminate blocks A_{\\sigma\\alpha} and A_{\\sigma\\beta}.\nThis makes block in the position of A_{\\sigma\\sigma}\\in\\mathbb{R}^{n\\times n} dense.\n\nCalculation of Cholesky of this block costs \\mathcal{O}(n^3) = \\mathcal{O}(N^{3/2}), where N = n^2 is the total number of nodes.\nSo, the complexity is \\mathcal{O}(N^{3/2})"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#packages-for-nested-dissection",
    "href": "lectures/lecture-11/lecture-11.html#packages-for-nested-dissection",
    "title": "Questions?",
    "section": "Packages for nested dissection",
    "text": "Packages for nested dissection\n\nMUltifrontal Massively Parallel sparse direct Solver (MUMPS)\nPardiso\nUmfpack as part of SuiteSparse\n\nAll of them have interfaces for C/C++, Fortran and Matlab\n\nNested dissection summary\n\nEnumeration: find a separator.\nDivide-and-conquer paradigm\nRecursively process two subsets of vertices after separation\nIn theory, nested dissection gives optimal complexity.\nIn practice, it beats others only for very large problems."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#separators-in-practice",
    "href": "lectures/lecture-11/lecture-11.html#separators-in-practice",
    "title": "Questions?",
    "section": "Separators in practice",
    "text": "Separators in practice\n\nComputing separators is not a trivial task.\nGraph partitioning heuristics has been an active research area for many years, often motivated by partitioning for parallel computation.\n\nExisting approaches:\n\nSpectral partitioning (uses eigenvectors of Laplacian matrix of graph) - more details below\nGeometric partitioning (for meshes with specified vertex coordinates) review and analysis\nIterative-swapping ((Kernighan-Lin, 1970), (Fiduccia-Matheysses, 1982)\nBreadth-first search (Lipton, Tarjan 1979)\nMultilevel recursive bisection (heuristic, currently most practical) (review and paper). Package for such kind of partitioning is called METIS, written in C, and available here"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#one-of-the-ways-to-construct-separators-spectral-graph-partitioning",
    "href": "lectures/lecture-11/lecture-11.html#one-of-the-ways-to-construct-separators-spectral-graph-partitioning",
    "title": "Questions?",
    "section": "One of the ways to construct separators – spectral graph partitioning",
    "text": "One of the ways to construct separators – spectral graph partitioning\n\nThe idea of spectral partitioning goes back to Miroslav Fiedler, who studied connectivity of graphs (paper).\nWe need to split the vertices into two sets.\nConsider +1/-1 labeling of vertices and the cost\n\nE_c(x) = \\sum_{j} \\sum_{i \\in N(j)} (x_i - x_j)^2, \\quad N(j) \\text{ denotes set of neighbours of a node } j. \n\nWe need a balanced partition, thus\n\n\\sum_i x_i =  0 \\quad \\Longleftrightarrow \\quad x^\\top e = 0, \\quad e = \\begin{bmatrix}1 & \\dots & 1\\end{bmatrix}^\\top,\nand since we have +1/-1 labels, we have\n\\sum_i x^2_i = n \\quad \\Longleftrightarrow \\quad \\|x\\|_2^2 = n."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#graph-laplacian",
    "href": "lectures/lecture-11/lecture-11.html#graph-laplacian",
    "title": "Questions?",
    "section": "Graph Laplacian",
    "text": "Graph Laplacian\nCost E_c can be written as (check why)\nE_c = (Lx, x)\nwhere L is the graph Laplacian, which is defined as a symmetric matrix with\nL_{ii} = \\mbox{degree of node $i$},\nL_{ij} = -1, \\quad \\mbox{if $i \\ne j$  and there is an edge},\nand 0 otherwise.\n\nRows of L sum to zero, thus there is an eigenvalue 0 and gives trivial eigenvector of all ones.\nEigenvalues are non-negative (why?)."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#partitioning-as-an-optimization-problem",
    "href": "lectures/lecture-11/lecture-11.html#partitioning-as-an-optimization-problem",
    "title": "Questions?",
    "section": "Partitioning as an optimization problem",
    "text": "Partitioning as an optimization problem\n\nMinimization of E_c with the mentioned constraints leads to a partitioning that tries to minimize number of edges in a separator, while keeping the partition balanced.\nWe now relax the integer quadratic programming to the continuous quadratic programming\n\nE_c(x) = (Lx, x)\\to \\min_{\\substack{x^\\top e =0, \\\\ \\|x\\|_2^2 = n}}"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator",
    "href": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator",
    "title": "Questions?",
    "section": "From Fiedler vector to separator",
    "text": "From Fiedler vector to separator\n\nThe solution to the minimization problem is given by the eigenvector (called Fiedler vector) corresponding to the second smallest eigenvalue of the graph Laplacian. Indeed,\n\n\n    \\min_{\\substack{x^\\top e =0, \\\\ \\|x\\|_2^2 = n}} (Lx, x) = n \\cdot \\min_{{x^\\top e =0}} \\frac{(Lx, x)}{(x, x)} = n \\cdot \\min_{{x^\\top e =0}} R(x), \\quad R(x) \\text{ is the Rayleigh quotient}\n\n\nSince e is the eigenvector, corresponding to the smallest eigenvalue, on the space x^\\top e =0 we get the second minimal eigevalue.\nThe sign x_i indicates the partitioning.\nIn computations, we need to find out, how to find this second minimal eigenvalue –– we at least know about power method, but it finds the largest. We will discuss iterative methods for eigenvalue problems later in our course.\nThis is the main goal of the iterative methods for large-scale linear problems, and can be achieved via few matrix-by-vector products.\n\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport networkx as nx\nkn = nx.read_gml('karate.gml')\nprint(\"Number of vertices = {}\".format(kn.number_of_nodes()))\nprint(\"Number of edges = {}\".format(kn.number_of_edges()))\nnx.draw_networkx(kn, node_color=\"red\") #Draw the graph\n\nNumber of vertices = 34\nNumber of edges = 78\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as spsplin\nLaplacian = nx.laplacian_matrix(kn).asfptype()\nplt.spy(Laplacian, markersize=5)\nplt.title(\"Graph laplacian\")\nplt.axis(\"off\")\nplt.show()\neigval, eigvec = spsplin.eigsh(Laplacian, k=3, which=\"SM\")\nprint(\"The 2 smallest eigenvalues =\", eigval)\n\n\n\n\n\n\n\n\nThe 2 smallest eigenvalues = [2.52665709e-17 4.68525227e-01 9.09247664e-01]\n\n\n\n#plt.scatter(np.arange(len(eigvec[:, 1])), np.sign(eigvec[:, 1]))\nplt.scatter(np.arange(len(eigvec[:, 1])), eigvec[:, 1])\n\nplt.show()\nprint(\"Sum of elements in Fiedler vector = {}\".format(np.sum(eigvec[:, 1].real)))\n\n\n\n\n\n\n\n\nSum of elements in Fiedler vector = -8.493206138382448e-15\n\n\n\nnx.draw_networkx(kn, node_color=np.sign(eigvec[:, 1]))\n\n\n\n\n\n\n\n\n\nSummary on demo\n\nHere we call SciPy sparse function to find fixed number of eigenvalues (and eigenvectors) that are smallest (other options are possible)\nDetails of the underlying method we will discuss soon\nFiedler vector gives simple separation of the graph\nTo separate graph on more than two parts you should use eigenvectors of laplacian as feature vectors and run some clustering algorithm, e.g. k-means"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator-1",
    "href": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator-1",
    "title": "Questions?",
    "section": "From Fiedler vector to separator",
    "text": "From Fiedler vector to separator\n\nElements of eigenvector v corresponding to the second smallest eigenvalue of the Laplacian indicate the partitioning of vertices\nIf we select some small positive \\tau &gt;0, then we can split the vertices in three groups\n\nv_i &lt; -\\tau\nv_i \\in [-\\tau, \\tau]\nv_i &gt; \\tau\n\nAfter that the separator is composed with the vertices corresponding to elements of v such that v_i \\in [-\\tau, \\tau]\nThe size of separator can be tuned by the magnitude of \\tau\nThe distribution of elements in v is important to identify the size of separator\n\n\nFiedler vector and algebraic connectivity of a graph\nDefinition. The algebraic connectivity of a graph is the second-smallest eigenvalue of the Laplacian matrix.\nClaim. The algebraic connectivity of a graph is greater than 0 if and only if a graph is a connected graph."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#practical-problems",
    "href": "lectures/lecture-11/lecture-11.html#practical-problems",
    "title": "Questions?",
    "section": "Practical problems",
    "text": "Practical problems\nComputing bisection recursively is expensive.\nAs an alternative, one typically computes multilevel bisection that consists of 3 phases.\n\nGraph coarsening: From a given graph, we join vertices into larger nodes, and get sequences of graphs G_1, \\ldots, G_m.\nAt the coarse level, we do high-quality bisection\nThen, we do uncoarsening: we propagate the splitting from G_k to G_{k-1} and improve the quality of the split by local optimization algorithms (refinement)."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#practical-problems-2",
    "href": "lectures/lecture-11/lecture-11.html#practical-problems-2",
    "title": "Questions?",
    "section": "Practical problems (2)",
    "text": "Practical problems (2)\n\nOnce the permutation has been computed, we need to implement the elimination, making use of efficient computational kernels.\nIf in the elemination we will be able to get the elements into blocks, we will be able to use BLAS-3 computations.\nIt is done by supernodal data structures:\nIf adjacent rows have the same sparsity structure, they can be stored in blocks:\nAlso, we can use such structure in efficient computations!\n\nDetails in this survey"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#take-home-message",
    "href": "lectures/lecture-11/lecture-11.html#take-home-message",
    "title": "Questions?",
    "section": "Take home message",
    "text": "Take home message\n\nSparse matrices & graphs ordering\nOrdering is important for LU fill-in: more details in the next lecture\nMarkowitz pivoting and minimal degree ordering\nOrderings from SciPy sparse package\nSeparators and how do they help in fill-in minimization\nNested dissection idea\nFiedler vector and spectral bipartitioning"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#plan-for-the-next-lecture",
    "href": "lectures/lecture-11/lecture-11.html#plan-for-the-next-lecture",
    "title": "Questions?",
    "section": "Plan for the next lecture",
    "text": "Plan for the next lecture\n\nBasic iterative methods for solving large linear systems\nConvergence\nAcceleration"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html",
    "href": "lectures/lecture-7/lecture-7.html",
    "title": "",
    "section": "",
    "text": "Eigenvectors and eigenvalues\nCharacteristic polynomial and why it is a bad idea\nPower method to find leading (maximum absolute value) eigenvalue and eigenvector\nGershgorin theorem\nSchur theorem: A = U T U^*\nNormal matrices: A^* A = A A^*\nAdvanced topic: pseudospectrum"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#recap-of-the-previous-lecture",
    "href": "lectures/lecture-7/lecture-7.html#recap-of-the-previous-lecture",
    "title": "",
    "section": "",
    "text": "Eigenvectors and eigenvalues\nCharacteristic polynomial and why it is a bad idea\nPower method to find leading (maximum absolute value) eigenvalue and eigenvector\nGershgorin theorem\nSchur theorem: A = U T U^*\nNormal matrices: A^* A = A A^*\nAdvanced topic: pseudospectrum"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#today-lecture",
    "href": "lectures/lecture-7/lecture-7.html#today-lecture",
    "title": "",
    "section": "Today lecture",
    "text": "Today lecture\n\nToday we will talk about matrix factorizations as general tool\nBasic matrix factorizations in numerical linear algebra:\n\nLU decomposition and Gaussian elimination — already covered\nQR decomposition and Gram-Schmidt algorithm\nSchur decomposition and QR-algorithm\nMethods for computing SVD decomposition in the second part\n\nWe already introduced QR decomposition some time ago, but now we are going to discuss it in more details."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#general-concept-of-matrix-factorization",
    "href": "lectures/lecture-7/lecture-7.html#general-concept-of-matrix-factorization",
    "title": "",
    "section": "General concept of matrix factorization",
    "text": "General concept of matrix factorization\n\nIn numerical linear algebra we need to solve different tasks, for example:\n\nSolve linear systems Ax = f\nCompute eigenvalues / eigenvectors\nCompute singular values / singular vectors\nCompute inverses, even sometimes determinants\nCompute matrix functions like \\exp(A), \\cos(A) (these are not elementwise functions)\n\nIn order to do this, we represent the matrix as a sum and/or product of matrices with simpler structure, such that we can solve mentioned tasks faster / in a more stable form.\nWhat is a simpler structure?"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#what-is-a-simpler-structure",
    "href": "lectures/lecture-7/lecture-7.html#what-is-a-simpler-structure",
    "title": "",
    "section": "What is a simpler structure",
    "text": "What is a simpler structure\n\nWe already encountered several classes of matrices with structure.\nFor dense matrices the most important classes are\n\nunitary matrices\nupper/lower triangular matrices\ndiagonal matrices"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#other-classes-of-structured-matrices",
    "href": "lectures/lecture-7/lecture-7.html#other-classes-of-structured-matrices",
    "title": "",
    "section": "Other classes of structured matrices",
    "text": "Other classes of structured matrices\n\nFor sparse matrices the sparse constraints are often included in the factorizations.\nFor Toeplitz matrices an important class of matrices is the class of matrices with low displacement rank, which is based on the low-rank matrices.\nThe class of low-rank matrices and block low-rank matrices appears in many applications."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#plan",
    "href": "lectures/lecture-7/lecture-7.html#plan",
    "title": "",
    "section": "Plan",
    "text": "Plan\nThe plan for today’s lecture is to discuss the decompositions one-by-one and point out: - How to compute a particular decomposition - When the decomposition exists - What is done in real life (LAPACK)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#decompositions-we-want-to-discuss-today",
    "href": "lectures/lecture-7/lecture-7.html#decompositions-we-want-to-discuss-today",
    "title": "",
    "section": "Decompositions we want to discuss today",
    "text": "Decompositions we want to discuss today\n\nLU factorization & Cholesky factorization — quick reminder, already done.\nQR decomposition and Gram-Schmidt algorithm\nOne slide about the SVD (more details in the second part of today lecture)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#plu-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#plu-decomposition",
    "title": "",
    "section": "PLU decomposition",
    "text": "PLU decomposition\n\nAny nonsingular matrix can be factored as\n\n A = P L U, \nwhere P is a permutation matrix, L is a lower triangular matrix, U is an upper triangular.\n\nMain goal of the LU decomposition is to solve linear systems, because\n\n A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, \nand this reduces to the solution of two linear systems\n L y = f,  \\quad U x = y \nwith lower and upper triangular matrices respectively.\nQ: what is the complexity of solving these linear systems?"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#positive-definite-matrices-and-cholesky-decomposition-reminder",
    "href": "lectures/lecture-7/lecture-7.html#positive-definite-matrices-and-cholesky-decomposition-reminder",
    "title": "",
    "section": "Positive definite matrices and Cholesky decomposition, reminder",
    "text": "Positive definite matrices and Cholesky decomposition, reminder\nIf the matrix is Hermitian positive definite, i.e. \n A = A^*, \\quad (Ax, x) &gt; 0, \\quad x \\ne 0, \nthen it can be factored as\n A = RR^*, \nwhere R is lower triangular.\nWe will need this for the QR decomposition."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition",
    "title": "",
    "section": "QR decomposition",
    "text": "QR decomposition\n\nThe next decomposition: QR decomposition.\nAgain from the name it is clear that a matrix is represented as a product\n\n\n    A = Q R,\n\nwhere Q is an column orthogonal (unitary) matrix and R is upper triangular.\n\nThe matrix sizes: Q is n \\times m, R is m \\times m if n\\geq m. See our poster for visualization of QR decomposition\nQR decomposition is defined for any rectangular matrix."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-applications",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-applications",
    "title": "",
    "section": "QR decomposition: applications",
    "text": "QR decomposition: applications\nThis decomposition plays a crucial role in many problems: - Computing orthogonal bases in a linear space - Used in the preprocessing step for the SVD - QR-algorithm for the computation of eigenvectors and eigenvalues (one of the 10 most important algorithms of the 20th century) is based on the QR decomposition - Solving overdetermined systems of linear equations (linear least-squares problem)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-and-least-squares-reminder",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-and-least-squares-reminder",
    "title": "",
    "section": "QR decomposition and least squares, reminder",
    "text": "QR decomposition and least squares, reminder\n\nSuppose we need to solve\n\n \\Vert A x - b \\Vert_2 \\rightarrow \\min_x, \nwhere A is n \\times m, n \\geq m.\n\nThen we factorize\n\n A = Q R, \nand use equation for pseudo-inverse matrix in the case of the full rank matrix A:\n x = A^{\\dagger}b = (A^*A)^{-1}A^*b = ((QR)^*(QR))^{-1}(QR)^*b = (R^*Q^*QR)^{-1}R^*Q^*b = R^{-1}Q^*b. \nthus x can be recovered from\nR x = Q^*b\n\nNote that this is a square system of linear equations with lower triangular matrix. What is the complexity of solving this system?"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#existence-of-qr-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#existence-of-qr-decomposition",
    "title": "",
    "section": "Existence of QR decomposition",
    "text": "Existence of QR decomposition\nTheorem. Every rectangular n \\times m matrix has a QR decomposition.\nThere are several ways to prove it and compute it:\n\nTheoretical: using the Gram matrices and Cholesky factorization\nGeometrical: using the Gram-Schmidt orthogonalization\nPractical: using Householder/Givens transformations\n\n\nProof using Cholesky decomposition\nIf we have the representation of the form\nA = QR,\nthen A^* A = ( Q R)^* (QR)  = R^* (Q^* Q) R = R^* R, the matrix A^* A is called Gram matrix, and its elements are scalar products of the columns of A.\n\n\nProof using Cholesky decomposition (full column rank)\n\nAssume that A has full column rank. Then, it is simple to show that A^* A is positive definite:\n\n (A^* A y, y) = (Ay, Ay) = \\Vert Ay \\Vert^2  &gt; 0, \\quad y\\not = 0. \n\nTherefore, A^* A = R^* R always exists.\nThen the matrix Q = A R^{-1} is unitary:\n\n (A R^{-1})^* (AR^{-1})= R^{-*} A^* A R^{-1} = R^{-*} R^* R R^{-1} = I. \n\n\nProof using Cholesky decomposition (rank-deficient case)\n\nWhen an n \\times m matrix does not have full column rank, it is said to be rank-deficient.\nThe QR decomposition, however, also exists.\nFor any rank-deficient matrix there is a sequence of full-column rank matrices A_k such that A_k \\rightarrow A (why?).\nEach A_k can be decomposed as A_k = Q_k R_k.\nThe set of all unitary matrices is compact, thus there exists a converging subsequence Q_{n_k} \\rightarrow Q (why?), and Q^* A_k \\rightarrow Q^* A = R, which is triangular."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#stability-of-qr-decomposition-via-cholesky-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#stability-of-qr-decomposition-via-cholesky-decomposition",
    "title": "",
    "section": "Stability of QR decomposition via Cholesky decomposition",
    "text": "Stability of QR decomposition via Cholesky decomposition\n\nSo, the simplest way to compute QR decomposition is then\n\nA^* A = R^* R,\nand\nQ = A R^{-1}.\n\nIt is a bad idea for numerical stability. Let us do some demo (for a submatrix of the Hilbert matrix).\n\n\nimport jax.numpy as jnp\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nn = 10\nr = 6\na = [[1.0 / (i + j + 0.5) for i in range(r)] for j in range(n)]\na = jnp.array(a)\nq, Rmat = jnp.linalg.qr(a)\ne = jnp.eye(r)\nprint('Built-in QR orth', jnp.linalg.norm(jnp.dot(q.T, q) - e))\ngram_matrix = a.T.dot(a)\nRmat1 = jnp.linalg.cholesky(gram_matrix)\n#q1 = jnp.dot(a, jnp.linalg.inv(Rmat1.T))\nq1 = jnp.linalg.solve(Rmat1, a.T).T\nprint('Via Cholesky:', jnp.linalg.norm(jnp.dot(q1.T, q1) - e))\n\nBuilt-in QR orth 8.184741451905057e-16\nVia Cholesky: 2.614491817325585e-05"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#second-way-gram-schmidt-orthogonalization",
    "href": "lectures/lecture-7/lecture-7.html#second-way-gram-schmidt-orthogonalization",
    "title": "",
    "section": "Second way: Gram-Schmidt orthogonalization",
    "text": "Second way: Gram-Schmidt orthogonalization\n\nQR decomposition is a mathematical way of writing down the Gram-Schmidt orthogonalization process.\n\nGiven a sequence of vectors a_1, \\ldots, a_m we want to find orthogonal basis q_1, \\ldots, q_m such that every a_i is a linear combination of such vectors.\n\nGram-Schmidt: 1. q_1 := a_1/\\Vert a_1 \\Vert 2. q_2 := a_2 - (a_2, q_1) q_1, \\quad q_2 := q_2/\\Vert q_2 \\Vert 3. q_3 := a_3 - (a_3, q_1) q_1 - (a_3, q_2) q_2, \\quad q_3 := q_3/\\Vert q_3 \\Vert 4. And so on\nNote that the transformation from Q to A has triangular structure, since from the k-th vector we subtract only the previous ones. It follows from the fact that the product of triangular matrices is a triangular matrix."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#modified-gram-schmidt",
    "href": "lectures/lecture-7/lecture-7.html#modified-gram-schmidt",
    "title": "",
    "section": "Modified Gram-Schmidt",
    "text": "Modified Gram-Schmidt\n\nGram-Schmidt can be very unstable (i.e., the produced vectors will be not orthogonal, especially if q_k has small norm).\nThis is called loss of orthogonality.\nThere is a remedy, called modified Gram-Schmidt method. Instead of doing\n\nq_k := a_k - (a_k, q_1) q_1 - \\ldots - (a_k, q_{k-1}) q_{k-1}\nwe do it step-by-step. First we set q_k := a_k and orthogonalize sequentially:\n\n   q_k := q_k - (q_k, q_1)q_1, \\quad q_k := q_{k} - (q_k,q_2)q_2, \\ldots\n\n\nIn exact arithmetic, it is the same. In floating point it is absolutely different!\nNote that the complexity is \\mathcal{O}(nm^2) operations"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-the-almost-practical-way",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-the-almost-practical-way",
    "title": "",
    "section": "QR decomposition: the (almost) practical way",
    "text": "QR decomposition: the (almost) practical way\n\nIf A = QR, then\n\n R = Q^* A, \nand we need to find a certain orthogonal matrix Q that brings a matrix into upper triangular form.\n- For simplicity, we will look for an n \\times n matrix such that\n Q^* A = \\begin{bmatrix} * & * & *  \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ & 0_{(n-m) \\times m} \\end{bmatrix} \n\nWe will do it column-by-column.\n\nFirst, we find a Householder matrix H_1 = (I - 2 uu^{\\top}) such that (we illustrate on a 4 \\times 3 matrix)\n\n H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & * & * \\\\ 0 & * & * \\end{bmatrix} \nThen,\n H_2 H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ 0 & 0 & * \\end{bmatrix}, \nwhere\n H_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & H'_2, \\end{bmatrix} \nand H'_2 is a 3 \\times 3 Householder matrix.\nFinally,\n H_3 H_2 H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ 0 & 0 & 0 \\end{bmatrix}, \nYou can try to implement it yourself, it is simple."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-real-life",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-real-life",
    "title": "",
    "section": "QR decomposition: real life",
    "text": "QR decomposition: real life\n\nIn reality, since this is a dense matrix factorization, you should implement the algorithm in terms of blocks (why?).\nInstead of using Householder transformation, we use block Householder transformation of the form\n\nH = I - 2UU^*, \nwhere U^* U = I.\n\nThis allows us to use BLAS-3 operations."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#rank-revealing-qr-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#rank-revealing-qr-decomposition",
    "title": "",
    "section": "Rank-revealing QR-decomposition",
    "text": "Rank-revealing QR-decomposition\n\nThe QR-decomposition can be also used to compute the (numerical) rank of the matrix, see Rank-Revealing QR Factorizations and the Singular Value Decomposition, Y. P. Hong; C.-T. Pan\nIt is done via so-called rank-revealing factorization.\nIt is based on the representation\n\nP A = Q R,\nwhere P is the permutation matrix (it permutes columns), and R has the block form\nR = \\begin{bmatrix} R_{11} & R_{12} \\\\ 0 & R_{22}\\end{bmatrix}.\n\nThe goal is to find P such that the norm of R_{22} is small, so you can find the numerical rank by looking at it.\nAn estimate is \\sigma_{r+1} \\leq \\Vert R_{22} \\Vert_2 (check why)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#summary",
    "href": "lectures/lecture-7/lecture-7.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nLU and QR decompositions can be computed using direct methods in finite amount of operations.\nWhat about Schur form and SVD?\nThey can not be computed by direct methods (why?) they can only be computed by iterative methods.\nAlthough iterative methods still have the same \\mathcal{O}(n^3) complexity in floating point arithmetic thanks to fast convergence."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#schur-form",
    "href": "lectures/lecture-7/lecture-7.html#schur-form",
    "title": "",
    "section": "Schur form",
    "text": "Schur form\n\nRecall that every matrix can be written in the Schur form\n\nA = Q T Q^*,\nwith upper triangular T and unitary Q and this decomposition gives eigenvalues of the matrix (they are on the diagonal of T).\n\nThe first and the main algorithm for computing the Schur form is the QR algorithm."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#qr-algorithm",
    "title": "",
    "section": "QR algorithm",
    "text": "QR algorithm\n\nThe QR algorithm was independently proposed in 1961 by Kublanovskaya and Francis.\n Do not mix QR algorithm and QR decomposition! \nQR decomposition is the representation of a matrix, whereas QR algorithm uses QR decomposition to compute the eigenvalues!"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#way-to-qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#way-to-qr-algorithm",
    "title": "",
    "section": "Way to QR algorithm",
    "text": "Way to QR algorithm\n\nConsider the equation\n\nA = Q T Q^*,\nand rewrite it in the form\n Q T = A Q. \n\nOn the left we can see QR factorization of the matrix AQ.\nWe can use this to derive fixed-point iteration for the Schur form, also known as QR algorithm."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#derivation-of-the-qr-algorithm-as-fixed-point-iteration",
    "href": "lectures/lecture-7/lecture-7.html#derivation-of-the-qr-algorithm-as-fixed-point-iteration",
    "title": "",
    "section": "Derivation of the QR algorithm as fixed-point iteration",
    "text": "Derivation of the QR algorithm as fixed-point iteration\nWe can write down the iterative process\n\n    Q_{k+1} R_{k+1} = A Q_k, \\quad Q_{k+1}^* A = R_{k+1} Q^*_k\n\nIntroduce\nA_k = Q^* _k A Q_k = Q^*_k Q_{k+1} R_{k+1} = \\widehat{Q}_k R_{k+1}\nand the new approximation reads\nA_{k+1} = Q^*_{k+1} A Q_{k+1} = ( Q_{k+1}^* A = R_{k+1} Q^*_k)  = R_{k+1} \\widehat{Q}_k.\nSo we arrive at the standard form of the QR algorithm.\nThe final formulas are then written in the classical QRRQ-form:\n\nStart from A_0 = A.\nCompute QR factorization of A_k = Q_k R_k.\nSet A_{k+1} = R_k Q_k.\n\nIterate until A_k is triangular enough (e.g. norm of subdiagonal part is small enough)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#what-about-the-convergence-and-complexity",
    "href": "lectures/lecture-7/lecture-7.html#what-about-the-convergence-and-complexity",
    "title": "",
    "section": "What about the convergence and complexity",
    "text": "What about the convergence and complexity\nStatement\nMatrices A_k are unitary similar to A\nA_k = Q^*_{k-1} A_{k-1} Q_{k-1} = (Q_{k-1} \\ldots Q_1)^* A (Q_{k-1} \\ldots Q_1)\nand the product of unitary matrices is a unitary matrix.\n\nComplexity of each step is \\mathcal{O}(n^3), if a general QR decomposition is done.\nOur hope is that A_k will be very close to the triangular matrix for suffiently large k.\n\n\nimport jax.numpy as jnp\nn = 5\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = jnp.array(a)\nniters = 1000\nfor k in range(niters):\n    q, rmat = jnp.linalg.qr(a)\n    a = rmat.dot(q)\nprint('Leading 3x3 block of a:')\nprint(a[:4, :4])\n\nLeading 3x3 block of a:\n[[ 1.68842566e+00  2.39874769e+00 -4.91278296e-01  4.55588011e-01]\n [-2.27337624e+00  1.13938512e+00 -1.02951549e+00  5.70268353e-01]\n [-1.39868453e-15 -3.77840299e-15  2.37335976e+00  1.79215981e+00]\n [-4.11289213e-16 -5.73344858e-16 -9.26030829e-01  2.20080588e+00]]"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#convergence-and-complexity-of-the-qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#convergence-and-complexity-of-the-qr-algorithm",
    "title": "",
    "section": "Convergence and complexity of the QR algorithm",
    "text": "Convergence and complexity of the QR algorithm\n\nThe convergence of the QR algorithm is from the largest eigenvalues to the smallest.\nAt least 2-3 iterations is needed for an eigenvalue.\nEach step is one QR factorization and one matrix-by-matrix product, as a result \\mathcal{O}(n^3) complexity.\n\nQ: does it mean \\mathcal{O}(n^4) complexity totally?\nA: fortunately, not.\n\nWe can speedup the QR algorithm by using shifts, since A_k - \\lambda I has the same Schur vectors.\nWe will discuss these details later"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#convergence-of-qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#convergence-of-qr-algorithm",
    "title": "",
    "section": "Convergence of QR algorithm",
    "text": "Convergence of QR algorithm\nDoes QR algorithm always convergence?\nProvide an example."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#counterexample",
    "href": "lectures/lecture-7/lecture-7.html#counterexample",
    "title": "",
    "section": "Counterexample",
    "text": "Counterexample\nFor a matrix A = \\begin{bmatrix} 0 & 1 \\\\\n                                  1 & 0 \\end{bmatrix}\nwe have A_k = A."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#connection-to-orthogonal-iteration",
    "href": "lectures/lecture-7/lecture-7.html#connection-to-orthogonal-iteration",
    "title": "",
    "section": "Connection to orthogonal iteration",
    "text": "Connection to orthogonal iteration\nIn the previous lecture, we considered power iteration, which is A^k v – approximation of the eigenvector.\nThe QR algorithm computes (implicitly) QR-factorization of the matrix A^k:\nA^k = A \\cdot \\ldots \\cdot A = Q_1 R_1 Q_1 R_1 \\ldots = Q_1 Q_2 R_2 Q_2 R_2 \\ldots (R_2 R_1) = \\ldots (Q_1 Q_2 \\ldots Q_k) (R_k \\ldots R_1)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#a-few-words-about-the-svd",
    "href": "lectures/lecture-7/lecture-7.html#a-few-words-about-the-svd",
    "title": "",
    "section": "A few words about the SVD",
    "text": "A few words about the SVD\n\nLast but not least: the singular value decomposition of matrix.\n\nA = U \\Sigma V^*.\n\nWe can compute it via eigendecomposition of\n\nA^* A = V^* \\Sigma^2 V,\nand/or\nAA^* = U^* \\Sigma^2 U\nwith QR algorithm, but it is a bad idea (c.f. Gram matrices).\n\nWe will discuss methods for computing SVD later."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#bonus-cs-cosine-sine-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#bonus-cs-cosine-sine-decomposition",
    "title": "",
    "section": "Bonus: CS (Cosine-Sine) decomposition",
    "text": "Bonus: CS (Cosine-Sine) decomposition\n\nLet Q be square unitary matrix with even number of rows and it is splitted in four equal size blocks\n\n Q = \\begin{bmatrix} Q_{11} & Q_{12} \\\\ Q_{21} & Q_{22} \\end{bmatrix} \n\nThen there exist unitary matrices U_1, U_2, V_1, V_2 such that\n\n \\begin{bmatrix} U_1^* & 0 \\\\ 0 & U_2^* \\end{bmatrix} \\begin{bmatrix} Q_{11} & Q_{12} \\\\ Q_{21} & Q_{22} \\end{bmatrix} \\begin{bmatrix} V_1 & 0 \\\\ 0 & V_2 \\end{bmatrix} = \\begin{bmatrix} C & S \\\\ -S & C \\end{bmatrix}, \nwhere C = \\mathrm{diag}(c) and S = \\mathrm{diag}(s) such that c_i \\geq 0, s_i \\geq 0 and c_i^2 + s_i^2 = 1\n\nQ: how many SVD do we have inside the CS decomposition?\nThe case of rectangular matrix with orthonormal columns\n\n \\begin{bmatrix} U_1^* & 0 \\\\ 0 & U_2^* \\end{bmatrix} \\begin{bmatrix} Q_{1} \\\\ Q_{2} \\end{bmatrix} V = \\begin{bmatrix} C \\\\ S \\end{bmatrix} \n\nThe algorithm for computing this decomposition is presented here\nThis decomposition naturally arises in the problem of finding distances and angles between subspaces"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#summary-1",
    "href": "lectures/lecture-7/lecture-7.html#summary-1",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nQR decomposition and Gram-Schmidt algorithm, reduction to a simpler form by Householder transformations\nSchur decomposition and QR algorithm"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#next-steps",
    "href": "lectures/lecture-7/lecture-7.html#next-steps",
    "title": "",
    "section": "Next steps",
    "text": "Next steps\n\nEfficient implementation of QR algorithm and its convergence\nEfficient computation of the SVD: 4 algorithms\nMore applications of the SVD\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html",
    "href": "lectures/lecture-1/lecture-1.html",
    "title": "Representation of numbers",
    "section": "",
    "text": "Today: - Part 1: floating point, vector norms - Part 2: matrix norms and stability concepts"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#syllabus",
    "href": "lectures/lecture-1/lecture-1.html#syllabus",
    "title": "Representation of numbers",
    "section": "",
    "text": "Today: - Part 1: floating point, vector norms - Part 2: matrix norms and stability concepts"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#fixed-point-representation",
    "href": "lectures/lecture-1/lecture-1.html#fixed-point-representation",
    "title": "Representation of numbers",
    "section": "Fixed Point Representation",
    "text": "Fixed Point Representation\n\nFixed point is the simplest way to represent real numbers digitally\n\nAlso known as Qm.n format, where:\n\nm bits for the integer part\nn bits for the fractional part\n\n\nKey Properties:\n\nRange: [-(2^m), 2^m - 2^{-n}]\nResolution: 2^{-n} (smallest representable difference)\nStorage: m + n + 1 bits total (including sign bit)\n\nLimitations:\n\nFixed range of representable numbers\nTrade-off between range (m) and precision (n)\nCannot efficiently handle very large or very small numbers"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#floating-point-formula",
    "href": "lectures/lecture-1/lecture-1.html#floating-point-formula",
    "title": "Representation of numbers",
    "section": "Floating point: formula",
    "text": "Floating point: formula\nf = (-1)^s 2^{(p-b)} \\left( 1 + \\frac{d_1}{2} + \\frac{d_2}{2^2}  + \\ldots + \\frac{d_m}{2^m}\\right),\nwhere s \\in \\{0, 1\\} is the sign bit, d_i \\in \\{0, 1\\} is the m-bit mantissa, p \\in \\mathbb{Z}; 0 \\leq p \\leq 2^e, e is the e-bit exponent, commonly defined as 2^e - 1\nCan be thought as a uniform m-bit grid between two sequential powers of 2."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#simple-examples",
    "href": "lectures/lecture-1/lecture-1.html#simple-examples",
    "title": "Representation of numbers",
    "section": "Simple examples",
    "text": "Simple examples\nThere are many ways to write a number in scientific notation, but there is always a unique normalized representation, with exactly one non-zero digit to the left of the decimal point.\nFor example: - 0.232 \\times 10^3 = 23.2 \\times 10^1 = 2.32 \\times 10^2 = \\ldots - 01001 = 1.001 \\times 2^3 = \\ldots\nExample 1: What’s the normalized representation of 00101101.101?\n0.0001101001110 = 1.110100111 \\times 2^{-4}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#multiplication-in-more-details",
    "href": "lectures/lecture-1/lecture-1.html#multiplication-in-more-details",
    "title": "Representation of numbers",
    "section": "Multiplication in more details",
    "text": "Multiplication in more details\nConsider two FP numbers x, y, whose exponents and fractions are x_e, y_e and x_m, y_m respectively, the vanilla FP Mul result is\n\n\\operatorname{Mul}(x, y)=\\left(1+x_m\\right) \\cdot 2^{x_e} \\cdot\\left(1+y_m\\right) \\cdot 2^{y_e}=\\left(1+x_m+y_m+x_m \\cdot y_m\\right) \\cdot 2^{x_e+y_e}\n\nRecent papers: Addition is all you need tries to argue that we can replace this multplication by addition and still get trainable neural networks (needs to be checked.)"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#fixed-vs-floating",
    "href": "lectures/lecture-1/lecture-1.html#fixed-vs-floating",
    "title": "Representation of numbers",
    "section": "Fixed vs Floating",
    "text": "Fixed vs Floating\nQ: What are the advantages/disadvantages of the fixed and floating points?\nA: In most cases, they work just fine.\n\nHowever, fixed point represents numbers within specified range and controls absolute accuracy.\nFloating point represent numbers with relative accuracy, and is suitable for the case when numbers in the computations have varying scale (i.e., 10^{-1} and 10^{5}).\nIn practice, if speed is of no concern, use float32 or float64."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#floating-point-numbers-on-a-logarithmic-scale",
    "href": "lectures/lecture-1/lecture-1.html#floating-point-numbers-on-a-logarithmic-scale",
    "title": "Representation of numbers",
    "section": "Floating point numbers on a logarithmic scale",
    "text": "Floating point numbers on a logarithmic scale\nLet’s visualize how floating point numbers are distributed on the real line:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define parameters for a small floating-point system\nb = 2  # base\ne_max = 3  # maximum exponent\nmantissa_bits = 2  # number of bits for mantissa\n\n# Generate all possible combinations of exponent and mantissa\nexponents = range(-e_max, e_max + 1)\nmantissas = np.linspace(0, 1 - 2**(-mantissa_bits), 2**mantissa_bits)\n\n# Calculate floating-point numbers\nfp_numbers = []\nfor e in exponents:\n    for m in mantissas:\n        fp_numbers.append((1 + m) * b**(e))\n\n# Sort the numbers for proper visualization\nfp_numbers.sort()\n\n# Create figure\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the floating-point numbers\nax.scatter(fp_numbers, [1] * len(fp_numbers), marker='|', s=100, color='blue')\nax.set_ylim(0.5, 1.5)\nax.set_yticks([])  # Remove y-axis ticks\n\nax.set_title(r'Distribution of Floating-Point Numbers (base=$%d$, max_exp=$%d$, mantissa_bits=$%d$)' % (b, e_max, mantissa_bits))\nax.set_xlabel('Value')\n\n# Add text explanation\nplt.figtext(0.1, -0.05, \n    r\"Base (b) = $%d$, Max exponent = $%d$, Mantissa bits = $%d$\" % (b, e_max, mantissa_bits) + \"\\n\" +\n    r\"Numbers are of the form $(1 + x_m) \\cdot 2^e$, where $x_m$ is the fraction and $e$ is the exponent.\",\n    wrap=True)\n#plt.tight_layout()\n#plt.show()\n\nText(0.1, -0.05, 'Base (b) = $2$, Max exponent = $3$, Mantissa bits = $2$\\nNumbers are of the form $(1 + x_m) \\\\cdot 2^e$, where $x_m$ is the fraction and $e$ is the exponent.')"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#ieee-754",
    "href": "lectures/lecture-1/lecture-1.html#ieee-754",
    "title": "Representation of numbers",
    "section": "IEEE 754",
    "text": "IEEE 754\nIn modern computers, the floating point representation is controlled by IEEE 754 standard which was published in 1985 and before that point different computers behaved differently with floating point numbers.\nIEEE 754 has: - Floating point representation (as described above), (-1)^s \\times c \\times b^q. - Two infinities, +\\infty and -\\infty - Two zeros: +0 and -0 - Two kinds of NaN: a quiet NaN (qNaN) and signalling NaN (sNaN) - qNaN does not throw exception in the level of floating point unit (FPU), until you check the result of computations - sNaN value throws exception from FPU if you use corresponding variable. This type of NaN can be useful for initialization purposes - C++11 proposes standard interface for creating different NaNs - Rules for rounding - Rules for \\frac{0}{0}, \\frac{1}{-0}, \\ldots\nPossible values are defined with - base b - accuracy p - number of digits - maximum possible value e_{\\max}\nand have the following restrictions - $ 0 c b^p - 1$ - 1 - e_{\\max} \\leq q + p - 1 \\leq e_{\\max}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#single-precision-double-precision",
    "href": "lectures/lecture-1/lecture-1.html#single-precision-double-precision",
    "title": "Representation of numbers",
    "section": "Single precision, double precision",
    "text": "Single precision, double precision\nThe two standard formats, called binary32 and binary64 (called also single and double formats). Recently, the format binary16 plays important role in learning deep neural networks.\n\n\n\nName\nCommon Name\nBase\nDigits\nEmin\nEmax\n\n\n\n\nbinary16\nhalf precision\n2\n11\n-14\n+ 15\n\n\nbinary32\nsingle precision\n2\n24\n-126\n+ 127\n\n\nbinary64\ndouble precision\n2\n53\n-1022\n+1023"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#examples",
    "href": "lectures/lecture-1/lecture-1.html#examples",
    "title": "Representation of numbers",
    "section": "Examples",
    "text": "Examples\n\nFor a number +0\n\nsign is 0\nexponent is 00000000000\nfraction is all zeros\n\nFor a number -0\n\nsign is 1\nexponent is 00000000000\nfraction is all zeros\n\nFor +infinity\n\nsign is 0\nexponent is 11111111111\nfraction is all zeros\n\n\nQ: what about -infinity and NaN ?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#accuracy-and-memory",
    "href": "lectures/lecture-1/lecture-1.html#accuracy-and-memory",
    "title": "Representation of numbers",
    "section": "Accuracy and memory",
    "text": "Accuracy and memory\nThe relative accuracy of single precision is 10^{-7}-10^{-8}, while for double precision is 10^{-14}-10^{-16}.\n Crucial note 1:  A float16 takes 2 bytes, float32 takes 4 bytes, float64, or double precision, takes 8 bytes.\n Crucial note 2:  These are the only two floating point-types supported in hardware (float32 and float64) + GPU/TPU different float types are supported.\n Crucial note 3:  You should use double precision in computational science and engineering and float32/float16 on GPU/Data Science.\nNow for large models float16 has become more and more robust."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#how-does-number-representation-format-affect-training-of-neural-networks-nn",
    "href": "lectures/lecture-1/lecture-1.html#how-does-number-representation-format-affect-training-of-neural-networks-nn",
    "title": "Representation of numbers",
    "section": "How does number representation format affect training of neural networks (NN)?",
    "text": "How does number representation format affect training of neural networks (NN)?\n\nWeights in layers (fully-connected, convolutional, activation functions) can be stored with different accuracies\nIt is important to improve energy efficiency of the devices that are used to train NNs\nProject DeepFloat from Facebook demonstrates how re-develop floating point operations in a way to ensure efficiency in training NNs, more details see in this paper\nAffect of the real numbers representation on the gradients of activation functions\nTypically, the first digit is one.\nSubnormal numbers have first digit 0 to represent zeros and numbers close to zero.\nSubnormal numbers fill the gap between positive and negative\nThey have performance issues, often flushed to zero by default.\n\n\n\nAnd on the learning curves\n\n\nPlots are taken from this paper"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#bfloat16-brain-floating-point",
    "href": "lectures/lecture-1/lecture-1.html#bfloat16-brain-floating-point",
    "title": "Representation of numbers",
    "section": "bfloat16 (Brain Floating Point)",
    "text": "bfloat16 (Brain Floating Point)\n\nThis format occupies 16 bits\n\n1 bit for sign\n8 bits for exponent\n7 bits for fraction \n\nTruncated single precision format from IEEE standard\nWhat is the difference between float32 and float16?\nThis format is utilized in Intel FPGA, Google TPU, Xeon CPUs and other platforms"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#tensor-float-from-nvidia-blog-post-about-this-format",
    "href": "lectures/lecture-1/lecture-1.html#tensor-float-from-nvidia-blog-post-about-this-format",
    "title": "Representation of numbers",
    "section": "Tensor Float from Nvidia (blog post about this format)",
    "text": "Tensor Float from Nvidia (blog post about this format)\n\nComparison with other formats\n\n\n\nResults\n\n\n\nPyTorch and Tensorflow supported this format are available in Nvidia NCG"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#mixed-precision-docs-from-nvidia",
    "href": "lectures/lecture-1/lecture-1.html#mixed-precision-docs-from-nvidia",
    "title": "Representation of numbers",
    "section": "Mixed precision (docs from Nvidia)",
    "text": "Mixed precision (docs from Nvidia)\n\nMain idea:\n\nMaintain copy of weights in single precision\nThen in every iteration\n\nMake a copy of weights in half-precision\nForward pass with weights in half-precision\nMultiply the loss by the scaling factor S\nBackward pass again in half precision\nMultiply the weight gradient with 1/S\nComplete the weight update (including gradient clipping, etc.)\n\nScaling factor S is a hyper-parameter\nConstant: a value so that its product with the maximum absolute gradient value is below 65504 (the maximum value representable in half precision).\nDynamic update based on the current gradient statistics\n\nPerformance comparison \nAutomatic mixed-precision extensions exist to simplify turning this option on, more details here"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#alternative-to-the-ieee-754-standard",
    "href": "lectures/lecture-1/lecture-1.html#alternative-to-the-ieee-754-standard",
    "title": "Representation of numbers",
    "section": "Alternative to the IEEE 754 standard",
    "text": "Alternative to the IEEE 754 standard\nIssues in IEEE 754: - overflow to infinity or zero - many different NaNs - invisible rounding errors - accuracy is very high or very poor - subnormal numbers – numbers between 0 and minimal possible represented number, i.e. significand starts from zero\nConcept of posits can replace floating point numbers, see this paper\n\n\nrepresent numbers with some accuracy, but provide limits of changing\nno overflows!\nexample of a number representation\n\n\n\nDivision accuracy demo\n\n\nimport random\nimport jax.numpy as jnp\nimport jax\n#from jax.config import config\n#config.update(\"jax_enable_x64\", True)\n#c = random.random()\n#print(c)\nc = jnp.float32(0.925924589693)\nprint(c)\na = jnp.float32(1.786875867457e-2)\nb = jnp.float32(c / a)\nprint('{0:10.16f}'.format(b))\nprint(abs(a * b - c)/abs(c))\n\nPlatform 'METAL' is experimental and not all JAX functionality may be correctly supported!\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1730112332.744245 4402733 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\nI0000 00:00:1730112332.761652 4402733 service.cc:145] XLA service 0x60000122a800 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1730112332.761811 4402733 service.cc:153]   StreamExecutor device (0): Metal, &lt;undefined&gt;\nI0000 00:00:1730112332.763332 4402733 mps_client.cc:406] Using Simple allocator.\nI0000 00:00:1730112332.763342 4402733 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator.\n\n\nMetal device set to: Apple M2 Pro\n0.9259246\n51.8180694580078125\n0.0\n\n\n\n\nSquare root accuracy demo\n\na = jnp.float32(1e-20)\nb = jnp.sqrt(a)\nprint(b.dtype)\nprint('{0:10.64f}'.format(abs(b * b - a)/abs(a)))\n\nfloat32\n0.0000000807793583135207882151007652282714843750000000000000000000\n\n\n\n\nExponent accuracy demo\n\na = jnp.float32(0.0001)\nb = jnp.exp(a)\nprint(b.dtype)\nprint((jnp.log(b) - a)/a)\n\nfloat32\n0.000115978764"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#more-complicated-example",
    "href": "lectures/lecture-1/lecture-1.html#more-complicated-example",
    "title": "Representation of numbers",
    "section": "More complicated example",
    "text": "More complicated example\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create x points in the range [-2e-15, 2e-15]\nx = np.linspace(-2e-15, 2e-15, 100)\n\n# Compute log(1+x)/x - 1 using method (a) and (b), being careful about x=0\ny_a = np.zeros_like(x)\ny_b = np.zeros_like(x)\nnonzero = x != 0\ny_a[nonzero] = (np.log(x[nonzero]+1))/x[nonzero] - 1  # (a)\ny_b[nonzero] = (np.log(x[nonzero]+1))/((1+x[nonzero])-1) - 1  # (b\n\n# Create the plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n\n# Plot for method (a)\nax1.plot(x, y_a, 'b-')\nax1.grid(True)\nax1.set_xlabel('x')\nax1.set_ylabel('log(1+x)/x - 1')\nax1.set_title('Method (a): (log(1+x))/x - 1')\n\n# Plot for method (b)\nax2.plot(x, y_b, 'r-')\nax2.grid(True)\nax2.set_xlabel('x')\nax2.set_ylabel('log(1+x)/x - 1')\nax2.set_title('Method (b): (log(1+x))/((1+x)-1) - 1')\n\n# Adjust layout and add a main title\nplt.tight_layout()\nfig.suptitle('Comparison of two methods for computing log(1+x)/x - 1 in double precision', fontsize=16)\nplt.subplots_adjust(top=0.88)\n\nplt.show()\n\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_35926/1372311441.py:12: RuntimeWarning: invalid value encountered in divide\n  y_b[nonzero] = (np.log(x[nonzero]+1))/((1+x[nonzero])-1) - 1  # (b)"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#summary-of-demos",
    "href": "lectures/lecture-1/lecture-1.html#summary-of-demos",
    "title": "Representation of numbers",
    "section": "Summary of demos",
    "text": "Summary of demos\n\nFor some values the inverse functions give exact answers\nThe relative accuracy should be preserved due to the IEEE standard\nDoes not hold for many modern GPU\nMore details about adoptation of IEEE 754 standard for GPU you can find here"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#loss-of-significance",
    "href": "lectures/lecture-1/lecture-1.html#loss-of-significance",
    "title": "Representation of numbers",
    "section": "Loss of significance",
    "text": "Loss of significance\n\nMany operations lead to the loss of digits loss of significance\nFor example, it is a bad idea to subtract two big numbers that are close, the difference will have fewer correct digits\nThis is related to algorithms and their properties (forward/backward stability), which we will discuss later"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#summation-algorithm",
    "href": "lectures/lecture-1/lecture-1.html#summation-algorithm",
    "title": "Representation of numbers",
    "section": "Summation algorithm",
    "text": "Summation algorithm\nHowever, the rounding errors can depend on the algorithm.\n\nConsider the simplest problem: given n floating point numbers x_1, \\ldots, x_n\nCompute their sum\n\nS = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.\n\nThe simplest algorithm is to add one-by-one\nWhat is the actual error for such algorithm?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#naïve-algorithm",
    "href": "lectures/lecture-1/lecture-1.html#naïve-algorithm",
    "title": "Representation of numbers",
    "section": "Naïve algorithm",
    "text": "Naïve algorithm\nNaïve algorithm adds numbers one-by-one:\ny_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.\n\nThe worst-case error is then proportional to \\mathcal{O}(n), while mean-squared error is \\mathcal{O}(\\sqrt{n}).\nThe Kahan algorithm gives the worst-case error bound \\mathcal{O}(1) (i.e., independent of n).\n Can you find the better algorithm?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#kahan-summation",
    "href": "lectures/lecture-1/lecture-1.html#kahan-summation",
    "title": "Representation of numbers",
    "section": "Kahan summation",
    "text": "Kahan summation\nThe following algorithm gives 2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2) error, where \\varepsilon is the machine precision.\n\nThe reason of the loss of significance in summation is operating with numbers of different magnitude\nThe main idea of Kahan summation is to keep track of small errors and aggregate them in separate variable\nThis approach is called compensated summation\n\ns = 0\nc = 0\nfor i in range(len(x)):\n    y = x[i] - c\n    t = s + y\n    c = (t - s) - y\n    s = t\n\nThere exists more advanced tricks to process this simple operation that are used for example in fsum function from math package, implementation check out here\n\n\nimport math\n\nimport jax.numpy as jnp\nimport numpy as np\nimport jax\nfrom numba import jit as numba_jit\n\nn = 10 ** 5\nsm = 1e-10\nx = jnp.ones(n, dtype=jnp.float32) * sm\nx = x.at[0].set(1)\n#x = jax.ops.index_update(x, [0], 1.)\ntrue_sum = 1.0 + (n - 1)*sm\napprox_sum = jnp.sum(x)\nmath_fsum = math.fsum(x)\n\n\n@jax.jit\ndef dumb_sum(x):\n    s = jnp.float32(0.0)\n    def b_fun(i, val):\n        return val + x[i] \n    s = jax.lax.fori_loop(0, len(x), b_fun, s)\n    return s\n\n\n@numba_jit(nopython=True)\ndef kahan_sum_numba(x):\n    s = np.float32(0.0)\n    c = np.float32(0.0)\n    for i in range(len(x)):\n        y = x[i] - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\n@jax.jit\ndef kahan_sum_jax(x):\n    s = jnp.float32(0.0)\n    c = jnp.float32(0.0)\n    def b_fun2(i, val):\n        s, c = val\n        y = x[i] - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n        return s, c\n    s, c = jax.lax.fori_loop(0, len(x), b_fun2, (s, c))\n    return s\n\nk_sum_numba = kahan_sum_numba(np.array(x))\nk_sum_jax = kahan_sum_jax(x)\nd_sum = dumb_sum(x)\nprint('Error in np sum: {0:3.1e}'.format(approx_sum - true_sum))\nprint('Error in Kahan sum Numba: {0:3.1e}'.format(k_sum_numba - true_sum))\nprint('Error in Kahan sum JAX: {0:3.1e}'.format(k_sum_jax - true_sum))\nprint('Error in dumb sum: {0:3.1e}'.format(d_sum - true_sum))\nprint('Error in math fsum: {0:3.1e}'.format(math_fsum - true_sum))\n\nError in np sum: 0.0e+00\nError in Kahan sum Numba: 1.4e-08\nError in Kahan sum JAX: 0.0e+00\nError in dumb sum: -1.0e-05\nError in math fsum: 1.3e-13"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#summary-of-floating-point",
    "href": "lectures/lecture-1/lecture-1.html#summary-of-floating-point",
    "title": "Representation of numbers",
    "section": "Summary of floating-point",
    "text": "Summary of floating-point\n\nYou should be really careful with floating point numbers, since it may give you incorrect answers due to rounding-off errors.\nFor many standard algorithms, the stability is well-understood and problems can be easily detected."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#vectors",
    "href": "lectures/lecture-1/lecture-1.html#vectors",
    "title": "Representation of numbers",
    "section": "Vectors",
    "text": "Vectors\n\nIn NLA we typically work not with numbers, but with vectors\nRecall that a vector in a fixed basis of size n can be represented as a 1D array with n numbers\nTypically, it is considered as an n \\times 1 matrix (column vector)\n\nExample: Polynomials with degree \\leq n form a linear space. Polynomial $ x^3 - 2x^2 + 1$ can be considered as a vector \\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix} in the basis \\{x^3, x^2, x, 1\\}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#vector-norm",
    "href": "lectures/lecture-1/lecture-1.html#vector-norm",
    "title": "Representation of numbers",
    "section": "Vector norm",
    "text": "Vector norm\n\nVectors typically provide an (approximate) description of a physical (or some other) object\nOne of the main question is how accurate the approximation is (1%, 10%)\nWhat is an acceptable representation, of course, depends on the particular applications. For example:\n\nIn partial differential equations accuracies 10^{-5} - 10^{-10} are the typical case\nIn data-based applications sometimes an error of 80\\% is ok, since the interesting signal is corrupted by a huge noise"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#distances-and-norms",
    "href": "lectures/lecture-1/lecture-1.html#distances-and-norms",
    "title": "Representation of numbers",
    "section": "Distances and norms",
    "text": "Distances and norms\n\nNorm is a qualitative measure of smallness of a vector and is typically denoted as \\Vert x \\Vert.\n\nThe norm should satisfy certain properties:\n\n\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (triangle inequality)\nIf \\Vert x \\Vert = 0 then x = 0\n\nThe distance between two vectors is then defined as\n d(x, y) = \\Vert x - y \\Vert."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#standard-norms",
    "href": "lectures/lecture-1/lecture-1.html#standard-norms",
    "title": "Representation of numbers",
    "section": "Standard norms",
    "text": "Standard norms\nThe most well-known and widely used norm is euclidean norm:\n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\nwhich corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#p-norm",
    "href": "lectures/lecture-1/lecture-1.html#p-norm",
    "title": "Representation of numbers",
    "section": "p-norm",
    "text": "p-norm\nEuclidean norm, or 2-norm, is a subclass of an important class of p-norms:\n \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}. \nThere are two very important special cases: - Infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value:\n \\Vert x \\Vert_{\\infty} = \\max_i | x_i| \n\n\nL_1 norm (or Manhattan distance) which is defined as the sum of modules of the elements of x:\n\n \\Vert x \\Vert_1 = \\sum_i |x_i| \n\nWe will give examples where L_1 norm is very important: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#equivalence-of-the-norms",
    "href": "lectures/lecture-1/lecture-1.html#equivalence-of-the-norms",
    "title": "Representation of numbers",
    "section": "Equivalence of the norms",
    "text": "Equivalence of the norms\nAll norms are equivalent in the sense that\n C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_* \nfor some positive constants C_1(n), C_2(n), x \\in \\mathbb{R}^n for any pairs of norms \\Vert \\cdot \\Vert_* and \\Vert \\cdot \\Vert_{**}. The equivalence of the norms basically means that if the vector is small in one norm, it is small in another norm. However, the constants can be large."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#computing-norms-in-python",
    "href": "lectures/lecture-1/lecture-1.html#computing-norms-in-python",
    "title": "Representation of numbers",
    "section": "Computing norms in Python",
    "text": "Computing norms in Python\nThe NumPy package has all you need for computing norms: np.linalg.norm function.\n\nimport numpy as np\nn = 100\na = np.random.randn(n)\nb = a + 1e-1 * np.random.normal((n,))\nprint('Relative error in L1 norm:', np.linalg.norm(a - b, 1) / np.linalg.norm(b, 1))\nprint('Relative error in L2 norm:', np.linalg.norm(a - b) / np.linalg.norm(b))\nprint('Relative error in Chebyshev norm:', np.linalg.norm(a - b, np.inf) / np.linalg.norm(b, np.inf))\n\nRelative error in L1 norm: 0.9929434299101272\nRelative error in L2 norm: 0.9870451639574119\nRelative error in Chebyshev norm: 0.7489012536858338"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#unit-disks-in-different-norms",
    "href": "lectures/lecture-1/lecture-1.html#unit-disks-in-different-norms",
    "title": "Representation of numbers",
    "section": "Unit disks in different norms",
    "text": "Unit disks in different norms\n\nA unit disk is a set of point such that \\Vert x \\Vert \\leq 1\nFor the euclidean norm a unit disk is a usual disk\nFor other norms unit disks look very different\n\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\np = 1 # Which norm do we use\nM = 40000 # Number of sampling points\nb = []\nfor i in range(M):\n    a = np.random.randn(2, 1)\n    if np.linalg.norm(a[:, 0], p) &lt;= 1:\n        b.append(a[:, 0])\nb = np.array(b)\nplt.plot(b[:, 0], b[:, 1], '.')\nplt.axis('equal')\nplt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))\n\nText(0.5, 1.0, 'Unit disk in the p-th norm, $p=1$')"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#why-l_1-norm-can-be-important",
    "href": "lectures/lecture-1/lecture-1.html#why-l_1-norm-can-be-important",
    "title": "Representation of numbers",
    "section": "Why L_1-norm can be important?",
    "text": "Why L_1-norm can be important?\nL_1 norm, as it was discovered quite recently, plays an important role in compressed sensing.\nThe simplest formulation of the considered problem is as follows:\n\nYou have some observations f\nYou have a linear model Ax = f, where A is an n \\times m matrix, A is known\nThe number of equations, n, is less than the number of unknowns, m\n\nThe question: can we find the solution?\nThe solution is obviously non-unique, so a natural approach is to find the solution that is minimal in the certain sense:\n\\begin{align*}\n& \\Vert x \\Vert \\rightarrow \\min_x \\\\\n\\mbox{subject to } & Ax = f\n\\end{align*}\n\nTypical choice of \\Vert x \\Vert = \\Vert x \\Vert_2 leads to the linear least squares problem (and has been used for ages).\nThe choice \\Vert x \\Vert = \\Vert x \\Vert_1 leads to the compressed sensing\nIt typically yields the sparsest solution"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#what-is-a-stable-algorithm",
    "href": "lectures/lecture-1/lecture-1.html#what-is-a-stable-algorithm",
    "title": "Representation of numbers",
    "section": "What is a stable algorithm?",
    "text": "What is a stable algorithm?\nAnd we finalize the lecture by the concept of stability.\n\nLet x be an object (for example, a vector)\nLet f(x) be the function (functional) you want to evaluate\n\nYou also have a numerical algorithm alg(x) that actually computes approximation to f(x).\nThe algorithm is called forward stable, if\n\\Vert \\text{alg}(x) - f(x) \\Vert  \\leq \\varepsilon \nThe algorithm is called backward stable, if for any x there is a close vector x + \\delta x such that\n\\text{alg}(x) = f(x + \\delta x)\nand \\Vert \\delta x \\Vert is small."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#classical-example",
    "href": "lectures/lecture-1/lecture-1.html#classical-example",
    "title": "Representation of numbers",
    "section": "Classical example",
    "text": "Classical example\nA classical example is the solution of linear systems of equations using Gaussian elimination which is similar to LU factorization (more details later)\nWe consider the Hilbert matrix with the elements\nA = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.\nAnd consider a linear system\nAx = f.\nWe will look into matrices in more details in the next lecture, and for linear systems in the upcoming weeks\n\nimport numpy as np\nn = 1000\na = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] # Hilbert matrix\nA = np.array(a)\nrhs =  np.random.randn(n)\nrhs = np.ones(n)\nsol = np.linalg.solve(A, rhs)\nprint(np.linalg.norm(A @ sol - rhs)/np.linalg.norm(rhs))\nplt.plot(sol)\n\n8.945456835802865e-08\n\n\n\n\n\n\n\n\n\n\nrhs = jnp.ones(n)\nsol = jnp.linalg.solve(A, rhs)\nprint(jnp.linalg.norm(A @ sol - rhs)/jnp.linalg.norm(rhs))\n#plt.plot(sol)\n\n0.0018351191"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#more-examples-of-instability",
    "href": "lectures/lecture-1/lecture-1.html#more-examples-of-instability",
    "title": "Representation of numbers",
    "section": "More examples of instability",
    "text": "More examples of instability\nHow to compute the following functions in numerically stable manner?\n\n\\log(1 - \\tanh^2(x))\n\\text{SoftMax}(x)_j = \\dfrac{e^{x_j}}{\\sum\\limits_{i=1}^n e^{x_i}}\n\n\nu = 30\neps = 1e-6\nprint(\"Original function:\", jnp.log(1 - jnp.tanh(u)**2))\neps_add = jnp.log(1 - jnp.tanh(u)**2 + eps)\nprint(\"Attempt to improve stability by adding a small constant:\", eps_add)\nprint(\"Use more numerically stable form:\", jnp.log(4) - 2 * jnp.log(jnp.exp(-u) + jnp.exp(u)))\n\nOriginal function: -inf\nAttempt to improve stability by adding a small constant: -13.815511\nUse more numerically stable form: -58.613705\n\n\n\nn = 5\nx = jax.random.normal(jax.random.PRNGKey(0), (n, ))\nx = x.at[0].set(1000)\nprint(jnp.exp(x) / jnp.sum(jnp.exp(x)))\nprint(jnp.exp(x - jnp.max(x)) / jnp.sum(jnp.exp(x - jnp.max(x))))\n\n[nan  0.  0.  0.  0.]\n[1. 0. 0. 0. 0.]"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#take-home-message",
    "href": "lectures/lecture-1/lecture-1.html#take-home-message",
    "title": "Representation of numbers",
    "section": "Take home message",
    "text": "Take home message\n\nFloating point (double, single, number of bytes), rounding error\nNorms are measures of smallness, used to compute the accuracy\n1, p and Euclidean norms\nL_1 is used in compressed sensing as a surrogate for sparsity (later lectures)\nForward/backward error (and stability of algorithms) (later lectures)"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#next-lecture",
    "href": "lectures/lecture-1/lecture-1.html#next-lecture",
    "title": "Representation of numbers",
    "section": "Next lecture",
    "text": "Next lecture\n\nMatrix norms: what is the difference between matrix and vector norms\nUnitary matrices, including elementary unitary matrices."
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html",
    "href": "seminars/seminar-5/Seminar-5.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\nimport scipy"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html#outline",
    "href": "seminars/seminar-5/Seminar-5.html#outline",
    "title": "",
    "section": "Outline",
    "text": "Outline\n\nLU decomposition as a sum of rank one matrices\nLU Decomposition with Pivoting\nOverdetermined Linear Systems (Least Squares) 3.1 Geometry of Normal Equation  3.2 Pseudoinverse  3.3 Pseudoinverse via SVD  3.4 QR approach  3.5 Linear Regression\n\nIn many applications, we encounter overdetermined systems of linear equations, where there are more equations than unknowns, represented as\n Ax = b. \nThese systems often lack an exact solution.\nTo address this, we can use the least squares method, which finds an approximate solution by minimizing the residual error:\n x = \\arg\\min_x \\| Ax - b \\|_2"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html#overdetermined-linear-systems",
    "href": "seminars/seminar-5/Seminar-5.html#overdetermined-linear-systems",
    "title": "",
    "section": "Overdetermined Linear Systems",
    "text": "Overdetermined Linear Systems\nMany applications lead to unsolvable linear equations Ax = b, where the number of equations is greater, than the number of unknowns.\nThe least squares method chooses the solution as\n x = \\arg\\min_x \\| Ax - b \\|_2\n\nThe Normal Equation\nThe solution to the least squares problem satisfies the normal equation:\n A^T A x = A^T b \nWhen $ A $ has full column rank, the matrix $ A^T A $ is positive definite, allowing us to solve this equation efficiently with Cholesky decomposition:\n A^T A = R^T R \nwhere $ R $ is an upper triangular matrix (and $ R^T $ is lower triangular). This leads to the following set of equations:\n\n\\begin{align}\n    R^T y &= A^T b \\\\\n    R x &= y\n\\end{align}\n\nHowever, solving the normal equations directly can be numerically unstable, especially for larger problems. This approach is generally safe for small problems, but for stability, other methods are recommended. If the pivots in Gaussian elimination are small, LU decomposition may fail, whereas Cholesky decomposition remains stable in these cases.\n\n# Cholesky Decomposition Method\ndef leastsq_chol(A, b):\n    R = scipy.linalg.cholesky(A.T @ A)\n    w = scipy.linalg.solve_triangular(R, A.T @ b, trans='T')\n    return scipy.linalg.solve_triangular(R, w)\n\n\n\nThe Pseudoinverse\nIn linear algebra, not all matrices have an inverse, particularly when a system of equations has no solution or many solutions. The Moore-Penrose pseudoinverse offers a way to find an approximate solution that minimizes error, even when a unique solution doesn’t exist.\nIn the lecture, the pseudoinverse is defined as:\n A^{\\dagger} = \\lim_{\\alpha \\to 0} (A^T A + \\alpha I)^{-1} A^T \nAlternatively,\n A^{\\dagger} = \\lim_{\\alpha \\to 0} A^T (A A^T + \\alpha I)^{-1} \nThese limits exist even if $ (A^T A)^{-1} $ or $ (A AT){-1} $ do not. Later, we will see how this relates to Tikhonov regularization.\nIf $ A $ has full column rank, the pseudoinverse simplifies to:\n A^{\\dagger} = (A^T A)^{-1} A^T \n\n\nComputing the Pseudoinverse Using SVD\nAnother way to compute the pseudoinverse is through the Singular Value Decomposition (SVD) $ A = U V^T $:\n A^{\\dagger} = V \\Sigma^{\\dagger} U^T \nIn this approach, we invert the diagonal entries of $ $ where possible.\nWith the pseudoinverse $ A^{} $, we can write the solution to $ Ax = b $ as:\n x = A^{\\dagger} b \n\n\nExample: Solving a System Using Pseudoinverse\nLet’s now consider the toy matrix and system of equations:\n \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x = \\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} \n\n# Pseudo-inverse Method using Matrix Inversion\ndef leastsq_pinv(A, b):\n    return np.linalg.inv(A.T @ A) @ A.T @ b\n\n\n# Pseudo-inverse Method using SVD\ndef leastsq_pinv_svd(A, b):\n    U, S, Vh = scipy.linalg.svd(A)\n    S_plus = np.zeros(A.shape).T\n    S_plus[:S.shape[0], :S.shape[0]] = np.linalg.inv(np.diag(S))\n    return Vh.T @ S_plus @ U.T @ b\n\nLet’s now return to our toy matrix and consider the system\n\\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x =\\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} \n\nA = np.array([[1., -1.], [2., 1.], [-1., 3.]])\nb = np.array([[0.], [3.], [2.]])\n\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\n\nA_pinv = np.linalg.pinv(A)\nx_sol = A_pinv @ b\nprint(\"\\n Solution by using pseudoinverse: \\n\", x_sol)\n\n\nx_1 = np.linspace(-3, 3, 100)\nx_2_1 = x_1\nx_2_2 = 3. - 2. * x_1\nx_2_3 =(2. - x_1) / 3\n\nplt.plot(x_1, x_2_1)\nplt.plot(x_1, x_2_2)\nplt.plot(x_1, x_2_3)\nplt.scatter(x_sol[0], x_sol[1], c='r')\n\nplt.xlim(0., 2.)\nplt.ylim(-0.5, 2.)\nplt.show()\n\n\n\nQR Decomposition for Solving Least Squares Problem\nIf $ A $ has full column rank, a QR decomposition exists for $ A $:\n A = QR \nThis allows us to rewrite the normal equations as:\n R^T Q^T Q R x = R^T Q^T b \nSince $ Q^T Q = I $, this further simplifies to:\n R x = Q^T b. \n\n# QR Decomposition Method\ndef leastsq_qr(A, b):\n    Q, R = scipy.linalg.qr(A, mode='economic')\n    return scipy.linalg.solve_triangular(R, Q.T @ b)\n\n\n\nLinear Regression\n\ndef SyntheticData(x, corr_col=False, noise=0.1, num_points=100):\n    A = np.random.randn(num_points, len(x) - 1)\n    A = np.hstack((A, np.ones((num_points, 1))))\n    if corr_col and len(x) &gt; 2:\n        A[:, 2] = A[:, 1] + np.random.rand(num_points) * noise * 1e-4\n    noise = np.random.randn(num_points, 1) * noise\n    b = A @ x.reshape((-1, 1)) + noise\n    return A, b\n\n\nx_true = np.array([2.0, 1.0])\n\nA, b = SyntheticData(x_true, num_points=1000)\n\nx_chol = leastsq_chol(A, b)\nx_pinv = leastsq_pinv(A, b)\nx_svd = leastsq_pinv_svd(A, b)\nx_qr = leastsq_qr(A, b)\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\nprint(\"True coefficients:\\n\", x_true)\nprint(\"Cholesky solution:\\n\", x_chol.flatten())\nprint(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\nprint(\"SVD solution:\\n\", x_svd.flatten())\nprint(\"QR solution:\\n\", x_qr.flatten())\n\nplt.figure(figsize=(10, 6))\nplt.scatter(A[:, 0], b, label='Data Points', color='blue', alpha=0.5)\n\nplt.plot(A[:, 0], A @ x_true, label='True Line', color='green')\nplt.plot(A[:, 0], A @ x_chol, label='Cholesky Fit', color='red')\nplt.plot(A[:, 0], A @ x_pinv, label='Pseudo-inverse Fit', color='purple')\nplt.plot(A[:, 0], A @ x_svd, label='SVD Fit', color='orange')\nplt.plot(A[:, 0], A @ x_qr, label='QR Fit', color='cyan')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('1D Linear Regression')\nplt.show()\n\nFinally, let’s consider the case where $ A $ does not have full column rank.\n\nx_true = np.array([2.0, 3.0, -1.0, 1.0])\n\nA, b = SyntheticData(x_true, corr_col=True, num_points=1000)\n\nx_chol = leastsq_chol(A, b)\nx_pinv = leastsq_pinv(A, b)\nx_svd = leastsq_pinv_svd(A, b)\nx_qr = leastsq_qr(A, b)\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\nprint(\"True coefficients:\\n\", x_true)\nprint(\"Cholesky solution:\\n\", x_chol.flatten())\nprint(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\nprint(\"SVD solution:\\n\", x_svd.flatten())\nprint(\"QR solution:\\n\", x_qr.flatten())\n\nIn such cases Tikhonov regularization is more effective for finding stable solutions.\nExercise Show that the problem\n \\min \\|x'\\|_2 \\quad \\text{s.t. } x' = \\arg\\min_x \\| Ax - b \\|_2^2 \nis equivalent to the following regularization problem:\n \\min_x \\| Ax - b \\|_2^2 + \\lambda \\|x\\|_2^2. \nNote that the analytical solution is available in this case as well:\n x^* = (A^T A + \\lambda I)^{-1}  X^T b.\nThe analytical solutions described earlier involve inverting the matrix A^T A (or A^T A + \\lambda I), which is computationally expensive. This brings us to iterative methods, which are generally more efficient and have become the primary approach for numerous applications.\nGradient descent is one of the most widely used optimization methods. It is important to note that the objective function (e.g., the loss function, which is the residual in our case: \\mathcal{L}(x) = \\|Ax - b\\|_2^2) should be differentiable with respect to the unknown $ x $. Using gradient descent, the weight vector at each step can be updated as follows:\n x^{k+1} = x^k - \\beta_k \\nabla \\mathcal{L}(x^k) \nExercise Compute the gradient of \\mathcal{L}_{\\lambda}(x) = \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2.\nTo further improve efficiency, one could use stochastic gradient descent (SGD), which computes the gradient over a randomly selected subset of data points, rather than the full dataset A.\nThese ideas will be explored further in the second homework assignment… stay tuned!"
  },
  {
    "objectID": "seminars/seminar-11/seminar-11.html",
    "href": "seminars/seminar-11/seminar-11.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport scipy.sparse\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nfrom sklearn.utils.extmath import randomized_svd\nThe best low-rank approximation can be computed by SVD.\nTheorem: Let r &lt; \\text{rank}(A), A_r = U_r \\Sigma_r V_r^*. Then\n\\min_{\\text{rank}(B)=r} \\|A - B\\|_2 = \\|A - A_r\\|_2 = \\sigma_{r+1}.\nThe same holds for \\|\\cdot\\|_F, but \\|A - A_r\\|_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{\\min (n,m)}^2}."
  },
  {
    "objectID": "seminars/seminar-11/seminar-11.html#low-rank-and-sparse-decomposition",
    "href": "seminars/seminar-11/seminar-11.html#low-rank-and-sparse-decomposition",
    "title": "",
    "section": "Low-rank and sparse decomposition",
    "text": "Low-rank and sparse decomposition\nA_r = U_r \\Sigma_r V_r^* S = A - A_r Ax = A_r x + Sx = U_r \\Sigma_r V_r^*x + Sx\nFor A: n \\times n and rank truncation r&lt;n:\nComplexity of Ax: \\mathcal{O}(n^2)\nComplexity of A_rx: \\mathcal{O}(nr)\nComplexity of Sx: \\mathcal{O}(nnz(S))\nIt becomes effective with:\nr&lt;&lt;n\nnnz(S)&lt;&lt;n^2\n\ndef decompose_matrix_with_sparse_correction_optimized(A, rank, threshold=1e-3):\n    U, sigma, Vt = randomized_svd(A, n_components=rank, n_iter=5, random_state=None)\n    U_r = U[:, :rank]\n    Sigma_r = sigma[:rank]\n    Vt_r = Vt[:rank, :]\n    B = (U_r * Sigma_r) @ Vt_r\n    S_dense = A - B\n    S_dense[np.abs(S_dense) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_dense)\n    return U_r, Sigma_r, Vt_r, S_sparse\n\ndef optimized_multiply(U, Sigma, Vt, S, x):\n    temp = Vt @ x\n    temp = Sigma * temp\n    Bx = U @ temp\n    Sx = S @ x\n    return Bx + Sx\n\n\n# Parameters\nsizes = np.arange(200, 4000, 200)\nthreshold = 0.6\nmax_rank = 0.05\n\n\nmses = []\nexact_times = []\napprox_times = []\nsvd_times = []\n\n\nfor i, n in enumerate(sizes):\n    A = np.random.rand(n, n)\n    x = np.random.rand(n)\n    rank = int(n*max_rank)\n    start_time = time.time()\n    U_r, Sigma_r, Vt_r, S_sparse = decompose_matrix_with_sparse_correction_optimized(A, rank, threshold)\n\n    svd_times.append(time.time() - start_time)\n\n    # Measure time for exact multiplication\n    start_time = time.time()\n    exact_result = A @ x\n    exact_times.append(time.time() - start_time)\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(sizes),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(sizes, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Size Grows\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(sizes, exact_times, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(sizes, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nn = 3000\nthreshold = 0.5\nmax_ranks = np.arange(0.0, 1.0, 0.05)\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\n\nU, Sigma, Vt, S = decompose_matrix_with_sparse_correction_optimized(A, n, 0)\n\n\n\nfor i, max_rank in enumerate(max_ranks):\n    rank = max(int(n*max_rank), 1)\n    U_r = U[:, :rank]\n    Sigma_r = Sigma[:rank]\n    Vt_r = Vt[:rank, :]\n    B = (U_r * Sigma_r) @ Vt_r\n    S_dense = A - B\n    S_dense[np.abs(S_dense) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_dense)\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(max_ranks, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Rank is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(max_ranks, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(max_ranks, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nn = 3200\nthresholds = np.arange(0.0, 1.0, 0.05)\nrank = 200\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\nU_r, Sigma_r, Vt_r, S = decompose_matrix_with_sparse_correction_optimized(A, rank, 0)\n\nfor i, threshold in enumerate(thresholds):\n    S_sparse = S.toarray()\n    S_sparse[np.abs(S_sparse) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_sparse)\n\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Density is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(thresholds, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\nU_r, Sigma_r, Vt_r, S = decompose_matrix_with_sparse_correction_optimized(A, rank, 0)\n\nfor i, threshold in enumerate(thresholds):\n    S_sparse = S.toarray()\n    S_sparse[np.abs(S_sparse) &lt; threshold] = 0\n    #S_sparse = scipy.sparse.csr_matrix(S_sparse)\n\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks))\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Density is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(thresholds, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html",
    "href": "seminars/seminar-1/seminar-1.html",
    "title": "",
    "section": "",
    "text": "Fixed vs Floating point, vector norms, and stability concepts"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#fixed-point",
    "href": "seminars/seminar-1/seminar-1.html#fixed-point",
    "title": "",
    "section": "Fixed point",
    "text": "Fixed point\nFixed point contains a 1-bit sign, m-bits integer, and n-bits fractional part: \n\\text{decimal} =\n(-1)^{\\text{sign}} \\times\n\\Big(\n\\sum_{i=0}^{m-1} \\text{integer}[i] \\cdot base^{m-1-i} +\n\\sum_{i=0}^{n-1} \\text{fractional}[i] \\cdot base^{-i-1}\n\\Big)\n\n\nrange [-2^m + 2^{-n}, 2^m - 2^{-n}]\nresolution 2^{-n}\ntotal storage is m + n + 1 bits\n\n\n\ndef binary_fixed_point_to_decimal(x, m=8, n=8):\n    \"\"\"\n    x - binary string of size 1 + m + n\n    m - size of an integer part\n    n - sze of a fractional part\n    \"\"\"\n    sign_part, integer_part, fractional_part = x[0], x[1:m+1], x[m+1:m+n+1]\n    sign_value = (-1) ** int(sign_part)\n    integer_value = sum([\n        int(v) * 2 ** i\n        for i, v in enumerate(integer_part[::-1])\n    ])\n    fractional_value = sum([\n        int(v) * 2 ** -(i + 1)\n        for i, v in enumerate(fractional_part)\n    ])\n    return sign_value * (integer_value + fractional_value)\n\nm, n = 8, 8\nx = '00000010100100000'\nprint(binary_fixed_point_to_decimal(x, m, n) == 5.125)\n\nTrue\n\n\n\nx = '11111111111111111' # Insert a string corresponding to a minimal possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == -(2 ** m - 2 ** (-n)))\n\nTrue\n\n\n\nx = '01111111111111111' # Insert a string corresponding to a maximal possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == 2 ** m - 2 ** (-n))\n\nTrue\n\n\n\nx = '00000000000000001' # Insert a string corresponding to an absolute minimal but nonzero possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == 2 ** (-n))\n\nTrue\n\n\n\nFloating point\nFloating point contains a 1-bit sign, m-bits exponent, and n-bits mantissa part:\n\n\\text{decimal} =\n(-1)^{\\text{sign}} \\times\nbase^{\\Big(\\sum_{i=0}^{m-1} \\text{exponent}[i] \\cdot base^{m-1-i} - (2^{m-1} - 1)\\Big)}\n\\times\n\\Big(1 + \\sum_{i=0}^{n-1} \\text{mantissa}[i] \\cdot base^{-i-1}\\Big)\n\n\nexponent values that are all 0 and all 1 are reserved for special numbers: NaN, infinity, etc.\ntotal storage is m + n + 1 bits\n\n\nHalf (float16) vs Single (float32) vs and Double (float32) Precision\n\nfloat16 - 16 bit total: 1 for a sign, m = 5 for exponent and n = 10 for mantissa\nfloat32 - 32 bits total: 1 for a sign, m = 8 for exponent and n = 23 for mantissa\nfloat64 - 64 bits total: 1 for a sign, m = 11 for exponent and n = 52 for mantissa\n\n\ndef binary_floating_point_to_decimal(x, m=8, n=23):\n    \"\"\"\n    x - binary string of size 1 + m + n\n    m - size of an exponent part\n    n - sze of a mantissa part\n    \"\"\"\n    sign_part, exponent_part, mantissa_part = x[0], x[1:m+1], x[m+1:n+m+1]\n    sign_value = (-1) ** int(sign_part)\n\n    mantissa_value = 1\n    for i, v in enumerate(mantissa_part):\n        mantissa_value += int(v) * (2 ** -(i + 1))\n\n    exponent_value = 0\n    for i, v in enumerate(exponent_part):\n        exponent_value += int(v) * 2 ** i\n    exponent_value -= (2 ** (m - 1) - 1)\n        \n    return sign_value * (2 ** exponent_value) * mantissa_value\n\nm, n = 8, 23\nx = '01000000101001000000000000000000'\nprint(binary_floating_point_to_decimal(x, m, n) == 5.125)\n\nTrue\n\n\n\n\nRounding Errors\nDue to the fact that float representations are only approximations to real numbers, some errors may occur.\nFor example, let’s consider a simple summation algorithm, where x_i are floating point numbers:\n\nf(x) = x_1 + x_2 + ... + x_n\n\nRealize a naïve algorithm from the lecture (add one-by-one) and check out the occuring error.\n[!] Set n as 1000 and all x_i as 0.1.\n\ntotal = 0.0\nfor _ in range(1000):\n    total += 0.1\n\nprint(\"Expected result: 100.0\")\nprint(f\"Actual result:\", total)\n\nExpected result: 100.0\nActual result: 99.9999999999986\n\n\nRealize a Kahan algorithm from the lecture and check out the occuring error.\n[!] Set n as 1000 and all x_i as 0.1.\n\ns = 0\nc = 0\nfor i in range(1000):\n    y = 0.1 - c\n    t = s + y\n    c = (t - s) - y\n    s = t\n\nprint(\"Expected result: 100.0\")\nprint(f\"Actual result:\", s)\n\nExpected result: 100.0\nActual result: 100.0\n\n\nExplanation: the value 0.1 cannot be represented precisely in binary so it becomes an approximation. When this approximation is added repeatedly, the small rounding errors accumulate, leading to a final result slightly less than 100.0.\n\n\nVectors and vector norms\nIn NLA we typically work not with numbers, but with vectors that are simply arrays of numbers of size n.\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])\n\nprint(f'Size of the x vector is {len(x)}')\nprint(f'Type of the vector elements is {type(x[0])}')\n\nSize of the x vector is 5\nType of the vector elements is &lt;class 'numpy.int64'&gt;\n\n\nAs you can see, this vector contains only integer values. Now convert them into float32 type.\n\nx = x.astype(np.float32)\nprint(f'Type of the vector elements is {type(x[0])}')\n\nType of the vector elements is &lt;class 'numpy.float32'&gt;\n\n\nIn order to measure smallness of a vector its norm \\|x\\| is used. The most important class is p-norms: \n\\|x\\|_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}\n Examples of p-norms: - Manhattan distance or L_1 norm - when p=1 - Euclidean norm or L_2 norm - when p=2 - Infinity norm, or Chebyshev norm - when p=+\\infty: $ |x|_{} = _i | x_i|$\nCompute norms for the x vector:\nHint: use np.linalg.norm\n\nprint('L1 norm:', np.linalg.norm(x, 1))\nprint('L2 norm:', np.linalg.norm(x, 2))\nprint('Chebyshev norm:', np.linalg.norm(x, np.inf))\n\nL1 norm: 15.0\nL2 norm: 7.4161983\nChebyshev norm: 5.0\n\n\nA unit disk for a p-norm is a set of point such that \\|x\\|_p = 1.\nVisualize p-norm unit disk for the following p-norms: p \\in (0.25, 0.75, 1.0, 2.0, 5.0, \\infty)\nHint: y = \\pm (1 - |x|^p)^{1/p}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef unit_disk(p):\n    x = np.linspace(-1, 1, 201)\n    y = (1 - np.abs(x) ** p) ** (1 / p)\n    x = np.hstack([x, x[1:][::-1], x[0]])\n    y = np.hstack([y, -y[1:][::-1], y[0]])\n    return x, y\n\nplt.figure(figsize=(4, 4))\nplt.axis('equal')\nfor p in (0.25, 0.5, 1.0, 2.0, 5.0, np.inf):\n    x, y = unit_disk(p)\n    plt.plot(x, y, label=f'$p$={p}')\nplt.legend(loc=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStability\nSuppose we have a vector x, function f(x), and an algorithm \\text{alg}(x) to approximate the function. Then the algorithm is called forward stable, if for some small \\varepsilon\n\n\\|\\text{alg}(x) - f(x)\\|  \\leq \\varepsilon\n\n[Task] Check the summation algorithms mentioned before (naive and Kahan) to be forward stable.\nSet each x_i as 0.1 againg and n as 100. \nf(x) = \\sum_{i=1}^{100} x_i, \\;\\;\nx_i = 0.1\n Record the error occuring in each step of summation: \n\\text{error}[i] = |0.1 \\cdot i - \\text{alg}(x)|\n\n\nN = 100\n\n# Naive\ntotal = 0.0\nerror_naive = []\nfor i in range(N):\n    total += 0.1\n    refer = (i + 1) / 10\n    error_naive.append(np.abs(refer - total))\n\n# Kahan\ns = 0\nc = 0\nerror_kahan = []\nfor i in range(N):\n    y = 0.1 - c\n    t = s + y\n    c = (t - s) - y\n    s = t\n    error_kahan.append(np.abs(c))\n\nplt.figure(figsize=(8, 4))\nplt.title(r'Forward stability of summation algorithms $\\varepsilon(n)$')\nplt.plot(error_naive, label='Naive')\nplt.plot(error_kahan, label='Kahan')\nplt.ylabel(r'$\\varepsilon$', rotation=0)\nplt.xlabel(r'$n$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWhat do you see?"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7 (unsolved).html",
    "href": "seminars/seminar-7/seminar-7 (unsolved).html",
    "title": "Recall determinant definition",
    "section": "",
    "text": "import numpy as np\nimport itertools\nfrom scipy.linalg import lu\nimport time\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7 (unsolved).html#determinant-of-triangular-matrix",
    "href": "seminars/seminar-7/seminar-7 (unsolved).html#determinant-of-triangular-matrix",
    "title": "Recall determinant definition",
    "section": "Determinant of triangular matrix",
    "text": "Determinant of triangular matrix\nDeterminant of an upper-(lower-)triangular matrix M is equal to \\prod_{i=1}^{n}M_{i,i}.\nProof: row expansion"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Здесь будет размещена информация о проектах, которые студенты выполняют в рамках курса."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "№\nТема лекции\nТема семинара\n\n\n\n\n1\nАрифметика чисел с плавающей запятой\nЧисла с плавающей запятой, число обусловленности\n\n\n2\nНормы матриц и унитарные матрицы\nИерархия памяти, умножение матриц, алгоритм Штрассена\n\n\n3\nРанг матрицы, скелетное разложение, SVD\nУдаление шума в изображениях с помощью SVD. Студенты экспериментируют с удалением шума из небольшого градиентного изображения путем усечения сингулярных значений\n\n\n4\nЛинейные системы\nГрадиентный спуск\n\n\n5\nСобственные значения и собственные векторы\nPageRank\n\n\n6\nОбзор разложений матриц. Вычисление QR-разложения и разложения Шура\nГрам-Шмидт\n\n\n7\nПроблема собственных значений симметричных матриц и SVD\n\n\n\n8\nРандомизированная линейная алгебра\nРандомизированное SVD, рандомизированное умножение матриц\n\n\n9\nОт плотной к разреженной линейной алгебре\nПрактика с разреженными линейными системами\n\n\n10\nВведение в итерационные методы\nГрадиентный спуск + многочлены Чебышёва + метод тяжелого шарика\n\n\n11\nВеликие итерационные методы\nГрам-Шмидт + идея метода сопряженных градиентов\n\n\n12\nИтерационные методы и предобуславливатели\nShampoo\n\n\n13\nСтруктурированные матрицы, БПФ, свертки, матрицы Тёплица\nПрактика с матрично-векторными операциями"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Вычислительная линейная алгебра",
    "section": "",
    "text": "Вычислительная линейная алгебра\n\nКурс для 1-го курса бакалавриата ФПМИ МФТИ в рамках программы “AI360”. 1 лекция + 1 семинар в неделю.\n\nYour browser does not support the video tag.\n\n\nTeam\n\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Иван Валерьевич Оселедец\n                    \n                    Лектор\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Максимович Меркулов\n                    \n                    Семинарист\n                  \n                \n              \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7.html",
    "href": "seminars/seminar-7/seminar-7.html",
    "title": "Recall determinant definition",
    "section": "",
    "text": "import numpy as np\nimport itertools\nfrom scipy.linalg import lu\nimport time\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7.html#determinant-of-triangular-matrix",
    "href": "seminars/seminar-7/seminar-7.html#determinant-of-triangular-matrix",
    "title": "Recall determinant definition",
    "section": "Determinant of triangular matrix",
    "text": "Determinant of triangular matrix\nDeterminant of an upper-(lower-)triangular matrix M is equal to \\prod_{i=1}^{n}M_{i,i}.\nProof: row expansion"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html",
    "href": "seminars/seminar-4/seminar-4.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#four-fundamental-subspaces",
    "href": "seminars/seminar-4/seminar-4.html#four-fundamental-subspaces",
    "title": "",
    "section": "Four Fundamental Subspaces",
    "text": "Four Fundamental Subspaces\nLet A = \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix}\n\nFind the column space \\text{col}(A) and null space \\text{null}(A) of A.\nDetermine if \\vec{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\in \\text{col}(A).\nDetermine if \\vec{u} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\text{null}(A).\n\n\ndef swap(matrix, row1, row2):\n    \"\"\"Swap two rows in a matrix.\"\"\"\n    matrix[[row1, row2]] = matrix[[row2, row1]]\n    return matrix\n\ndef scale(matrix, row, scalar):\n    \"\"\"Multiply all entries in a specified row by a scalar.\"\"\"\n    matrix[row] *= scalar\n    return matrix\n\ndef replace(matrix, row1, row2, scalar):\n    \"\"\"Replace a row by itself plus a scalar multiple of another row.\"\"\"\n    matrix[row1] += scalar * matrix[row2]\n    return matrix\n\n\nM = np.array([[1, -1, 0], [2, 1, 0], [-1, 3, 0]], dtype=float)\n\n\nM1 = replace(M.copy(), 1, 0, -2)\nM2 = replace(M1, 2, 0, 1)\nM3 = scale(M2, 1, 1/3)\nM4 = replace(M3, 2, 1, -2)\n\nprint(\"Reduced Matrix M4:\")\nprint(M4)\n\n\nM_augm = np.array([[1, -1, 1], [2, 1, 2], [-1, 3, 1]], dtype=float)\nM\n\n\n# Row Reduction Example\nM1 = replace(M_augm.copy(), 1, 0, -2)\nM2 = replace(M1, 2, 0, 1)\nM3 = scale(M2, 1, 1/3)\nM4 = replace(M3, 2, 1, -2)\nM4\n\nA = \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & -1 \\\\ 0 & 3 \\\\ 0 & 2 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \n\\mathrm{im}(A) = \\mathrm{col}(A) = \\mathrm{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 3 \\end{bmatrix} \\right\\}\n\\mathrm{im}(A^T) =  \\mathrm{col}(A^T) = \\mathrm{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\right\\}\n\\mathrm{ker}(A) = \\mathrm{null}(A) = \\{ 0 \\}\n\\mathrm{ker}(A^T) = \\mathrm{null}(A^T) =  \\mathrm{span}\\left\\{ \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right\\}"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#skeleton-decomposition",
    "href": "seminars/seminar-4/seminar-4.html#skeleton-decomposition",
    "title": "",
    "section": "Skeleton decomposition",
    "text": "Skeleton decomposition\nA = \\sum_{\\alpha = 1}^r U_{\\alpha} V_{\\alpha}^T\n##Low-rank Approximation\n\nimg = Image.open('./sk_campus_img.jpg').convert('L')\nimg_array = np.array(img)\noriginal_shape = img_array.shape\n\nplt.figure(figsize=(8, 4))\nplt.imshow(img_array, cmap='gray')\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n\nU, S, Vt = np.linalg.svd(img_array, full_matrices=False)  # economy SVD\nU.shape, S.shape, Vt.shape\n\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.semilogy(S / S[0])\nplt.xlabel(r\"$k$\")\nplt.ylabel(r\"$\\sigma_k / \\sigma_1$\")\nplt.title(r\"$\\sigma_k / \\sigma_1$\")\nplt.grid()\n\n\ncumulative_energy = np.cumsum(S**2) / np.sum(S**2)\nplt.subplot(1, 2, 2)\nplt.plot(cumulative_energy)\nplt.xlabel(r\"$k$\")\nplt.ylabel(r\"Cumulative Energy\")\nplt.title(r\"$(\\sum_{i=1}^k \\sigma_i) / \\sum_{i=0}^n \\sigma_i)$\")\nplt.grid()\n\nplt.show()\n\n\ndef reconstruct_image(k):\n    return (U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :])\n\n\nranks = [5, 20, 50, 100, original_shape[1]]\nplt.figure(figsize=(15, 5))\n\nfor i, rank in enumerate(ranks, 1):\n    plt.subplot(1, len(ranks), i)\n    recon_img = reconstruct_image(rank)\n    plt.imshow(recon_img, cmap='gray')\n    plt.title(f'Rank {rank}') if rank!= original_shape[1] else plt.title(f'Original Image')\n    plt.axis('off')\n\nplt.suptitle(\"Low-Rank Approximations of Image\")\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#svd-and-its-applications",
    "href": "seminars/seminar-4/seminar-4.html#svd-and-its-applications",
    "title": "",
    "section": "SVD and its Applications",
    "text": "SVD and its Applications\nSingular Value Decomposition (SVD) is a versatile tool in numerical linear algebra, implemented in many programming languages, typically relying on the LAPACK (Linear Algebra Package) library written in Fortran for its underlying computations.\n\nGeometric interpretation\n\ndef plot_transformed_circle_and_vectors(A, plot_singular_vectors=False, singular_values=None, singular_vectors=None,\n                                        circle_color='black', vector_colors=['blue', 'deeppink'],\n                                        singular_vector_colors=['red', 'green'],\n                                        singular_labels=[r'$\\sigma_1 u_1$', r'$\\sigma_2 u_2$'],\n                                        label_offset=0.2, xlim=(-8, 8), ylim=(-8, 8)):\n    theta = np.linspace(0, 2 * np.pi, 300)\n    unit_circle = np.vstack((np.cos(theta), np.sin(theta)))\n\n    transformed_circle = A @ unit_circle\n\n    plt.plot(transformed_circle[0, :], transformed_circle[1, :], color=circle_color, alpha=0.5)\n\n    e1_transformed = A @ np.array([1, 0])\n    e2_transformed = A @ np.array([0, 1])\n\n    for i, vec in enumerate([e1_transformed, e2_transformed]):\n        color = vector_colors[i]\n        plt.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color=color)\n\n    if plot_singular_vectors and singular_values is not None and singular_vectors is not None:\n        for i, (sigma, vec) in enumerate(zip(singular_values, singular_vectors.T)):\n            vec_scaled = sigma * vec\n            color = singular_vector_colors[i]\n            label = singular_labels[i]\n            plt.quiver(0, 0, vec_scaled[0], vec_scaled[1], angles='xy', scale_units='xy', scale=1, color=color)\n            plt.text(vec_scaled[0] * (1 + label_offset), vec_scaled[1] * (1 + label_offset), label, color=color, fontsize=12)\n\n    plt.axvline(x=0, color='black', lw=1)\n    plt.axhline(y=0, color='black', lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n\n\nA = np.array([[1, 3], [4, 5]])\n\nU, D, Vt = np.linalg.svd(A)\n\nprint('Unit circle (before transformation):')\nplot_transformed_circle_and_vectors(np.eye(2), xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\nplt.show()\n\nprint('1st rotation by V (right singular vectors):')\nplot_transformed_circle_and_vectors(Vt.T, xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\nplt.show()\n\nprint('Scaling by D:')\nscaling_matrix = np.diag(D) @ Vt\nplot_transformed_circle_and_vectors(scaling_matrix, xlim=(-8, 8), ylim=(-8, 8))\nplt.show()\n\nprint('2nd rotation by U (final transformation by A):')\nfinal_transformation = U @ np.diag(D) @ Vt\nplot_transformed_circle_and_vectors(final_transformation, xlim=(-8, 8), ylim=(-8, 8))\nplt.show()\n\n\nA = np.array([[1, 3], [4, 5]])\n\nU, D, Vt = np.linalg.svd(A)\n\nprint(\"Transformed unit circle, basis vectors, and singular vectors:\")\nplot_transformed_circle_and_vectors(A,\n                                    plot_singular_vectors=True,\n                                    singular_values=D,\n                                    singular_vectors=U,\n                                    singular_labels=[r'$\\sigma_1 u_1$', r'$\\sigma_2 u_2$'])\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#applications-in-image-reconstruction",
    "href": "seminars/seminar-4/seminar-4.html#applications-in-image-reconstruction",
    "title": "",
    "section": "Applications in Image Reconstruction",
    "text": "Applications in Image Reconstruction\nMatrix completion, commonly used for filling missing data, can be applied to image and recommendation systems. A well-known example is movie recommendation systems, where a ratings matrix is often only partially filled, as users have not rated every movie. To provide accurate recommendations, we aim to predict these missing ratings.\nThis task is feasible because user ratings tend to follow patterns, meaning the ratings matrix is often low-rank; only a limited amount of information is needed to approximate it well.\nA similar approach applies to images, where pixel values often depend on neighboring pixels, making low-rank approximations effective for reconstructing images with missing or corrupted data."
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#svd-in-facial-recognition-eigenfaces",
    "href": "seminars/seminar-4/seminar-4.html#svd-in-facial-recognition-eigenfaces",
    "title": "",
    "section": "SVD in Facial Recognition: Eigenfaces",
    "text": "SVD in Facial Recognition: Eigenfaces\nThe “Eigenfaces for Recognition” paper introduced a novel approach to facial recognition. Unlike earlier methods that focused on detecting individual features (e.g., eyes or nose), Eigenfaces uses SVD to extract and encode essential information from face images. This encoding allows for efficient comparisons between faces by compressing the most relevant facial information into a low-dimensional representation. This method paved the way for data-driven approaches in face recognition, relying on similarities within the encoded space rather than feature-by-feature comparison.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\n\nfrom sklearn.datasets import fetch_lfw_people\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom sklearn.datasets import fetch_lfw_people\n\n\nlfw_dataset = fetch_lfw_people(min_faces_per_person=100)\nX, y, target_names = lfw_dataset.data, lfw_dataset.target, lfw_dataset.target_names\nh, w = lfw_dataset.images.shape[1:3]\nprint(f\"Number of samples: {X.shape[0]}, Image size: {h}x{w}, Unique classes: {len(target_names)}\")\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n\nU, S, VT = np.linalg.svd(X_train, full_matrices=False)\n\nnum_components = 100\nface_space = VT[:num_components, :]\n\nX_train_transformed = X_train @ face_space.T\nX_test_transformed = X_test @ face_space.T\n\nplt.figure(figsize=(8, 4))\nplt.semilogy(np.arange(len(S)), S / S[0], marker=\"o\")\nplt.title(\"$\\sigma_k / \\sigma_1$\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"$\\sigma_k / \\sigma_1$\")\nplt.grid()\nplt.show()\n\n\ndef plot_reconstructed_images(original, transformed, face_space, h, w, index=0):\n    reconstructed = transformed[index] @ face_space\n    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n    ax[0].imshow(original[index].reshape(h, w), cmap=\"gray\")\n    ax[0].set_title(\"Original Image\")\n    ax[1].imshow(reconstructed.reshape(h, w), cmap=\"gray\")\n    ax[1].set_title(\"Reconstructed Image\")\n    plt.show()\n\nplot_reconstructed_images(X_train, X_train_transformed, face_space, h, w, index=0)\n\nclf_knn = KNeighborsClassifier().fit(X_train_transformed, y_train)\nclf_mlp = MLPClassifier(hidden_layer_sizes=(1024,), batch_size=256, early_stopping=True).fit(X_train_transformed, y_train)\n\ny_pred_knn = clf_knn.predict(X_test_transformed)\ny_pred_mlp = clf_mlp.predict(X_test_transformed)\nprint(\"k-NN Classifier Report:\\n\", classification_report(y_test, y_pred_knn, target_names=target_names))\nprint(\"MLP Classifier Report:\\n\", classification_report(y_test, y_pred_mlp, target_names=target_names))\n\nIt might seem discouraging, but it’s worthwhile to check if the data is imbalanced and go through the steps again (exercise)."
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html",
    "href": "seminars/seminar-2/seminar-2.html",
    "title": "Matrix Norms Calculation",
    "section": "",
    "text": "Matrix norms, unitary matrices\nimport numpy as np"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#householder-martices",
    "href": "seminars/seminar-2/seminar-2.html#householder-martices",
    "title": "Matrix Norms Calculation",
    "section": "Householder Martices",
    "text": "Householder Martices\nHouseholder matrix is the matrix of the form: H \\equiv H(v) = I - 2 vv^*, where v is an n \\times 1 column and v^* v = 1.\nIt is also a reflection:  Hx = x - 2(v^* x) v\nAttention! If it does not work, remember about vector norm"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#build-your-own-from-a-vector",
    "href": "seminars/seminar-2/seminar-2.html#build-your-own-from-a-vector",
    "title": "Matrix Norms Calculation",
    "section": "Build your own from a vector",
    "text": "Build your own from a vector\n\ndef build_householder(v):\n    # v - vector of size n\n    v = v/np.linalg.norm(v)\n    a = np.identity(v.size)\n    \n    H = a - 2*np.outer(v, v)\n    \n    return H\n\n\nv = np.random.normal(size=(3))\nprint(v)\nh = build_householder(v)\nprint(h)"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#see-how-it-reflects-vectors",
    "href": "seminars/seminar-2/seminar-2.html#see-how-it-reflects-vectors",
    "title": "Matrix Norms Calculation",
    "section": "See how it reflects vectors",
    "text": "See how it reflects vectors\n\nv = np.random.normal(size=(3))\nh = build_householder(v)\n\nx = np.random.normal(size=(3))\nprint(x)\nprint(np.matmul(h, x))\n\n\nv = np.array([0,  1,  -1])\nh = build_householder(v)\nx = np.array([0,  1,  0])\nprint(np.round(h , decimals=2))\nprint(x)\nprint(np.round(h @ x.T, decimals=2))"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#optional-check-that-it-indeed-is-also-a-reflection",
    "href": "seminars/seminar-2/seminar-2.html#optional-check-that-it-indeed-is-also-a-reflection",
    "title": "Matrix Norms Calculation",
    "section": "Optional: check that it indeed is also a reflection",
    "text": "Optional: check that it indeed is also a reflection\n Hx = x - 2(v^* x) v\n\nv = np.random.normal(size=(3))\nv = v/np.linalg.norm(v)\nh = build_householder(v)\n\nx = np.random.normal(size=(3))\n\nv = np.array([0,  1,  -1])\nv = v/np.linalg.norm(v)\nh = build_householder(v)\nx = np.array([0,  1,  0])\n\nhx = h @ x.T\n\nreflected =  x - 2 * np.dot(v.T,x) * v\n\nprint(\"initial vector: \", x)\nprint(\"transofrmed by matrix: \", hx)\nprint(\"reflected by vector: \", reflected)"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#check-unitarity",
    "href": "seminars/seminar-2/seminar-2.html#check-unitarity",
    "title": "Matrix Norms Calculation",
    "section": "Check unitarity",
    "text": "Check unitarity\n\nUse numpy tools to check if the matrix is unitary using formula U* x U = I\n\nn = 3\n\n\nv = np.random.normal(size=(n))\nh = build_householder(v)\n\nprint(np.round(h @ h, decimals=2))\n\n\ndef householder_transform(A):\n    \"\"\"\n    Transforms the matrix A into an upper triangular matrix using Householder reflections.\n    \n    Parameters:\n        A (numpy.ndarray): The matrix to be transformed.\n    \n    Returns:\n        R (numpy.ndarray): The upper triangular matrix after applying Householder transformations.\n    \"\"\"\n    A = A.copy()\n    m, n = A.shape\n    for j in range(min(m, n)):\n        # Create the vector x for the current column\n        x = A[j:, j]\n        \n        # Calculate the norm of x and the Householder vector v\n        norm_x = np.linalg.norm(x)\n        if norm_x == 0:\n            continue\n        sign = -1 if x[0] &lt; 0 else 1\n        v = x.copy()\n        v[0] += sign * norm_x  # Adjust the first element of v for the reflection\n        v /= np.linalg.norm(v)  # Normalize v\n        \n        # Apply the Householder transformation to A[j:, j:]\n        A[j:, j:] -= 2 * np.outer(v, v @ A[j:, j:])\n    \n    return A\n\n# Example matrix\nA = np.array([\n    [4, 1, -2, 2],\n    [1, 2, 0, 1],\n    [-2, 0, 3, -2],\n    [2, 1, -2, -1]\n], dtype=float)\n\nR = householder_transform(A)\nprint(\"Upper triangular matrix R:\\n\", R)\n\n\n\nBonus task: check that it also preserves the norm. You can check it for your own custom norm if you created one!\n \\frac{\\Vert x - \\widehat{x} \\Vert}{\\Vert x \\Vert} \\leq \\varepsilon. \n \\frac{\\Vert y - \\widehat{y} \\Vert}{\\Vert y \\Vert } = \\frac{\\Vert U ( x - \\widehat{x}) \\Vert}{\\Vert U  x\\Vert}  \\leq \\varepsilon. \n\nv = np.random.normal(size=(n))\nv = v/np.linalg.norm(v)\nh = build_householder(v)\nepsilon = 0.001\n\nx = np.random.normal(size=(n))\nx_hat = x + epsilon * np.random.normal(size=(n)) # approximaton of x\ny = x - x_hat                                    # error of approximation\n\ninitial_error = np.linalg.norm(y)/np.linalg.norm(x)\ntransformed_error = np.linalg.norm(h @ y.T)/np.linalg.norm(h @ x.T)\n        \nprint(\"initial error:     \", initial_error)\nprint(\"transformed error: \", transformed_error)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html",
    "href": "lectures/lecture-8/lecture-8.html",
    "title": "Questions?",
    "section": "",
    "text": "QR decomposition and Gram-Schmidt algorithm\nSchur decomposition and QR-algorithm (basic)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#recap-of-the-previous-part",
    "href": "lectures/lecture-8/lecture-8.html#recap-of-the-previous-part",
    "title": "Questions?",
    "section": "",
    "text": "QR decomposition and Gram-Schmidt algorithm\nSchur decomposition and QR-algorithm (basic)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#plan-for-today",
    "href": "lectures/lecture-8/lecture-8.html#plan-for-today",
    "title": "Questions?",
    "section": "Plan for today",
    "text": "Plan for today\nToday we will talk about:\n\nAlgorithms for the symmetric eigenvalue problems\n\nQR algorithm (in more details)\nDivide-and-Conquer\nbisection\n\nAlgorithms for SVD computation"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#schur-form-computation",
    "href": "lectures/lecture-8/lecture-8.html#schur-form-computation",
    "title": "Questions?",
    "section": "Schur form computation",
    "text": "Schur form computation\n\nRecall that we are trying to avoid \\mathcal{O}(n^3) complexity for each iteration.\nThe idea is to make a matrix have a simpler structure so that each step of QR algorithm becomes cheaper.\nIn case of a general matrix we can use the Hessenberg form."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#hessenberg-form",
    "href": "lectures/lecture-8/lecture-8.html#hessenberg-form",
    "title": "Questions?",
    "section": "Hessenberg form",
    "text": "Hessenberg form\nThe matrix A is said to be in the Hessenberg form, if\na_{ij} = 0, \\quad \\mbox{if } i \\geq j+2.\nH = \\begin{bmatrix} * & * & * & * & * \\\\ * & * & * & * & * \\\\ 0 & * & * & * & *\\\\ 0 & 0 & * & * & *\\\\ 0 & 0 & 0 & * & * \\\\ \\end{bmatrix}."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#reduction-any-matrix-to-hessenberg-form",
    "href": "lectures/lecture-8/lecture-8.html#reduction-any-matrix-to-hessenberg-form",
    "title": "Questions?",
    "section": "Reduction any matrix to Hessenberg form",
    "text": "Reduction any matrix to Hessenberg form\n\nBy applying Householder reflections we can reduce any matrix to the Hessenberg form\n\nU^* A U = H\n\nThe only difference with Schur decomposition is that we have to map the first column to the vector with two non-zeros, and the first element is not changed.\nThe computational cost of such reduction is \\mathcal{O}(n^3) operations.\nIn a Hessenberg form, computation of one iteration of the QR algorithm costs \\mathcal{O}(n^2) operations (e.g. using Givens rotations, how?), and the Hessenberg form is preserved by the QR iteration (check why)."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#symmetric-hermitian-case",
    "href": "lectures/lecture-8/lecture-8.html#symmetric-hermitian-case",
    "title": "Questions?",
    "section": "Symmetric (Hermitian) case",
    "text": "Symmetric (Hermitian) case\n\nIn the symmetric case, we have A = A^*, then H = H^* and the upper Hessenberg form becomes tridiagonal matrix.\nFrom now on we will talk about the case of symmetric tridiagonal form.\nAny symmetric (Hermitian) matrix can be reduced to the tridiagonal form by Householder reflections.\nKey point is that tridiagonal form is preserved by the QR algorithm, and the cost of one step can be reduced to \\mathcal{O}(n)!"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#qr-algorithm-iterations",
    "href": "lectures/lecture-8/lecture-8.html#qr-algorithm-iterations",
    "title": "Questions?",
    "section": "QR algorithm: iterations",
    "text": "QR algorithm: iterations\n\nThe iterations of the QR algorithm have the following form:\n\nA_k = Q_k R_k, \\quad A_{k+1} = R_k Q_k.\n\nIf A_0 = A is  tridiagonal symmetric matrix , this form is preserved by the QR algorithm.\n\nLet us see..\n\n%matplotlib inline\nimport jax.numpy as jnp\nimport jax\nimport matplotlib.pyplot as plt\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n#Generate a random tridiagonal matrix\n\nn = 20\nd = jax.random.normal(jax.random.PRNGKey(0), (n, ))\nsub_diag = jax.random.normal(jax.random.PRNGKey(1), (n-1,))\n\nmat = jnp.diag(d) + jnp.diag(sub_diag, -1) + jnp.diag(sub_diag, 1)\nmat1 = jnp.abs(mat)\nmat1 = mat1/jnp.max(mat1.flatten())\nplt.spy(mat)\nq, r = jnp.linalg.qr(mat)\nplt.figure()\nb = r.dot(q)\n# b[abs(b) &lt;= 1e-12] = 0\nb = b.at[abs(b) &lt;= 1e-12].set(0.0)\n#b = jax.ops.index_update(b, jax.ops.index[abs(b) &lt;= 1e-12], 0)\nplt.spy(b)\n#plt.figure()\n#plt.imshow(np.abs(r.dot(q)))\nb[0, :]\n\nArray([-1.47168069, -0.75157939,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ],      dtype=float64)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#tridiagonal-form",
    "href": "lectures/lecture-8/lecture-8.html#tridiagonal-form",
    "title": "Questions?",
    "section": "Tridiagonal form",
    "text": "Tridiagonal form\n\nIn the tridiagonal form, you do not have to compute the Q matrix: you only have to compute the triadiagonal part that appears after the iterations\n\nA_k = Q_k R_k, \\quad A_{k+1}  = R_k Q_k,\nin the case when A_k = A^*_k and is also tridiagonal.\n\nSuch matrix is defined by \\mathcal{O}(n) parameters; computation of the QR is more complicated, but it is possible to compute A_{k+1} directly without computing Q_k.\nThis is called implicit QR-step."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#theorem-on-implicit-qr-iteration",
    "href": "lectures/lecture-8/lecture-8.html#theorem-on-implicit-qr-iteration",
    "title": "Questions?",
    "section": "Theorem on implicit QR iteration",
    "text": "Theorem on implicit QR iteration\nAll the implicit QR algorithms are based on the following theorem:\nLet\nQ^* A Q = H\nbe an irreducible upper Hessenberg matrix. Then, the first column of the matrix Q defines all of its other columns. It can be found from the equation\nA Q = Q H."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#convergence-of-the-qr-algorithm",
    "href": "lectures/lecture-8/lecture-8.html#convergence-of-the-qr-algorithm",
    "title": "Questions?",
    "section": "Convergence of the QR-algorithm",
    "text": "Convergence of the QR-algorithm\n\nThe convergence of the QR-algorithm is a very delicate issue (see E. E. Tyrtyshnikov, “Brief introduction to numerical analysis” for details).\n\nSummary. If we have a decomposition of the form\nA = X \\Lambda X^{-1}, \\quad A = \\begin{bmatrix}A_{11} & A_{12} \\\\ A_{21} & A_{22}\\end{bmatrix}\nand\n\n\\Lambda = \\begin{bmatrix} \\Lambda_1 & 0 \\\\\n0 & \\Lambda_2 \\end{bmatrix}, \\quad \\lambda(\\Lambda_1)=\\{\\lambda_1,\\dots,\\lambda_m\\}, \\ \\lambda(\\Lambda_2)=\\{\\lambda_{m+1},\\dots,\\lambda_r\\},\n\nand there is a gap between the eigenvalues of \\Lambda_1 and \\Lambda_2 (|\\lambda_1|\\geq \\dots \\geq |\\lambda_m| &gt; |\\lambda_{m+1}| \\geq\\dots \\geq |\\lambda_r| &gt;0), then the A^{(k)}_{21} block of A_k in the QR-iteration goes to zero with\n\\Vert A^{(k)}_{21} \\Vert \\leq  C q^k, \\quad q = \\left| \\frac{\\lambda_{m+1}}{\\lambda_{m}}  \\right |,\nwhere m is the size of \\Lambda_1.\nSo we need to increase the gap! It can be done by the QR algorithm with shifts."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#qr-algorithm-with-shifts",
    "href": "lectures/lecture-8/lecture-8.html#qr-algorithm-with-shifts",
    "title": "Questions?",
    "section": "QR-algorithm with shifts",
    "text": "QR-algorithm with shifts\nA_{k} - s_k I = Q_k R_k, \\quad A_{k+1} = R_k Q_k + s_k I\nThe convergence rate for a shifted version is then\n\\left| \\frac{\\lambda_{m+1} - s_k}{\\lambda_{m} - s_k}  \\right |,\nwhere \\lambda_m is the m-th largest eigenvalue of the matrix in modulus. - If the shift is close to the eigenvalue, then the convergence speed is better. - There are different strategies to choose shifts. - Introducing shifts is a general strategy to improve convergence of iterative methods of finding eigenvalues. - In next slides we will illustrate how to choose shift on a simpler algorithm."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#shifts-and-power-method",
    "href": "lectures/lecture-8/lecture-8.html#shifts-and-power-method",
    "title": "Questions?",
    "section": "Shifts and power method",
    "text": "Shifts and power method\n\nRemember the power method for the computation of the eigenvalues.\n\nx_{k+1} := A x_k, \\quad x_{k+1} := \\frac{x_{k+1}}{\\Vert x_{k+1} \\Vert}.\n\nIt converges to the eigenvector corresponding to the largest eigenvalue in modulus.\nThe convergence can be very slow.\nLet us try to use shifting strategy. If we shift the matrix as\n\n  A := A - \\lambda_k I\nand the corresponding eigenvalue becomes small (but we need large). - That is not what we wanted!"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#inverse-iteration-and-rayleigh-quotient-iteration",
    "href": "lectures/lecture-8/lecture-8.html#inverse-iteration-and-rayleigh-quotient-iteration",
    "title": "Questions?",
    "section": "Inverse iteration and Rayleigh quotient iteration",
    "text": "Inverse iteration and Rayleigh quotient iteration\n\nTo make a small eigenvalue large, we need to invert the matrix, and that gives us inverse iteration\n\nx_{k+1} = (A - \\lambda I)^{-1} x_k,\nwhere \\lambda is the shift which is approximation to the eigenvalue that we want.\n\nAs it was for the power method, the convergence is linear.\nTo accelerate convergence one can use the Rayleigh quotient iteration (inverse iteration with adaptive shifts) which is given by the selection of the adaptive shift:\n\nx_{k+1} = (A - \\lambda_k I)^{-1} x_k,\n\\lambda_k = \\frac{(Ax_k, x_k)}{(x_k, x_k)}\n\nIn the symmetric case A = A^* the convergence is locally cubic and locally quadratic otherwise."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-1",
    "href": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-1",
    "title": "Questions?",
    "section": "Singular values and eigenvalues (1)",
    "text": "Singular values and eigenvalues (1)\n\nNow let us talk about singular values and eigenvalues.\nSVD\n\nA = U \\Sigma V^*\nexists for any matrix.\n\nIt can be also viewed as a reduction of a given matrix to the diagonal form by means of two-sided unitary transformations:\n\n\\Sigma = U^* A V.\n\nBy two-sided Householder transformation we can reduce any matrix to the bidiagonal form B (how?)."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-2",
    "href": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-2",
    "title": "Questions?",
    "section": "Singular values and eigenvalues (2)",
    "text": "Singular values and eigenvalues (2)\n\nImplicit QR algorithm (with shifts) gives the way of computing the eigenvalues (and Schur form).\nBut we cannot apply QR algorithm directly to the bidiagonal matrix, as it is not diagonalizable in general case.\nHowever, the problem of the computation of the SVD can be reduced to the symmetric eigenvalue problem in two ways:\n\n\nWork with the tridiagonal matrix\n\nT = B^* B\n\nWork with the extended matrix\n\nT = \\begin{bmatrix} 0 & B \\\\ B^* & 0 \\end{bmatrix}\n\nThe case 1 is OK if you do not form T directly!\nThus, the problem of computing singular values can be reduced to the problem of the computation of the eigenvalues of symmetric tridiagonal matrix."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#algorithms-for-the-sev-symmetric-eigenvalue-problem",
    "href": "lectures/lecture-8/lecture-8.html#algorithms-for-the-sev-symmetric-eigenvalue-problem",
    "title": "Questions?",
    "section": "Algorithms for the SEV (symmetric eigenvalue problem)",
    "text": "Algorithms for the SEV (symmetric eigenvalue problem)\nDone: - QR algorithm: the “gold standard” of the eigenvalue computations - RQI-iteration: Rayleigh quotient iteration is implicitly performed at each step of the QR algorithm\nNext slides: - Divide-and-conquer - Bisection method - Jacobi method"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#divide-and-conquer",
    "href": "lectures/lecture-8/lecture-8.html#divide-and-conquer",
    "title": "Questions?",
    "section": "Divide-and-conquer",
    "text": "Divide-and-conquer\n\nSuppose we have a tridiagonal matrix, and we split it into two blocks:\n\nT = \\begin{bmatrix} T'_1 & B \\\\ B^{\\top} & T'_2 \\end{bmatrix}\n\nWe can write the matrix T as\n\nT = \\begin{bmatrix} T_1 & 0 \\\\ 0 & T_2 \\end{bmatrix} + b_m v v^*\nwhere vv^* is rank 1 matrix, v = (0,\\dots,0,1,1,0,\\dots,0)^T.\n\nSuppose we have decomposed T_1 and T_2 already:\n\nT_1 = Q_1 \\Lambda_1 Q^*_1, \\quad T_2 = Q_2 \\Lambda_2 Q^*_2\n\nThen (check),\n\n\\begin{bmatrix} Q^*_1 & 0 \\\\ 0 & Q^*_2 \\end{bmatrix} T\\begin{bmatrix} Q_1 & 0 \\\\ 0 & Q_2 \\end{bmatrix} = D + \\rho u u^{*}, \\quad D = \\begin{bmatrix} \\Lambda_1 & 0 \\\\ 0 & \\Lambda_2\\end{bmatrix}\n\nI.e. we have reduced the problem to the problem of the computation of the eigenvalues of  diagonal plus low-rank matrix"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#diagonal-plus-low-rank-matrix",
    "href": "lectures/lecture-8/lecture-8.html#diagonal-plus-low-rank-matrix",
    "title": "Questions?",
    "section": "Diagonal-plus-low-rank matrix",
    "text": "Diagonal-plus-low-rank matrix\nIt is tricky to compute the eigenvalues of the matrix\nD + \\rho u u^* \nThe characteristic polynomial has the form\n\\det(D + \\rho uu^* - \\lambda I) = \\det(D - \\lambda I)\\det(I + \\rho (D - \\lambda I)^{-1} uu^*) = 0.\nThen (prove!!)\n\\det(I + \\rho (D - \\lambda I)^{-1} uu^*) = 1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{d_i - \\lambda} = 0\nHint: find \\det(I + w u^*) using the fact that \\text{det}(C) = \\prod_{i=1}^n\\lambda_i(C) and \\text{trace}(C) = \\sum_{i=1}^n \\lambda_i."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#characteristic-equation",
    "href": "lectures/lecture-8/lecture-8.html#characteristic-equation",
    "title": "Questions?",
    "section": "Characteristic equation",
    "text": "Characteristic equation\n1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{d_i - \\lambda} = 0\nHow to find the roots?\n\nimport jax.numpy as jnp\n\nlm = jnp.array([1, 2, 3, 4])\nM = len(lm)\nD = jnp.array(lm)\na = jnp.min(lm)\nb = jnp.max(lm)\nt = jnp.linspace(-1, 6, 1000)\nu = 0.5 * jnp.ones(M)\nrho = 1\ndef fun(lam):\n    return 1 + rho * jnp.sum(u**2/(D - lam))\nres = [fun(lam) for lam in t]\nplt.figure(figsize=(10,8))\nplt.plot(t, res, 'k')\n#plt.plot(jnp.zeros_like(t))\nplt.ylim([-6, 6])\nplt.tight_layout()\nplt.yticks(fontsize=24)\nplt.xticks(fontsize=24)\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nThe function has only one root at [d_i, d_{i+1}]\nWe have proved, by the way, the Cauchy interlacing theorem (what happens to the eigenvalues under rank-1 perturbation)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#how-to-find-the-root",
    "href": "lectures/lecture-8/lecture-8.html#how-to-find-the-root",
    "title": "Questions?",
    "section": "How to find the root",
    "text": "How to find the root\n\nA Newton method will fail (draw a picture with a tangent line).\nNote that Newton method is just approximation of a function f(\\lambda) by a linear function.\nMuch better approximation is the hyperbola:\n\nf(\\lambda) \\approx c_0 + \\frac{c_1}{d_i - \\lambda} + \\frac{c_2}{d_{i+1} - \\lambda}.\n\nTo fit the coefficients, we have to evaluate f(\\lambda) and f'(\\lambda) in the particular point.\nAfter that, the approximation can be recovered from solving quadratic equation"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#important-issues",
    "href": "lectures/lecture-8/lecture-8.html#important-issues",
    "title": "Questions?",
    "section": "Important issues",
    "text": "Important issues\n\nFirst, stability: this method was abandoned for a long time due to instability of the computation of the eigenvectors.\nIn the recursion, we need to compute the eigenvectors of the D + \\rho uu^* matrix.\nThe exact expression for the eigenvectors is just (let us check!)\n\n(D - \\alpha_i I)^{-1}u, where \\alpha_i is the computed root.\n\nThe reason of instability:\n\nif \\alpha_i and \\alpha_{i+1} are close, then the corresponding eigenvectors are collinear, but they have to be orthogonal\nif \\alpha_i and \\alpha_{i+1} are close, then they close to d_i, therefore matrices D - \\alpha_i I and D - \\alpha_{i+1} I are close to singular"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#loewner-theorem",
    "href": "lectures/lecture-8/lecture-8.html#loewner-theorem",
    "title": "Questions?",
    "section": "Loewner theorem",
    "text": "Loewner theorem\n\nThe solution came is to use a strange Loewner theorem:\n\nIf \\alpha_i and d_i satisfy the interlacing theorem\nd_n &lt; \\alpha_n &lt; \\ldots &lt; d_{i+1} &lt; \\alpha_{i+1} \\ldots\nThen there exists a vector \\widehat{u} such that \\alpha_i are exact eigenvalues of the matrix\n\\widehat{D} = D + \\widehat{u} \\widehat{u}^*.\n\nSo, you first compute the eigenvalues, then compute \\widehat{u} and only then the eigenvectors."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#divide-and-conquer-and-the-fast-multipole-method",
    "href": "lectures/lecture-8/lecture-8.html#divide-and-conquer-and-the-fast-multipole-method",
    "title": "Questions?",
    "section": "Divide and conquer and the Fast Multipole Method",
    "text": "Divide and conquer and the Fast Multipole Method\nIn the computations of divide and conquer we have to evaluate the sums of the form\nf(\\lambda) = 1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{(d_i - \\lambda)},\nand have to do it at least for n points.\n\nThe complexity is then \\mathcal{O}(n^2), as for the QR algorithm.\nCan we make it \\mathcal{O}(n \\log n)?\nThe answer is yes, but we have to replace the computations by the approximate ones by the help of Fast Multipole Method.\n\nLets explain a little bit…"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#few-more-algorithms",
    "href": "lectures/lecture-8/lecture-8.html#few-more-algorithms",
    "title": "Questions?",
    "section": "Few more algorithms",
    "text": "Few more algorithms\n\nAbsolutely different approach is based on the bisection.\nGiven a matrix A its inertia is defined as a triple\n\n(\\nu, \\zeta, \\pi),\nwhere \\nu is the number of negative, \\zeta - zero and \\pi - positive eigenvalues.\n\nIf X is non-singular, then\n\nInertia(A) = Inertia(X^* A X)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#bisection-via-gaussian-elimination",
    "href": "lectures/lecture-8/lecture-8.html#bisection-via-gaussian-elimination",
    "title": "Questions?",
    "section": "Bisection via Gaussian elimination",
    "text": "Bisection via Gaussian elimination\n\nGiven z we can do the Gaussian elimination:\n\nA - zI = L D L^*,\nand inertia of the diagonal matrix is trivial to compute.\n\nThus, if we want to find all the eigenvalues in the interval a, b\nUsing inertia, we can easily count the number of eigenvalues in an interval.\nIllustration: if Inertia(A)=(5,0,2) and after shift Inertia(A-zI)=(4,0,3), z\\in[a,b] then we know that \\lambda(A)\\in[a,z]."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-method",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-method",
    "title": "Questions?",
    "section": "Jacobi method",
    "text": "Jacobi method\n\nRecall what a Jacobi (Givens rotations) are\nIn a plane they correspong to a 2 \\times 2 orthogonal matrix of the form\n\n\\begin{pmatrix} \\cos \\phi & \\sin \\phi \\\\ -\\sin \\phi & \\cos \\phi \\end{pmatrix},\nand in the n-dimensional case we select two variables i and j and rotate."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-method-cont.",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-method-cont.",
    "title": "Questions?",
    "section": "Jacobi method (cont.)",
    "text": "Jacobi method (cont.)\n\nThe idea of the Jacobi method is to minimize sum of squares of off-diagonal elements:\n\n\\Gamma(A) = \\mathrm{off}( U^* A U), \\quad \\mathrm{off}^2(X) = \\sum_{i \\ne j} \\left|X_{ij}\\right|^2 = \\|X \\|^2_F - \\sum\\limits_{i=1}^n x^2_{ii}.\nby applying succesive Jacobi rotations U to zero off-diagonal elements.\n\nWhen the “pivot” is chosen, it is easy to eliminate it.\nThe main question is then what is the order of sweeps we have to make (i.e. in which order to eliminate).\nIf we always eliminate the largest off-diagonal elements the method has quadratic convergence.\nIn practice, a cyclic order (i.e., (1, 2), (1, 3), \\ldots, (2, 3), \\ldots) is used."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-method-convergence",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-method-convergence",
    "title": "Questions?",
    "section": "Jacobi method: convergence",
    "text": "Jacobi method: convergence\n\nTo show convergence, we firstly show that\n\n \\text{off}(B) &lt; \\text{off}(A), \nwhere B = U^*AU.\n\nIn this case we use the unitary invariance of Frobenius norm and denote by p and q the indices that is changed after rotation:\n\n$ ^2(A) = ^2(B) = |B|^2_F - {i=1}^n b^2{ii} = | A |^2_F - {i p, q} b^2{ii} - (b^2_{pp} + b^2_{qq}) = | A |^2_F - {i p, q} a^2{ii} - (a^2_{pp} + 2a^2_{pq} + a^2_{qq}) = | A |^2_F - {i =1}^n a^2{ii} - 2a^2_{pq} = ^2(A) - 2a^2_{pq} &lt; ^2(A)$\n\nWe show that the ‘’size’’ of off-diagonal elements decreases after Jacobi rotation.\nIf we always select the largest off-diagonal element a_{pq} = \\gamma to eliminate (pivot), then we have\n\n |a_{ij}| \\leq \\gamma, \nthus\n \\text{off}(A)^2 \\leq 2 N \\gamma^2, \nwhere 2N = n(n-1) is the number of off-diagonal elements.\n\nOr rewrite this inequality in the form\n\n2\\gamma^2 \\geq \\frac{\\text{off}^2(A)}{N}.\nNow we use relations \\Gamma^2(A) = \\text{off}^2(A) - 2\\gamma^2 \\leq \\text{off}^2(A) - \\dfrac{\\text{off}^2(A)}{N} and get\n \\Gamma(A) \\leq \\sqrt{\\left(1 - \\frac{1}{N}\\right)} \\text{off}(A). \n\nAften N steps we have the factor\n\n\\left(1 - \\frac{1}{N}\\right)^{\\frac{N}{2}} \\approx e^{-\\frac{1}{2}},\ni.e. linear convergence. However, the convergence is locally quadratic (given without proof here)."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-summary",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-summary",
    "title": "Questions?",
    "section": "Jacobi: summary",
    "text": "Jacobi: summary\nJacobi method was the first numerical method for the eigenvalues, proposed in 1846.\n\nLarge constant\nVery accurate (high relative error for small eigenvalues)\nGood parallel capabilities"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#summary-for-this-part",
    "href": "lectures/lecture-8/lecture-8.html#summary-for-this-part",
    "title": "Questions?",
    "section": "Summary for this part",
    "text": "Summary for this part\n\nMany algorithms for the computation of the SEV solution:\n\nQR\nDivide-and-conquer\nBisection\nJacobi"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#next-lecture",
    "href": "lectures/lecture-8/lecture-8.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nWe start sparse and/or structured NLA."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html",
    "href": "lectures/lecture-6/lecture-6.html",
    "title": "Matrix decomposition: the Schur form",
    "section": "",
    "text": "Linear systems\nGaussian elimination\nLU decomposition\nCondition number as a measure of forward stability of the problem"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#recap-of-the-previous-lecture",
    "href": "lectures/lecture-6/lecture-6.html#recap-of-the-previous-lecture",
    "title": "Matrix decomposition: the Schur form",
    "section": "",
    "text": "Linear systems\nGaussian elimination\nLU decomposition\nCondition number as a measure of forward stability of the problem"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#today-lecture",
    "href": "lectures/lecture-6/lecture-6.html#today-lecture",
    "title": "Matrix decomposition: the Schur form",
    "section": "Today lecture",
    "text": "Today lecture\nToday we will talk about: - Eigenvectors and their applications (PageRank) - Gershgorin circles - Computing eigenvectors using power method - Schur theorem - Normal matrices"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#what-is-an-eigenvector",
    "href": "lectures/lecture-6/lecture-6.html#what-is-an-eigenvector",
    "title": "Matrix decomposition: the Schur form",
    "section": "What is an eigenvector?",
    "text": "What is an eigenvector?\n\nDefinition. A vector x \\ne 0 is called an eigenvector of a square matrix A if there exists a number \\lambda such that\n\n Ax = \\lambda x. \n\nThe number \\lambda is called an eigenvalue. The name eigenpair is also used.\nSince A - \\lambda I should have a non-trivial kernel, eigenvalues are the roots of the characteristic polynomial\n\n \\det (A - \\lambda I) = 0."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#eigendecomposition",
    "href": "lectures/lecture-6/lecture-6.html#eigendecomposition",
    "title": "Matrix decomposition: the Schur form",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\nIf matrix A of size n\\times n has n eigenvectors s_i, i=1,\\dots,n:\n As_i = \\lambda_i s_i, \nthen this can be written as\n A S = S \\Lambda, \\quad\\text{where}\\quad S=(s_1,\\dots,s_n), \\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n), \nor equivalently\n A = S\\Lambda S^{-1}. \n\nThis is called eigendecomposition of a matrix. Matrices that can be represented by their eigendecomposition are called diagonalizable.\n\n\nExistence\n\nWhat classes of matrices are diagonalizable?\nSimple example can be matrices with all different eigenvalues.\nMore generally, matrix is diagonalizable iff algebraic multiplicity of each eigenvalue (mutiplicity of eigenvalue in the characteristic polynomial) is equal to its geometric multiplicity (dimension of eigensubspace).\nFor our purposes the most important class of diagonalizable matrices is the class of normal matrices:\n\nAA^* = A^* A.\n\nYou will learn how to prove that normal matrices are diagonalizable after a few slides (Schur decomposition topic).\n\n\nExample\n\nYou can simply check that, e.g. matrix\n\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\nhas one eigenvalue 1 of multiplicity 2 (since its characteristic polynomial is p(\\lambda)=(1-\\lambda)^2), but only one eigenvector \\begin{pmatrix} c \\\\ 0  \\end{pmatrix} and hence the matrix is not diagonalizable."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#why-eigenvectors-and-eigenvalues-are-important",
    "href": "lectures/lecture-6/lecture-6.html#why-eigenvectors-and-eigenvalues-are-important",
    "title": "Matrix decomposition: the Schur form",
    "section": "Why eigenvectors and eigenvalues are important?",
    "text": "Why eigenvectors and eigenvalues are important?\n\nEigenvectors are both important auxiliary tools and also play important role in applications.\n\nCan you give some examples?"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#applications-of-eigenvalueseigenvectors",
    "href": "lectures/lecture-6/lecture-6.html#applications-of-eigenvalueseigenvectors",
    "title": "Matrix decomposition: the Schur form",
    "section": "Applications of eigenvalues/eigenvectors",
    "text": "Applications of eigenvalues/eigenvectors\n\nCommunication theory: theoretical limit on the amount of information transferred\nDesigning bridges (mechanical engineering)\nDesigning hifi-audio systems\nQuantum chemistry: all our microworld is governed by the Schrodinger equation which is an eigenvalue problem:\n\n H \\psi = E \\psi, \n\nModel order reduction of complex systems\nGraph analysis (PageRank, graph clustering)"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#eigenvalues-are-vibrational-frequencies",
    "href": "lectures/lecture-6/lecture-6.html#eigenvalues-are-vibrational-frequencies",
    "title": "Matrix decomposition: the Schur form",
    "section": "Eigenvalues are vibrational frequencies",
    "text": "Eigenvalues are vibrational frequencies\nA typical computation of eigenvectors / eigenvectors is for studying\n\nVibrational computations of mechanical structures\nModel order reduction of complex systems\n\n\nfrom IPython.display import YouTubeVideo \nYouTubeVideo(\"VcCcMZo6J6w\")"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#google-pagerank",
    "href": "lectures/lecture-6/lecture-6.html#google-pagerank",
    "title": "Matrix decomposition: the Schur form",
    "section": "Google PageRank",
    "text": "Google PageRank\n\nOne of the most famous eigenvectors computation is the Google PageRank.\nIt is not actively used by Google nowadays, but it was one of the main features in its early stages. The question is how do we rank webpages, which one is important, and which one is not.\nAll we know about the web is which page refers to which. PageRank is defined by a recursive definition.\nDenote by p_i the importance of the i-th page.\nThen we define this importance as an average value of all importances of all pages that refer to the current page. It gives us a linear system\n\n p_i = \\sum_{j \\in N(i)} \\frac{p_j}{L(j)}, \nwhere L(j) is the number of outgoing links on the j-th page, N(i) are all the neighbours of the i-th page. It can be rewritten as\n p = G p, \\quad G_{ij} = \\frac{1}{L(j)} \nor as an eigenvalue problem\n\n   Gp = 1 p,\n\ni.e. the eigenvalue 1 is already known. Note that G is left stochastic, i.e. its columns sum up to 1. Check that any left stochastic matrix has maximum eigenvalue equal to 1."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#demo",
    "href": "lectures/lecture-6/lecture-6.html#demo",
    "title": "Matrix decomposition: the Schur form",
    "section": "Demo",
    "text": "Demo\n\nWe can compute PageRank using some Python packages.\nWe will use networkx package for working with graphs that can be installed using\n\nconda install networkx\n\nOther packages to work with graphs in Python are igraph and graph-tool\nThey can be useful in your projects\nWe will use a simple example of Zachary karate club network.\nThis data was manually collected in 1977, and is a classical social network dataset.\n\n\nimport jax.numpy as jnp\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport networkx as nx\nkn = nx.read_gml('karate.gml')\n#nx.write_gml(kn, 'karate2.gml')\nnx.draw_networkx(kn, node_color=\"red\") #Draw the graph\n\n\n\n\n\n\n\n\n\nNow we can actually compute the PageRank using the NetworkX built-in function.\nWe also plot the size of the nodes larger if its PageRank is larger.\n\n\npr = nx.algorithms.link_analysis.pagerank(kn)\npr_vector = list(pr.values())\npr_vector = jnp.array(pr_vector) * 3000\nnx.draw_networkx(kn, node_size=pr_vector, node_color=\"red\", labels=None)"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#computations-of-eigenvalues",
    "href": "lectures/lecture-6/lecture-6.html#computations-of-eigenvalues",
    "title": "Matrix decomposition: the Schur form",
    "section": "Computations of eigenvalues",
    "text": "Computations of eigenvalues\n\nHow to compute eigenvalues and eigenvectors?\n\nThere are two types of eigenproblems:\n\nfull eigenproblem (all eigenvalues & eigenvectors are required)\npartial eigenvalues (minimal/maximal eigenvalues, eigenvalues within the specified region are required)"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#computation-of-the-eigenvalues-via-characteristic-equations",
    "href": "lectures/lecture-6/lecture-6.html#computation-of-the-eigenvalues-via-characteristic-equations",
    "title": "Matrix decomposition: the Schur form",
    "section": "Computation of the eigenvalues via characteristic equations",
    "text": "Computation of the eigenvalues via characteristic equations\nThe eigenvalue problem has the form\n Ax = \\lambda x, \nor\n (A - \\lambda I) x = 0, \ntherefore matrix A - \\lambda I has non-trivial kernel and should be singular.\nThat means, that the determinant\n p(\\lambda) = \\det(A - \\lambda I) = 0. \n\nThe equation is called characteristic equations and is a polynomial of order n.\nThe n-degree polynomial has n complex roots!"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#recall-the-determinant",
    "href": "lectures/lecture-6/lecture-6.html#recall-the-determinant",
    "title": "Matrix decomposition: the Schur form",
    "section": "Recall the determinant",
    "text": "Recall the determinant\nThe determinant of a square matrix A is defined as\n\\det A = \\sum_{\\sigma \\in S_n} \\mathrm{sgn}({\\sigma})\\prod^n_{i=1} a_{i, \\sigma_i},\nwhere - S_n is the set of all permutations of the numbers 1, \\ldots, n - \\mathrm{sgn} is the signature of the permutation ( (-1)^p, where p is the number of transpositions to be made)."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#properties-of-determinant",
    "href": "lectures/lecture-6/lecture-6.html#properties-of-determinant",
    "title": "Matrix decomposition: the Schur form",
    "section": "Properties of determinant",
    "text": "Properties of determinant\nDeterminant has many nice properties:\n1. \\det(AB) = \\det(A) \\det(B)\n2. If we have one row as a sum of two vectors, determinant is a sum of two determinants\n3. “Minor expansion”: we can expand determinant through a selected row or column.\n\nIf you do it via minor expansion, we get exponential complexity in n.\nCan we do \\mathcal{O}(n^3)?"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#eigenvalues-and-characteristic-equation",
    "href": "lectures/lecture-6/lecture-6.html#eigenvalues-and-characteristic-equation",
    "title": "Matrix decomposition: the Schur form",
    "section": "Eigenvalues and characteristic equation",
    "text": "Eigenvalues and characteristic equation\n\nNow we go back to the eigenvalues.\nThe characteristic equation can be used to compute the eigenvalues, which leads to naïve algorithm:\n\np(\\lambda) = \\det(A - \\lambda I)\n\nCompute coefficients of the polynomial\nCompute the roots\n\nIs this a good idea?\nGive your feedback\nWe can do a short demo of this\n\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\nimport jax\nn = 40\na = [[1.0 / (i - j + 0.5) for i in range(n)] for j in range(n)]\na = jnp.array(a)\nev = jnp.linalg.eigvals(a)\n#There is a special numpy function for chacteristic polynomial\ncf = jnp.poly(a)\nev_roots = jnp.roots(cf)\n#print('Coefficients of the polynomial:', cf)\n#print('Polynomial roots:', ev_roots)\nplt.scatter(ev_roots.real, ev_roots.imag, marker='x', label='roots')\nb = a + 0 * jax.random.normal(jax.random.PRNGKey(0,), (n, n))\nev_b = jnp.linalg.eigvals(b)\nplt.scatter(ev_b.real, ev_b.imag, marker='o', label='Lapack')\n#plt.scatter(ev_roots.real, ev_roots.imag, marker='o', label='Brute force')\nplt.legend(loc='best')\nplt.xlabel('Real part')\nplt.ylabel('Imaginary part')\n\nText(0, 0.5, 'Imaginary part')\n\n\n\n\n\n\n\n\n\n\nMorale\n\nDo not do that, unless you have a reason.\nPolynomial rootfinding is very ill-conditioned (can be much better, but not with monomials \\{1,x,x^2,\\dots\\}!). Note that Gram matrix of monomials\n\nh_{ij} = \\int_0^1 x^i x^j\\, dx = \\frac{1}{i+j+1},\nis the Hilbert matrix, which has exponential decay of singular values. - So, monomials are “almost” linearly dependent."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#gershgorin-circles",
    "href": "lectures/lecture-6/lecture-6.html#gershgorin-circles",
    "title": "Matrix decomposition: the Schur form",
    "section": "Gershgorin circles",
    "text": "Gershgorin circles\n\nThere is a very interesting theorem that sometimes helps to localize the eigenvalues.\nIt is called Gershgorin theorem.\nIt states that all eigenvalues \\lambda_i,  i = 1, \\ldots, n are located inside the union of Gershgorin circles C_i, where C_i is a disk on the complex plane with center a_{ii} and radius\n\nr_i = \\sum_{j \\ne i} |a_{ij}|.\n\nMoreover, if the circles do not intersect they contain only one eigenvalue per circle.\nThe proof is instructive since it uses the concepts we looked at the previous lectures."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#proof",
    "href": "lectures/lecture-6/lecture-6.html#proof",
    "title": "Matrix decomposition: the Schur form",
    "section": "Proof",
    "text": "Proof\nFirst, we need to show that if the matrix A is strictly diagonally dominant, i.e. \n\n   |a_{ii}| &gt; \\sum_{j \\ne i} |a_{ij}|,\n then such matrix is non-singular.\nWe separate the diagonal part and off-diagonal part, and get\n\n    A = D + S = D( I + D^{-1}S),\n\nand \\Vert D^{-1} S\\Vert_1 &lt; 1. Therefore, by using the Neumann series, the matrix I + D^{-1}S is invertible and hence A is invertible.\nNow the proof follows by contradiction:\n\nif any of the eigenvalues lies outside all of the circles, the matrix (A - \\lambda I) is strictly diagonally dominant\nthus it is invertible\nthat means, that (A - \\lambda I) x = 0 means x = 0."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#a-short-demo",
    "href": "lectures/lecture-6/lecture-6.html#a-short-demo",
    "title": "Matrix decomposition: the Schur form",
    "section": "A short demo",
    "text": "A short demo\n\nimport jax.numpy as jnp\n%matplotlib inline\nn = 3\nfig, ax = plt.subplots(1, 1)\na = [[5, 1, 1], [1, 0, 0.5], [2, 0, 10]]\n#a = [[1.0 / (i - j + 0.5) for i in xrange(n)] for j in xrange(n)]\na = jnp.array(a)\n#a = np.diag(np.arange(n))\na = a + 2 * jax.random.normal(jax.random.PRNGKey(0), (n, n))\n#u = np.random.randn(n, n)\n#a = np.linalg.inv(u).dot(a).dot(u)\nxg = jnp.diag(a).real\nyg = jnp.diag(a).imag\nrg = jnp.zeros(n)\nev = jnp.linalg.eigvals(a)\nfor i in range(n):\n    rg = jax.ops.index_update(rg, jax.ops.index[i], jnp.sum(jnp.abs(a[i, :])) - jnp.abs(a[i, i]))\n    crc = plt.Circle((xg[i], yg[i]), radius=rg[i], fill=False)\n    ax.add_patch(crc)\nplt.scatter(ev.real, ev.imag, color='r', label=\"Eigenvalues\")\nplt.axis('equal')\nplt.legend()\nax.set_title('Eigenvalues and Gershgorin circles')\nfig.tight_layout()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[7], line 17\n     15 ev = jnp.linalg.eigvals(a)\n     16 for i in range(n):\n---&gt; 17     rg = jax.ops.index_update(rg, jax.ops.index[i], jnp.sum(jnp.abs(a[i, :])) - jnp.abs(a[i, i]))\n     18     crc = plt.Circle((xg[i], yg[i]), radius=rg[i], fill=False)\n     19     ax.add_patch(crc)\n\nAttributeError: module 'jax.ops' has no attribute 'index_update'\n\n\n\n\n\n\n\n\n\n\nNote: There are more complicated figures, like Cassini ovals, that include the spectrum\n C_{ij} = \\{z\\in\\mathbb{C}: |a_{ii} - z|\\cdot |a_{jj} - z|\\leq r_i r_j\\}, \\quad r_i = \\sum_{l\\not= i} |a_{il}|."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#power-method",
    "href": "lectures/lecture-6/lecture-6.html#power-method",
    "title": "Matrix decomposition: the Schur form",
    "section": "Power method",
    "text": "Power method\n\nWe are often interested in the computation of the part of the spectrum, like the largest eigenvalues or smallest eigenvalues.\nAlso it is interesting to note that for the Hermitian matrices (A = A^*) the eigenvalues are always real (prove it!).\n\nA power method is the simplest method for the computation of the largest eigenvalue in modulus.\nIt is also our first example of the iterative method and Krylov method."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#power-method-1",
    "href": "lectures/lecture-6/lecture-6.html#power-method-1",
    "title": "Matrix decomposition: the Schur form",
    "section": "Power method",
    "text": "Power method\n\nThe eigenvalue problem\n\nAx = \\lambda x, \\quad \\Vert x \\Vert_2 = 1 \\ \\text{for stability}.\ncan be rewritten as a fixed-point iteration. - This iteration is called power method and finds the largest in modulus eigenvalue of A.\nPower method has the form\n x_{k+1} = A x_k, \\quad x_{k+1} := \\frac{x_{k+1}}{\\Vert x_{k+1} \\Vert_2}\nand\n x_{k+1}\\to v_1,\nwhere Av_1 = \\lambda_1 v_1 and \\lambda_1 is the largest eigenvalue and v_1 is the corresponding eigenvector.\n\nOn the (k+1)-th iteration approximation to \\lambda_1 can be found as\n\n \\lambda^{(k+1)} = (Ax_{k+1}, x_{k+1}), \n\nNote that \\lambda^{(k+1)} is not required for the (k+2)-th iteration, but might be useful to measure error on each iteration: \\|Ax_{k+1} - \\lambda^{(k+1)}x_{k+1}\\|.\nThe convergence is geometric, but the convergence ratio is q^k, where q = \\left|\\frac{\\lambda_{2}}{\\lambda_{1}}\\right| &lt; 1, for \\lambda_1&gt;\\lambda_2\\geq\\dots\\geq \\lambda_n and k is the number of iteration.\nIt means, the convergence can be artitrary small. To prove it, it is sufficient to consider a 2 \\times 2 diagonal matrix."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#convergence-analysis-for-aa",
    "href": "lectures/lecture-6/lecture-6.html#convergence-analysis-for-aa",
    "title": "Matrix decomposition: the Schur form",
    "section": "Convergence analysis for A=A^*",
    "text": "Convergence analysis for A=A^*\nLet’s have a more precise look at the power method when A is Hermitian. In two slides you will learn that every Hermitian matrix is diagonalizable. Therefore, there exists orthonormal basis of eigenvectors v_1,\\dots,v_n such that Av_i = \\lambda_i v_i. Let us decompose x_0 into a sum of v_i with coefficients c_i:\n x_0 = c_1 v_1 + \\dots + c_n v_n. \nSince v_i are eigenvectors, we have\n\n\\begin{split}\nx_1 &= \\frac{Ax_0}{\\|Ax_0\\|} = \\frac{c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n}{\\|c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n \\|}  \\\\\n&\\vdots\\\\\nx_k &= \\frac{Ax_{k-1}}{\\|Ax_{k-1}\\|} = \\frac{c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n}{\\|c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n \\|}\n\\end{split}\n\nNow you see, that\n\nx_k = \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\frac{ v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k}v_2 + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}v_n}{\\left\\|v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k}v_2 + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}v_n\\right\\|},\n\nwhich converges to v_1 since \\left| \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\right| = 1 and \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k \\to 0 if |\\lambda_2|&lt;|\\lambda_1|."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#things-to-remember-about-the-power-method",
    "href": "lectures/lecture-6/lecture-6.html#things-to-remember-about-the-power-method",
    "title": "Matrix decomposition: the Schur form",
    "section": "Things to remember about the power method",
    "text": "Things to remember about the power method\n\nPower method gives estimate of the largest eigenvalue in modulus or spectral radius of the given matrix\nOne step requires one matrix-by-vector product. If the matrix allows for an \\mathcal{O}(n) matvec (for example, it is sparse), then power method is tractable for larger n.\nConvergence can be slow\nIf only a rough estimate is needed, only a few iterations are sufficient\nThe solution vector is in the Krylov subspace \\{x_0, Ax_0,\\dots,A^{k}x_0\\} and has the form \\mu A^k x_0, where \\mu is the normalization constant."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#schur-theorem",
    "href": "lectures/lecture-6/lecture-6.html#schur-theorem",
    "title": "Matrix decomposition: the Schur form",
    "section": "Schur theorem",
    "text": "Schur theorem\nTheorem: Every A \\in \\mathbb{C}^{n \\times n} matrix can be represented in the Schur form A = UTU^*, where U is unitary and T is upper triangular.\nSketch of the proof. 1. Every matrix has at least 1 non-zero eigenvector (take a root of characteristic polynomial, (A-\\lambda I) is singular, has non-trivial nullspace). Let\nAv_1 = \\lambda_1 v_1, \\quad \\Vert v_1 \\Vert_2 = 1\n\nLet U_1 = [v_1,v_2,\\dots,v_n], where v_2,\\dots, v_n are any vectors othogonal to v_1. Then\n\n U^*_1 A U_1 = \\begin{pmatrix} \\lambda_1 & *  \\\\ 0 & A_2  \\end{pmatrix}, \nwhere A_2 is an (n-1) \\times (n-1) matrix. This is called block triangular form. We can now work with A_2 only and so on.\nNote: Since we need eigenvectors in this proof, this proof is not a practical algorithm."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#application-of-the-schur-theorem",
    "href": "lectures/lecture-6/lecture-6.html#application-of-the-schur-theorem",
    "title": "Matrix decomposition: the Schur form",
    "section": "Application of the Schur theorem",
    "text": "Application of the Schur theorem\n\nImportant application of the Schur theorem: normal matrices.\nDefinition. Matrix A is called normal matrix, if\n\n AA^* = A^* A. \nQ: Examples of normal matrices?\nExamples: Hermitian matrices, unitary matrices."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#normal-matrices",
    "href": "lectures/lecture-6/lecture-6.html#normal-matrices",
    "title": "Matrix decomposition: the Schur form",
    "section": "Normal matrices",
    "text": "Normal matrices\nTheorem: A is a normal matrix, iff A = U \\Lambda U^*, where U is unitary and \\Lambda is diagonal.\nSketch of the proof: - One way is straightforward (if the decomposition holds, the matrix is normal).\n- The other is more complicated. Consider the Schur form of the matrix A. Then AA^* = A^*A means TT^* = T^* T.\n- By looking at the elements we immediately see, that the only upper triangular matrix T that satisfies TT^* = T^* T is a diagonal matrix!\n\nImportant consequence\nTherefore, every normal matrix is unitary diagonalizable, which means that it can be diagonalized by unitary matrix U.\nIn other words every normal matrix has orthogonal basis of eigenvectors."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#how-we-compute-the-schur-decomposition",
    "href": "lectures/lecture-6/lecture-6.html#how-we-compute-the-schur-decomposition",
    "title": "Matrix decomposition: the Schur form",
    "section": "How we compute the Schur decomposition?",
    "text": "How we compute the Schur decomposition?\n\nEverything is fine, but how we compute the Schur form?\nThis will be covered in the next lecture."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#variational-principle-for-eigenvalues",
    "href": "lectures/lecture-6/lecture-6.html#variational-principle-for-eigenvalues",
    "title": "Matrix decomposition: the Schur form",
    "section": "Variational principle for eigenvalues",
    "text": "Variational principle for eigenvalues\n\nIn many cases, minimal/maximal eigenvalues are needed\nThen, if A is a Hermitian matrix, the Rayleigh quotient is defined as\n\nR_A(x) = \\frac{(Ax, x)}{(x, x)},\nand the maximal eigenvalue is the maximum of R_A(x), and the minimal eigenvalue is the minimal of R_A(x).\n\nThus, we can use optimization method to find these extreme eigenvalues.\nThis approach will be crucial in the case of partial eigenvalue problem for large sparse matrices\n\nNow, “advanced” concept."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#spectrum-and-pseudospectrum",
    "href": "lectures/lecture-6/lecture-6.html#spectrum-and-pseudospectrum",
    "title": "Matrix decomposition: the Schur form",
    "section": "Spectrum and pseudospectrum",
    "text": "Spectrum and pseudospectrum\n\nFor linear dynamical systems given by the matrix A, spectrum can tell a lot about the system (i.e. stability, …)\nHowever, for non-normal matrices, spectrum can be unstable with respect to small perturbations.\nIn order to measure such perturbation, the notion of pseudospectrum has been developed."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#pseudospectrum",
    "href": "lectures/lecture-6/lecture-6.html#pseudospectrum",
    "title": "Matrix decomposition: the Schur form",
    "section": "Pseudospectrum",
    "text": "Pseudospectrum\nWe consider the union of all possible eigenvalues of all perturbations of the matrix A.\n\\Lambda_{\\epsilon}(A) = \\{ \\lambda \\in \\mathbb{C}: \\exists E, x \\ne 0: (A + E) x = \\lambda x, \\quad \\Vert E \\Vert_2 \\leq \\epsilon. \\}\n\nFor small E and normal A these will be circules around eigenvalues, for non-normal matrices, the structure can be much different. More details: http://www.cs.ox.ac.uk/pseudospectra/"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#summary-of-todays-lecture",
    "href": "lectures/lecture-6/lecture-6.html#summary-of-todays-lecture",
    "title": "Matrix decomposition: the Schur form",
    "section": "Summary of todays lecture",
    "text": "Summary of todays lecture\n\nEigenvalues, eigenvectors\nGershgorin theorem\nPower method\nSchur theorem\nNormal matrices\nSome advanced topics"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#next-lecture",
    "href": "lectures/lecture-6/lecture-6.html#next-lecture",
    "title": "Matrix decomposition: the Schur form",
    "section": "Next lecture",
    "text": "Next lecture\n\nReview of the considered matrix decompositions\nPractical way to compute QR decomposition\nAlmost practical method for computing eigenvalues and eigenvectors\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html",
    "href": "lectures/lecture-9/lecture-9.html",
    "title": "Questions?",
    "section": "",
    "text": "SVD and algorithms for its computations: divide-and-conquer, QR, Jacobi, bisection."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#brief-recap-of-the-previous-lecture",
    "href": "lectures/lecture-9/lecture-9.html#brief-recap-of-the-previous-lecture",
    "title": "Questions?",
    "section": "",
    "text": "SVD and algorithms for its computations: divide-and-conquer, QR, Jacobi, bisection."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#todays-lecture",
    "href": "lectures/lecture-9/lecture-9.html#todays-lecture",
    "title": "Questions?",
    "section": "Todays lecture",
    "text": "Todays lecture\nToday, we will do a brief dive into the randomized NLA.\nA good read is (https://arxiv.org/pdf/2002.01387.pdf)"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#random-numbers",
    "href": "lectures/lecture-9/lecture-9.html#random-numbers",
    "title": "Questions?",
    "section": "Random numbers",
    "text": "Random numbers\nAll the computations that we considered up to today were deterministic.\nHowever, reduction of complexity can be done by using randomized (stochastic) computation.\nExample: randomized matrix multiplication."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#checking-matrix-equality",
    "href": "lectures/lecture-9/lecture-9.html#checking-matrix-equality",
    "title": "Questions?",
    "section": "Checking matrix equality",
    "text": "Checking matrix equality\nWe can check, if $ A B = C$ in \\mathcal{O}(n^2) operations.\nHow?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#freivalds-algorithm",
    "href": "lectures/lecture-9/lecture-9.html#freivalds-algorithm",
    "title": "Questions?",
    "section": "Freivalds algorithm",
    "text": "Freivalds algorithm\nChecks by multiplying by random vectors!\nComplexity is k n^2, probability is of failure is \\frac{1}{2^k}."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#matrix-multiplication",
    "href": "lectures/lecture-9/lecture-9.html#matrix-multiplication",
    "title": "Questions?",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nBut can we multiply matrices faster using randomization ideas?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#randomized-matrix-multiplication",
    "href": "lectures/lecture-9/lecture-9.html#randomized-matrix-multiplication",
    "title": "Questions?",
    "section": "Randomized matrix multiplication",
    "text": "Randomized matrix multiplication\n\nWe know that matrix multiplication AB costs O(mnp) for matrices m \\times p and p \\times n\nWe can construct approximation of this product by sampling rows and columns of the multipliers\n\nQ: how to sample them?\nA: generate probabilities from their norms!\n\nSo the final approximation expression\n\n AB \\approx \\sum_{t=1}^k \\frac{1}{kp_{i_t}} A^{(i_t)} B_{(i_t)}, \nwhere A^{(i_t)} is a column of A and B_{(i_t)} is a row of B\n\nComplexity reduction from O(mnp) to O(mnk)\n\n\nimport numpy as np\n\nn = 1\np = 10000\nm = 1\nA = np.random.randn(n, p)\nB = np.random.randn(p, m)\nC = A @ A.T\n\ndef randomized_matmul(A, B, k):\n    p1 = A.shape[1]\n    p = np.linalg.norm(A, axis=0) * np.linalg.norm(B, axis=1)\n    p = p\n    p = p.ravel() / p.sum()\n    n = A.shape[1]\n    p = np.ones(p1)\n    p = p/p.sum()\n    idx = np.random.choice(np.arange(n), (k,), False, p)\n    #d = 1 / np.sqrt(k * p[idx])\n    d = 1.0/np.sqrt(k)#np.sqrt(p1)/np.sqrt(k*p[idx])\n    A_sketched = A[:, idx]*np.sqrt(p1)/np.sqrt(k)#* d[None, :]\n    B_sketched = B[idx, :]*np.sqrt(p1)/np.sqrt(k) #* d[:, None]\n    C = A_sketched @ B_sketched\n    print(d)\n    return C\n\ndef randomized_matmul_topk(A, B, K):\n    \n    norm_mult = np.linalg.norm(A,axis=0) * np.linalg.norm(B,axis=1)\n    top_k_idx = np.sort(np.argsort(norm_mult)[::-1][:K])\n    \n    A_top_k_cols = A[:, top_k_idx]\n    B_top_k_rows = B[top_k_idx, :]\n\n    C_approx = A_top_k_cols @ B_top_k_rows\n    return C_approx\n\nnum_items = 3000\nC_appr_samples = randomized_matmul(A, B, num_items)\nprint(C_appr_samples, 'appr')\nprint(C, 'true')\nC_appr_topk = randomized_matmul_topk(A, B, num_items)\nprint(np.linalg.norm(C_appr_topk - C, 2) / np.linalg.norm(C, 2))\nprint(np.linalg.norm(C_appr_samples - C, 2) / np.linalg.norm(C, 2))\n\n0.018257418583505537\n[[-209.68265641]] appr\n[[10065.73675927]] true\n1.012091041179466\n1.020831327246555"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#approximation-error",
    "href": "lectures/lecture-9/lecture-9.html#approximation-error",
    "title": "Questions?",
    "section": "Approximation error",
    "text": "Approximation error\n \\mathbb{E} [\\|AB - CR\\|^2_F] = \\frac{1}{k} \\left(\\sum_{i=1}^n \\| A^{(i)} \\|_2 \\| B_{(i)} \\|_2\\right)^2   - \\frac{1}{k}\\|AB\\|_F^2 \n\nOther sampling probabilities are possible\nUse approximation  AB \\approx ASD(SD)^\\top B  = ACC^{\\top}B can replace sampling and scaling with another matrix that\n\nreduces the dimension\nsufficiently accurately approximates\n\n\nQ: what matrices can be used?\n\nStochastic trace estimator\nMany problems can be written in the form of the trace estimation:\n\\mathrm{Tr}(A) = \\sum_{i} A_{ii}.\nCan we compute the trace of the matrix if we only have access to matrix-by-vector products?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#two-estimators",
    "href": "lectures/lecture-9/lecture-9.html#two-estimators",
    "title": "Questions?",
    "section": "Two estimators",
    "text": "Two estimators\nThe randomized trace estimators can be computed from the following formula:\n\\mathrm{Tr}(A) = E_w w^* A w, \\quad E ww^* = 1\nIn order to sample, we pick k independent samples of w_k, get random variable X_k and average the results.\nGirard trace estimator: Sample w \\sim N(0, 1)\nThen, \\mathrm{Var} X_k = \\frac{2}{k} \\sum_{i, j=1}^n \\vert A_{ij} \\vert^2 = \\frac{2}{k} \\Vert A \\Vert^2_F\nHutchinson trace estimator: Let w be a Rademacher random vector (i.e., elements are sampled from the uniform distribution.\nIt gives the minimal variance estimator."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#intdim",
    "href": "lectures/lecture-9/lecture-9.html#intdim",
    "title": "Questions?",
    "section": "Intdim",
    "text": "Intdim\nThe variance of the trace can be estimated in terms of intrinsic dimension (intdim) for symmetric positive definite matrices.\nIt is defined as \\mathrm{intdim}(A) = \\frac{\\mathrm{Tr}(A)}{\\Vert A \\Vert_F}. It is easy to show that\n1 \\leq \\mathrm{intdim}(A) \\leq ?.\nThen, the probability of the large deviation can be estimated as\nP( \\vert \\overline{X}_k - \\mathrm{Tr}(A) \\vert \\geq t \\mathrm{Tr}(A)) \\leq \\frac{2}{k \\mathrm{intdim}(A) t^2}"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#better-bounds-for-spd-matrices",
    "href": "lectures/lecture-9/lecture-9.html#better-bounds-for-spd-matrices",
    "title": "Questions?",
    "section": "Better bounds for SPD matrices",
    "text": "Better bounds for SPD matrices\nIf A is SPD, then\nP(\\overline{X}_k \\geq \\tau \\mathrm{Tr}(A) ) \\leq \\exp\\left(-1/2 \\mathrm{intdim}(A) (\\sqrt{\\tau} - 1)^2)\\right) \nSimilar inequality holds for the lower bound.\nThis estimate is much better.\nAn interesting (and often mislooked) property of stochastic estimator is that it comes with a stochastic variance estimate (from samples!)\nWarning: we still need \\varepsilon^{-2} samples to get to the accuracy \\varepsilon when using independent samples.\n\nDistances between languages (original paper)"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#where-do-stochastic-methods-also-help",
    "href": "lectures/lecture-9/lecture-9.html#where-do-stochastic-methods-also-help",
    "title": "Questions?",
    "section": "Where do stochastic methods also help?",
    "text": "Where do stochastic methods also help?\n\nSVD\nLinear systems"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#randomized-svd-halko-et-al-2011",
    "href": "lectures/lecture-9/lecture-9.html#randomized-svd-halko-et-al-2011",
    "title": "Questions?",
    "section": "Randomized SVD (Halko et al, 2011)",
    "text": "Randomized SVD (Halko et al, 2011)\n\nProblem statement reminder\n\n A \\approx U\\Sigma V^\\top, \nwhere A is of size m \\times n, U is of size m \\times k and V is of size n \\times k.\n\nWe have already known that the complexity of rank-k approximation is O(mnk)\nHow can we reduce this complexity?\nAssume we know orthogonal matrix Q of size m \\times k such that\n\nA \\approx Q Q^{\\top}A \n\nIn other words, columns of Q represent orthogonal basis in the column space of matrix A\nThen the following deterministic steps can give the factors U, \\Sigma and V corresponding of SVD of matrix A\n\nForm k \\times n matrix B = Q^{\\top}A\nCompute SVD of small matrix B = \\hat{U}\\Sigma V^{\\top}\nUpdate left singular vectors U = Q\\hat{U}\n\nIf k \\ll \\min(m, n) then these steps can be performed fast\nIf Q forms exact basis in column space of A, then U, \\Sigma and V are also exact!\nSo, how to compose matrix Q?\n\n\nRandomized approximation of basis in column space of A\n\nThe main approach\n\nGenerate k + p Gaussian vectors of size m and form matrix G\nCompute Y = AG\nCompute QR decomposition of Y and use the resulting matrix Q as an approximation of the basis\n\nParameter p is called oversampling parameter and is needed to improve approximation of the leading k left singular vectors later\nComputing of Y can be done in parallel\nHere we need only matvec function for matrix A rather than its elements as a 2D array - black-box concept!\nInstead of Gaussian random matrix one can use more structured but still random matrix that can be multiplied by A fast\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn = 1000\nk = 100\nm = 200\n# Lowrank matrix\nA = np.random.randn(n, k)\nB = np.random.randn(k, m)\nA = A @ B\n\n# Random matrix\n# A = np.random.randn(n, m)\n\ndef randomized_svd(A, rank, p):\n    m, n = A.shape\n    G = np.random.randn(n, rank + p)\n    Y = A @ G\n    Q, _ = np.linalg.qr(Y)\n    B = Q.T @ A\n    u, S, V = np.linalg.svd(B)\n    U = Q @ u\n    return U, S, V\n\nrank = 100\np = 5\nU, S, V = randomized_svd(A, rank, p)\nprint(\"Error from randomized SVD\", np.linalg.norm(A - U[:, :rank] * S[None, :rank] @ V[:rank, :]))\nplt.semilogy(S[:rank] / S[0], label=\"Random SVD\")\nu, s, v = np.linalg.svd(A)\nprint(\"Error from exact SVD\", np.linalg.norm(A - u[:, :rank] * s[None, :rank] @ v[:rank, :]))\nplt.semilogy(s[:rank] / s[0], label=\"Exact SVD\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\nError from randomized SVD 1.7704601563939492e-11\nError from exact SVD 1.195330542835496e-11\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as spsplin\n# More details about Facebook package for computing randomized SVD is here: https://research.fb.com/blog/2014/09/fast-randomized-svd/ \nimport fbpca\nn = 1000\nm = 200\nA = np.random.randn(n, m)\nk = 10\np = 10\n%timeit spsplin.svds(A, k=k)\n%timeit randomized_svd(A, k, p)\n%timeit fbpca.pca(A, k=k, raw=False) \n\n60.5 ms ± 11.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n8.07 ms ± 3.32 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n3.09 ms ± 177 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nConvergence theorem\nThe averaged error of the presented algorithm, where k is target rank and p is oversampling parameter, is the following - in Frobenius norm\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_F \\leq \\left( 1 + \\frac{k}{p-1} \\right)^{1/2}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^2_j \\right)^{1/2}  \n\nin spectral norm\n\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_2 \\leq \\left( 1 + \\sqrt{\\frac{k}{p-1}} \\right)\\sigma_{k+1} + \\frac{e\\sqrt{k+p}}{p}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^2_j \\right)^{1/2} \nThe expectation is taken w.r.t. random matrix G generated in the method described above.\nCompare these upper bounds with Eckart-Young theorem. Are these bounds good?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#accuracy-enhanced-randomized-svd",
    "href": "lectures/lecture-9/lecture-9.html#accuracy-enhanced-randomized-svd",
    "title": "Questions?",
    "section": "Accuracy enhanced randomized SVD",
    "text": "Accuracy enhanced randomized SVD\n\nMain idea: power iteration\nIf A = U \\Sigma V^\\top, then $A^{(q)} = (AA{})qA = U {2q+1}V$, where q some small natural number, e.g. 1 or 2\nThen we sample from A^{(q)}, not from A\n\n Y = (AA^{\\top})^qAG \\qquad Q, R = \\mathtt{qr}(Y) \n\nThe main reason: if singular values of A decays slowly, the singular values of A^{(q)} will decay faster\n\n\nn = 1000\nm = 200\nA = np.random.randn(n, m)\ns = np.linalg.svd(A, compute_uv=False)\nAq = A @ A.T @ A\nsq = np.linalg.svd(Aq, compute_uv=False)\nplt.semilogy(s / s[0], label=\"$A$\")\nplt.semilogy(sq / sq[0], label=\"$A^{(1)}$\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\n\n\n\n\n\n\n\n\nLoss of accuracy with rounding errors\n\nCompose A^{(q)} naively leads to condition number grows and loss of accuracy\n\nQ: how can we battle with this issue?\nA: sequential orthogonalization!\n\ndef more_accurate_randomized_svd(A, rank, p, q):\n    m, n = A.shape\n    G = np.random.randn(n, rank + p)\n    Y = A @ G\n    Q, _ = np.linalg.qr(Y)\n    for i in range(q):\n        W = A.T @ Q\n        W, _ = np.linalg.qr(W)\n        Q = A @ W\n        Q, _ = np.linalg.qr(Q)\n    B = Q.T @ A\n    u, S, V = np.linalg.svd(B)\n    U = Q @ u\n    return U, S, V\n\nn = 1000\nm = 200\nA = np.random.randn(n, m)\n\nrank = 100\np = 20\nU, S, V = randomized_svd(A, rank, p)\nprint(\"Error from randomized SVD\", np.linalg.norm(A - U[:, :rank] * S[None, :rank] @ V[:rank, :]))\nplt.semilogy(S[:rank] / S[0], label=\"Random SVD\")\n\nUq, Sq, Vq = more_accurate_randomized_svd(A, rank, p, 5)\nprint(\"Error from more accurate randomized SVD\", np.linalg.norm(A - Uq[:, :rank] * Sq[None, :rank] @ Vq[:rank, :]))\nplt.semilogy(Sq[:rank] / Sq[0], label=\"Accurate random SVD\")\n\nu, s, v = np.linalg.svd(A)\nprint(\"Error from exact SVD\", np.linalg.norm(A - u[:, :rank] * s[None, :rank] @ v[:rank, :]))\nplt.semilogy(s[:rank] / s[0], label=\"Exact SVD\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\nError from randomized SVD 286.99760873015225\nError from more accurate randomized SVD 250.2388642432797\nError from exact SVD 249.3503301291079\n\n\n\n\n\n\n\n\n\n\n%timeit spsplin.svds(A, k=k)\n%timeit fbpca.pca(A, k=k, raw=False)\n%timeit randomized_svd(A, k, p) \n%timeit more_accurate_randomized_svd(A, k, p, 1)\n%timeit more_accurate_randomized_svd(A, k, p, 2)\n%timeit more_accurate_randomized_svd(A, k, p, 5)\n\n347 ms ± 60.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n82.3 ms ± 6.93 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n68.7 ms ± 4.99 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n118 ms ± 6.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n176 ms ± 13.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n352 ms ± 43.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\nConvergence theorem\nThe presented above method provides the following upper bound\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_2 \\leq \\left[\\left( 1 + \\sqrt{\\frac{k}{p-1}} \\right)\\sigma^{2q+1}_{k+1} + \\frac{e\\sqrt{k+p}}{p}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^{2(2q+1)}_j \\right)^{1/2}\\right]^{1/(2q+1)} \nConsider the worst case, where no lowrank structure exists in the given matrix.\nQ: what is the degree of suboptimality w.r.t. Eckart-Young theorem?\n\n\nSummary on randomized SVD\n\nEfficient method to get approximate SVD\nSimple to implement\nIt can be extended to one-pass method, where matrix A is needed only to construct Q\nIt requires only matvec with target matrix"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#kaczmarz-method-to-solve-linear-systems",
    "href": "lectures/lecture-9/lecture-9.html#kaczmarz-method-to-solve-linear-systems",
    "title": "Questions?",
    "section": "Kaczmarz method to solve linear systems",
    "text": "Kaczmarz method to solve linear systems\n\nWe have already discussed how to solve overdetermined linear systems Ax = f in the least-squares manner\n\npseudoinverse matrix\nQR decomposition\n\nOne more approach is based on iterative projections a.k.a. Kaczmarz method or algebraic reconstruction technique in compoutational tomography domain\nInstead of solving all equations, pick one randomly, which reads\n\na^{\\top}_i x = f_i,\nand given an approximation x_k try to find x_{k+1} as\nx_{k+1} = \\arg \\min_x \\frac12 \\Vert x - x_k \\Vert^2_2, \\quad \\mbox{s.t.} \\quad  a^{\\top}_i x = f_i.\n\nA simple analysis gives\n\nx_{k+1} = x_k - \\frac{(a_i, x_k) - f_i}{(a_i, a_i)} a_i. \n\nA cheap update, but the analysis is quite complicated.\nYou can recognize in this method stochastic gradient descent with specific step size equal to \\frac{1}{\\|a_i\\|_2^2} for every sample"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#convergence-theorem-2",
    "href": "lectures/lecture-9/lecture-9.html#convergence-theorem-2",
    "title": "Questions?",
    "section": "Convergence theorem",
    "text": "Convergence theorem\n\nAssume we generate i according to the distribution over the all available indices proportional to norms of the rows, i.e. \\mathbb{P}[i = k] = \\frac{\\|a_k\\|_2^2}{\\| A \\|^2_F}. This method is called Randomized Kaczmarz method (RKM)\nWhy sampling strategy is important here?\nInvestigation of the best sampling is provided here\nIf the overdetermined linear system is consistent, then\n\n \\mathbb{E}[\\|x_{k+1} - x^*\\|^2_2] \\leq \\left(1 - \\frac{1}{\\kappa^2_F(A)}\\right) \\mathbb{E}[\\|x_{k} - x^*\\|^2_2], \nwhere \\kappa_F(A) = \\frac{\\| A \\|_F}{\\sigma_{\\min}(A)} and \\sigma_{\\min}(A) is a minimal non-zero singular value of A. This result was presented in (Strohmer and Vershynin, 2009)\n\nIf the overdetermined linear system is inconsistent, then\n\n \\mathbb{E}[\\|x_{k+1} - x^*\\|^2_2] \\leq \\left(1 - \\frac{1}{\\kappa^2_F(A)}\\right) \\mathbb{E}[\\|x_{k} - x^*\\|^2_2] + \\frac{\\|r^*\\|_2^2}{\\| A \\|^2_F}, \nwhere r^* = Ax^* - f\n\nInconsistent overdetermined linear system\n\nIt was shown in (Needell, 2010) that RKM does not converge to A^{\\dagger}f\nTo address this issue Randomized extended Kaczmarz method was proposed in (A Zouzias, N Freris, 2013)\nThe main idea is to use two steps of RKM:\n\nthe first step is for system A^\\top z = 0 starting from z_k\n\n z^{k+1} = z^{k} - \\frac{a^\\top_{:, j} z^k}{\\| a_{:, j} \\|_2^2}a_{:, j}  \n\nthe second step is for system Ax = f - z_{k+1} starting from x_k\n\nx^{k+1} = x^k - \\frac{a_{i,:}x_k - f_i + z^{k+1}_i}{\\|a_{i,:}\\|_2^2}a^{\\top}_{i,:} \n\nHere a_{:, j} denotes the j-th column of A and a_{i, :} denotes the i-th row of A\n\nIf z^0 \\in f + \\mathrm{range}(A) and x^0 \\in \\mathrm{range}(A^\\top), then REK converges exponentially to A^{\\dagger}f"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#sampling-and-sketching",
    "href": "lectures/lecture-9/lecture-9.html#sampling-and-sketching",
    "title": "Questions?",
    "section": "Sampling and sketching",
    "text": "Sampling and sketching\n\nSampling of a particular row can be considered as a particular case of more general approach called sketching\nIdea: replace matrix A with another matrix SA, where matrix SA has significantly smaller number of rows but preserves some important properties of matrix A\nPossible choices:\n\nrandom projection\nrandom row selection\n\nExample: linear least squares problem \\|Ax - b\\|_2^2 \\to \\min_x transforms to \\| (SA)y - Sb \\|_2^2 \\to \\min_y and we expect that x \\approx y\nBlendenpick solver is based on that idea and outperforms LAPACK routine\nMore details see in Sketching as a Tool for Numerical Linear Algebra by D. Woodruff"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#coherence",
    "href": "lectures/lecture-9/lecture-9.html#coherence",
    "title": "Questions?",
    "section": "Coherence",
    "text": "Coherence\nThe key idea is the coherence of the matrix.\nLet A be n \\times r and U be an orthogonal matrix whose columns form the basis of the column space of A.\nThen, coherence is defined as\n\\mu(A) = \\max \\Vert U_{i, *} \\Vert^2\nCoherence is always smaller than 1 and bigger than \\frac{r}{n}, it has nothing to do with the condition number.\nWhat does it mean?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#coherence-1",
    "href": "lectures/lecture-9/lecture-9.html#coherence-1",
    "title": "Questions?",
    "section": "Coherence",
    "text": "Coherence\nSmall coherence means, that sampling rows uniformly gives a good preconditioner (will be covered later in the course, and why it is important)\nOne can do S A = QR, and look at the condition number of AR^{-1}.\n\nSummary on randomized methods in solving linear systems\n\nEasy to use family of methods\nEspecially useful in problems with streaming data\nExisting theoretical bounds for convergence\nMany interpretations in different domains (SGD in deep learning, ART in computational tomography)\n\n\n\nSummary on randomized matmul\n\nSimple method to get approximation of result\nCan be used if the high accuracy is not crucial\nEspecially useful for large dense matrices"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#next-lecture",
    "href": "lectures/lecture-9/lecture-9.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nWe start sparse and/or structured NLA."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html",
    "href": "lectures/lecture-10/lecture-10.html",
    "title": "Questions?",
    "section": "",
    "text": "Randomized matmul\nHutchinson trace estimator\nRandomized SVD\nKarcmarz"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#recap-of-the-previous-lecture",
    "href": "lectures/lecture-10/lecture-10.html#recap-of-the-previous-lecture",
    "title": "Questions?",
    "section": "",
    "text": "Randomized matmul\nHutchinson trace estimator\nRandomized SVD\nKarcmarz"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#plan-of-todays-lecture",
    "href": "lectures/lecture-10/lecture-10.html#plan-of-todays-lecture",
    "title": "Questions?",
    "section": "Plan of todays lecture",
    "text": "Plan of todays lecture\n\nSimple topic in parallel computing in NLA (on the matvec example)\nSparse matrix part (with a separate plan)"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#large-scale-dense-matrices",
    "href": "lectures/lecture-10/lecture-10.html#large-scale-dense-matrices",
    "title": "Questions?",
    "section": "Large scale dense matrices",
    "text": "Large scale dense matrices\n\nIf the size of the dense matrix is huge, then it can not be stored in memory\nPossible options\n\nThis matrix is structured, e.g. block Toeplitz with Toeptitz blocks (next lectures). Then the compressed storage is possible\nFor unstructured dense matrices distributed memory helps\nMPI for processing distributed storing matrices\n\n\n\nDistributed memory and MPI\n\nSplit matrix into blocks and store them on different machines\nEvery machine has its own address space and can not damage data on other machines\nIn this case machines communicate with each other to aggregate results of computations\nMPI (Message Passing Interface) is a standard for parallel computing in distributed memory\n\n\n\nExample: matrix-by-vector product\n\nAssume you want to compute Ax and matrix A can not be stored in available memory\nThen you can split it on blocks and distribute blocks on separate machines\nPossible strategies\n\n1D blocking splits only rows on blocks\n2D blocking splits both rows and columns\n\n\n\n1D blocking scheme\n\n\n\nTotal time of computing matvec with 1D blocking\n\nEach machine has $n / p $ complete rows and n / p elements of vector\nTotal operations are n^2 / p\nTotal time for sending and writing data are t_s \\log p + t_w n, where t_s time unit for sending and t_w time unit for writing\n\n\n\n2D blocking scheme\n\n\n\nTotal time of computing matvec with 2D blocking\n\nEach machine has $n / $ size block and n / \\sqrt{p} elements of vector\nTotal operations are n^2 / p\nTotal time for sending and writing data are approximately t_s \\log p + t_w (n/\\sqrt{p}) \\log p, where t_s time unit for sending and t_w time unit for writing\n\n\n\n\nPackages supported distributed storage\n\nScaLAPACK\nTrilinos\n\nIn Python you can use mpi4py for parallel programming of your algorithm.\n\nPyTorch supports distributed training and data storage, see details here\n\n\n\nSummary on large unstructered matrix processing\n\nDistributed manner of storage\nMPI\nPackages that use parallel computations\nDifferent blocking strategies"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#sparse-matrices-intro",
    "href": "lectures/lecture-10/lecture-10.html#sparse-matrices-intro",
    "title": "Questions?",
    "section": "Sparse matrices intro",
    "text": "Sparse matrices intro\n\nFor dense linear algebra problems, we are limited by the memory to store the full matrix, it is N^2 parameters.\nThe class of sparse matrices where most of the elements are zero, allows us at least to store such matrices.\n\nThe question if we can:\n\nsolve linear systems\nsolve eigenvalue problems\n\nwith sparse matrices"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#plan-for-the-next-part-of-the-lecture",
    "href": "lectures/lecture-10/lecture-10.html#plan-for-the-next-part-of-the-lecture",
    "title": "Questions?",
    "section": "Plan for the next part of the lecture",
    "text": "Plan for the next part of the lecture\nNow we will talk about sparse matrices, where they arise, how we store them, how we operate with them.\n\nFormats: list of lists and compressed sparse row format, relation to graphs\nMatrix-by-vector product\nParallell processing of sparse matrices\nFast direct solvers for Gaussian elimination (start)"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#applications-of-sparse-matrices",
    "href": "lectures/lecture-10/lecture-10.html#applications-of-sparse-matrices",
    "title": "Questions?",
    "section": "Applications of sparse matrices",
    "text": "Applications of sparse matrices\nSparse matrices arise in:\n\npartial differential equations (PDE), mathematical modelling\ngraphs mining, e.g. social networks analysis\nrecommender systems\nwherever relations between objects are “sparse”.\n\n\nSparse matrices are ubiquitous in PDE\nThe simplest partial differential equation (PDE), called\nLaplace equation:\n\n   \\Delta T = \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} = f(x,y), \\quad x,y\\in \\Omega\\equiv[0,1]^2,\n\n\n    T_{\\partial\\Omega} = 0.\n\n\nDiscretization\n\\frac{\\partial^2 T}{\\partial x^2} \\approx \\frac{T(x+h) + T(x-h) - 2T(x)}{h^2} + \\mathcal{O}(h^2),\nsame for \\frac{\\partial^2 T}{\\partial y^2}, and we get a linear system.\nFirst, let us consider one-dimensional case:\nAfter the discretization of the one-dimensional Laplace equation with Dirichlet boundary conditions we have\n\\frac{u_{i+1} + u_{i-1} - 2u_i}{h^2} = f_i,\\quad i=1,\\dots,N-1\n u_{0} = u_N = 0 or in the matrix form\n A u = f, and (for n = 5) A=-\\frac{1}{h^2}\\begin{bmatrix} 2 & -1 & 0 & 0 & 0\\\\ -1 & 2 & -1 & 0 &0 \\\\ 0 & -1 & 2& -1 & 0 \\\\ 0 & 0 & -1 & 2  &-1\\\\ 0 & 0 & 0 & -1 & 2 \\end{bmatrix}\nThe matrix is triadiagonal and sparse\n(and also Toeplitz: all elements on the diagonal are the same)\n\n\nBlock structure in 2D\nIn two dimensions, we get equation of the form\n-\\frac{4u_{ij} -u_{(i-1)j} - u_{(i+1)j} - u_{i(j-1)}-u_{i(j+1)}}{h^2} = f_{ij},\nor in the Kronecker product form\n\\Delta_{2D} = \\Delta_{1D} \\otimes I + I \\otimes \\Delta_{1D},\nwhere \\Delta_{1D} is a 1D Laplacian, and \\otimes is a Kronecker product of matrices.\nFor matrices A\\in\\mathbb{R}^{n\\times m} and B\\in\\mathbb{R}^{l\\times k} its Kronecker product is defined as a block matrix of the form\n\n   A\\otimes B = \\begin{bmatrix}a_{11}B & \\dots & a_{1m}B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n1}B & \\dots & a_{nm}B\\end{bmatrix}\\in\\mathbb{R}^{nl\\times mk}.\n\nIn the block matrix form the 2D-Laplace matrix can be written in the following form:\nA = -\\frac{1}{h^2}\\begin{bmatrix} \\Delta_1 + 2I & -I & 0 & 0 & 0\\\\ -I & \\Delta_1 + 2I  & -I & 0 &0 \\\\ 0 & -I & \\Delta_1 + 2I & -I & 0 \\\\ 0 & 0 & -I & \\Delta_1 + 2I   &-I\\\\ 0 & 0 & 0 & -I & \\Delta_1 + 2I \\end{bmatrix}\n\nShort list of Kronecker product properties\n\nIt is bilinear\n(A\\otimes B) (C\\otimes D) = AC \\otimes BD\nLet \\mathrm{vec}(X) be vectorization of matrix X columnwise. Then \\mathrm{vec}(AXB) = (B^T \\otimes A) \\mathrm{vec}(X).\n\n\n\n\n\nSparse matrices help in computational graph theory\n\nGraphs are represented with adjacency matrix, which is usually sparse\nNumerical solution of graph theory problems are based on processing of this sparse matrix\n\nCommunity detection and graph clustering\nLearning to rank\nRandom walks\nOthers\n\nExample: probably the largest publicly available hyperlink graph consists of 3.5 billion web pages and 128 billion hyperlinks, more details see here\nMore medium scale graphs to test your algorithms are available in Stanford Large Network Dataset Collection\n\n\n\nSuiteSpare matrix collection (formerly Florida sparse matrix collection)\nMore sparse matrices you can find in SuiteSparse matrix collection which contains all sorts of matrices for different applications.\n\nfrom IPython.display import IFrame\nIFrame(\"http://yifanhu.net/GALLERY/GRAPHS/search.html\", width=700, height=450)\n\n\n        \n        \n\n\n\n\nSparse matrices and deep learning\n\nDNN has a lot of parameters\nSome of them may be redundant\nHow to prune the parameters without significantly accuracy reduction?\nSparse variational dropout method leads to significantly sparse filters in DNN almost without accuracy decreasing"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#sparse-matrix-construction",
    "href": "lectures/lecture-10/lecture-10.html#sparse-matrix-construction",
    "title": "Questions?",
    "section": "Sparse matrix: construction",
    "text": "Sparse matrix: construction\n\nWe can create sparse matrix using scipy.sparse package (actually this is not the best sparse matrix package)\nWe can go to really large sizes (at least, to store this matrix in the memory)\n\nPlease note the following functions - Create sparse matrices with given diagonals spdiags - Kronecker product of sparse matrices kron - There is also overloaded arithmectics for sparse matrices\n\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nfrom scipy.sparse import csc_matrix, csr_matrix\nimport matplotlib.pyplot as plt\nimport scipy.linalg\nimport scipy.sparse.linalg\n%matplotlib inline\nn = 128\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csc_matrix(A)\nplt.spy(A, aspect='equal', marker='.', markersize=5)\n\n\n\n\n\n\n\n\n\nSparsity pattern\n\nThe spy command plots the sparsity pattern of the matrix: the (i, j) pixel is drawn, if the corresponding matrix element is non-zero.\nSparsity pattern is really important for the understanding the complexity of the sparse linear algebra algorithms.\nOften, only the sparsity pattern is needed for the analysis of “how complex” the matrix is.\n\n\n\nSparse matrix: definition\n\nThe definition of “sparse matrix” is that the number of non-zero elements is much less than the total number of elements.\nYou can do the basic linear algebra operations (like solving linear systems at the first place) faster, than if working for with the full matrix."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#what-we-need-to-find-out-to-see-how-it-actually-works",
    "href": "lectures/lecture-10/lecture-10.html#what-we-need-to-find-out-to-see-how-it-actually-works",
    "title": "Questions?",
    "section": "What we need to find out to see how it actually works",
    "text": "What we need to find out to see how it actually works\n\nQuestion 1: How to store the sparse matrix in memory?\nQuestion 2: How to multiply sparse matrix by vector fast?\nQuestion 3: How to solve linear systems with sparse matrices fast?\n\n\nSparse matrix storage\nThere are many storage formats, important ones:\n\nCOO (Coordinate format)\nLIL (Lists of lists)\nCSR (compressed sparse row)\nCSC (compressed sparse column)\nBlock variants\n\nIn scipy there are constructors for each of these formats, e.g. \nscipy.sparse.lil_matrix(A).\n\nCoordinate format (COO)\nThe simplest format is to use coordinate format to represent the sparse matrix as positions and values of non-zero elements.\ni, j, val\nwhere i, j are integer array of indices, val is the real array of matrix elements.  So we need to store 3\\cdot nnz elements, where nnz denotes number of nonzeroes in the matrix.\nQ: What is good and what is bad in this format?\n\n\nMain disadvantages\n\nIt is not optimal in storage (why?)\nIt is not optimal for matrix-by-vector product (why?)\nIt is not optimal for removing elements as you must make nnz operations to find one element (this is good in LIL format)\n\nFirst two disadvantages are solved by compressed sparse row (CSR) format.\n\n\nCompressed sparse row (CSR)\nIn the CSR format a matrix is stored as 3 different arrays:\nia, ja, sa\nwhere:\n\nia (row start) is an integer array of length n+1\nja (column indices) is an integer array of length nnz\nsa (values) is an real-value array of length nnz\n\n\nSo, we got 2\\cdot{\\bf nnz} + n+1 elements.\n\n\n\nSparse matrices in PyTorch and Tensorflow\n\nPyTorch supports sparse matrices stored in COO format\nIncompletre backward operation for these matrices, see summary here\nTensorflow also supports sparse matrices in COO format\nThe list of supported operations is here and gradient support is also limited\n\n\n\nCSR helps in sparse matrix by vector product (SpMV)\n\n   for i in range(n):\n        \n        for k in range(ia[i]:ia[i+1]):\n            \n            y[i] += sa[k] * x[ja[k]]\nLet us do a short timing test\n\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nimport scipy.sparse.linalg\nfrom scipy.sparse import csc_matrix, csr_matrix, coo_matrix\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 1024\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\nB = coo_matrix(A)\n%timeit A.dot(rhs)\n%timeit B.dot(rhs)\n\n3.25 ms ± 19.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n18.6 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nAs you see, CSR is faster, and for more unstructured patterns the gain will be larger.\n\n\nSparse matrices and efficiency\n\nSparse matrices give complexity reduction.\n\nBut they are not very good for parallel/GPU implementation.\n\nThey do not give maximal efficiency due to random data access.\n\nTypically, peak efficiency of 10\\%-15\\% is considered good.\n\n\n\nRecall how we measure efficiency of linear algebra operations\nThe standard way to measure the efficiency of linear algebra operations on a particular computing architecture is to use flops (number of floating point operations per second)\nWe can measure peak efficiency of an ordinary matrix-by-vector product.\n\nimport numpy as np\nimport time\nn = 4000\nk = 1400\na = np.random.randn(n, n)\nv = np.random.randn(n, k)\nt = time.time()\nnp.dot(a, v)\nt = time.time() - t\nprint('Time: {0: 3.1e}, Efficiency: {1: 3.1e} Gflops'.\\\n      format(t,  ((k*2 * n ** 2)/t) / 10 ** 9))\n\nTime:  2.0e-01, Efficiency:  2.3e+02 Gflops\n\n\n\nn = 4000000\nk = 40\nex = np.ones(n)\na = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.random.randn(n, k)\nt = time.time()\na.dot(rhs)\nt = time.time() - t\nprint('Time: {0: 3.1e}, Efficiency: {1: 3.1e} Gflops'.\\\n      format(t,  (3 * n * k) / t / 10 ** 9))\n\nTime:  2.6e-01, Efficiency:  1.9e+00 Gflops\n\n\n\n\nRandom data access and cache misses\n\nInitially all matrix and vector entries are stored in RAM (Random Access Memory)\nIf you want to compute matvec, the part of matrix and vector elements are moved to cache (fast and small capacity memory, see lecture about Strassen algorithm)\nAfter that, CPU takes data from cache to proccess it and then returns result in cache, too\nIf CPU needs some data that is not already in cache, this situation is called cache miss\nIf cache miss happens, the required data is moved from RAM to cache\n\nQ: what if cache does not have free space?\n\nThe larger number of cache misses, the slower computations\n\n\n\nCache scheme and LRU (Least recently used)\n\n\nCSR sparse matrix by vector product\n\n   for i in range(n):\n        \n        for k in range(ia[i]:ia[i+1]):\n            \n            y[i] += sa[k] * x[ja[k]]\n            \n\nWhat part of operands is strongly led cache misses?\n\nHow this issue can be solved?\n\n\n\n\nReordering reduces cache misses\n\nIf ja stores sequential elements, then they will be moved to cache altogether and number of cache misses decreases\nThis happens when sparse matrix is banded or at least block diagonal\nWe can convert given sparse matrix to banded or block diagonal with permutations\nLet P be row permutation matrix and Q be column permutation matrix\nA_1 = PAQ is a matrix, which has less bandwith than A\ny = Ax \\to \\tilde{y} = A_1 \\tilde{x}, where \\tilde{x} = Q^{\\top}x and \\tilde{y} = Py\nSeparated block diagonal form is a cache-oblivious format for sparse matrix by vector product\nIt can be extended for 2D, where separated not only rows, but also columns\n\n\nExample\n\nSBD in 1D \n\n\n\n\nSparse transpose matrix-by-vector product\n\nIn some cases it is important to compute not only Ax for sparse A, but also A^{\\top}x\nMort details will be discussed in the lecture about Krylov methods for non-symmetric linear systems\nTransposing is computationally expensive\nHere is proposed compressed sparse block format of storage appropriate for this case\n\n\nCompressed sparse block (CSB)\n\nSplit matrix in blocks\nStore block indices and indices of data inside each block\nThus, feasible number of bits to store indices\nOrdering of the blocks and inside blocks is impoprtant for parallel implementation\nSwitching between blockrow to blockcolumn makes this format appropriate to transpose matrix by vector product"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#solve-linear-systems-with-sparse-matrices",
    "href": "lectures/lecture-10/lecture-10.html#solve-linear-systems-with-sparse-matrices",
    "title": "Questions?",
    "section": "Solve linear systems with sparse matrices",
    "text": "Solve linear systems with sparse matrices\n\nDirect methods (start today and continue in the next lecture)\n\nLU decomposition\nNumber of reordering techniques to minimize fill-in\n\nKrylov methods\n\nLet us start with small demo of solving sparse linear system…\n\nn = 1024\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\nsol = sp.sparse.linalg.spsolve(A, rhs)\n_, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(sol)\nax1.set_title('Not reshaped solution')\nax2.contourf(sol.reshape((n, n), order='f'))\nax2.set_title('Reshaped solution')\n\nText(0.5, 1.0, 'Reshaped solution')"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#take-home-message",
    "href": "lectures/lecture-10/lecture-10.html#take-home-message",
    "title": "Questions?",
    "section": "Take home message",
    "text": "Take home message\n\nAbout parallel matrix-by-vector product and different blocking.\nCSR format for storage\nCache and parallel issues in sparse matrix processing\nReordering and blocking as a way to solve these issues"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html",
    "href": "lectures/lecture-5/lecture-5.html",
    "title": "",
    "section": "",
    "text": "Matrix rank\nSkeleton decomposition\nLow-rank approximation\nSingular Value Decomposition (SVD)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#previous-lecture",
    "href": "lectures/lecture-5/lecture-5.html#previous-lecture",
    "title": "",
    "section": "",
    "text": "Matrix rank\nSkeleton decomposition\nLow-rank approximation\nSingular Value Decomposition (SVD)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#today-lecture",
    "href": "lectures/lecture-5/lecture-5.html#today-lecture",
    "title": "",
    "section": "Today lecture",
    "text": "Today lecture\n\nLinear systems\nInverse matrix\nCondition number\nGaussian elimination"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#linear-systems",
    "title": "",
    "section": "Linear systems",
    "text": "Linear systems\n\nLinear systems of equations are the basic tool in NLA.\nThey appear as:\n\nLinear regression problems\nDiscretization of partial differential/integral equations\nLinearizations of nonlinear regression problems\nOptimization (i.e., Gauss-Newton and Newton-Raphson methods, KKT)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#linear-equations-and-matrices",
    "href": "lectures/lecture-5/lecture-5.html#linear-equations-and-matrices",
    "title": "",
    "section": "Linear equations and matrices",
    "text": "Linear equations and matrices\n\nFrom school we know about linear equations.\nA linear system of equations can be written in the form\n\n\\begin{align*}\n    &2 x + 3 y = 5\\quad &\\longrightarrow \\quad &2x + 3 y + 0 z = 5\\\\\n    &2 x + 3z = 5\\quad &\\longrightarrow\\quad &2 x + 0 y + 3 z = 5\\\\\n    &x + y = 2\\quad &\\longrightarrow\\quad  & 1 x + 1 y + 0 z = 2\\\\\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#matrix-form",
    "href": "lectures/lecture-5/lecture-5.html#matrix-form",
    "title": "",
    "section": "Matrix form",
    "text": "Matrix form\n\n\\begin{pmatrix}\n2 & 3 & 0 \\\\\n2 & 0 & 3 \\\\\n1 & 1 & 0 \\\\\n\\end{pmatrix}\\begin{pmatrix}\nx \\\\\ny \\\\\nz\n\\end{pmatrix} =\n\\begin{pmatrix}\n5 \\\\\n5 \\\\\n2\n\\end{pmatrix}\n\nor simply\n A u = f,  \nwhere A is a 3 \\times 3 matrix and f is right-hand side"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#overunder-determined-linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#overunder-determined-linear-systems",
    "title": "",
    "section": "Over/under determined linear systems",
    "text": "Over/under determined linear systems\nIf the system Au = f has\n\nmore equations than unknowns it is called overdetermined system (generically, no solution)\nless equations than unknowns it is called underdetermined system (solution is non-unique, to make it unique additional assumptions have to be made)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#existence-of-solutions",
    "href": "lectures/lecture-5/lecture-5.html#existence-of-solutions",
    "title": "",
    "section": "Existence of solutions",
    "text": "Existence of solutions\nA solution to the linear system of equations with a square matrix A\nA u = f\nexists, iff * \\det A \\ne 0\nor\n\nmatrix A has full rank."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#scales-of-linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#scales-of-linear-systems",
    "title": "",
    "section": "Scales of linear systems",
    "text": "Scales of linear systems\nIn different applications, the typical size of the linear systems can be different.\n\nSmall: n \\leq 10^4 (full matrix can be stored in memory, dense matrix)\nMedium: n = 10^4 - 10^6 (typically, sparse or structured matrix)\nLarge: n = 10^8 - 10^9 (typically sparse matrix + parallel computations)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#linear-systems-can-be-big",
    "href": "lectures/lecture-5/lecture-5.html#linear-systems-can-be-big",
    "title": "",
    "section": "Linear systems can be big",
    "text": "Linear systems can be big\n\nWe take a continious problem, discretize it on a mesh with N elements and get a linear system with N\\times N matrix.\n\nExample of a mesh around A319 aircraft (taken from GMSH website).\n\n\nThe main difficulty is that these systems are big: millions or billions of unknowns!"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#linear-systems-can-be-structured",
    "href": "lectures/lecture-5/lecture-5.html#linear-systems-can-be-structured",
    "title": "",
    "section": "Linear systems can be structured",
    "text": "Linear systems can be structured\n\nStoring N^2 elements of a matrix is prohibitive even for N = 100000.\n\nQ: how to work with such matrices?\nA: fortunately, those matrices are structured and require \\mathcal{O}(N) parameters to be stored.\n\nThe most widespread structure are sparse matrices: such matrices have only \\mathcal{O}(N) non-zeros!\nExample (one of the famous matrices around for n = 5):\n\n\n  \\begin{pmatrix}\n  2 & -1 & 0 & 0 & 0 \\\\\n  -1 & 2 & -1 & 0 & 0 \\\\\n  0 & -1 & 2 & -1 & 0 \\\\\n  0 & 0 &-1& 2 & -1  \\\\\n  0 & 0 & 0 & -1 & 2 \\\\\n  \\end{pmatrix}\n\n\nAt least you can store such matrices\nAlso you can multiply such matrix by vector fast\nBut how to solve linear systems?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#main-questions-about-linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#main-questions-about-linear-systems",
    "title": "",
    "section": "Main questions about linear systems",
    "text": "Main questions about linear systems\n\nWhat is the accuracy we get from the solution (due to rounding errors)?\nHow we compute the solution? (LU-decomposition, Gaussian elimination)\nWhat is the complexity of the solution of linear systems?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#how-to-solve-linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#how-to-solve-linear-systems",
    "title": "",
    "section": "How to solve linear systems?",
    "text": "How to solve linear systems?\nImportant: forget about determinants and the Cramer rule (it is good for 2 \\times 2 matrices still)!"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#how-to-solve-linear-systems-1",
    "href": "lectures/lecture-5/lecture-5.html#how-to-solve-linear-systems-1",
    "title": "",
    "section": "How to solve linear systems?",
    "text": "How to solve linear systems?\nThe main tool is variable elimination.\n\\begin{align*}\n    &2 y + 3 x = 5 \\quad&\\longrightarrow \\quad &y = 5/2 -  3/2 x \\\\\n    &2 x + 3z = 5 \\quad&\\longrightarrow\\quad &z = 5/3 - 2/3 x\\\\\n    &z + y = 2 \\quad&\\longrightarrow\\quad  & 5/2 + 5/3 - (3/2 + 2/3) x = 2,\\\\\n\\end{align*}\nand that is how you find x (and all previous ones).\nThis process is called Gaussian elimination and is one of the most widely used algorithms."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#gaussian-elimination",
    "href": "lectures/lecture-5/lecture-5.html#gaussian-elimination",
    "title": "",
    "section": "Gaussian elimination",
    "text": "Gaussian elimination\nGaussian elimination consists of two steps: 1. Forward step 2. Backward step"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#forward-step",
    "href": "lectures/lecture-5/lecture-5.html#forward-step",
    "title": "",
    "section": "Forward step",
    "text": "Forward step\n\nIn the forward step, we eliminate x_1:\n\n\n   x_1 = f_1 - (a_{12} x_2 + \\ldots + a_{1n} x_n)/a_{11},\n\nand then substitute this into the equations 2, \\ldots, n.\n\nThen we eliminate x_2 and so on from the second equation.\nThe important thing is that the pivots (that we divide over) are not equal to 0."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#backward-step",
    "href": "lectures/lecture-5/lecture-5.html#backward-step",
    "title": "",
    "section": "Backward step",
    "text": "Backward step\nIn the backward step: - solve equation for x_n - put it into the equation for x_{n-1} and so on, until we compute all x_i, i=1,\\ldots, n."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#gaussian-elimination-and-lu-decomposition",
    "href": "lectures/lecture-5/lecture-5.html#gaussian-elimination-and-lu-decomposition",
    "title": "",
    "section": "Gaussian elimination and LU decomposition",
    "text": "Gaussian elimination and LU decomposition\n\nGaussian elimination is the computation of one of the most important matrix decompositions: LU-decomposition.\n\nDefinition: LU-decomposition of the square matrix A is the representation\nA =  LU,\nwhere - L is lower triangular (elements strictly above the diagonal are zero) - U is upper triangular matrix (elements strictly below the diagonal are zero)\nThis factorization is non-unique, so it is typical to require that the matrix L has ones on the diagonal.\nMain goal of the LU decomposition is to solve linear system, because\n A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, \nand this reduces to the solution of two linear systems forward step\n L y = f, \nand backward step\n U x = y. \nDoes LU decomposition always exist?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#complexity-of-the-gaussian-eliminationlu-decomposition",
    "href": "lectures/lecture-5/lecture-5.html#complexity-of-the-gaussian-eliminationlu-decomposition",
    "title": "",
    "section": "Complexity of the Gaussian elimination/LU decomposition",
    "text": "Complexity of the Gaussian elimination/LU decomposition\n\nEach elimination step requires \\mathcal{O}(n^2) operations.\nThus, the cost of the naive algorithm is \\mathcal{O}(n^3).\n\nThink a little bit: can Strassen algorithm help here?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#block-lu-decomposition",
    "href": "lectures/lecture-5/lecture-5.html#block-lu-decomposition",
    "title": "",
    "section": "Block LU-decomposition",
    "text": "Block LU-decomposition\nWe can try to compute block version of LU-decomposition:\n\\begin{pmatrix} A_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{pmatrix} = \\begin{pmatrix} L_{11} & 0 \\\\\nL_{21} & L_{22}\n\\end{pmatrix} \\begin{pmatrix} U_{11} & U_{12} \\\\\n0 & U_{22}\n\\end{pmatrix} \n\nThere are two basic operations: compute LU-factorization of half-matrices + matrix-by-matrix product."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#existence-of-the-lu-decomposition",
    "href": "lectures/lecture-5/lecture-5.html#existence-of-the-lu-decomposition",
    "title": "",
    "section": "Existence of the LU-decomposition",
    "text": "Existence of the LU-decomposition\n\nThe LU-decomposition algorithm does not fail if we do not divide by zero at every step of the Gaussian elimination.\n\nQ: when it is so, for which class of matrices?\nA: it is true for strictly regular matrices."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#strictly-regular-matrices-and-lu-decomposition",
    "href": "lectures/lecture-5/lecture-5.html#strictly-regular-matrices-and-lu-decomposition",
    "title": "",
    "section": "Strictly regular matrices and LU decomposition",
    "text": "Strictly regular matrices and LU decomposition\n\nDefinition. A matrix A is called strictly regular, if all of its leading principal minors (i.e, submatrices in the first k rows and k columns) are non-singular.\nIn this case, there always exists an LU-decomposition. The reverse is also true (check!).\n\nCorollary: If L is unit triangular (ones on the diagonal), then LU-decomposition is unique. \nProof: Indeed, L_1 U_1 = L_2 U_2 means L_2^{-1} L_1 = U_2 U_1^{-1}. $L_2^{-1} L_1 $ is lower triangular with ones on the diagonal. U_2 U_1^{-1} is upper triangular. Thus, L_2^{-1} L_1 = U_2 U_1^{-1} = I and L_1 = L_2, U_1 = U_2."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#lu-for-positive-definite-hermitian-matrices-cholesky-factorization",
    "href": "lectures/lecture-5/lecture-5.html#lu-for-positive-definite-hermitian-matrices-cholesky-factorization",
    "title": "",
    "section": "LU for positive definite Hermitian matrices (Cholesky factorization)",
    "text": "LU for positive definite Hermitian matrices (Cholesky factorization)\n\nStrictly regular matrices have LU-decomposition.\nAn important subclass of strictly regular matrices is the class of Hermitian positive definite matrices\n\nDefinition. A matrix A is called  positive definite  if for any x: \\Vert x \\Vert \\ne 0 we have\n\n(x, Ax) &gt; 0.\n - if this holds for x \\in \\mathbb{C}^n, then the matrix A has to be hermitian - if this holds for x \\in \\mathbb{R}^n, then the matrix A can be non symmetric\n\nClaim: A Hermitian positive definite matrix A is strictly regular and has Cholesky factorization of the form\n\nA = RR^*,\nwhere R is a lower triangular matrix.\n\nLet us try to prove this fact (on the whiteboard).\nIt is sometimes referred to as “square root” of the matrix."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#computing-lu-decomposition",
    "href": "lectures/lecture-5/lecture-5.html#computing-lu-decomposition",
    "title": "",
    "section": "Computing LU-decomposition",
    "text": "Computing LU-decomposition\n\nIn many cases, computing LU-decomposition once is a good idea!\nOnce the decomposition is found (it costs \\mathcal{O}(n^3) operations), then solving linear systems with L and U costs only \\mathcal{O}(n^2) operations.\n\nCheck:\n\nSolving linear systems with triangular matrices is easy (why?).\nHow we compute the L and U factors?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#when-lu-fails",
    "href": "lectures/lecture-5/lecture-5.html#when-lu-fails",
    "title": "",
    "section": "When LU fails",
    "text": "When LU fails\n\nWhat happens, if the matrix is not strictly regular (or the pivots in the Gaussian elimination are really small?).\nThere is classical 2 \\times 2 example of a matrix with a bad LU decomposition.\nThe matrix we look at is\n\n\n    A = \\begin{pmatrix}\n    \\varepsilon & 1 \\\\\n    1 & 1\n    \\end{pmatrix}\n\n\nIf \\varepsilon is sufficiently small, we might fail. In contrast the Cholesky factorization is always stable.\n\nLet us do some demo here.\n\nimport jax.numpy as jnp\nimport jax\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\neps = 1.8e-17\na = [[eps, 1],[1.0,  1]]\na = jnp.array(a)\na0 = a.copy()\nn = a.shape[0]\nL = jnp.zeros((n, n))\nU = jnp.zeros((n, n))\nfor k in range(n): #Eliminate one row\n    L = L.at[k, k].set(1.0)\n    #L = jax.ops.index_update(L, jax.ops.index[k, k], 1)\n    for i in range(k+1, n):\n        L = L.at[i, k].set(a[i, k]/a[k, k])\n        #L = jax.ops.index_update(L, jax.ops.index[i, k], a[i, k] / a[k, k])\n        for j in range(k+1, n):\n            a = a.at[i, j].add(-L[i, k]*a[k, j])\n            #a = jax.ops.index_add(a, jax.ops.index[i, j], -L[i, k] * a[k, j])\n    for j in range(k, n):\n        U = U.at[k, j].set(a[k, j])\n        #U = jax.ops.index_update(U, jax.ops.index[k, j], a[k, j])\nprint('L * U - A:\\n', jnp.dot(L, U) - a0)\nL\n\nL * U - A:\n [[ 0.  0.]\n [ 0. -1.]]\n\n\nArray([[1.00000000e+00, 0.00000000e+00],\n       [5.55555556e+16, 1.00000000e+00]], dtype=float64)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#the-concept-of-pivoting",
    "href": "lectures/lecture-5/lecture-5.html#the-concept-of-pivoting",
    "title": "",
    "section": "The concept of pivoting",
    "text": "The concept of pivoting\n\nWe can do pivoting, i.e. permute rows and columns to maximize A_{kk} that we divide over.\nThe simplest but effective strategy is the row pivoting: at each step, select the index that is maximal in modulus, and put it onto the diagonal.\nIt gives us the decomposition\n\nA = P L U,\nwhere P is a permutation matrix.\nQ. What makes row pivoting good?\nA. It is made good by the fact that\n | L_{ij}|&lt;1, \nbut the elements of U can grow, up to 2^n! (in practice, this is very rarely encountered).\n\nCan you come up with a matrix where the elements of U grow as much as possible?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#stability-of-linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#stability-of-linear-systems",
    "title": "",
    "section": "Stability of linear systems",
    "text": "Stability of linear systems\n\nThere is a fundamental problem of solving linear systems which is independent on the algorithm used.\nIt occures when elements of a matrix are represented as floating point numbers or there is some measurement noise.\n\nLet us illustrate this issue on the following example.\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n%matplotlib inline\nn = 40\na = [[1.0/(i + j + 0.5) for i in range(n)] for j in range(n)]\na = jnp.array(a)\nrhs = jax.random.normal(jax.random.PRNGKey(0), (n,)) #Right-hand side\nx = jnp.linalg.solve(a, rhs) #This function computes LU-factorization and solves linear system\n\n#And check if everything is fine\ner = jnp.linalg.norm(a.dot(x) - rhs) / jnp.linalg.norm(rhs)\nprint(er)\nplt.plot(x)\nplt.grid(True)\n\n142.6518312130805\n\n\n\n\n\n\n\n\n\n\nAs you see, the error grows with larger n, and we have to find out why.\n\nImportant point is that it is not a problem of the algorithm: it is a problem of representing the matrix in the memory.\nThe error occurs in the moment when the matrix elements are evaluated approximately."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#questions-from-the-demo",
    "href": "lectures/lecture-5/lecture-5.html#questions-from-the-demo",
    "title": "",
    "section": "Questions from the demo",
    "text": "Questions from the demo\n\nWhat was the problem in the previous example?\nWhy the error grows so quickly?\nAnd here is one of the main concepts of numerical linear algebra: the concept of condition number of a matrix.\n\nBut before that we have to define the inverse."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#inverse-definition",
    "href": "lectures/lecture-5/lecture-5.html#inverse-definition",
    "title": "",
    "section": "Inverse: definition",
    "text": "Inverse: definition\n\nThe inverse of a matrix A is defined as a matrix X denoted by A^{-1} such that\n\n AX = XA = I,  \nwhere I is the identity matrix (i.e., I_{ij} = 0 if i \\ne j and 1 otherwise). - The computation of the inverse is linked to the solution of linear systems. Indeed, the i-th column of the product gives\n A x_i = e_i,\nwhere e_i is the i-th column of the identity matrix. - Thus, we can apply Gaussian elimination to solve this system. Moreover, if there are no divisions by zero in this process (and the pivots do not depend on the right-hand side), then it is possible to solve the system."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#inverse-matrix-and-linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#inverse-matrix-and-linear-systems",
    "title": "",
    "section": "Inverse matrix and linear systems",
    "text": "Inverse matrix and linear systems\nIf we have computed A^{-1}, the solution of linear system\nAx = f\nis just x = A^{-1} f.\nIndeed,\n A(A^{-1} f) = (AA^{-1})f = I f = f."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#neumann-series",
    "href": "lectures/lecture-5/lecture-5.html#neumann-series",
    "title": "",
    "section": "Neumann series",
    "text": "Neumann series\n\nTo study, why there can be such big errors in a solution (see the example above on the Hilbert matrix) we need an important auxiliary result.\n\nNeumann series:\nIf a matrix F is such that \\Vert F \\Vert &lt; 1 holds, then the matrix (I - F) is invertible and\n(I - F)^{-1} = I + F + F^2 + F^3 + \\ldots = \\sum_{k=0}^{\\infty} F^k.\nNote that it is a matrix version of the geometric progression.\nQ: what norm is considered here? What is the “best possible” norm here?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#proof",
    "href": "lectures/lecture-5/lecture-5.html#proof",
    "title": "",
    "section": "Proof",
    "text": "Proof\nThe proof is constructive. First of all, prove that the series \\sum_{k=0}^{\\infty} F^k converges.\nLike in the scalar case, we have\n (I - F) \\sum_{k=0}^N F^k = (I - F^{N+1}) \\rightarrow I, \\quad N \\to +\\infty \nIndeed,\n \\| (I - F^{N+1}) - I\\| = \\|F^{N+1}\\| \\leqslant \\|F\\|^{N+1} \\to 0, \\quad N\\to +\\infty. \nWe can also estimate the norm of the inverse:\n \\left\\Vert \\sum_{k=0}^N F^k \\right\\Vert \\leq \\sum_{k=0}^N \\Vert F \\Vert^k \\Vert I \\Vert \\leq \\frac{\\Vert I \\Vert}{1 - \\Vert F \\Vert}"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#small-perturbation-of-the-inverse",
    "href": "lectures/lecture-5/lecture-5.html#small-perturbation-of-the-inverse",
    "title": "",
    "section": "Small perturbation of the inverse",
    "text": "Small perturbation of the inverse\n\nUsing this result, we can estimate, how the perturbation of the matrix influences the inverse matrix.\nWe assume that the perturbation E is small in the sense that \\Vert A^{-1} E \\Vert &lt; 1.\nThen\n\n(A + E)^{-1} = \\sum_{k=0}^{\\infty} (-A^{-1} E)^k A^{-1}\nand moreover,\n \\frac{\\Vert (A + E)^{-1} - A^{-1} \\Vert}{\\Vert A^{-1} \\Vert} \\leq \\frac{\\Vert A^{-1} \\Vert \\Vert E \\Vert \\Vert I \\Vert}{1 - \\Vert A^{-1} E \\Vert}. \nAs you see, the norm of the inverse enters the estimate."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#condition-number-of-a-linear-system",
    "href": "lectures/lecture-5/lecture-5.html#condition-number-of-a-linear-system",
    "title": "",
    "section": "Condition number of a linear system",
    "text": "Condition number of a linear system\nNow consider the perturbed linear system:\n (A + \\Delta A) \\widehat{x} = f + \\Delta f."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#estimates",
    "href": "lectures/lecture-5/lecture-5.html#estimates",
    "title": "",
    "section": "Estimates",
    "text": "Estimates\n\n\\begin{split}\n\\widehat{x} - x &= (A + \\Delta A)^{-1} (f + \\Delta f) - A^{-1} f =\\\\\n&= \\left((A + \\Delta A)^{-1} - A^{-1}\\right)f + (A + \\Delta A)^{-1} \\Delta f = \\\\\n& = \\Big[ \\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k - I \\Big]A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k \\Big]A^{-1} \\Delta f  \\\\\n&= \\Big[\\sum_{k=1}^{\\infty} (-A^{-1} \\Delta A)^k\\Big] A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k \\Big] A^{-1} \\Delta f,\n\\end{split}\n\ntherefore\n\n\\begin{split}\n\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq\n&\\frac{1}{\\|A^{-1}f\\|} \\Big[ \\frac{\\|A^{-1}\\|\\|\\Delta A\\|}{1 - \\|A^{-1}\\Delta A\\|}\\|A^{-1}f\\| + \\frac{1}{1 - \\|A^{-1} \\Delta A\\|} \\|A^{-1} \\Delta f\\|  \\Big] \\\\\n\\leq & \\frac{\\|A\\|\\|A^{-1}\\|}{1 - \\|A^{-1}\\Delta A\\|} \\frac{\\|\\Delta A\\|}{\\|A\\|} + \\frac{\\|A^{-1}\\|}{1 - \\|A^{-1}\\Delta A\\|} \\frac{\\|\\Delta f\\|}{\\|A^{-1}f\\|}\\\\\n\\end{split}\n\nNote that \\|AA^{-1}f\\| \\leq \\|A\\|\\|A^{-1}f\\|, therefore \\| A^{-1} f \\| \\geq \\frac{\\|f\\|}{\\|A\\|}\nNow we are ready to get the final estimate\n\n\\begin{split}\n\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq\n&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A^{-1}\\Delta A\\|} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) \\leq \\\\\n\\leq\n&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A\\|\\|A^{-1}\\|\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) \\equiv \\\\\n\\equiv &\\frac{\\mathrm{cond}(A)}{1 - \\mathrm{cond}(A)\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big)\n\\end{split}\n\nThe crucial role is played by the condition number \\mathrm{cond}(A) = \\Vert A \\Vert \\Vert A^{-1} \\Vert."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#condition-number",
    "href": "lectures/lecture-5/lecture-5.html#condition-number",
    "title": "",
    "section": "Condition number",
    "text": "Condition number\n\nThe larger the condition number, the less number of digits we can recover. Note, that the condition number is different for different norms.\nNote, that if \\Delta A = 0, then\n\n \\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq \\mathrm{cond}(A) \\frac{\\|\\Delta f\\|}{\\|f\\|} \n\nThe spectral norm of the matrix is equal to the largest singular value, and the singular values of the inverse matrix are equal to the inverses of the singular values.\n\nThus, the condition number in spectral norm is equal to the ratio of the largest singular value and the smallest singular value.\n\n \\mathrm{cond}_2 (A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#hilbert-matrix-again",
    "href": "lectures/lecture-5/lecture-5.html#hilbert-matrix-again",
    "title": "",
    "section": "Hilbert matrix (again)",
    "text": "Hilbert matrix (again)\n\nWe can also try to test how tight is the estimate, both with ones in the right-hand side, and with a random vector in the right-hand side.\nThe results are strickingly different\n\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nn = 30\na = [[1.0/(i + j + 0.5) for i in range(n)] for j in range(n)]\na = jnp.array(a)\n#rhs = jax.random.normal(jax.random.PRNGKey(10), [n]) \nrhs = jnp.ones(n) #Right-hand side\n#rhs = (jnp.arange(n))\n#rhs = (-1)**rhs\nf = jnp.linalg.solve(a, rhs)\n\n#And check if everything is fine\ner = jnp.linalg.norm(a.dot(f) - rhs) / jnp.linalg.norm(rhs)\ncn = jnp.linalg.cond(a, 2)\nprint('Error:', er, 'Log Condition number:', jnp.log10(cn))\n\n#u1, s1, v1 = jnp.linalg.svd(a)\n#cf = u1.T@rhs\nplt.plot(u1[:, 12])\n#cf/s1\n\nError: 3.7302540452318564e-08 Log Condition number: 19.282605181744575\n\n\n\n\n\n\n\n\n\nAnd with random right-hand side…\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nn = 100\na = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)]\na = jnp.array(a)\nrhs = jax.random.normal(jax.random.PRNGKey(-1), (n, )) #Right-hand side\nf = jnp.linalg.solve(a, rhs)\n\n#And check if everything is fine\ner = jnp.linalg.norm(a.dot(f) - rhs) / jnp.linalg.norm(rhs)\ncn = jnp.linalg.cond(a)\nprint('Error:', er, 'Condition number:', cn)\n\nu, s, v = jnp.linalg.svd(a)\nrhs = jax.random.normal(jax.random.PRNGKey(1), (n, ))\n# rhs = jnp.ones((n,))\nplt.plot(u.T.dot(rhs))\nplt.grid(True)\nplt.xlabel(\"Index of vector elements\", fontsize=20)\nplt.ylabel(\"Elements of vector\", fontsize=20)\nplt.xticks(fontsize=18)\n_ = plt.yticks(fontsize=18)\n\nError: 17.674315761144477 Condition number: 4.073996146476839e+19\n\n\n\n\n\n\n\n\n\nCan you think about an explanation?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#overdetermined-linear-systems",
    "href": "lectures/lecture-5/lecture-5.html#overdetermined-linear-systems",
    "title": "",
    "section": "Overdetermined linear systems",
    "text": "Overdetermined linear systems\n\nImportant class of problems are overdetermined linear systems, when the number of equations is greater, than the number of unknowns.\nThe simplest example that you all know, is linear fitting, fitting a set of 2D points by a line.\nThen, a typical way is to minimize the residual (least squares)\n\n\\Vert A x - b \\Vert_2 \\rightarrow \\min"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#overdetermined-system-and-gram-matrix",
    "href": "lectures/lecture-5/lecture-5.html#overdetermined-system-and-gram-matrix",
    "title": "",
    "section": "Overdetermined system and Gram matrix",
    "text": "Overdetermined system and Gram matrix\nThe optimality condition is 0\\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right), where \\nabla denotes gradient. Therefore,\n 0 \\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right) = 2(A^*A x - A^*b) = 0. \nThus,\n \\quad A^* A x = A^* b \n\nThe matrix A^* A is called Gram matrix and the system is called normal equation.\nThis is not a good way to do it, since the condition number of A^* A is a square of condition number of A (check why)."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#pseudoinverse",
    "href": "lectures/lecture-5/lecture-5.html#pseudoinverse",
    "title": "",
    "section": "Pseudoinverse",
    "text": "Pseudoinverse\n\nMatrix A^* A can be singular in general case.\nTherefore, we need to introduce the concept of pseudoinverse matrix A^{\\dagger} such that solution of the linear least squares problem can formally be written as\n\nx = A^{\\dagger} b.\n\nThe matrix A^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* is called Moore-Penrose pseudoinverse of the matrix A.\nIf matrix A has full column rank, then A^* A is non-singular and we get\n\nA^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^*. \n\nIf matrix A is squared and non-singular we get standard inverse of A:\n\nA^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^* = A^{-1} A^{-*} A^* = A^{-1}\n\nIf A has linearly dependent columns, then A^\\dagger b gives solution that has minimal Euclidean norm"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#compute-pseudoinverse-via-svd",
    "href": "lectures/lecture-5/lecture-5.html#compute-pseudoinverse-via-svd",
    "title": "",
    "section": "Compute pseudoinverse via SVD",
    "text": "Compute pseudoinverse via SVD\nLet A = U \\Sigma V^* be the SVD of A. Then,\nA^{\\dagger} = V \\Sigma^{\\dagger} U^*,\nwhere \\Sigma^{\\dagger} consists of inverses of non-zero singular values of A. Indeed,\n\\begin{align*}\nA^{\\dagger} &= \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = \\lim_{\\alpha \\rightarrow 0}( \\alpha VV^* + V \\Sigma^2 V^*)^{-1} V \\Sigma U^* \\\\ & = \\lim_{\\alpha \\rightarrow 0}( V(\\alpha I + \\Sigma^2) V^*)^{-1} V \\Sigma U^* = V \\lim_{\\alpha \\rightarrow 0}(\\alpha I + \\Sigma^2)^{-1} \\Sigma U^* = V \\Sigma^{\\dagger} U^*.\n\\end{align*}\n\nOne can check that \\Sigma^{\\dagger} contains just the inversion of nonzero singular values.\nIf singular values are small, one can skip inverting them. This will result in a solution which is less sensitive to the noise in the right-hand side.\nThe condition number for the Euclidean norm is still just the ratio of the largest and the smallest non-zero singular values."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#a-canonical-way-to-solve-linear-least-squares",
    "href": "lectures/lecture-5/lecture-5.html#a-canonical-way-to-solve-linear-least-squares",
    "title": "",
    "section": "A canonical way to solve linear least squares",
    "text": "A canonical way to solve linear least squares\nIs to use the QR decomposition.\n\nAny matrix can be factored into a product\n\n A = Q R, \nwhere Q is unitary, and R is upper triangular (details in the next lectures).\n\nThen, if A has full column rank, then\n\n x = A^{\\dagger}b = (A^*A)^{-1}A^*b = ((QR)^*(QR))^{-1}(QR)^*b = (R^*Q^*QR)^{-1}R^*Q^*b = R^{-1}Q^*b.  \n\nThus, finding optimal x is equivalent to solving\n\n Rx = Q^* b. \n\nSince R is upper triangular, the solving of this linear system costs \\mathcal{O}(n^2).\nAlso it is more stable, than using the pseudo-inverse matrix directly."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#padding-into-a-bigger-system",
    "href": "lectures/lecture-5/lecture-5.html#padding-into-a-bigger-system",
    "title": "",
    "section": "Padding into a bigger system",
    "text": "Padding into a bigger system\n\nInstead of solving A^* A x = A^* b, we introduce a new variable r = Ax - b and then have\n\nA^* r = 0, \\quad r = Ax - b,\nor in the block form\n \\begin{pmatrix} 0 & A^* \\\\ A & -I \\end{pmatrix} \\begin{pmatrix} x \\\\ r \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ b \\end{pmatrix}, \nthe total size of the system is (n + m) square, and the condition number is the same as for A - How we define the condition number of a rectangular matrix?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#example-of-ls",
    "href": "lectures/lecture-5/lecture-5.html#example-of-ls",
    "title": "",
    "section": "Example of LS",
    "text": "Example of LS\nConsider a two-dimensional example. Suppose we have a linear model\ny = ax + b\nand noisy data (x_1, y_1), \\dots (x_n, y_n). Then the linear system on coefficients will look as follows\n\n\\begin{split}\na x_1 &+ b &= y_1 \\\\\n&\\vdots \\\\\na x_n &+ b &= y_n \\\\\n\\end{split}\n\nor in a matrix form\n\n\\begin{pmatrix}\nx_1 & 1 \\\\\n\\vdots & \\vdots \\\\\nx_n & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na \\\\\nb\n\\end{pmatrix} =\n\\begin{pmatrix}\ny_1 \\\\\n\\vdots  \\\\\ny_n \\\\\n\\end{pmatrix},\n\nwhich represents overdetermined system.\n\n%matplotlib inline\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\na_exact = 1.\nb_exact = 2.\n\nn = 10\nxi = jnp.arange(n)\nyi = a_exact * xi + b_exact + 2 * jax.random.normal(jax.random.PRNGKey(1), (n, ))\n\nA = jnp.array([xi, jnp.ones(n)])\ncoef = jnp.linalg.pinv(A).T.dot(yi) # coef is [a, b]\n\nplt.plot(xi, yi, 'o', label='$(x_i, y_i)$')\nplt.plot(xi, coef[0]*xi + coef[1], label='Least squares')\nplt.legend(loc='best', fontsize=18)\nplt.grid(True)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#lacking-for-structure",
    "href": "lectures/lecture-5/lecture-5.html#lacking-for-structure",
    "title": "",
    "section": "Lacking for structure",
    "text": "Lacking for structure\n\nA typical 3D-problem requires a 100 \\times 100 \\times 100 discretization\nThis gives a linear system with 10^6 unknowns, right-hand side takes 8 megabytes of memory\nThis matrix has 10^6 \\times 10^6 = 10^{12} elements, takes 8 terabytes of memory.\n\nFortunately, the matrices in real-life are not dense, but have certain structure:\n\nSparse matrices\nLow-rank matrices\nToeplitz matrices (shift-invariant property)\nSparse in certain bases"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#summary",
    "href": "lectures/lecture-5/lecture-5.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nLinear systems can be solved by Gaussian elimination, complexity is \\mathcal{O}(n^3).\nLinear systems can be solved by LU-decomposition, complexity is \\mathcal{O}(n^3) for the decomposition, \\mathcal{O}(n^2) for each solve\nLinear least squares can be solved by normal equation (bad)\nLinear least squares can be solved by QR-decomposition (good) or by augmentation (not bad)\nWithout structure, we can solve up to 10^4 linear systems on a laptop (memory restrictions)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#next-lecture",
    "href": "lectures/lecture-5/lecture-5.html#next-lecture",
    "title": "",
    "section": "Next lecture",
    "text": "Next lecture\n\nEigenvectors & eigenvalues\nSchur theorem\n\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html",
    "href": "lectures/lecture-3/lecture-3.html",
    "title": "GPU Memory Architecture",
    "section": "",
    "text": "Floating point arithmetics and related issues\nStable algorithms: backward and forward stability\nMost important matrix norms: spectral and Frobenius\nUnitary matrices preserve these norms\nThere are two “basic” classes of unitary matrices: Householder and Givens matrices"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#recap-of-the-previous-lectures",
    "href": "lectures/lecture-3/lecture-3.html#recap-of-the-previous-lectures",
    "title": "GPU Memory Architecture",
    "section": "",
    "text": "Floating point arithmetics and related issues\nStable algorithms: backward and forward stability\nMost important matrix norms: spectral and Frobenius\nUnitary matrices preserve these norms\nThere are two “basic” classes of unitary matrices: Householder and Givens matrices"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#examples-of-peak-performance",
    "href": "lectures/lecture-3/lecture-3.html#examples-of-peak-performance",
    "title": "GPU Memory Architecture",
    "section": "Examples of peak performance",
    "text": "Examples of peak performance\nFlops –– floating point operations per second.\nGiga = 2^{30} \\approx 10^9,\nTera = 2^{40} \\approx 10^{12},\nPeta = 2^{50} \\approx 10^{15},\nExa = 2^{60} \\approx 10^{18}\nWhat is the peak perfomance of:\n\nModern CPU\nModern GPU\nLargest supercomputer of the world?\n\n\nClock frequency of CPU vs. performance in flops\nFLOPS = sockets * (cores per socket) * (number of clock cycles per second) * (number of floating point operations per cycle).\n\nTypically sockets = 1\nNumber of cores is typically 2 or 4\nNumber of ticks per second is familiar clock frequency\nNumber of floating point operations per tick depends on the particular CPU\n\n\nModern CPU (Intel Core i7) –– 400 Gflops\nModern GPU Nvidia DGX H100 – depends on the precision!\nLargest supercomputer in the world –– 1.102 Exaflop/s –– peak performanse"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#matrix-by-vector-multiplication-matvec",
    "href": "lectures/lecture-3/lecture-3.html#matrix-by-vector-multiplication-matvec",
    "title": "GPU Memory Architecture",
    "section": "Matrix-by-vector multiplication (matvec)",
    "text": "Matrix-by-vector multiplication (matvec)\nMultiplication of an n\\times n matrix A by a vector x of size n\\times 1 (y=Ax):\n\ny_{i} = \\sum_{j=1}^n a_{ij} x_j\n\nrequires n^2 mutliplications and n(n-1) additions. Thus, the overall complexity is 2n^2 - n =  \\mathcal{O}(n^2)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#how-bad-is-mathcalon2",
    "href": "lectures/lecture-3/lecture-3.html#how-bad-is-mathcalon2",
    "title": "GPU Memory Architecture",
    "section": "How bad is \\mathcal{O}(n^2)?",
    "text": "How bad is \\mathcal{O}(n^2)?\n\nLet A be the matrix of pairwise gravitational interaction between planets in a galaxy.\nThe number of planets in an average galaxy is 10^{11}, so the size of this matrix is 10^{11} \\times 10^{11}.\nTo model evolution in time we have to multiply this matrix by vector at each time step.\nTop supercomputers do around 10^{16} floating point operations per second (flops), so the time required to multiply the matrix A by a vector is approximately\n\n\\begin{align*}\n\\frac{(10^{11})^2 \\text{ operations}}{10^{16} \\text{ flops}} = 10^6 \\text{ sec} \\approx 11.5 \\text{ days}\n\\end{align*}\nfor one time step. If we could multiply it with \\mathcal{O}(n) complexity, we would get\n\\begin{align*}\n\\frac{10^{11} \\text{ operations}}{10^{16} \\text{ flops}} = 10^{-5} \\text{ sec}.\n\\end{align*}\nHere is the YouTube video that illustrates collision of two galaxisies which was modelled by \\mathcal{O}(n \\log n) algorithm:\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"7HF5Oy8IMoM\")"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#can-we-beat-mathcalon2",
    "href": "lectures/lecture-3/lecture-3.html#can-we-beat-mathcalon2",
    "title": "GPU Memory Architecture",
    "section": "Can we beat \\mathcal{O}(n^2)?",
    "text": "Can we beat \\mathcal{O}(n^2)?\n\nGenerally speaking NO.\nThe point is that we have \\mathcal{O}(n^2) input data, so there is no way to be faster for a general matrix.\nFortunately, we can be faster for certain types of matrices. Here are some examples:\n\nThe simplest example may be a matrix of all ones, which can be easily multiplied with only n-1 additions. This matrix is of rank one. More generally we can multiply fast by low-rank  matrices (or by matrices that have low-rank blocks)\nSparse matrices (contain \\mathcal{O}(n) nonzero elements)\nStructured matrices:\n\nFourier\nCirculant\nToeplitz\nHankel"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#matrix-by-matrix-product",
    "href": "lectures/lecture-3/lecture-3.html#matrix-by-matrix-product",
    "title": "GPU Memory Architecture",
    "section": "Matrix-by-matrix product",
    "text": "Matrix-by-matrix product\nConsider composition of two linear operators:\n\ny = Bx\nz = Ay\n\nThen, z = Ay =  A B x = C x, where C is the matrix-by-matrix product."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#matrix-by-matrix-product-mm-classics",
    "href": "lectures/lecture-3/lecture-3.html#matrix-by-matrix-product-mm-classics",
    "title": "GPU Memory Architecture",
    "section": "Matrix-by-matrix product (MM): classics",
    "text": "Matrix-by-matrix product (MM): classics\nDefinition. A product of an n \\times k matrix A and a k \\times m matrix B is a n \\times m matrix C with the elements\n\n   c_{ij} = \\sum_{s=1}^k a_{is} b_{sj}, \\quad i = 1, \\ldots, n, \\quad j = 1, \\ldots, m\n\nFor m=k=n complexity of a naïve algorithm is 2n^3 - n^2 = \\mathcal{O}(n^3)."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#discussion-of-mm",
    "href": "lectures/lecture-3/lecture-3.html#discussion-of-mm",
    "title": "GPU Memory Architecture",
    "section": "Discussion of MM",
    "text": "Discussion of MM\n\nMatrix-by-matrix product is the core for almost all efficient algorithms in numerical linear algebra.\nBasically, all the dense NLA algorithms are reduced to a sequence of matrix-by-matrix products.\nEfficient implementation of MM reduces the complexity of numerical algorithms by the same factor.\nHowever, implementing MM is not easy at all!"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#efficient-implementation-for-mm",
    "href": "lectures/lecture-3/lecture-3.html#efficient-implementation-for-mm",
    "title": "GPU Memory Architecture",
    "section": "Efficient implementation for MM",
    "text": "Efficient implementation for MM\nQ1: Is it easy to multiply a matrix by a matrix in the most efficient way?"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#answer-no-it-is-not-easy",
    "href": "lectures/lecture-3/lecture-3.html#answer-no-it-is-not-easy",
    "title": "GPU Memory Architecture",
    "section": "Answer: no, it is not easy",
    "text": "Answer: no, it is not easy\nIf you want it as fast as possible, using the computers that are at hand."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#demo",
    "href": "lectures/lecture-3/lecture-3.html#demo",
    "title": "GPU Memory Architecture",
    "section": "Demo",
    "text": "Demo\nLet us do a short demo and compare a np.dot() procedure which in my case uses MKL with a hand-written matrix-by-matrix routine in Python and also its numba version.\n\nimport numpy as np\ndef matmul(a, b):\n    n = a.shape[0]\n    k = a.shape[1]\n    m = b.shape[1]  \n    c = np.zeros((n, m))\n    for i in range(n):\n        for j in range(m):\n            for s in range(k):\n                c[i, j] += a[i, s] * b[s, j]\n                \n    return c\n\n\nimport numpy as np\nfrom numba import jit # Just-in-time compiler for Python, see http://numba.pydata.org \n\n@jit(nopython=True)\ndef numba_matmul(a, b):\n    n = a.shape[0]\n    k = a.shape[1]\n    m = b.shape[1]\n    c = np.zeros((n, m))\n    for i in range(n):\n        for j in range(m):\n            for s in range(k):\n                c[i, j] += a[i, s] * b[s, j]\n    return c\n\nThen we just compare computational times.\nGuess the answer.\n\nimport jax.numpy as jnp\n#from jax.config import config\n#config.update(\"jax_enable_x64\", True)\n\nn = 10\na = np.random.randn(n, n)\nb = np.random.randn(n, n)\n\na_jax = jnp.array(a)\nb_jax = jnp.array(b)\n\n%timeit matmul(a, b)\n%timeit numba_matmul(a, b)\n%timeit a @ b\n%timeit (a_jax @ b_jax)#.block_until_ready()\n\nPlatform 'METAL' is experimental and not all JAX functionality may be correctly supported!\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1730369320.414505 6713609 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\nI0000 00:00:1730369320.451663 6713609 service.cc:145] XLA service 0x600000ce5700 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1730369320.451824 6713609 service.cc:153]   StreamExecutor device (0): Metal, &lt;undefined&gt;\nI0000 00:00:1730369320.453663 6713609 mps_client.cc:406] Using Simple allocator.\nI0000 00:00:1730369320.453675 6713609 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator.\n\n\nMetal device set to: Apple M2 Pro\n260 μs ± 3.48 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n819 ns ± 2.87 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n656 ns ± 3.57 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n46.9 μs ± 1.02 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nIs this answer correct for any dimensions of matrices?\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndim_range = [10*i for i in range(1, 11)]\ntime_range_matmul = []\ntime_range_numba_matmul = []\ntime_range_np = []\nfor n in dim_range:\n    print(\"Dimension = {}\".format(n))\n    a = np.random.randn(n, n)\n    b = np.random.randn(n, n)\n\n    t = %timeit -o -q matmul(a, b)\n    time_range_matmul.append(t.best)\n    t = %timeit -o -q numba_matmul(a, b)\n    time_range_numba_matmul.append(t.best)\n    t = %timeit -o -q np.dot(a, b)\n    time_range_np.append(t.best)\n\nDimension = 10\nDimension = 20\nDimension = 30\nDimension = 40\nDimension = 50\nDimension = 60\nDimension = 70\nDimension = 80\nDimension = 90\nDimension = 100\n\n\n\nplt.plot(dim_range, time_range_matmul, label=\"Matmul\")\nplt.plot(dim_range, time_range_numba_matmul, label=\"Matmul Numba\")\nplt.plot(dim_range, time_range_np, label=\"Numpy\")\nplt.legend(fontsize=18)\nplt.xlabel(\"Dimension\", fontsize=18)\nplt.ylabel(\"Time\", fontsize=18)\nplt.yscale(\"log\")"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#why-is-naïve-implementation-slow",
    "href": "lectures/lecture-3/lecture-3.html#why-is-naïve-implementation-slow",
    "title": "GPU Memory Architecture",
    "section": "Why is naïve implementation slow?",
    "text": "Why is naïve implementation slow?\nIt is slow due to two issues:\n\nIt does not use the benefits of fast memory (cache) and in general memory architecture\nIt does not use available parallelization ability (especially important for GPU)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#memory-architecture",
    "href": "lectures/lecture-3/lecture-3.html#memory-architecture",
    "title": "GPU Memory Architecture",
    "section": "Memory architecture",
    "text": "Memory architecture\n\n\nFast memory is small\nBigger memory is slow"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#typical-memory-hierarchy-specifications",
    "href": "lectures/lecture-3/lecture-3.html#typical-memory-hierarchy-specifications",
    "title": "GPU Memory Architecture",
    "section": "Typical Memory Hierarchy Specifications",
    "text": "Typical Memory Hierarchy Specifications\n\n\n\nMemory Type\nSize\nAccess Time\nNotes\n\n\n\n\nCPU Registers\nFew KB\n&lt;1 ns\nFastest, directly accessed by CPU\n\n\nL1 Cache\n32-64 KB\n1-4 ns\nSplit into instruction and data cache\n\n\nL2 Cache\n256 KB - 1 MB\n4-10 ns\nUnified cache\n\n\nL3 Cache\n2-32 MB\n10-20 ns\nShared between CPU cores\n\n\nMain Memory (RAM)\n8-32 GB\n100 ns\nPrimary system memory\n\n\nSSD\n256 GB - 2 TB\n10-100 μs\nFast secondary storage\n\n\nHard Drive\n1-10 TB\n5-10 ms\nSlowest but largest storage\n\n\n\nKey observations: - Access time increases ~10x at each level - Size increases ~10-100x at each level - Effective use of faster memory levels is crucial for performance"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#cache-lines-and-cache-coherence",
    "href": "lectures/lecture-3/lecture-3.html#cache-lines-and-cache-coherence",
    "title": "GPU Memory Architecture",
    "section": "Cache Lines and Cache Coherence",
    "text": "Cache Lines and Cache Coherence\n\nCache memory is organized into cache lines - fixed-size blocks (typically 64 bytes)\nWhen CPU needs data, it loads entire cache line containing that data\nThis is efficient when accessing sequential memory (spatial locality)\n\nCache coherence ensures that: - Multiple CPU cores see consistent view of memory - When one core modifies data, other cores are notified - Prevents race conditions and data inconsistency\nWhy it matters for matrix operations: - Sequential access to matrix rows/columns affects cache line utilization - Poor cache line usage = more cache misses = slower performance - Multi-threaded code needs coherent caches for correctness"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#making-algorithms-more-computationally-intensive",
    "href": "lectures/lecture-3/lecture-3.html#making-algorithms-more-computationally-intensive",
    "title": "GPU Memory Architecture",
    "section": "Making algorithms more computationally intensive",
    "text": "Making algorithms more computationally intensive\nImplementation in NLA: use block version of algorithms. \nThis approach is a core of BLAS (Basic Linear Algebra Subroutines), written in Fortran many years ago, and still rules the computational world.\nSplit the matrix into blocks! For illustration consider splitting in 2 \\times 2 block matrix:\n\n   A = \\begin{bmatrix}\n         A_{11} & A_{12} \\\\\n         A_{21} & A_{22}\n        \\end{bmatrix}, \\quad B = \\begin{bmatrix}\n         B_{11} & B_{12} \\\\\n         B_{21} & B_{22}\n        \\end{bmatrix}\nThen,\nAB = \\begin{bmatrix}A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\\\\n            A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\end{bmatrix}.\nIf A_{11}, B_{11} and their product fit into the cache memory (which is 20 Mb (L3) for the recent Intel Chip), then we load them only once into the memory."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#blas",
    "href": "lectures/lecture-3/lecture-3.html#blas",
    "title": "GPU Memory Architecture",
    "section": "BLAS",
    "text": "BLAS\nBLAS has three levels: 1. BLAS-1, operations like c = a + b 2. BLAS-2, operations like matrix-by-vector product 3. BLAS-3, matrix-by-matrix product\nWhat is the principal differences between them?\nThe main difference is the number of operations vs. the number of input data!\n\nBLAS-1: \\mathcal{O}(n) data, \\mathcal{O}(n) operations\nBLAS-2: \\mathcal{O}(n^2) data, \\mathcal{O}(n^2) operations\nBLAS-3: \\mathcal{O}(n^2) data, \\mathcal{O}(n^3) operations"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#why-blas-is-so-important-and-actual",
    "href": "lectures/lecture-3/lecture-3.html#why-blas-is-so-important-and-actual",
    "title": "GPU Memory Architecture",
    "section": "Why BLAS is so important and actual?",
    "text": "Why BLAS is so important and actual?\n\nThe state-of-the-art implementation of the basic linear algebra operations\nProvides standard names for operations in any new implementations (e.g. ATLAS, OpenBLAS, MKL). You can call matrix-by-matrix multiplication function (GEMM), link your code with any BLAS implementation and it will work correctly\nFormulate new algorithms in terms of BLAS operations\nThere are wrappers for the most popular languages"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#packages-related-to-blas",
    "href": "lectures/lecture-3/lecture-3.html#packages-related-to-blas",
    "title": "GPU Memory Architecture",
    "section": "Packages related to BLAS",
    "text": "Packages related to BLAS\n\nATLAS - Automatic Tuned Linear Algebra Software. It automatically adapts to a particular system architechture.\nLAPACK - Linear Algebra Package. It provides high-level linear algebra operations (e.g. matrix factorizations), which are based on calls of BLAS subroutines.\nIntel MKL - Math Kernel Library. It provides re-implementation of BLAS and LAPACK, optimized for Intel processors. Available in Anaconda Python distribution:\n\nconda install mkl\n\nOpenBLAS is an optimized BLAS library based on GotoBLAS.\nPyTorch supports some calls from BLAS and LAPACK\nFor NVIDIA GPUs, cuBLAS provides a GPU-accelerated implementation of BLAS.\nFor AMD GPUs, rocBLAS is part of the ROCm platform and offers a BLAS implementation optimized for AMD hardware.\n\nFor comparison of OpenBLAS and Intel MKL, see this review"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#faster-algorithms-for-matrix-multiplication",
    "href": "lectures/lecture-3/lecture-3.html#faster-algorithms-for-matrix-multiplication",
    "title": "GPU Memory Architecture",
    "section": "Faster algorithms for matrix multiplication",
    "text": "Faster algorithms for matrix multiplication\nRecall that matrix-matrix multiplication costs \\mathcal{O}(n^3) operations. However, storage is \\mathcal{O}(n^2).\nQuestion: is it possible to reduce number operations down to \\mathcal{O}(n^2)?\nAnswer: a quest for \\mathcal{O}(n^2) matrix-by-matrix multiplication algorithm is not yet done.\n\nStrassen gives \\mathcal{O}(n^{2.807\\dots}) –– sometimes used in practice\nCurrent world record \\mathcal{O}(n^{2.37\\dots}) –– big constant, not practical, based on Coppersmith-Winograd_algorithm.\nIt improved the previous record (Williams 2012) by 3\\cdot 10^{-7}\nThe papers still study multiplication of 3 \\times 3 matrices and interpret it from different sides (Heule, et. al. 2019)\n\nConsider Strassen in more details."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#naïve-multiplication",
    "href": "lectures/lecture-3/lecture-3.html#naïve-multiplication",
    "title": "GPU Memory Architecture",
    "section": "Naïve multiplication",
    "text": "Naïve multiplication\nLet A and B be two 2\\times 2 matrices. Naïve multiplication C = AB\n\n\\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22}  \\end{bmatrix}  =\n\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}  \\end{bmatrix}\n\\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22}  \\end{bmatrix} =\n\\begin{bmatrix}\na_{11}b_{11} + a_{12}b_{21} & a_{11}b_{21} + a_{12}b_{22} \\\\\na_{21}b_{11} + a_{22}b_{21} & a_{21}b_{21} + a_{22}b_{22}\n\\end{bmatrix}\n\ncontains 8 multiplications and 4 additions."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#strassen-algorithm",
    "href": "lectures/lecture-3/lecture-3.html#strassen-algorithm",
    "title": "GPU Memory Architecture",
    "section": "Strassen algorithm",
    "text": "Strassen algorithm\nIn the work Gaussian elimination is not optimal (1969) Strassen found that one can calculate C using 18 additions and only 7 multiplications: \n\\begin{split}\nc_{11} &= f_1 + f_4 - f_5 + f_7, \\\\\nc_{12} &= f_3 + f_5, \\\\\nc_{21} &= f_2 + f_4, \\\\\nc_{22} &= f_1 - f_2 + f_3 + f_6,\n\\end{split}\n where \n\\begin{split}\nf_1 &= (a_{11} + a_{22}) (b_{11} + b_{22}), \\\\\nf_2 &= (a_{21} + a_{22}) b_{11}, \\\\\nf_3 &= a_{11} (b_{12} - b_{22}), \\\\\nf_4 &= a_{22} (b_{21} - b_{11}), \\\\\nf_5 &= (a_{11} + a_{12}) b_{22}, \\\\\nf_6 &= (a_{21} - a_{11}) (b_{11} + b_{12}), \\\\\nf_7 &= (a_{12} - a_{22}) (b_{21} + b_{22}).\n\\end{split}\n\nFortunately, these formulas hold even if a_{ij} and b_{ij}, i,j=1,2 are block matrices.\nThus, Strassen algorithm looks as follows. - First of all we split matrices A and B of sizes n\\times n, n=2^d  into 4 blocks of size \\frac{n}{2}\\times \\frac{n}{2} - Then we calculate multiplications in the described formulas recursively\nThis leads us again to the divide and conquer idea."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#example-of-strassen-algorithm",
    "href": "lectures/lecture-3/lecture-3.html#example-of-strassen-algorithm",
    "title": "GPU Memory Architecture",
    "section": "Example of Strassen algorithm",
    "text": "Example of Strassen algorithm\nLet’s multiply two 2x2 matrices using Strassen’s method:\nA = \\begin{bmatrix} 2 & 3 \\\\ 4 & 1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 7 \\\\ 6 & 8 \\end{bmatrix}\nCalculate the 7 products f_1 through f_7:\n\\begin{align*}\nf_1 &= (2 + 1)(5 + 8) = 3 \\cdot 13 = 39 \\\\\nf_2 &= (4 + 1)(5) = 5 \\cdot 5 = 25 \\\\\nf_3 &= (2)(7 - 8) = 2 \\cdot (-1) = -2 \\\\\nf_4 &= (1)(6 - 5) = 1 \\cdot 1 = 1 \\\\\nf_5 &= (2 + 3)(8) = 5 \\cdot 8 = 40 \\\\\nf_6 &= (4 - 2)(5 + 7) = 2 \\cdot 12 = 24 \\\\\nf_7 &= (3 - 1)(6 + 8) = 2 \\cdot 14 = 28\n\\end{align*}\nNow compute the elements of result matrix C:\n\\begin{align*}\nc_{11} &= f_1 + f_4 - f_5 + f_7 = 39 + 1 - 40 + 28 = 28 \\\\\nc_{12} &= f_3 + f_5 = -2 + 40 = 38 \\\\\nc_{21} &= f_2 + f_4 = 25 + 1 = 26 \\\\\nc_{22} &= f_1 - f_2 + f_3 + f_6 = 39 - 25 - 2 + 24 = 36\n\\end{align*}\nTherefore:\nC = \\begin{bmatrix} 28 & 38 \\\\ 26 & 36 \\end{bmatrix}\nYou can verify this equals the result of standard matrix multiplication!"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#complexity-of-the-strassen-algorithm",
    "href": "lectures/lecture-3/lecture-3.html#complexity-of-the-strassen-algorithm",
    "title": "GPU Memory Architecture",
    "section": "Complexity of the Strassen algorithm",
    "text": "Complexity of the Strassen algorithm\n\nNumber of multiplications\nCalculation of number of multiplications is a trivial task. Let us denote by M(n) number of multiplications used to multiply 2 matrices of sizes n\\times n using the divide and conquer concept. Then for naïve algorithm we have number of multiplications\n M_\\text{naive}(n) = 8 M_\\text{naive}\\left(\\frac{n}{2} \\right) = 8^2 M_\\text{naive}\\left(\\frac{n}{4} \\right)\n= \\dots = 8^{d-1} M(2) = 8^{d} M(1) = 8^{d} = 8^{\\log_2 n} = n^{\\log_2 8} = n^3 \nSo, even when using divide and coquer idea we can not be better than n^3.\nLet us calculate number of multiplications for the Strassen algorithm:\n M_\\text{strassen}(n) = 7 M_\\text{strassen}\\left(\\frac{n}{2} \\right) = 7^2 M_\\text{strassen}\\left(\\frac{n}{4} \\right)\n= \\dots = 7^{d-1} M(1) = 7^{d} = 7^{\\log_2 n} = n^{\\log_2 7} \n\n\nNumber of additions\nThere is no point to estimate number of addtitions A(n) for naive algorithm, as we already got n^3 multiplications.\nFor the Strassen algorithm we have:\n A_\\text{strassen}(n) = 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 \nsince on the first level we have to add \\frac{n}{2}\\times \\frac{n}{2} matrices 18 times and then go deeper for each of the 7 multiplications. Thus,\n\n \\begin{split}\nA_\\text{strassen}(n) =& 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 = 7 \\left(7 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 18 \\left( \\frac{n}{4} \\right)^2 \\right) + 18 \\left( \\frac{n}{2} \\right)^2 =\n7^2 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 7\\cdot 18 \\left( \\frac{n}{4} \\right)^2 +  18 \\left( \\frac{n}{2} \\right)^2 = \\\\\n=& \\dots = 18 \\sum_{k=1}^d 7^{k-1} \\left( \\frac{n}{2^k} \\right)^2 = \\frac{18}{4} n^2 \\sum_{k=1}^d \\left(\\frac{7}{4} \\right)^{k-1} = \\frac{18}{4} n^2 \\frac{\\left(\\frac{7}{4} \\right)^d - 1}{\\frac{7}{4} - 1} = 6 n^2 \\left( \\left(\\frac{7}{4} \\right)^d - 1\\right) \\leqslant 6 n^2 \\left(\\frac{7}{4} \\right)^d = 6 n^{\\log_2 7}\n\\end{split}\n \n(since 4^d = n^2 and 7^d = n^{\\log_2 7}).\nAsymptotic behavior of A(n) could be also found from the master theorem.\n\n\nTotal complexity\nTotal complexity is M_\\text{strassen}(n) + A_\\text{strassen}(n)= 7 n^{\\log_2 7}. Strassen algorithm becomes faster when\n\\begin{align*}\n2n^3 &&gt; 7 n^{\\log_2 7}, \\\\\nn &&gt; 667,\n\\end{align*}\nso it is not a good idea to get to the bottom level of recursion."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#typical-workflow-with-the-starpu-library-requires-only-master-thread-to-run-user-code",
    "href": "lectures/lecture-3/lecture-3.html#typical-workflow-with-the-starpu-library-requires-only-master-thread-to-run-user-code",
    "title": "GPU Memory Architecture",
    "section": "Typical workflow with the StarPU library requires only master thread to run user code:",
    "text": "Typical workflow with the StarPU library requires only master thread to run user code:\n\nInit StarPU and all other related libraries (e.g., MPI, cuBLAS).\nRegister data with StarPU.\nSubmit tasks, that operate on registered data, into a pool of tasks. Tasks are inserted asynchronously, i.e., master thread continues sequential flow through the program without waiting for the result.\nWait for all tasks to complete.\nUnregister data and free memory.\nDeinit StarPU and all related libraries (opposite to the initialization order)."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#strassen-algorithm-and-tensor-rank-advanced-topic",
    "href": "lectures/lecture-3/lecture-3.html#strassen-algorithm-and-tensor-rank-advanced-topic",
    "title": "GPU Memory Architecture",
    "section": "Strassen algorithm and tensor rank (advanced topic)",
    "text": "Strassen algorithm and tensor rank (advanced topic)\n\nIt is not clear how Strassen found these formulas.\nHowever, now we can see that they are not artificial.\nThere is a general approach based on the so-called tensor decomposition technique.\nHere by tensor we imply a multidimensional array - generalization of the matrix concept to many dimensions.\n\nLet us enumerate elements in the 2\\times 2 matrices as follows\n\n\\begin{bmatrix} c_{1} & c_{3} \\\\ c_{2} & c_{4}  \\end{bmatrix} =\n\\begin{bmatrix} a_{1} & a_{3} \\\\ a_{2} & a_{4}  \\end{bmatrix}\n\\begin{bmatrix} b_{1} & b_{3} \\\\ b_{2} & b_{4}  \\end{bmatrix}=\n\\begin{bmatrix}\na_{1}b_{1} + a_{3}b_{2} & a_{1}b_{3} + a_{3}b_{4} \\\\\na_{2}b_{1} + a_{4}b_{2} & a_{2}b_{3} + a_{4}b_{4}\n\\end{bmatrix}\n\nThis can be written as\n c_k = \\sum_{i=1}^4 \\sum_{j=1}^4 x_{ijk} a_i b_j, \\quad k=1,2,3,4 \nx_{ijk} is a 3-dimensional array, that consists of zeros and ones:\n\n\\begin{split}\nx_{\\ :,\\ :,\\ 1} =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{pmatrix}\n\\quad\nx_{\\ :,\\ :,\\ 2} =\n\\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix} \\\\\nx_{\\ :,\\ :,\\ 3} =\n\\begin{pmatrix}\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{pmatrix}\n\\quad\nx_{\\ :,\\ :,\\ 4} =\n\\begin{pmatrix}\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\\end{split}\n\n\nTrilinear decomposition\nTo get Strassen algorithm we should do the following trick –– decompose x_{ijk} in the following way\n x_{ijk} = \\sum_{\\alpha=1}^r u_{i\\alpha} v_{j\\alpha} w_{k\\alpha}. \nThis decomposition is called trilinear tensor decomposition and has a meaning of separation of variables: we have a sum of r (called rank) summands with separated i, j and k.\n\n\nStrassen via trilinear\nNow we have\n c_k = \\sum_{\\alpha=1}^r w_{k\\alpha} \\left(\\sum_{i=1}^4  u_{i\\alpha} a_i \\right) \\left( \\sum_{j=1}^4 v_{j\\alpha} b_j\\right), \\quad k=1,2,3,4. \nMultiplications by u_{i\\alpha} or v_{j\\alpha} or w_{k\\alpha} do not require recursion since u, v and w are known precomputed matrices. Therefore, we have only r multiplications of \\left(\\sum_{i=1}^4  u_{i\\alpha} a_i \\right) \\left( \\sum_{j=1}^4 v_{j\\alpha} b_j\\right) where both factors depend on the input data.\nAs you might guess array x_{ijk} has rank r=7, which leads us to 7 multiplications and to the Strassen algorithm!"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#alphatensor",
    "href": "lectures/lecture-3/lecture-3.html#alphatensor",
    "title": "GPU Memory Architecture",
    "section": "AlphaTensor",
    "text": "AlphaTensor\nRecent AlphaTensor paper has shown how modern deep reinforcement learning can be used to get new decompositions of tensors."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#selected-results",
    "href": "lectures/lecture-3/lecture-3.html#selected-results",
    "title": "GPU Memory Architecture",
    "section": "Selected results",
    "text": "Selected results\n\nBetter ranks for certain matrix sizes\nNew variants for 4x4 Strassen that work on real hardware faster (but only for this specific hardware!)\nBetter antisymmetric matrix-by-vector product"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#summary-of-mm-part",
    "href": "lectures/lecture-3/lecture-3.html#summary-of-mm-part",
    "title": "GPU Memory Architecture",
    "section": "Summary of MM part",
    "text": "Summary of MM part\n\nMM is the core of NLA. You have to think in block terms, if you want high efficiency\nThis is all about computer memory hierarchy\nConcept of block algorithms\n(Advanced topic) Strassen and trilinear form"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html",
    "href": "lectures/lecture-15/lecture-15.html",
    "title": "",
    "section": "",
    "text": "Krylov methods: Arnoldi relation, CG, GMRES\nPreconditioners\n\nJacobi\nGauss-Seidel\nSSOR\nILU and its modifications"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#previous-lecture",
    "href": "lectures/lecture-15/lecture-15.html#previous-lecture",
    "title": "",
    "section": "",
    "text": "Krylov methods: Arnoldi relation, CG, GMRES\nPreconditioners\n\nJacobi\nGauss-Seidel\nSSOR\nILU and its modifications"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#other-structured-matrices",
    "href": "lectures/lecture-15/lecture-15.html#other-structured-matrices",
    "title": "",
    "section": "Other structured matrices",
    "text": "Other structured matrices\n\nUp to now, we discussed preconditioning only for sparse matrices\nBut iterative methods work well for any matrices that have fast black-box matrix-by-vector product\nImportant class of such matrices are Toeplitz matrices (and Hankel matrices) and their multilevel variants\n\nThey are directly connected to the convolution operation and Fast Fourier Transform."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolution",
    "href": "lectures/lecture-15/lecture-15.html#convolution",
    "title": "",
    "section": "Convolution",
    "text": "Convolution\n\nOne of the key operation in signal processing/machine learning is the convolution of two functions.\nLet x(t) and y(t) be two given functions. Their convolution is defined as\n\n(x * y)(t) = \\int_{-\\infty}^{\\infty} x(\\tau) y(t -  \\tau) d \\tau."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolution-theorem-and-fourier-transform",
    "href": "lectures/lecture-15/lecture-15.html#convolution-theorem-and-fourier-transform",
    "title": "",
    "section": "Convolution theorem and Fourier transform",
    "text": "Convolution theorem and Fourier transform\nA well-known fact: a convolution in the time domain is a product in the frequency domain.\n\nTime-frequency transformation is given by the Fourier transform:\n\n\\widehat{x}(w) = (\\mathcal{F}(x))(w) = \\int_{-\\infty}^{\\infty} e^{i w t} x(t) dt.\n\nThen,\n\n\\mathcal{F}(x * y) = \\mathcal{F}(x) \\mathcal{F}(y).\n\nThus, the “algorithm” for the computation of the convolution can be:\n\n\nCompute Fourier transform of x(t) and y(t).\nCompute their product\nCompute inverse Fourier transform"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#discrete-convolution-operation",
    "href": "lectures/lecture-15/lecture-15.html#discrete-convolution-operation",
    "title": "",
    "section": "Discrete convolution operation",
    "text": "Discrete convolution operation\n(x * y)(t) = \\int_{-\\infty}^{\\infty} x(\\tau) y(t -  \\tau) d \\tau.\nLet us approximate the integral by a quadrature sum on a uniform grid, and store the signal at equidistant points.\nThen we are left with the summation\nz_i = \\sum_{j=0}^{n-1} x_j y_{i - j},\nwhich is called discrete convolution. This can be thought as an application of a filter with coefficients x to a signal y.\nThere are different possible filters for different purposes, but they all utilize the shift-invariant structure."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#discrete-convolution-and-toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#discrete-convolution-and-toeplitz-matrices",
    "title": "",
    "section": "Discrete convolution and Toeplitz matrices",
    "text": "Discrete convolution and Toeplitz matrices\nA discrete convolution can be thought as a matrix-by-vector product:\nz_i = \\sum_{j=0}^{n-1} x_j y_{i - j}, \\Leftrightarrow z = Ax\nwhere the matrix A elements are given as a_{ij} = y_{i-j}, i.e., they depend only on the difference between the row index and the column index."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-matrices-definition",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-matrices-definition",
    "title": "",
    "section": "Toeplitz matrices: definition",
    "text": "Toeplitz matrices: definition\nA matrix is called Toeplitz if its elements are defined as\na_{ij} = t_{i - j}.\n\nA Toeplitz matrix is completely defined by its first column and first row (i.e., 2n-1 parameters).\nIt is a dense matrix, however it is a structured matrix (i.e., defined by \\mathcal{O}(n) parameters).\nAnd the main operation in the discrete convolution is the product of Toeplitz matrix by vector.\nCan we compute it faster than \\mathcal{O}(n^2)?"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-and-circulant-matrix",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-and-circulant-matrix",
    "title": "",
    "section": "Toeplitz and circulant matrix",
    "text": "Toeplitz and circulant matrix\n\nFor a special class of Toeplitz matrices, named circulant matrices the fast matrix-by-vector product can be done.\nA matrix C is called circulant, if\n\nC_{ij} = c_{i - j \\mod n},\ni.e. it periodicaly wraps\nC = \\begin{bmatrix}\nc_0 & c_3 & c_2 & c_1 \\\\\nc_1 & c_0 & c_3 & c_2 \\\\\nc_2 & c_1 & c_0 & c_3 \\\\\nc_3 & c_2 & c_1 & c_0 \\\\\n\\end{bmatrix}.\n\n\nThese matrices have the same eigenvectors, given by the Discrete Fourier Transform (DFT)."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#spectral-theorem-for-circulant-matrices",
    "href": "lectures/lecture-15/lecture-15.html#spectral-theorem-for-circulant-matrices",
    "title": "",
    "section": "Spectral theorem for circulant matrices",
    "text": "Spectral theorem for circulant matrices\nTheorem:\nAny circulant matrix can be represented in the form\nC = \\frac{1}{n} F^* \\Lambda F,\nwhere F is the Fourier matrix with the elements\nF_{kl} = w_n^{kl}, \\quad k, l = 0, \\ldots, n-1, \\quad w_n = e^{-\\frac{2 \\pi i}{n}},\nand matrix \\Lambda = \\text{diag}(\\lambda) is the diagonal matrix and\n\\lambda = F c, \nwhere c is the first column of the circulant matrix C.\nThe proof will be later: now we need to study the FFT matrix."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fourier-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fourier-matrix",
    "title": "",
    "section": "Fourier matrix",
    "text": "Fourier matrix\nThe Fourier matrix is defined as:\n\nF_n =\n\\begin{pmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & w^{1\\cdot 1}_n & w^{1\\cdot 2}_n & \\dots & w^{1\\cdot (n-1)}_n\\\\\n1 & w^{2\\cdot 1}_n & w^{2\\cdot 2}_n & \\dots & w^{2\\cdot (n-1)}_n\\\\\n\\dots & \\dots & \\dots &\\dots &\\dots \\\\\n1 & w^{(n-1)\\cdot 1}_n & w^{(n-1)\\cdot 2}_n & \\dots & w^{(n-1)\\cdot (n-1)}_n\\\\\n\\end{pmatrix},\n\nor equivalently\n F_n = \\{ w_n^{kl} \\}_{k,l=0}^{n-1}, \nwhere\nw_n = e^{-\\frac{2\\pi i}{n}}.\nProperties: * Symmetric (not Hermitian!) * Unitary up to a scaling factor: F_n^* F_n = F_n F_n^* = nI (check this fact). Therefore F_n^{-1} = \\frac{1}{n}F^*_n * Can be multiplied by a vector (called discrete Fourier transform or DFT) with \\mathcal{O}(n \\log n) complexity (called fast Fourier transform or FFT)! FFT helps to analyze spectrum of a signal and, as we will see later, helps to do fast mutiplications with certain types of matrices.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nN = 1000\ndt = 1.0 / 800.0\nx = np.linspace(0.0, N*dt, N)\ny = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x) + 0.2*np.sin(300.0 * 2.0*np.pi*x)\nplt.plot(x, y)\nplt.xlabel('Time')\nplt.ylabel('Signal')\nplt.title('Initial signal')\n\nText(0.5, 1.0, 'Initial signal')\n\n\n\n\n\n\n\n\n\n\nyf = np.fft.fft(y)\nxf = np.linspace(0.0, 1.0/(2.0*dt), N//2)\nplt.plot(xf, 2.0/N * np.abs(yf[0:N//2])) #Note: N/2 to N will give negative frequencies\nplt.xlabel('Frequency')\nplt.ylabel('Amplitude')\nplt.title('Discrete Fourier transform')\n\nText(0.5, 1.0, 'Discrete Fourier transform')"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-fourier-transform-fft",
    "href": "lectures/lecture-15/lecture-15.html#fast-fourier-transform-fft",
    "title": "",
    "section": "Fast Fourier transform (FFT)",
    "text": "Fast Fourier transform (FFT)\nHere we consider a matrix interpretation of the standard Cooley-Tukey algorithm (1965), which has underlying divide and conquer idea. Note that in packages more advanced versions are used.\n\nLet n be a power of 2.\nFirst of all we  permute the rows  of the Fourier matrix such that the first n/2 rows of the new matrix had row numbers 1,3,5,\\dots,n-1 and the last n/2 rows had row numbers 2,4,6\\dots,n.\nThis permutation can be expressed in terms of multiplication by permutation matrix P_n:\n\n\nP_n =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 0 & 1 & 0 &\\dots & 0 & 0 \\\\\n\\vdots & & & & & & \\vdots \\\\\n0 & 0 & 0 & 0 &\\dots & 1 & 0 \\\\\n\\hline\n0 & 1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 0 & 0 & 1 &\\dots & 0 & 0 \\\\\n\\vdots & & & & & & \\vdots \\\\\n0 & 0 & 0 & 0 &\\dots & 0 & 1\n\\end{pmatrix},\n\nHence,\n\nP_n F_n =\n\\begin{pmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & w^{2\\cdot 1}_n & w^{2\\cdot 2}_n & \\dots & w^{2\\cdot (n-1)}_n\\\\\n1 & w^{4\\cdot 1}_n & w^{4\\cdot 2}_n & \\dots & w^{4\\cdot (n-1)}_n\\\\\n\\vdots & & & & \\vdots\\\\\n1 & w^{(n-2)\\cdot 1}_n & w^{(n-2)\\cdot 2}_n & \\dots & w^{(n-2)\\cdot (n-1)}_n\\\\\n\\hline\n1 & w^{1\\cdot 1}_n & w^{1\\cdot 2}_n & \\dots & w^{1\\cdot (n-1)}_n\\\\\n1 & w^{3\\cdot 1}_n & w^{3\\cdot 2}_n & \\dots & w^{3\\cdot (n-1)}_n\\\\           \n\\vdots & & & & \\vdots\\\\\n1 & w^{(n-1)\\cdot 1}_n & w^{(n-1)\\cdot 2}_n & \\dots & w^{(n-1)\\cdot (n-1)}_n\\\\\n\\end{pmatrix},\n\nNow let us imagine that we separated its columns and rows by two parts each of size n/2.\nAs a result we get 2\\times 2 block matrix that has the following form\n\nP_n F_n =\n\\begin{pmatrix}\n\\left\\{w^{2kl}_n\\right\\} & \\left\\{w_n^{2k\\left(\\frac{n}{2} + l\\right)}\\right\\} \\\\\n\\left\\{w_n^{(2k+1)l}\\right\\} & \\left\\{w_n^{(2k+1)\\left(\\frac{n}{2} + l\\right)}\\right\\}\n\\end{pmatrix},\n\\quad k,l = 0,\\dots, \\frac{n}{2}-1.\n\nSo far it does not look like something that works faster :) But we will see that in a minute. Lets have a more precise look at the first block \\left\\{w^{2kl}_n\\right\\}:\n\nw^{2kl}_n = e^{-2kl\\frac{2\\pi i}{n}} = e^{-kl\\frac{2\\pi i}{n/2}} = w^{kl}_{n/2}.\n\nSo this block is exactly twice smaller Fourier matrix F_{n/2}!\n\nThe block \\left\\{w_n^{(2k+1)l}\\right\\} can be written as\n\nw_n^{(2k+1)l} = w_n^{2kl + l} = w_n^{l} w_n^{2kl} = w_n^{l} w_{n/2}^{kl},\n\nwhich can be written as W_{n/2}F_{n/2}, where\nW_{n/2} = \\text{diag}(1,w_n,w_n^2,\\dots,w_n^{n/2-1}).\nDoing the same tricks for the other blocks we will finally get\n\nP_n F_n =\n\\begin{pmatrix}\nF_{n/2} & F_{n/2} \\\\\nF_{n/2}W_{n/2} & -F_{n/2}W_{n/2}\n\\end{pmatrix} =\n\\begin{pmatrix}\nF_{n/2} & 0 \\\\\n0 & F_{n/2}\n\\end{pmatrix}\n\\begin{pmatrix}\nI_{n/2} & I_{n/2} \\\\\nW_{n/2} & -W_{n/2}\n\\end{pmatrix}.\n\n\nThus, we reduced multiplication by F_n to 2 multiplications by F_{n/2} and cheap multiplications by diagonal matrices.\nIf we apply the obtained expressions recursively to F_{n/2}, we will get \\mathcal{O}(n\\log n)  complexity.\n\n\n#FFT vs full matvec\nimport time\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg\n\nn = 10000\nF = sp.linalg.dft(n)\nx = np.random.randn(n)\n\ny_full = F.dot(x)\n\nfull_mv_time = %timeit -q -o F.dot(x)\nprint('Full matvec time =', full_mv_time.average)\n\ny_fft = np.fft.fft(x)\nfft_mv_time = %timeit -q -o np.fft.fft(x)\nprint('FFT time =', fft_mv_time.average)\n\nprint('Relative error =', (np.linalg.norm(y_full - y_fft)) / np.linalg.norm(y_full))\n\nFull matvec time = 0.016554963692857142\nFFT time = 6.18095107428571e-05\nRelative error = 1.5329028805883414e-12"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#circulant-matrices",
    "href": "lectures/lecture-15/lecture-15.html#circulant-matrices",
    "title": "",
    "section": "Circulant matrices",
    "text": "Circulant matrices\nFFT helps to multiply fast by certain types of matrices. We start from a circulant matrix:\n\nC =\n\\begin{pmatrix}\nc_0 & c_{n-1} & c_{n-2} & \\dots & c_1 \\\\\nc_{1} & c_{0} & c_{n-1} & \\dots & c_2 \\\\\nc_{2} & c_{1} & c_0 & \\dots & c_3 \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\nc_{n-1} & c_{n-2} & c_{n-3} & \\dots & c_0\n\\end{pmatrix}\n\nTheorem. Let C be a circulant matrix of size n\\times n and let c be it’s first column , then\n\nC = \\frac{1}{n} F_n^* \\text{diag}(F_n c) F_n\n\nProof. - Consider a number\n\\lambda (\\omega) = c_0 + \\omega c_1 + \\dots + \\omega^{n-1} c_{n-1},\nwhere \\omega is any number such that \\omega^n=1. - Lets multiply \\lambda by 1,\\omega,\\dots, \\omega^{n-1}:\n\n\\begin{split}\n\\lambda & = c_0 &+& \\omega c_1 &+& \\dots &+& \\omega^{n-1} c_{n-1},\\\\\n\\lambda\\omega & = c_{n-1} &+& \\omega c_0 &+& \\dots &+& \\omega^{n-1} c_{n-2},\\\\\n\\lambda\\omega^2 & = c_{n-2} &+& \\omega c_{n-1} &+& \\dots &+& \\omega^{n-1} c_{n-3},\\\\\n&\\dots\\\\\n\\lambda\\omega^{n-1} & = c_{1} &+& \\omega c_{2} &+& \\dots &+& \\omega^{n-1} c_{0}.\n\\end{split}\n\n\nTherefore,\n\n\n\\lambda(\\omega) \\cdot \\begin{pmatrix} 1&\\omega & \\dots& \\omega^{n-1} \\end{pmatrix} =\n\\begin{pmatrix} 1&\\omega&\\dots& \\omega^{n-1} \\end{pmatrix} \\cdot C.\n\n\nWriting this for \\omega = 1,w_n, \\dots, w_n^{n-1} we get\n\n\n\\Lambda F_n = F_n C\n\nand finally\n\nC = \\frac{1}{n} F^*_n \\Lambda F_n, \\quad \\text{where}\\quad \\Lambda = \\text{diag}(F_nc) \\qquad\\blacksquare"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-circulant-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-circulant-matrix",
    "title": "",
    "section": "Fast matvec with circulant matrix",
    "text": "Fast matvec with circulant matrix\n\nRepresentation $C = F^* (F_n c) F_n $ gives us an explicit way to multiply a vector x by C in \\mathcal{O}(n\\log n) operations.\nIndeed,\n\n\nCx = \\frac{1}{n} F_n^* \\text{diag}(F_n c) F_n x = \\text{ifft}\\left( \\text{fft}(c) \\circ \\text{fft}(x)\\right)\n\nwhere \\circ denotes elementwise product (Hadamard product) of two vectors (since \\text{diag}(a)b = a\\circ b) and ifft denotes inverse Fourier transform F^{-1}_n.\n\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg\n\ndef circulant_matvec(c, x):\n    return np.fft.ifft(np.fft.fft(c) * np.fft.fft(x))\n\nn = 5000\nc = np.random.random(n)\nC = sp.linalg.circulant(c)\nx = np.random.randn(n)\n\n\ny_full = C.dot(x)\nfull_mv_time = %timeit -q -o C.dot(x)\nprint('Full matvec time =', full_mv_time.average)\n\n\ny_fft = circulant_matvec(c, x)\nfft_mv_time = %timeit -q -o circulant_matvec(c, x)\nprint('FFT time =', fft_mv_time.average)\n\nprint('Relative error =', (np.linalg.norm(y_full - y_fft)) / np.linalg.norm(y_full))\n\nFull matvec time = 0.003428939464285707\nFFT time = 0.0001001016214142856\nRelative error = 1.307050346126901e-15"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-matrices",
    "title": "",
    "section": "Toeplitz matrices",
    "text": "Toeplitz matrices\nNow we get back to Toeplitz matrices!\n\nT =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{-3}& \\dots & t_{1-n} \\\\\nt_{1} & t_{0} & t_{-1} & t_{-2}& \\dots & t_{2-n} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} &\\dots & t_{3-n} \\\\\nt_{3} & t_{2} & t_1 & t_0 & \\dots & t_{4-n} \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\\\\\nt_{n-1} & t_{n-2} & t_{n-3} & t_{n-4} &\\dots &t_0\n\\end{pmatrix},\n\nor equivalently T_{ij} = t_{i-j}.\nMatvec operation can be written as\n\ny_i = \\sum_{j=1}^n t_{i-j} x_j,\n\nwhich can be interpreted as a discrete convolution of filter t_i and signal x_i. For simplicity the size of the filter t is such that the sizes of the input and output signals are the same. Generally, filter size can be arbitrary.\nFast convolution computation has a variety of applications, for instance, in signal processing or partial differential and integral equations. For instance, here is the smoothing of a signal:\n\nfrom scipy import signal\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nalpha = 0.01\nsig = np.repeat([0., 1., 0.], 100)\nfilt = np.exp(-alpha * (np.arange(100)-50)**2)\nfiltered = signal.convolve(sig, filt, mode='same') / sum(filt)\n\nfig, (ax_orig, ax_filt, ax_filtered) = plt.subplots(3, 1, sharex=True)\nax_orig.plot(sig)\nax_orig.margins(0, 0.1)\nax_filt.plot(filt)\nax_filt.margins(0, 0.1)\nax_filtered.plot(filtered)\nax_filtered.margins(0, 0.1)\n\nax_orig.set_title('Original signal')\nax_filt.set_title('Filter')\nax_filtered.set_title('Convolution')\n\nfig.tight_layout()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-toeplitz-matrix",
    "title": "",
    "section": "Fast matvec with Toeplitz matrix",
    "text": "Fast matvec with Toeplitz matrix\nKey point: the multiplication by a Toeplitz matrix can be reduced to the multiplication by a circulant.\n\nIndeed, every Toeplitz matrix of size n\\times n can be embedded into a Circulant matrix C of size (2n - 1) \\times (2n - 1):\n\n\nC =\n\\begin{pmatrix}\nT & \\dots \\\\\n\\dots & \\dots\n\\end{pmatrix}.\n\n\nThe 3\\times 3 matrix T = \\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} \\\\\nt_{1} & t_{0} & t_{-1} \\\\\nt_{2} & t_{1} & t_0 \\\\\n\\end{pmatrix} can be embedded as follows\n\n\nC =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{2} & t_{1}\\\\\nt_{1} & t_{0} & t_{-1} & t_{-2} & t_{2} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} & t_{-2} \\\\\nt_{-2}& t_{2} & t_{1} & t_0 & t_{-1}  \\\\\nt_{-1} & t_{-2} & t_{2} & t_{1} & t_0  \n\\end{pmatrix}.\n\n\nFor matvec $\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\n=\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} \\\\\nt_{1} & t_{0} & t_{-1} \\\\\nt_{2} & t_{1} & t_0 \\\\\n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n$ we pad vector x with zeros:\n\n\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\star \\\\ \\star \\end{pmatrix} =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{2} & t_{1}\\\\\nt_{1} & t_{0} & t_{-1} & t_{-2} & t_{2} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} & t_{-2} \\\\\nt_{-2}& t_{2} & t_{1} & t_0 & t_{-1}  \\\\\nt_{-1} & t_{-2} & t_{2} & t_{1} & t_0  \n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix}=\n\\text{ifft}(\\text{fft}(\\begin{pmatrix} t_0 \\\\ t_{1} \\\\ t_{2} \\\\ t_{-2} \\\\ t_{-1} \\end{pmatrix})\\circ \\text{fft}(\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix})).\n\n\nNote that you do not need to form and store the whole matrix T\nFrom the Cooley-Tukey algorithm follows that the preferable size of circulant matrix is 2^k for some k. You can do it with zero padding of the appropriate size."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#multilevel-toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#multilevel-toeplitz-matrices",
    "title": "",
    "section": "Multilevel Toeplitz matrices",
    "text": "Multilevel Toeplitz matrices\nThe 2-dimensional convolution is defined as\n\ny_{i_1i_2} = \\sum_{j_1,j_2=1}^n t_{i_1-j_1, i_2-j_2} x_{j_1 j_2}.\n\nNote that x and y are 2-dimensional arrays and T is 4-dimensional. To reduce this expression to matrix-by-vector product we have to reshape x and y into long vectors:\n\n\\text{vec}(x) =\n\\begin{pmatrix}\nx_{11} \\\\ \\vdots \\\\ x_{1n} \\\\ \\hline \\\\ \\vdots \\\\ \\hline \\\\ x_{n1} \\\\ \\vdots \\\\ x_{nn}\n\\end{pmatrix},\n\\quad\n\\text{vec}(y) =\n\\begin{pmatrix}\ny_{11} \\\\ \\vdots \\\\ y_{1n} \\\\ \\hline \\\\ \\vdots \\\\ \\hline \\\\ y_{n1} \\\\ \\vdots \\\\ y_{nn}\n\\end{pmatrix}.\n\nIn this case matrix T is block Toeplitz with Toeplitz blocks: (BTTB)\n\nT =\n\\begin{pmatrix}\nT_0 & T_{-1} & T_{-2} &  \\dots & T_{1-n} \\\\\nT_{1} & T_{0} & T_{-1} & \\dots & T_{2-n} \\\\\nT_{2} & T_{1} & T_0 & \\dots & T_{3-n} \\\\\n\\dots & \\dots & \\dots &  \\dots & \\dots\\\\\nT_{n-1} & T_{n-2} & T_{n-3}  &\\dots &T_0\n\\end{pmatrix},\n\\quad \\text{where} \\quad\nT_k = t_{k, i_2 - j_2}\\quad  \\text{are Toeplitz matrices}"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-multilevel-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-multilevel-toeplitz-matrix",
    "title": "",
    "section": "Fast matvec with multilevel Toeplitz matrix",
    "text": "Fast matvec with multilevel Toeplitz matrix\nTo get fast matvec we need to embed block Toeplitz matrix with Toeplitz blocks into the block circulant matrix with circulant blocks. The analog of \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\star \\\\ \\star \\end{pmatrix} =\n\\text{ifft}(\\text{fft}(\\begin{pmatrix} t_0 \\\\ t_{1} \\\\ t_{2} \\\\ t_{-2} \\\\ t_{-1} \\end{pmatrix})\\circ\\text{fft}(\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix})). will look like  \\begin{pmatrix} y_{11} & y_{12} & y_{13} & \\star & \\star \\\\\ny_{21} & y_{22} & y_{23} & \\star & \\star \\\\  \ny_{31} & y_{32} & y_{33} & \\star & \\star \\\\\n\\star & \\star & \\star & \\star & \\star \\\\  \n\\star & \\star & \\star & \\star & \\star \\\\  \n\\end{pmatrix} = \\text{ifft2d}(\\text{fft2d}(\\begin{pmatrix} t_{0,0} & t_{1,0} & t_{2,0} & t_{-2,0} & t_{-1,0} \\\\\nt_{0,1} & t_{1,1} & t_{2,1} & t_{-2,1} & t_{-1,1} \\\\  \nt_{0,2} & t_{1,2} & t_{2,2} & t_{-2,2} & t_{-1,2} \\\\\nt_{0,-2} & t_{1,-2} & t_{2,-2} & t_{-2,-2} & t_{-1,-2} \\\\\nt_{0,-1} & t_{1,-1} & t_{2,-1} & t_{-2,-1} & t_{-1,-1}\n\\end{pmatrix}) \\circ \\text{fft2d}(\\begin{pmatrix}x_{11} & x_{12} & x_{13} & 0 & 0 \\\\\nx_{21} & x_{22} & x_{23} & 0 & 0 \\\\  \nx_{31} & x_{32} & x_{33} & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\  \n0 & 0 & 0 & 0 & 0 \\\\  \n\\end{pmatrix})), where fft2d is 2-dimensional fft that consists of one-dimensional transforms, applied first to rows and and then to columns (or vice versa).\n\n# Blurring and Sharpening Lena by convolution\n\nfrom scipy import signal\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib  inline\nfrom scipy import misc\nimport imageio\n\nfilter_size = 5\nfilter_blur = np.ones((filter_size, filter_size)) / filter_size**2\nlena = imageio.imread('./lena512.jpg')\n#lena = misc.face()\n#lena = lena[:, :, 0]\nblurred = signal.convolve2d(lena, filter_blur, boundary='symm', mode='same')\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nax[0, 0].imshow(lena[200:300, 200:300], cmap='gray')\nax[0, 0].set_title('Original Lena')\nax[0, 1].imshow(blurred[200:300, 200:300], cmap='gray')\nax[0, 1].set_title('Blurred Lena')\nax[1, 0].imshow((lena - blurred)[200:300, 200:300], cmap='gray')\nax[1, 0].set_title('Lena $-$ Blurred Lena')\nax[1, 1].imshow(((lena - blurred)*3 + blurred)[200:300, 200:300], cmap='gray')\nax[1, 1].set_title('$3\\cdot$(Lena $-$ Blurred Lena) + Blurred Lena')\nfig.tight_layout()\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In [7], line 8\n      6 get_ipython().run_line_magic('matplotlib', ' inline')\n      7 from scipy import misc\n----&gt; 8 import imageio\n     10 filter_size = 5\n     11 filter_blur = np.ones((filter_size, filter_size)) / filter_size**2\n\nModuleNotFoundError: No module named 'imageio'"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#solving-linear-systems-with-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#solving-linear-systems-with-toeplitz-matrix",
    "title": "",
    "section": "Solving linear systems with Toeplitz matrix",
    "text": "Solving linear systems with Toeplitz matrix\n\nConvolution is ok; but what about deconvolution, or solving linear systems with Toeplitz matrices?\n\nT x = f.\n\nFor the periodic case, where T = C is circulant,\n\nwe have the spectral theorem\nC = \\frac{1}{n}F^* \\Lambda F, \\quad C^{-1} = \\frac{1}{n}F^* \\Lambda^{-1} F,\nbut for a general Toeplitz matrices, it is not a trivial question."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#iterative-methods",
    "href": "lectures/lecture-15/lecture-15.html#iterative-methods",
    "title": "",
    "section": "Iterative methods",
    "text": "Iterative methods\n\nNot-a-bad recipe for Toeplitz linear system is to use iterative method (fast matvec is available).\nA good choice for a preconditioner is a circulant matrix."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#circulant-preconditioner",
    "href": "lectures/lecture-15/lecture-15.html#circulant-preconditioner",
    "title": "",
    "section": "Circulant preconditioner",
    "text": "Circulant preconditioner\n\nA natural idea is to use circulants as preconditioners, since they are easy to invert.\nThe first preconditioner was the preconditioner by Raymond Chan and Gilbert Strang, who proposed to take the first column of the matrix and use it to generate the circulant.\nThe second preconditioner is the Tony Chan preconditioner, which is also very natural:\n\nC = \\arg \\min_P \\Vert P - T \\Vert_F.\n\nA simple formula for the entries of C can be derived.\n\n\nimport numpy as np\nimport scipy.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport scipy as sp\n\nn = 100\nc = np.zeros(n)\nc[0] = -2\nc[1] = 1\nTm = sp.linalg.toeplitz(c, c)\n\n\nc1 = sp.linalg.circulant(c) #Strang preconditioner\nFmat = 1.0/np.sqrt(n) * np.fft.fft(np.eye(n)) #Poor man's Fourier matrix\n\nd2 = np.diag(Fmat.conj().dot(Tm).dot(Fmat))\nc2 = Fmat.dot(np.diag(d2)).dot(Fmat.conj().T)\n\n\nmat = np.linalg.inv(c1).dot(Tm)\nev = np.linalg.eigvals(mat).real\nplt.plot(np.sort(ev), np.ones(n), 'o')\nplt.xlabel('Eigenvalues for Strang preconditioner')\nplt.gca().get_yaxis().set_visible(False)\n\nmat = np.linalg.inv(c2).dot(Tm)\nev = np.linalg.eigvals(mat).real\nplt.figure()\nplt.plot(np.sort(ev), np.ones(n), 'o')\nplt.xlabel('Eigenvalues for T. Chan Preconditioner')\nplt.gca().get_yaxis().set_visible(False)"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolutions-in-neural-networks",
    "href": "lectures/lecture-15/lecture-15.html#convolutions-in-neural-networks",
    "title": "",
    "section": "Convolutions in neural networks",
    "text": "Convolutions in neural networks\n\nThe revolution in deep learning and computer vision is related to using Convolutional Neural Networks (CNN)\nThe most famous examples are\n\nAlexNet, 2012\nGoogLeNet, 2014\nVGG, 2015\n\nFurther improvements are based on more advanced tricks like normalizations, skip connections and so on\nMore details will be presented in Deep learning/computer vision courses"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#what-is-convolution-in-nn",
    "href": "lectures/lecture-15/lecture-15.html#what-is-convolution-in-nn",
    "title": "",
    "section": "What is convolution in NN?",
    "text": "What is convolution in NN?\n\nIn neural networks the convolution means not convolution but cross-correlation!\n\n (x \\star y)(t) = \\int_{-\\infty}^{+\\infty} x(\\tau)y(\\tau + t)d \\tau \n\nCompare with the definition of convolution from the first slide\n\n(x * y)(t) = \\int_{-\\infty}^{+\\infty} x(\\tau) y(t -  \\tau) d \\tau.\n\nSource is here\n\nConvolution and cross-correlation are related as\n\n x(t) \\star y(t) = x(-t) * y(t) \n\nHow this operation is performed in neural networks?\n\n\nSource of gif is here\n\nAlso nice presentation about the difference of this operations, detailed comparison and PyTorch examples is here"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#additional-remarks-about-this-operation",
    "href": "lectures/lecture-15/lecture-15.html#additional-remarks-about-this-operation",
    "title": "",
    "section": "Additional remarks about this operation",
    "text": "Additional remarks about this operation\n\nThis operation reflects the relations between the neighbour pixels in image\nMultiple filters/kernels can fit different features of the image\nThis is still linear transformation of the input, but it focus on local properties of data\nIt can be efficiently computed with GPU since the single set of simple instructions have to be applied to multiple data\nThe trained parameters here are filters that is used to produce the output, for batch of 3D images (RGB) they are 4-dimensional tensors"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#how-the-trained-filters-and-result-image-representation-looks-like",
    "href": "lectures/lecture-15/lecture-15.html#how-the-trained-filters-and-result-image-representation-looks-like",
    "title": "",
    "section": "How the trained filters and result image representation looks like",
    "text": "How the trained filters and result image representation looks like\n\nimport torchvision.models as models\n\nvgg16 = models.vgg16(pretrained=True)\nprint(vgg16)\n\n\nfor ch in vgg16.children():\n    features = ch\n    break\nprint(features)\nfor name, param in features.named_parameters(): \n    print(name, param.shape)\n\n\nplt.figure(figsize=(20, 17))\nfor i, filter in enumerate(features[2].weight):\n    plt.subplot(16, 16, i+1)\n    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\nfrom PIL import Image\nimg = Image.open(\"./tiger.jpeg\")\nplt.imshow(img)\nplt.show()\n\n\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimg_ = transform(img)\nimg_ = img_.unsqueeze(0)\nprint(img_.size())\nplt.imshow(img_[0].permute(1, 2, 0))\n\n\n# After the first convolutional layer\noutput1 = features[0](img_)\nprint(output1.size())\nplt.figure(figsize=(20, 17))\nfor i, f_map in enumerate(output1[0]):\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()\n\n\n# After the second convolutional layer\noutput2 = features[2](features[1](output1))\nprint(output2.size())\nplt.figure(figsize=(20, 17))\nfor i, f_map in enumerate(output2[0]):\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()\n\n\noutput3 = features[5](features[4](features[3](output2)))\nprint(output3.size())\nplt.figure(figsize=(20, 20))\nfor i, f_map in enumerate(output3[0]):\n    if i + 1 == 65:\n        break\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#image-transformations",
    "href": "lectures/lecture-15/lecture-15.html#image-transformations",
    "title": "",
    "section": "Image transformations",
    "text": "Image transformations\n\nDifferent filters highlight different features of the image\nPooling operation reduces the spatial dimensions and further convolutional layer moves it to channels dimension\nAfter all feature block one gets 512 channels (we start from 3 (RGB)) and significant reduction of the spatial size of image\nSuch operation extracts useful features to help classifier work better\nExactly this property of VGG-type networks is one of the ingredients of style transfer networks, see more details here"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#low-rank-and-fourier-transform",
    "href": "lectures/lecture-15/lecture-15.html#low-rank-and-fourier-transform",
    "title": "",
    "section": "Low-rank and Fourier transform",
    "text": "Low-rank and Fourier transform\nRecent achievements for learning structured matrices include [Monarch matrices] (https://arxiv.org/pdf/2204.00595.pdf) and here Monarch mixer\nMonarch matrix is given as\n\\mathbf{M}=\\left(\\prod_{i=1}^p \\mathbf{P}_i \\mathbf{B}_i\\right) \\mathbf{P}_0\nwhere each \\mathbf{P}_i is related to the ‘base \\sqrt[p]{N}’ variant of the bit-reversal permutation, and \\mathbf{B}_i is a block-diagonal matrix with block size b. Setting b=\\sqrt[p]{N} achieves sub-quadratic compute cost. For example, for p=2, b=\\sqrt{N}, Monarch matrices require O\\left(N^{3 / 2}\\right) compute in sequence length N."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#low-rank-characteristic-of-monarch-matrices",
    "href": "lectures/lecture-15/lecture-15.html#low-rank-characteristic-of-monarch-matrices",
    "title": "",
    "section": "Low-rank characteristic of Monarch matrices",
    "text": "Low-rank characteristic of Monarch matrices\nIf we treat Monarch matrix as a block matrix with m \\times m block, the elements have the form:\nM_{\\ell j k i}=L_{j \\ell k} R_{k j i}\nThis is rank-1 approximation in disguise."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#small-demo",
    "href": "lectures/lecture-15/lecture-15.html#small-demo",
    "title": "",
    "section": "Small demo",
    "text": "Small demo\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nn = 1024\n\nf = np.fft.fft(np.eye(n))\n\n\nprint('Rectangular block, approximate low rank')\nprint(np.linalg.svd(f[:16, :n//16])[1])\n\nprint('Special low-rank submatrices, exact low-rank')\nm1 = f[::16, ::n//16]\nprint(m1.shape)\nprint(np.linalg.svd(m1)[1])"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#take-home-message",
    "href": "lectures/lecture-15/lecture-15.html#take-home-message",
    "title": "",
    "section": "Take home message",
    "text": "Take home message\n\nToeplitz and circulant matrices\nSpectral theorem\nFFT\nMultilevel Toeplitz matrices\nIntro to convolutional neural networks\n\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In [1], line 5\n      3     styles = open(\"./styles/custom.css\", \"r\").read()\n      4     return HTML(styles)\n----&gt; 5 css_styling()\n\nCell In [1], line 3, in css_styling()\n      2 def css_styling():\n----&gt; 3     styles = open(\"./styles/custom.css\", \"r\").read()\n      4     return HTML(styles)\n\nFile ~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282, in _modified_open(file, *args, **kwargs)\n    275 if file in {0, 1, 2}:\n    276     raise ValueError(\n    277         f\"IPython won't let you open fd={file} by default \"\n    278         \"as it is likely to crash IPython. If you know what you are doing, \"\n    279         \"you can use builtins' open.\"\n    280     )\n--&gt; 282 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: './styles/custom.css'"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html",
    "href": "lectures/lecture-13/lecture-13.html",
    "title": "Questions?",
    "section": "",
    "text": "Concept of iterative methods for linear systems:\n\nRichardson iteration and its convergence\nChebyshev iteration"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#solution-of-linear-systems-and-minimization-of-functionals",
    "href": "lectures/lecture-13/lecture-13.html#solution-of-linear-systems-and-minimization-of-functionals",
    "title": "Questions?",
    "section": "Solution of linear systems and minimization of functionals",
    "text": "Solution of linear systems and minimization of functionals\n\nInstead of solving a linear system, we can minimize the residual:\n\nR(x) = \\Vert A x - f \\Vert^2_2.\n\nThe condition \\nabla R(x) = 0 gives\n\nA^* A x = A^* f,\nthus it has squared condition number, so direct minimization of the residual by standard optimization methods is rarely used.\n\nFor the symmetric positive definite case there is a much simpler functional."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#energy-functional",
    "href": "lectures/lecture-13/lecture-13.html#energy-functional",
    "title": "Questions?",
    "section": "Energy functional",
    "text": "Energy functional\nLet A = A^* &gt; 0, then the following functional\n\\Phi(x) = (Ax, x)  - 2(f, x)\nis called energy functional.\n\nProperties of energy functional\n\nIt is strictly convex (check!)\n\n \\Phi(\\alpha x + (1 - \\alpha)y) &lt; \\alpha \\Phi(x) + (1 - \\alpha) \\Phi(y)\n\nSince it is strictly convex, it has unique local minimum, which is also global\nIts global minimum x_* satisfies\n\nA x_* = f.\nIndeed,\n\\nabla \\Phi = 2(Ax - f).\nand the first order optimality condition \\nabla \\Phi (x_*) = 0 yields\nA x_* = f."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#approximation-of-the-solution-by-a-subspace",
    "href": "lectures/lecture-13/lecture-13.html#approximation-of-the-solution-by-a-subspace",
    "title": "Questions?",
    "section": "Approximation of the solution by a subspace",
    "text": "Approximation of the solution by a subspace\n\nGiven a linear M-dimensional subspace \\{y_1, \\dots, y_M\\}, we want to find an approximate solution in this basis, i.e. \n\nA x \\approx f, \\quad x = x_0 +  \\sum_{k=1}^M c_k y_k,\nwhere c is the vector of coefficients.\n\nIn the symmetric positive definite case we need to minimize\n\n(Ax, x) - 2(f, x)\nsubject to x = x_0 + Y c,\nwhere Y=[y_1,\\dots,y_M] is n \\times M and vector c has length M.\n\nUsing the representation of x, we have the following minimization for c:\n\n\\widehat{\\Phi}(c) = (A Y c, Y c) + 2 (Y^*Ax_0, c) - 2(f, Y c) = (Y^* A Y c, c) - 2(Y^* (f - Ax_0), c).\n\nNote that this is the same functional, but for the Galerkin projection of A\n\nY^* A Y c = Y^* (f - Ax_0) = Y^* r_0,\nwhich is an M \\times M linear system with symmetric positive definite matrix if Y has full column rank.\nBut how to choose Y?"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-random-projection",
    "href": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-random-projection",
    "title": "Questions?",
    "section": "Selection of the subspace: random projection",
    "text": "Selection of the subspace: random projection\n\nWe can generate Y with random numbers and then orthogonalize\nWhat quality of x will we get in this case?\nHow the derived quality relates to the conditioning of the matrix?\nSelection of the proper random matrix is important topic in dimensionality reduction theory and relates to random projection approach\n\n\nimport numpy as np\n\nn = 100\nA = np.random.randn(n, n)\nQ, _ = np.linalg.qr(A)\nk = 70\nA = Q.T @ np.diag([1e-6] * k + list(np.random.rand(n-k))) @ Q\nx_true = np.random.randn(n)\nrhs = A @ x_true\nM = n - k\nY = np.random.randn(n, M)\nA_proj = Y.T @ A @ Y\nrhs_proj = Y.T @ rhs\nprint(A_proj.shape)\nc = np.linalg.solve(A_proj, rhs_proj)\nx_proj = Y @ c\nprint(np.linalg.norm(A @ x_proj - rhs) / np.linalg.norm(rhs))\n\n(30, 30)\n0.0004114360569253815"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-krylov-subspace",
    "href": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-krylov-subspace",
    "title": "Questions?",
    "section": "Selection of the subspace: Krylov subspace",
    "text": "Selection of the subspace: Krylov subspace\nIn the Krylov subspace we generate the whole subspace from a single vector r_0 = f - Ax_0:\ny_0\\equiv k_0 = r_0, \\quad y_1\\equiv k_1 = A r_0, \\quad y_2\\equiv k_2 = A^2 r_0, \\ldots, \\quad y_{M-1}\\equiv k_{M-1} = A^{M-1} r_0.\nThis gives the Krylov subpace of the M-th order\n\\mathcal{K}_M(A, r_0) = \\mathrm{Span}(r_0, Ar_0, \\ldots, A^{M-1} r_0).\n\nIt is known to be quasi-optimal space given only matrix-vector product operation.\nKey reference here is “On the numerical solution of equation by which are determined in technical problems the frequencies of small vibrations of material systems”, A. N. Krylov, 1931, text in russian"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#solution-x_-lies-in-the-krylov-subspace-x_-in-mathcalk_na-f",
    "href": "lectures/lecture-13/lecture-13.html#solution-x_-lies-in-the-krylov-subspace-x_-in-mathcalk_na-f",
    "title": "Questions?",
    "section": "Solution x_* lies in the Krylov subspace: x_* \\in \\mathcal{K}_n(A, f)",
    "text": "Solution x_* lies in the Krylov subspace: x_* \\in \\mathcal{K}_n(A, f)\n\nAccording to Cayley–Hamilton theorem: p(A) = 0, where p(\\lambda) = \\det(A - \\lambda I)\np(A)f = A^nf + a_1A^{n-1}f + \\ldots + a_{n-1}Af + a_n f = 0\nA^{-1}p(A)f = A^{n-1}f + a_1A^{n-2}f + \\ldots + a_{n-1}f + a_nA^{-1}f = 0\nx_* = A^{-1}f = -\\frac{1}{a_n}(A^{n-1}f + a_1A^{n-2}f + \\ldots + a_{n-1}f)\nThus, x_* \\in \\mathcal{K}_n(A, f)"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#ill-conditioned-of-the-natural-basis",
    "href": "lectures/lecture-13/lecture-13.html#ill-conditioned-of-the-natural-basis",
    "title": "Questions?",
    "section": "Ill-conditioned of the natural basis",
    "text": "Ill-conditioned of the natural basis\nThe natural basis in the Krylov subspace is very ill-conditioned, since\nk_i = A^i r_0 \\rightarrow \\lambda_\\max^i v,\nwhere v is the eigenvector, corresponding to the maximal eigenvalue of A, i.e. k_i become more and more collinear for large i.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.sparse as spsp\n%matplotlib inline\n\nn = 100\nex = np.ones(n);\nA = spsp.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nf = np.ones(n)\nx0 = np.random.randn(n)\n\nsubspace_order = 10\nkrylov_vectors = np.zeros((n, subspace_order))\nkrylov_vectors[:, 0] = f - A.dot(x0)\nfor i in range(1, subspace_order):\n    krylov_vectors[:, i] = A.dot(krylov_vectors[:, i-1])\n    \ns = np.linalg.svd(krylov_vectors, compute_uv=False)\nprint(\"Condition number = {}\".format(s.max() / s.min()))\n\nCondition number = 610650129.027377\n\n\nSolution: Compute orthogonal basis in the Krylov subspace."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#good-basis-in-a-krylov-subspace",
    "href": "lectures/lecture-13/lecture-13.html#good-basis-in-a-krylov-subspace",
    "title": "Questions?",
    "section": "Good basis in a Krylov subspace",
    "text": "Good basis in a Krylov subspace\nIn order to have stability, we first orthogonalize the vectors from the Krylov subspace using Gram-Schmidt orthogonalization process (or, QR-factorization).\nK_j = \\begin{bmatrix} r_0 & Ar_0 & A^2 r_0 & \\ldots & A^{j-1} r_0\\end{bmatrix} = Q_j R_j, \nand the solution will be approximated as\nx \\approx x_0 + Q_j c."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#short-way-to-arnoldi-relation",
    "href": "lectures/lecture-13/lecture-13.html#short-way-to-arnoldi-relation",
    "title": "Questions?",
    "section": "Short way to Arnoldi relation",
    "text": "Short way to Arnoldi relation\nStatement. The Krylov matrix K_j satisfies an important recurrent relation (called Arnoldi relation)\nA Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1},\nwhere H_j is upper Hessenberg, and Q_{j+1} = [q_0,\\dots,q_j] has orthogonal columns that spans columns of K_{j+1}.\nLet us prove it (consider j = 3 for simplicity):\nA \\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} = \\begin{bmatrix} k_1 & k_2 & k_3 \\end{bmatrix} = \\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} \\begin{bmatrix} 0 & 0 & \\alpha_0 \\\\ 1 & 0  & \\alpha_1 \\\\ 0 & 1  & \\alpha_2 \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 & 0 & k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2 \\end{bmatrix}, \nwhere \\alpha_s will be selected later. Denote \\widehat{k}_3 = k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2.\nIn the matrix form,\nA K_3 = K_3 Z + \\widehat k_3 e^{\\top}_2,\nwhere Z is the lower shift matrix with the last column (\\alpha_0,\\alpha_1,\\alpha_2)^T, and e_2 is the last column of the identity matrix.\nLet\nK_3 = Q_3 R_3\nbe the QR-factorization. Then,\nA Q_3 R_3 = Q_3 R_3 Z + \\widehat{k}_3 e^{\\top}_2,\n A Q_3 = Q_3 R_3 Z R_3^{-1} + \\widehat{k}_3 e^{\\top}_2 R_3^{-1}.\nNote that\ne^{\\top}_2 R_3^{-1} = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\end{bmatrix}  = \\gamma e^{\\top}_2, and\nR_3 Z R_3^{-1} = \\begin{bmatrix} * & * & * \\\\* & * & * \\\\  0 & * & * \\\\ \\end{bmatrix},\nin the general case it will be an upper Hessenberg matrix H, i.e. a matrix that\nH_{ij} = 0, \\quad \\mbox{if } i &gt; j + 1."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#almost-arnoldi-relation",
    "href": "lectures/lecture-13/lecture-13.html#almost-arnoldi-relation",
    "title": "Questions?",
    "section": "(Almost) Arnoldi relation",
    "text": "(Almost) Arnoldi relation\nLet Q_j be the orthogonal basis in the Krylov subspace, then we have almost the Arnoldi relation\nA Q_j = Q_j H_j +  \\gamma\\widehat{k}_j e^{\\top}_{j-1},\nwhere H_j is an upper Hessenberg matrix, and\n\\widehat{k}_j = k_j - \\sum_{s=0}^{j-1} \\alpha_s k_s.\nWe select \\alpha_s in such a way that\nQ^*_j \\widehat{k}_j = 0.\nThen, \\widehat{k}_j = h_{j, j-1} q_j, where q_j is the last column of Q_{j+1}."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#arnoldi-relation-final-formula",
    "href": "lectures/lecture-13/lecture-13.html#arnoldi-relation-final-formula",
    "title": "Questions?",
    "section": "Arnoldi relation: final formula",
    "text": "Arnoldi relation: final formula\nWe have\nA Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1}.\n\nThis is the crucial formula for the efficient generation of such subspaces.\nFor non-symmetric case, it is just modified Gram-Schmidt.\nFor the symmetric case, we have a much simpler form (Lanczos process)."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#lanczos-process",
    "href": "lectures/lecture-13/lecture-13.html#lanczos-process",
    "title": "Questions?",
    "section": "Lanczos process",
    "text": "Lanczos process\nIf A = A^*, then\nQ^*_j A Q_j = H_j, \nthus H_j is hermitian, and thus it is tridiagonal, H_j = T_j.\nThis gives a short-term recurrence relation to generate the Arnoldi vectors q_j without full orthogonalization."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#lanczos-process-2",
    "href": "lectures/lecture-13/lecture-13.html#lanczos-process-2",
    "title": "Questions?",
    "section": "Lanczos process (2)",
    "text": "Lanczos process (2)\n A Q_j = Q_j T_j + t_{j, j-1} q_j e^{\\top}_{j-1}.\nIn order to get q_j, we need to compute just the last column of\nt_{j, j-1} q_j = (A Q_j - Q_j T_j) e_{j-1} = A q_{j-1} - t_{j-1, j-1} q_{j-1} - t_{j-2, j-1} q_{j-2}. \nThe coefficients \\alpha_j = t_{j-1, j-1} and \\beta_j = t_{j-2, j-1} can be recovered from orthogonality constraints\n(q_j, q_{j-1}) = 0, \\quad (q_j, q_{j-2}) = 0\nAll the other constraints will be satisfied automatically!!\nAnd we only need to store two vectors to get the new one."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#from-direct-lanczos-method-to-the-conjugate-gradient",
    "href": "lectures/lecture-13/lecture-13.html#from-direct-lanczos-method-to-the-conjugate-gradient",
    "title": "Questions?",
    "section": "From direct Lanczos method to the conjugate gradient",
    "text": "From direct Lanczos method to the conjugate gradient\nWe can now get from the Lanczos recurrence to the famous conjugate gradient method.\nWe have for A = A^* &gt; 0\nA Q_j = Q_j T_j + T_{j, j-1} q_j.\nRecall that when we minimize energy functional in basis Y we get a system Y^* A Y c = Y^* f,. Here Y = Q_j, so the approximate solution of Ax \\approx f with x_j = x_0 + Q_j c_j can be found by solving a small system\nQ^*_j A Q_j c_j = T_j c_j = Q^*_j r_0 .\nSince f is the first Krylov subspace, then Note!!! (recall what the first column in Q_j is)\nQ^*_j r_0  = \\Vert r_0 \\Vert_2 e_0 = \\gamma e_0.\nWe have a tridiagonal system of equations for c:\nT_j c_j = \\gamma e_0\nand x_j = Q_j c_j.\nWe could stop at this point, but we want short recurrent formulas instead of solving linear system with matrix T_j at each step.\nDerivation of the following update formulas is not required on the oral exam!\n\nSince A is positive definite, T_j is also positive definite, and it allows an LU decomposition\nT_j = L_j U_j, where L_j is a bidiagonal matrix with ones on the diagonal, U_j is a upper bidiagonal matrix.\n\n T_j = \\begin{bmatrix} a_1 & b_1 &  & \\\\ b_1 & a_2 & b_2 & \\\\ & \\ddots & \\ddots & \\ddots & \\\\ & & b_{j-1} & a_{j-1} & b_j \\\\ & & & b_j & a_j \\end{bmatrix} = \\begin{bmatrix} 1 & &  & \\\\ c_1 & 1 &  & \\\\ & \\ddots & \\ddots &  & \\\\ & & c_{j-1} & 1 & \\\\ & & & c_j & 1 \\end{bmatrix} \\begin{bmatrix} d_1 & b_1 &  & \\\\ & d_1 & b_2 & \\\\ & & \\ddots & \\ddots & \\\\ & & & d_{j-1} & b_j \\\\ & & & & d_j \\end{bmatrix} \n\nWe need to define one subdiagonal in L (with elements c_1, \\ldots, c_{j-1}), main diagonal of U_j (with elements d_0, \\ldots, d_{j-1} and superdiagonal of U_j (with elements b_1, \\ldots, b_{j-1}).\nThey have convenient recurrences:\n\nc_i = b_i/d_{i-1}, \\quad d_i = \\begin{cases} a_1, & \\mbox{if } i = 1, \\\\\na_i - c_i b_i, & \\mbox{if } i &gt; 1. \\end{cases}\n\nFor the solution we have\n\nx_j = Q_j T^{-1}_j \\gamma e_0  = \\gamma Q_j (L_j U_j)^{-1} e_0  = \\gamma Q_j U^{-1}_j L^{-1}_j e_0.\n\nWe introduce two new quantities:\n\nP_j = Q_j U^{-1}_j, \\quad z_j = \\gamma L^{-1}_j e_0.\n\nNow we have the following equation for x_j:\n\n x_j = P_j z_j\n\nDue to the recurrence relations, we have\n\nP_j = \\begin{bmatrix} P_{j-1} & p_j \\end{bmatrix}, \nand\nz_j = \\begin{bmatrix} z_{j-1} \\\\ \\xi_{j} \\end{bmatrix}.\n\nFor p_j and \\xi_j we have short-term recurrence relations (due to bidiagonal structure)\n\np_j = \\frac{1}{d_j}\\left(q_j - b_j p_{j-1} \\right), \\quad \\xi_j = -c_j \\xi_{j-1}.\n\nThus, we arrive at short-term recurrence for x_j:\n\nx_j = P_j z_j = P_{j-1} z_{j-1} + \\xi_j p_j = x_{j-1} + \\xi_j p_j.\nand q_j are found from the Lanczos relation (see slides above).\n\nThis method for solving linear systems is called a direct Lanczos method. It is closely related to the conjugate gradient method."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#direct-lanczos-method",
    "href": "lectures/lecture-13/lecture-13.html#direct-lanczos-method",
    "title": "Questions?",
    "section": "Direct Lanczos method",
    "text": "Direct Lanczos method\nWe have the direct Lanczos method, where we store\np_{j-1}, q_j, x_{j-1} to get a new estimate of x_j.\nThe main problem is with q_j: we have the three-term recurrence, but in the floating point arithmetic the orthogonality is can be lost, leading to numerical errors.\nLet us do some demo.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy as sp\nimport scipy.sparse as spsp\nfrom scipy.sparse import csc_matrix\n\nn = 128\nex = np.ones(n);\nA = spsp.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\n\nnit = 64\nq1 = rhs/np.linalg.norm(rhs)\nq2 = A.dot(q1)\nq2 = q2 - np.dot(q2, q1)*q1\nq2 = q2/np.linalg.norm(q2)\nqall = [q1, q2]\nfor i in range(nit):\n    qnew = A.dot(qall[-1])\n    qnew = qnew - np.dot(qnew, qall[-1])*qall[-1]\n    qnew = qnew/np.linalg.norm(qnew)\n    qnew = qnew - np.dot(qnew, qall[-2])*qall[-2]\n    qnew = qnew/np.linalg.norm(qnew)\n    qall.append(qnew)\nqall_mat = np.vstack(qall).T\nprint(np.linalg.norm(qall_mat.T.dot(qall_mat) - np.eye(qall_mat.shape[1])))\n\n1.9605915654183865"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#conjugate-gradient-method",
    "href": "lectures/lecture-13/lecture-13.html#conjugate-gradient-method",
    "title": "Questions?",
    "section": "Conjugate gradient method",
    "text": "Conjugate gradient method\nInstead of q_j (last vector in the modified Gram-Schmidt process), it is more convenient to work with the residual\nr_j = f - A x_j.\nThe resulting recurrency has the form\nx_j = x_{j-1} + \\alpha_{j-1} p_{j-1}\nr_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}\np_j = r_j + \\beta_j p_{j-1}.\nHence the name conjugate gradient: to the gradient r_j we add a conjugate direction p_j.\nWe have orthogonality of residuals (check!):\n(r_i, r_j) = 0, \\quad i \\ne j\nand A-orthogonality of conjugate directions (check!):\n (A p_i, p_j) = 0,\nwhich can be checked from the definition.\nThe equations for \\alpha_j and \\beta_j can be now defined explicitly from these two properties."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#cg-final-formulas",
    "href": "lectures/lecture-13/lecture-13.html#cg-final-formulas",
    "title": "Questions?",
    "section": "CG final formulas",
    "text": "CG final formulas\nWe have (r_{j}, r_{j-1}) = 0 = (r_{j-1} - \\alpha_{j-1} A p_{j-1}, r_{j-1}),\nthus\n\\alpha_{j-1} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, r_{j-1})} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, p_{j-1} - \\beta_{j-1}p_{j-2})} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, p_{j-1})}.\nIn the similar way, we have\n\\beta_{j-1} = \\frac{(r_j, r_j)}{(r_{j-1}, r_{j-1})}.\nRecall that\nx_j = x_{j-1} + \\alpha_{j-1} p_{j-1}\nr_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}\np_j = r_j + \\beta_j p_{j-1}.\nOnly one matrix-by-vector product per iteration."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#cg-derivation-overview",
    "href": "lectures/lecture-13/lecture-13.html#cg-derivation-overview",
    "title": "Questions?",
    "section": "CG derivation overview",
    "text": "CG derivation overview\n\nWant to find x_* in Krylov subspace\nBut natural basis is ill-conditioned, therefore we need orthogonalization\nDerive recurrent equation for sequential orthogonalization of the Krylov subspace basis\n\nArnoldi process for non-symmetric matrix\nLanczos process for symmetrix matrix\n\nClever re-writing of these formulas gives short recurrence"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#some-history",
    "href": "lectures/lecture-13/lecture-13.html#some-history",
    "title": "Questions?",
    "section": "Some history",
    "text": "Some history\nMore details here: https://www.siam.org/meetings/la09/talks/oleary.pdf\nWhen Hestenes worked on conjugate bases in 1936, he was advised by a Harvard professor that it was too obvious for publication - CG doesn’t work on slide rules.  - CG has little advantage over Gauss elimination for computation with calculators. - CG is not well suited for a room of human “computers” – too much data exchange."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#properties-of-the-cg-method",
    "href": "lectures/lecture-13/lecture-13.html#properties-of-the-cg-method",
    "title": "Questions?",
    "section": "Properties of the CG method",
    "text": "Properties of the CG method\n\nWe need to store 3 vectors.\nSince it generates A-orthogonal sequence p_1, \\ldots, p_N, after n steps it should stop (i.e., p_{N+1} = 0.)\nIn practice it does not have this property in finite precision, thus after its invention in 1952 by Hestens and Stiefel it was labeled unstable.\nIn fact, it is a brilliant iterative method."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#a-optimality",
    "href": "lectures/lecture-13/lecture-13.html#a-optimality",
    "title": "Questions?",
    "section": "A-optimality",
    "text": "A-optimality\nEnergy functional can be written as\n(Ax, x) - 2(f, x) = (A (x - x_*), (x - x_*)) - (Ax _*, x_*),\nwhere A x_* = f. Up to a constant factor,\n (A(x - x_*), (x -x_*)) = \\Vert x - x_* \\Vert^2_A\nis the A-norm of the error."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#convergence",
    "href": "lectures/lecture-13/lecture-13.html#convergence",
    "title": "Questions?",
    "section": "Convergence",
    "text": "Convergence\nThe CG method computes x_k that minimizes the energy functional over the Krylov subspace, i.e. x_k = p(A)f, where p is a polynomial of degree k+1, so\n\\Vert x_k - x_* \\Vert_A  =  \\inf\\limits_{p} \\Vert \\left(p(A) - A^{-1}\\right) f \\Vert_A. \nUsing eigendecomposition of A we have\nA = U \\Lambda U^*, \\quad  g = U^* f, and\n\\Vert x - x_* \\Vert^2_A = \\displaystyle{\\inf_p} \\Vert \\left(p(\\Lambda) - \\Lambda^{-1}\\right) g \\Vert_\\Lambda^2 = \\displaystyle{\\inf_p}\n\\displaystyle{\\sum_{i=1}^n} \\frac{(\\lambda_i p(\\lambda_i) - 1)^2 g^2_i}{\\lambda_i} = \\displaystyle{\\inf_{q, q(0) = 1}} \\displaystyle{\\sum_{i=1}^n} \\frac{q(\\lambda_i)^2 g^2_i}{\\lambda_i}\nSelection of the optimal q depends on the eigenvalue distribution."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#absolute-and-relative-error",
    "href": "lectures/lecture-13/lecture-13.html#absolute-and-relative-error",
    "title": "Questions?",
    "section": "Absolute and relative error",
    "text": "Absolute and relative error\nWe have\n\\Vert x - x_* \\Vert^2_A \\leq \\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} \\inf_{q, q(0)=1} \\max_{j} q({\\lambda_j})^2\nThe first term is just\n\\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} = (A^{-1} f, f) = \\Vert x_* \\Vert^2_A.\nAnd we have relative error bound\n\\frac{\\Vert x - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})|,\nso if matrix has only 2 different eigenvalues, then there exists a polynomial of degree 2 such that q({\\lambda_1}) =q({\\lambda_2})=0, so in this case CG converges in 2 iterations.\n\n\nIf eigenvalues are clustered and there are l outliers, then after first \\mathcal{O}(l) iterations CG will converge as if there are no outliers (and hence the effective condition number is smaller).\nThe intuition behind this fact is that after \\mathcal{O}(l) iterations the polynomial has degree more than l and thus is able to zero l outliers. \n\nLet us find another useful upper-bound estimate of convergence. Since\n\n\\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})| \\leq \\inf_{q, q(0)=1} \\max_{\\lambda\\in[\\lambda_\\min,\\lambda_\\max]} |q({\\lambda})|\n\nThe last term is just the same as for the Chebyshev acceleration, thus the same upper convergence bound holds:\n\\frac{\\Vert x_k - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\gamma \\left( \\frac{\\sqrt{\\mathrm{cond}(A)}-1}{\\sqrt{\\mathrm{cond}(A)}+1}\\right)^k."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#finite-termination-clusters",
    "href": "lectures/lecture-13/lecture-13.html#finite-termination-clusters",
    "title": "Questions?",
    "section": "Finite termination & clusters",
    "text": "Finite termination & clusters\n\nIf A has only m different eigenvalues, CG converges in m iterations (proof in the blackboard).\nIf A has m “clusters” of eigenvalues, CG converges cluster-by-cluster.\n\nAs a result, better convergence than Chebyshev acceleration, but slightly higher cost per iteration."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#summary",
    "href": "lectures/lecture-13/lecture-13.html#summary",
    "title": "Questions?",
    "section": "Summary",
    "text": "Summary\nCG is the method of choice for symmetric positive definite systems:\n\n\\mathcal{O}(n) memory\nSquare root of condition number in the estimates\nAutomatic ignoring of the outliers/clusters\nA-optimality property"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#non-linear-conjugate-gradient-method",
    "href": "lectures/lecture-13/lecture-13.html#non-linear-conjugate-gradient-method",
    "title": "Questions?",
    "section": "Non-linear conjugate gradient method",
    "text": "Non-linear conjugate gradient method\n\nCG minimizes the energy functional, which is quadratic in x\nCG formulas were used as starting point in developing methods to minimize arbitrary convex function\nMost popular CG extensions (so-called non-linear CG method) are\n\nHestenes-Stiefel method\nPolak-Ribiere method - original paper in French\nFletcher–Reeves method"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#non-symmetric-systems-and-the-generalized-minimal-residual-method-gmres-y.-saad-m.-schultz-1986",
    "href": "lectures/lecture-13/lecture-13.html#non-symmetric-systems-and-the-generalized-minimal-residual-method-gmres-y.-saad-m.-schultz-1986",
    "title": "Questions?",
    "section": "Non-symmetric systems and the generalized minimal residual method (GMRES) (Y. Saad, M. Schultz, 1986)",
    "text": "Non-symmetric systems and the generalized minimal residual method (GMRES) (Y. Saad, M. Schultz, 1986)\nBefore we discussed symmetric positive definite systems. What happens if A is non-symmetric?\nWe can still orthogonalize the Krylov subspace using Arnoldi process, and get\nA Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1}.\nLet us rewrite the latter expression as\n A Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1} = Q_{j+1} \\widetilde H_j, \\quad \\widetilde H_j =\n\\begin{bmatrix} h_{0,0} & h_{0,1} & \\dots & h_{0,j-2} & h_{0,j-1} \\\\ h_{1,0} & h_{1,1} & \\dots & h_{1,j-2} & h_{1,j-1} \\\\ 0& h_{2,2} &  \\dots & h_{2,j-2} & h_{2,j-1} \\\\\n0& 0 & \\ddots & \\vdots & \\vdots  \\\\\n0& 0 &  & h_{j,j-1} & h_{j-1,j-1} \\\\ 0& 0 & \\dots & 0 & h_{j,j-1}\\end{bmatrix}\nThen, if we need to minimize the residual over the Krylov subspace, we have\nx_j = x_0 + Q_j c_j \nand x_j has to be selected as\n \\Vert A x_j - f \\Vert_2 =  \\Vert A Q_j c_j - r_0 \\Vert_2 \\rightarrow \\min_{c_j}.\nUsing the Arnoldi recursion, we have\n \\Vert Q_{j+1} \\widetilde H_j c_j -  r_0 \\Vert_2 \\rightarrow \\min_{c_j}.\nUsing the orthogonal invariance under multiplication by unitary matrix, we get\n \\Vert \\widetilde H_j c_j - \\gamma e_0 \\Vert_2 \\rightarrow \\min_{c_j},\nwhere we have used that Q^*_{j+1} r_0 = \\gamma e_0, \\gamma = \\Vert r_0 \\Vert\n\nThis is just a linear least squares with (j+1) equations and j unknowns.\nThe matrix is also upper Hessenberg, thus its QR factorization can be computed in a very cheap way.\nThis allows the computation of c_j. This method is called GMRES (generalized minimal residual)"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#summary-of-the-gmres",
    "href": "lectures/lecture-13/lecture-13.html#summary-of-the-gmres",
    "title": "Questions?",
    "section": "Summary of the GMRES",
    "text": "Summary of the GMRES\n\nMinimizes the residual directly\nNo normal equations\nMemory grows with the number of iterations as \\mathcal{O}(nj), so restarts typically implemented (just start GMRES from the new initial guess).\n\n\nimport scipy.sparse.linalg as la\nfrom scipy.sparse import csc_matrix, csr_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n%matplotlib inline\nn = 150\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\n\nplt.figure(figsize=(10, 5))\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nfor restart in [5, 40, 200]:\n    hist = []\n    def callback(rk):\n        hist.append(np.linalg.norm(rk) / np.linalg.norm(rhs))\n    st = time.time()\n    sol = la.gmres(A, rhs, x0=np.zeros(n*n), maxiter=200, restart=restart, callback=callback, tol=1e-16)\n    current_time = time.time() - st\n    ax1.semilogy(np.array(hist), label='rst={}'.format(restart))\n    ax2.semilogy([current_time * i / len(hist) for i in range(len(hist))], np.array(hist), label='rst={}'.format(restart))\n    \n\nax1.legend(loc='best')\nax2.legend(loc='best')\nax1.set_xlabel(\"Number of outer iterations\", fontsize=20)\nax2.set_xlabel(\"Time, sec\", fontsize=20)\nax1.set_ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nax2.set_ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nplt.sca(ax1)\nplt.yticks(fontsize=20)\nplt.sca(ax2)\nplt.yticks(fontsize=20)\nf.tight_layout()\n\n&lt;Figure size 1000x500 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as la\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Example from http://www.caam.rice.edu/~embree/39961.pdf\n\nA = np.array([[1, 1, 1],\n              [0, 1, 3],\n              [0, 0, 1]]\n            )\nrhs = np.array([2, -4, 1])\nx0 = np.zeros(3)\n\nfor restart in [1, 2, 3]:\n    hist = []\n    def callback(rk):\n        hist.append(np.linalg.norm(rk)/np.linalg.norm(rhs))\n    _ = la.gmres(A, rhs, x0=x0, maxiter=20, restart=restart, callback=callback)\n    plt.semilogy(np.array(hist), label='rst={}'.format(restart))\nplt.legend(fontsize=22)\nplt.xlabel(\"Number of outer iterations\", fontsize=20)\nplt.ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#next-lecture",
    "href": "lectures/lecture-13/lecture-13.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nIterative methods continued (BiCG, MINRES)\nPreconditioners"
  }
]