[
  {
    "objectID": "lectures/lecture-14/lecture-14.html#previous-part",
    "href": "lectures/lecture-14/lecture-14.html#previous-part",
    "title": "Questions?",
    "section": "Previous part",
    "text": "Previous part\n\nArnoldi orthogonalization of Krylov subspaces\nLanczos for the symmetric case\nEnergy functional and conjugate gradient method\nConvergence analysis\nNon-symmetric case: idea of GMRES"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#now-we-will-cover-the-following-topics",
    "href": "lectures/lecture-14/lecture-14.html#now-we-will-cover-the-following-topics",
    "title": "Questions?",
    "section": "Now we will cover the following topics",
    "text": "Now we will cover the following topics\n\nMore iterative methods: MINRES, BiCG and BiCGStab\nThe concept of preconditioners"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#what-methods-to-use",
    "href": "lectures/lecture-14/lecture-14.html#what-methods-to-use",
    "title": "Questions?",
    "section": "What methods to use",
    "text": "What methods to use\n\nIf a matrix is symmetric (Hermitian) positive definite, use CG method.\nIf a matrix is symmetric but indefinite, we can use MINRES method (GMRES applied to a symmetric system)\nIf a matrix is non-symmetric and not very big, use GMRES\nIf a matrix is non-symmetric and we can store limited amount of vectors, use either: GMRES with restarts, or BiCGStab (the latter of the product with A^{\\top} is also available).\n\n\nMore detailed flowchart from this book"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#minres",
    "href": "lectures/lecture-14/lecture-14.html#minres",
    "title": "Questions?",
    "section": "MINRES",
    "text": "MINRES\nThe MINRES method is GMRES applied to a symmetric system.\n\nWe search the solution in the subspace spanned by columns of Q_j\n\n x_j = x_0 + Q_j c_j \n\nWe minimize the residual norm\n\n\\Vert Ax_j - f\\Vert_2 = \\Vert Ax_0 + AQ_j c_j - f\\Vert_2 =  \\Vert A Q_j c_j - r_0 \\Vert_2 = \\Vert Q_j H_j c_j + h_{j, j-1} q_j c_j - r_0 \\Vert_2 = \\Vert Q_{j+1} \\widetilde{H}_{j+1}  c_j - r_0 \\Vert_2 \\rightarrow \\min_{c_j}\nwhich is equivalent to a linear least squares with an almost tridiagonal matrix\n\\Vert \\widetilde{H}_{j+1} c_{j} - \\gamma e_0 \\Vert_2 \\rightarrow \\min_{c_{j}}.\n\nIn a similar fashion, we can derive short-term recurrences.\nA careful implementation of MINRES requires at most 5 vectors to be stored."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#difference-between-minres-and-cg",
    "href": "lectures/lecture-14/lecture-14.html#difference-between-minres-and-cg",
    "title": "Questions?",
    "section": "Difference between MINRES and CG",
    "text": "Difference between MINRES and CG\n\nMINRES minimizes \\Vert Ax_k - f \\Vert_2 over the Krylov subspace\nCG minimize (Ax, x) - 2(f, x) over the Krylov subspace\nMINRES works for indefinite (i.e., non-positive definite) problems.\n\nCG stores less vectors (3 instead of 5).\nNow, let us talk about non-symmetric systems.\n\nimport scipy.sparse.linalg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\n# Create a symmetric indefinite matrix\nn = 200\ndiag = np.ones(n)\n# Create a discretized 1D Helmholtz operator: -d^2/dx^2 - k^2\nh = 1.0/n  # Grid spacing\nk = 0     # Wave number\n\n# Create diagonals for tridiagonal matrix\nmain_diag = -2.00/h**2 + k**2  # Main diagonal: -2/h^2 + k^2 \noff_diag = 1.0/h**2           # Off diagonals: 1/h^2\n\n# Create arrays for the three diagonals\nmain = np.ones(n) * main_diag\nupper = np.ones(n) * off_diag  \nlower = np.ones(n) * off_diag\n\n# Construct tridiagonal matrix using sparse diagonals\n# The error occurs because spdiags expects diagonals to be provided in a 2D array\n# where each row represents a diagonal. Let's stack them correctly:\nA = sp.sparse.spdiags([lower, main, upper], [-1, 0, 1], n, n, format='csr')\n# Random right-hand side\nnp.random.seed(42)\nb = np.random.randn(n)\n\n# Lists to store residual norms\nres_minres = []\nres_cg = []\n\ndef callback_minres(xk):\n    res_minres.append(np.linalg.norm(b - A @ xk))\n    \ndef callback_cg(xk):\n    res_cg.append(np.linalg.norm(b - A @ xk))\n\n# Solve with MINRES\nx_minres = scipy.sparse.linalg.minres(A, b, callback=callback_minres, maxiter=500, rtol=1e-10)\n\n# Try to solve with CG (may not converge due to indefiniteness)\ntry:\n    x_cg = scipy.sparse.linalg.cg(A, b, callback=callback_cg, maxiter=500, rtol=1e-10)\nexcept:\n    print(\"CG failed to converge as expected for indefinite system\")\n\n# Plot convergence\nplt.figure(figsize=(8, 6))\nplt.semilogy(res_minres, 'b-', label='MINRES')\nif len(res_cg) &gt; 0:\n    plt.semilogy(res_cg, 'r--', label='CG')\nplt.xlabel('Iteration')\nplt.ylabel('Residual norm')\nplt.title('Convergence comparison for symmetric indefinite system')\nplt.legend()\nplt.grid(True)\nnp.max(np.linalg.eigvals(A.todense()))\n\nnp.float64(-9.771444747833783)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#non-symmetric-systems",
    "href": "lectures/lecture-14/lecture-14.html#non-symmetric-systems",
    "title": "Questions?",
    "section": "Non-symmetric systems",
    "text": "Non-symmetric systems\n\nThe main disadvantage of GMRES: we have to store all the vectors, so the memory cost grows with each step.\nWe can do restarts (i.e. get a new residual and a new Krylov subspace): we find some approximate solution x and now solve the linear system for the correction:\n\nA(x + e) = f, \\quad Ae = f - Ax,\nand generate the new Krylov subspace from the residual vector. This spoils the convergence, as we will see from the demo.\n\nimport scipy.sparse.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rc(\"text\", usetex=False)\nimport numpy as np\nimport scipy as sp\n\nn = 300\nex = np.ones(n);\nA = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.random.randn(n)\n\nres_gmres_rst = []\nres_gmres = []\ndef gmres_rst_cl(r):\n    res_gmres_rst.append(np.linalg.norm(r))\n    \ndef gmres_rst(r):\n    res_gmres.append(np.linalg.norm(r))\n\nsol = scipy.sparse.linalg.gmres(A, rhs, restart=20, callback=gmres_rst_cl)\nsol = scipy.sparse.linalg.gmres(A, rhs, restart=n, callback=gmres_rst)\n\nlim = 300\nplt.semilogy(res_gmres_rst[:lim], marker='.',color='k', label='GMRES, restart=20')\nplt.semilogy(res_gmres[:lim], marker='x',color='r', label='GMRES, no restart')\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Residual norm', fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.legend(fontsize=20)\n\n\n\n\n\n\n\n\n\nHow to avoid such spoiling of convergence?\n\nBiConjugate Gradient method (named BiCG, proposed by Fletcher, original paper) avoids that using “short recurrences” like in the CG method."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#idea-of-biconjugate-gradient",
    "href": "lectures/lecture-14/lecture-14.html#idea-of-biconjugate-gradient",
    "title": "Questions?",
    "section": "Idea of biconjugate gradient",
    "text": "Idea of biconjugate gradient\nIdea of BiCG method is to use the normal equations:\nA^* A x = A^* f,\nand apply the CG method to it.\n\nThe condition number has squared, thus we need stabilization.\nThe stabilization idea proposed by Van der Vorst et al. improves the stability (later in the lecture)\n\nLet us do some demo for a simple non-symmetric matrix to demonstrate instability of BiCG method.\n\nres_all_bicg = []\ndef bicg_cl(x):\n    res_all_bicg.append(np.linalg.norm(A.dot(x) - rhs))\n    \nsol = scipy.sparse.linalg.bicg(A, rhs, x0=np.zeros(n), callback=bicg_cl)\nplt.semilogy(res_all_bicg, label='BiCG')\nplt.semilogy(res_gmres_rst[:n], label='GMRES, restart=20')\nplt.semilogy(res_gmres, label='GMRES, no restart')\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Residual norm', fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.legend(fontsize=20)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#biconjugate-gradients",
    "href": "lectures/lecture-14/lecture-14.html#biconjugate-gradients",
    "title": "Questions?",
    "section": "BiConjugate Gradients",
    "text": "BiConjugate Gradients\nThere are two options:\n\nUse \\mathcal{K}(A^* A, A^* f) to generate the subspace. That leads to square of condition number\nInstead, use two Krylov subspaces \\mathcal{K}(A) and \\mathcal{K}(A^*) to generate two basises that are biorthogonal (so-called biorthogonal Lanczos).\n\nThe goal is to compute the Petrov-Galerkin projection\nW^* A V \\widehat{x} = W^* f\nwith columns W from the Krylov subspace of A^*, V from A (cf. with CG case).\nThat may lead to instabilities if we try to recompute the solutions in the efficient way. It is related to the pivoting (which we did not use in CG), and it is not naturally implemented here."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#notes-about-bicg",
    "href": "lectures/lecture-14/lecture-14.html#notes-about-bicg",
    "title": "Questions?",
    "section": "Notes about BiCG",
    "text": "Notes about BiCG\nA practical implementation of BiCG uses two-sided Lanczos process: generating Krylov subspace for A and A^{\\top}\nIn partucular\n\n\\alpha_j = \\frac{(r_j, \\hat{r}_j)}{(Ap_j, \\hat{p}_j)}\n$x_{j+1} = x_j + _j p_j $\nr_{j+1} = r_j - \\alpha_j Ap_j\n\\hat{r}_{j+1} = \\hat{r}_j - \\alpha_j A^{\\top}\\hat{p}_j\n\\beta_j = \\frac{(r_{j+1}, \\hat{r}_{j+1})}{(r_j, \\hat{r}_j)}\np_{j+1} = r_{j+1} + \\beta_j p_j\n\\hat{p}_{j+1} = \\hat{r}_{j+1} - \\beta_j \\hat{p}_j\n\nNow we move to the stable version of the BiCG method"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#bicgstab",
    "href": "lectures/lecture-14/lecture-14.html#bicgstab",
    "title": "Questions?",
    "section": "BiCGStab",
    "text": "BiCGStab\n\nBiCGStab is frequently used, and represent a stabilized version of BiCG. It has faster and smoother convergence than original BiCG method.\nThe formulas can be found, for example, here\nIt is a combination of BiCG step followed by GMRES(1) step in order to smooth the convergence.\nFor more details, please consult the book “Iterative Krylov Methods for Large Linear Systems” by H. Van-der Vorst.\n\nA short demo to compare “Stabilized” vs “Non-stabilized” versions.\n\nimport scipy.sparse.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\nn = 600\n\nex = np.ones(n);\nA = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr') \nrhs = np.random.randn(n)\n\n# ee = sp.sparse.eye(n)\n# A = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr')\n# A = sp.sparse.kron(A, ee) + sp.sparse.kron(ee, A)\n# rhs = np.ones(n * n)\n\nprint(\"Dimension of the linear system = {}\".format(A.shape[0]))\n\nres_all_bicg = []\nres_all_bicgstab = []\ndef bicg_cl(x):\n    res_all_bicg.append(np.linalg.norm(A.dot(x) - rhs))\n\ndef bicgstab_cl(x):\n    res_all_bicgstab.append(np.linalg.norm(A.dot(x) - rhs))\n\nres_gmres_rst = []\nres_gmres = []\ndef gmres_rst_cl(r):\n    res_gmres_rst.append(np.linalg.norm(r))\n    \ndef gmres_rst(r):\n    res_gmres.append(np.linalg.norm(r))\n\nsol2 = scipy.sparse.linalg.gmres(A, rhs, restart=20, callback=gmres_rst_cl)\nsol2 = scipy.sparse.linalg.gmres(A, rhs, restart=n, callback=gmres_rst)\n\n    \nsol2 = scipy.sparse.linalg.bicg(A, rhs, x0=np.zeros(A.shape[0]), callback=bicg_cl)\nsol2 = scipy.sparse.linalg.bicgstab(A, rhs, x0=np.zeros(A.shape[0]), callback=bicgstab_cl)\nres_all_bicg = np.array(res_all_bicg)/res_all_bicg[0]\nres_all_bicgstab = np.array(res_all_bicgstab)/res_all_bicgstab[0]\nres_gmres_rst = np.array(res_gmres_rst)/res_gmres_rst[0]\nres_gmres = np.array(res_gmres)/res_gmres[0]\n\nlim = 500\nplt.semilogy(res_all_bicgstab[:lim], marker='.',color='k', label='BiCGStab')\nplt.semilogy(res_all_bicg[:lim], marker='x',color='r', label='BiCG')\nplt.semilogy(res_gmres_rst[:lim], marker='.',color='y', label='GMRES res(n)')\nplt.semilogy(res_gmres[:lim], marker='x',color='g', label='GMRES')\n\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Retative residual norm', fontsize=20)\nplt.legend(loc='best', fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\nDimension of the linear system = 600"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#nonlinear-gmres-or-anderson-acceleration",
    "href": "lectures/lecture-14/lecture-14.html#nonlinear-gmres-or-anderson-acceleration",
    "title": "Questions?",
    "section": "“Nonlinear GMRES” or Anderson acceleration",
    "text": "“Nonlinear GMRES” or Anderson acceleration\nWe can apply the GMRES-like idea to speed up the convergence of a given fixed-point iteration\nx_{k+1} = \\Phi(x_k).\nThis was actually older than the GMRES, and known as an Direct Inversion in Iterated Subspaces in Quantum Chemistry, or Anderson Acceleration.\nIdea: use history for the update\nx_{k+1} = \\Phi(x_k) + \\sum_{s=1}^m \\alpha_s (x_{k - s} - \\Phi(x_{k - s})), \nand the parameters \\alpha_s are selected to minimize the norm of the residual\n \\min_{\\alpha} \\left \\| \\sum_{s=1}^m \\alpha_s (x_{k - s} - \\Phi(x_{k - s})) \\right\\|_2, \\quad \\sum_{s=1}^m \\alpha_s = 1\nMore details see in the original paper"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#battling-the-condition-number",
    "href": "lectures/lecture-14/lecture-14.html#battling-the-condition-number",
    "title": "Questions?",
    "section": "Battling the condition number",
    "text": "Battling the condition number\n\nThe condition number problem is un-avoidable if only the matrix-by-vector product is used.\nThus we need an army of preconditioners to solve it.\nThere are several general purpose preconditioners that we can use, but often for a particular problem a special design is needed."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#preconditioner-general-concept",
    "href": "lectures/lecture-14/lecture-14.html#preconditioner-general-concept",
    "title": "Questions?",
    "section": "Preconditioner: general concept",
    "text": "Preconditioner: general concept\nThe general concept of the preconditioner is simple:\nGiven a linear system\nA x = f,\nwe want to find the matrix P_R and/or P_L such that\n\nCondition number of AP_R^{-1} (right preconditioner) or P^{-1}_LA (right preconditioner) or P^{-1}_L A P_R^{-1} is better than for A\nWe can easily solve P_Ly = g or P_Ry = g for any g (otherwise we could choose e.g. P_L = A)\n\nThen we solve for (right preconditioner)\n AP_R^{-1} y = f \\quad \\Rightarrow \\quad P_R x = y\nor (left preconditioner)\n P_L^{-1} A x = P_L^{-1}f, or even both  P_L^{-1} A P_R^{-1} y = P_L^{-1}f \\quad \\Rightarrow \\quad P_R x = y.\nThe best choice is of course P = A, but this does not make life easier.\nOne of the ideas is to use other iterative methods (beside Krylov) as preconditioners."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#other-iterative-methods-as-preconditioners",
    "href": "lectures/lecture-14/lecture-14.html#other-iterative-methods-as-preconditioners",
    "title": "Questions?",
    "section": "Other iterative methods as preconditioners",
    "text": "Other iterative methods as preconditioners\nThere are other iterative methods that we have not mentioned.\n\nJacobi method\nGauss-Seidel\nSOR(\\omega) (Successive over-relaxation) and its symmetric modification SSOR(\\omega)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#jacobi-method-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#jacobi-method-as-preconditioner",
    "title": "Questions?",
    "section": "Jacobi method (as preconditioner)",
    "text": "Jacobi method (as preconditioner)\nConsider again the matrix with non-zero diagonal. To get the Jacobi method you express the diagonal element:\na_{ii} x_i = -\\sum_{i \\ne j} a_{ij} x_j + f_i\nand use this to iteratively update x_i:\n x_i^{(k+1)} = -\\frac{1}{a_{ii}}\\left( \\sum_{i \\ne j} a_{ij} x_j^{(k)} + f_i \\right),\nor in the matrix form\n\nx^{(k+1)} = D^{-1}\\left((D-A)x^{(k)} + f\\right)\n\nwhere D = \\mathrm{diag}(A) and finally\n\nx^{(k+1)} = x^{(k)} - D^{-1}(Ax^{(k)} - f).\n\nSo, Jacobi method is nothing, but simple Richardson iteration with \\tau=1 and left preconditioner P = D - diagonal of a matrix. Therefore we will refer to P = \\mathrm{diag}(A) as Jacobi preconditioner. Note that it can be used for any other method like Chebyshev or Krylov-type methods."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#properties-of-the-jacobi-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#properties-of-the-jacobi-preconditioner",
    "title": "Questions?",
    "section": "Properties of the Jacobi preconditioner",
    "text": "Properties of the Jacobi preconditioner\nJacobi preconditioner:\n\nVery easy to compute and apply\nWorks well for diagonally dominant matrices (remember the Gershgorin circle theorem!)\nUseless if all diagonal entries are the same (proportional to the identity matrix)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-as-preconditioner",
    "title": "Questions?",
    "section": "Gauss-Seidel (as preconditioner)",
    "text": "Gauss-Seidel (as preconditioner)\nAnother well-known method is Gauss-Seidel method.\nIts canonical form is very similar to the Jacobi method, with a small difference. When we update x_i as\nx_i^{(k+1)} := -\\frac{1}{a_{ii}}\\left( \\sum_{j =1}^{i-1} a_{ij} x_j^{(k+1)} +\\sum_{j = i+1}^n a_{ij} x_j^{(k)} - f_i \\right)\nwe use it in the later updates. In the Jacobi method we use the full vector from the previous iteration.\nIts matrix form is more complicated."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-matrix-version",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-matrix-version",
    "title": "Questions?",
    "section": "Gauss-Seidel: matrix version",
    "text": "Gauss-Seidel: matrix version\nGiven A = A^{*} &gt; 0 we have\nA = L + D + L^{*},\nwhere D is the diagonal of A, L is lower-triangular part with zero on the diagonal.\nOne iteration of the GS method reads\n\nx^{(k+1)} = x^{(k)} - (L + D)^{-1}(Ax^{(k)} - f).\n\nand we refer to the preconditioner P = L+D as Gauss-Seidel preconditioner.\nGood news: $(I - (L+D)^{-1} A) &lt; 1, $ where \\rho is the spectral radius,\ni.e. for a positive definite matrix GS-method always converges."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-and-coordinate-descent",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-and-coordinate-descent",
    "title": "Questions?",
    "section": "Gauss-Seidel and coordinate descent",
    "text": "Gauss-Seidel and coordinate descent\nGS-method can be viewed as a coordinate descent method, applied to the energy functional\nF(x) = (Ax, x) - 2(f, x)\nwith the iteration\nx_i := \\arg \\min_z F(x_1, \\ldots, x_{i-1}, z, x_{i+1}, \\ldots, x_d).\nMoreover, the order in which we eliminate variables, is really important!"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#side-note-nonlinear-gauss-seidel-a.k.a-coordinate-descent",
    "href": "lectures/lecture-14/lecture-14.html#side-note-nonlinear-gauss-seidel-a.k.a-coordinate-descent",
    "title": "Questions?",
    "section": "Side note: Nonlinear Gauss-Seidel (a.k.a coordinate descent)",
    "text": "Side note: Nonlinear Gauss-Seidel (a.k.a coordinate descent)\nIf F is given, and we optimize one coordinate at a time, we have\nx_i := \\arg \\min_z F(x_1, \\ldots, x_{i-1}, z, x_{i+1}, \\ldots, x_d).\nNote the convergence result for block coordinate descent for the case of a general functional F:\nit converges locally with the speed of the GS-method applied to the Hessian\nH = \\nabla^2 F\nof the functional.\nThus, if F is twice differentiable and x_* is the local minimum, then H &gt; 0 can Gauss-Seidel converges."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#successive-overrelaxation-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#successive-overrelaxation-as-preconditioner",
    "title": "Questions?",
    "section": "Successive overrelaxation (as preconditioner)",
    "text": "Successive overrelaxation (as preconditioner)\nWe can even introduce a parameter \\omega into the GS-method preconditioner, giving a successive over-relaxation (SOR(\\omega)) method:\n\nx^{(k+1)} = x^{(k)} - \\omega (D + \\omega L)^{-1}(Ax^{(k)} - f).\n\nP = \\frac{1}{\\omega}(D+\\omega L).\n\nConverges for 0&lt;\\omega &lt; 2.\nOptimal selection of \\omega is not trivial. If the Jacobi method converges, then \\omega^* = \\frac{2}{1 + \\sqrt{1 - \\rho_J^2}}, where \\rho_J is spectral radius of Jacobi iterations\nNote that \\omega = 1 gives us a Gauss-Seidel preconditioner."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#preconditioners-for-sparse-matrices",
    "href": "lectures/lecture-14/lecture-14.html#preconditioners-for-sparse-matrices",
    "title": "Questions?",
    "section": "Preconditioners for sparse matrices",
    "text": "Preconditioners for sparse matrices\n\nIf A is sparse, one iteration of Jacobi, GS and SOR method is cheap (what complexity?).\nFor GS, we need to solve linear system with a sparse triangular matrix L, which costs \\mathcal{O}(nnz).\nFor sparse matrices, however, there are more complicated algorithms, based on the idea of approximate LU-decomposition.\nRemember the motivation for CG: possibility of the early stopping, how to do approximate LU-decomposition for a sparse matrix?"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#remember-the-gaussian-elimination",
    "href": "lectures/lecture-14/lecture-14.html#remember-the-gaussian-elimination",
    "title": "Questions?",
    "section": "Remember the Gaussian elimination",
    "text": "Remember the Gaussian elimination\n\nDecompose the matrix A in the form\n\nA = P_1 L U P^{\\top}_2, \nwhere P_1 and P_2 are certain permutation matrices (which do the pivoting).\n\nThe most natural idea is to use sparse L and U.\nIt is not possible without fill-in growth for example for matrices, coming from 2D/3D Partial Differential equations (PDEs).\nWhat to do?"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#incomplete-lu",
    "href": "lectures/lecture-14/lecture-14.html#incomplete-lu",
    "title": "Questions?",
    "section": "Incomplete LU",
    "text": "Incomplete LU\n\nSuppose you want to eliminate a variable x_1, and the equations have the form\n\n5 x_1 + x_4 + x_{10} = 1, \\quad 3 x_1 + x_4 + x_8 = 0, \\ldots,\nand in all other equations x_1 are not present.\n\nAfter the elimination, only x_{10} will enter additionally to the second equation (new fill-in).\n\nx_4 + x_8 + 3(1 - x_4 - x_{10})/5 = 0\n\nIn the Incomplete LU case (actually, ILU(0)) we just throw away the new fill-in."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#incomplete-lu-formal-definition",
    "href": "lectures/lecture-14/lecture-14.html#incomplete-lu-formal-definition",
    "title": "Questions?",
    "section": "Incomplete-LU: formal definition",
    "text": "Incomplete-LU: formal definition\nWe run the usual LU-decomposition cycle, but avoid inserting non-zeros other than the initial non-zero pattern.\n    L = np.zeros((n, n))\n    U = np.zeros((n, n))\n    for k in range(n): #Eliminate one row   \n        L[k, k] = 1\n        for i in range(k+1, n):\n            L[i, k] = a[i, k] / a[k, k]\n            for j in range(k+1, n):\n                a[i, j] = a[i, j] - L[i, k] * a[k, j]  #New fill-ins appear here\n        for j in range(k, n):\n            U[k, j] = a[k, j]"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#iluk",
    "href": "lectures/lecture-14/lecture-14.html#iluk",
    "title": "Questions?",
    "section": "ILU(k)",
    "text": "ILU(k)\n\nYousef Saad (who is the author of GMRES) also had a seminal paper on the Incomplete LU decomposition\nA good book on the topic is Iterative methods for sparse linear systems by Y. Saad, 2003\nAnd he proposed ILU(k) method, which has a nice interpretation in terms of graphs."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#iluk-idea",
    "href": "lectures/lecture-14/lecture-14.html#iluk-idea",
    "title": "Questions?",
    "section": "ILU(k): idea",
    "text": "ILU(k): idea\n\nThe idea of ILU(k) is very instructive and is based on the connection between sparse matrices and graphs.\nSuppose you have an n \\times n matrix A and a corresponding adjacency graph.\nThen we eliminate one variable (vertex) and get a smaller system of size (n-1) \\times (n-1).\nNew edges (=fill-in) appears between high-order neighbors."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#lu-graphs",
    "href": "lectures/lecture-14/lecture-14.html#lu-graphs",
    "title": "Questions?",
    "section": "LU & graphs",
    "text": "LU & graphs\n\nThe new edge can appear only between vertices that had common neighbour: it means, that they are second-order neigbours.\n\nThis is also a sparsity pattern of the matrix A^2.\nThe ILU(k) idea is to leave only the elements in L and U that are k-order neighbours in the original graph.\nThe ILU(2) is very efficient, but for some reason completely abandoned (i.e. there is no implementation in MATLAB and SciPy).\nThere is an original Sparsekit software by Saad, which works quite well."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#ilu-thresholded-ilut",
    "href": "lectures/lecture-14/lecture-14.html#ilu-thresholded-ilut",
    "title": "Questions?",
    "section": "ILU Thresholded (ILUT)",
    "text": "ILU Thresholded (ILUT)\nA much more popular approach is based on the so-called thresholded LU.\nYou do the standard Gaussian elimination with fill-ins, but either:\n\nThrow away elements that are smaller than threshold, and/or control the amount of non-zeros you are allowed to store.\nThe smaller is the threshold, the better is the preconditioner, but more memory it takes.\n\nIt is denoted ILUT(\\tau)."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#symmetric-positive-definite-case",
    "href": "lectures/lecture-14/lecture-14.html#symmetric-positive-definite-case",
    "title": "Questions?",
    "section": "Symmetric positive definite case",
    "text": "Symmetric positive definite case\n\nIn the SPD case, instead of incomplete LU you can use Incomplete Cholesky, which is twice faster and consumes twice less memory.\nHowever, Incomplete Cholesky may not exist.\nBoth ILUT and Ichol are implemented in SciPy and you can try."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#second-order-lu-preconditioners",
    "href": "lectures/lecture-14/lecture-14.html#second-order-lu-preconditioners",
    "title": "Questions?",
    "section": "Second-order LU preconditioners",
    "text": "Second-order LU preconditioners\n\nThere is a more efficient (but much less popular due to the limit of open-source implementations) second-order LU factorization proposed by I. Kaporin\nThe idea (for symmetric matrices) is to approximate the matrix in the form\n\nA \\approx U_2 U^{\\top}_2 + U^{\\top}_2 R_2 + R^{\\top}_2 U_2,\nwhich is just the expansion of the UU^{\\top} with respect to the perturbation of U.\n\nU_1 and U_2 are upper-triangular and sparse, whereare R_2 is small with respect to the drop tolerance parameter."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#summary-of-this-part",
    "href": "lectures/lecture-14/lecture-14.html#summary-of-this-part",
    "title": "Questions?",
    "section": "Summary of this part",
    "text": "Summary of this part\n\nJacobi, Gauss-Seidel, SSOR (as preconditioners)\nIncomplete LU, three flavours: ILU(k), ILUT, ILU2"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#previous-lecture",
    "href": "lectures/lecture-12/lecture-12.html#previous-lecture",
    "title": "The main topics for today",
    "section": "Previous lecture",
    "text": "Previous lecture\n\nDirect methods for LU\nGraph separators"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#matrix-as-a-black-box",
    "href": "lectures/lecture-12/lecture-12.html#matrix-as-a-black-box",
    "title": "The main topics for today",
    "section": "Matrix as a black box",
    "text": "Matrix as a black box\n\nWe have now an absolutely different view on a matrix: matrix is now a linear operator, that acts on a vector,\nand this action can be computed in \\mathcal{O}(N) operations.\nThis is the only information we know about the matrix: the  matrix-by-vector product (matvec) \nCan we solve linear systems using only matvecs?\nOf course, we can multiply by the colums of the identity matrix, and recover the full matrix, but it is not what we need."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#richardson-iteration",
    "href": "lectures/lecture-12/lecture-12.html#richardson-iteration",
    "title": "The main topics for today",
    "section": "Richardson iteration",
    "text": "Richardson iteration\nThe simplest idea is the “simple iteration method” or Richardson iteration.\nAx = f, \\tau  (Ax - f) = 0, x - \\tau (Ax - f) = x, x_{k+1} = x_k - \\tau (Ax_k - f),\nwhere \\tau is the iteration parameter, which can be always chosen such that the method converges."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#connection-to-odes",
    "href": "lectures/lecture-12/lecture-12.html#connection-to-odes",
    "title": "The main topics for today",
    "section": "Connection to ODEs",
    "text": "Connection to ODEs\n\nThe Richardson iteration has a deep connection to the Ordinary Differential Equations (ODE).\nConsider a time-dependent problem\n\n\\frac{dy}{dt} + A y = f, \\quad y(0) = y_0.\n\nThen y(t) \\rightarrow A^{-1} f as t \\rightarrow \\infty, and the Euler scheme reads\n\n\\frac{y_{k+1} - y_k}{\\tau} = -A y_k + f.\nwhich leads to the Richardson iteration\n\n    y_{k+1} = y_k - \\tau(Ay_k -f)"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#convergence-of-the-richardson-method",
    "href": "lectures/lecture-12/lecture-12.html#convergence-of-the-richardson-method",
    "title": "The main topics for today",
    "section": "Convergence of the Richardson method",
    "text": "Convergence of the Richardson method\n\nLet x_* be the solution; introduce an error e_k = x_{k} - x_*, then\n\n\n     e_{k+1} = (I - \\tau A) e_k,\n\ntherefore if \\Vert I - \\tau A \\Vert &lt; 1 in any norm, the iteration converges.\n\nFor symmetric positive definite case it is always possible to select \\tau such that the method converges.\nWhat about the non-symmetric case? Below demo will be presented…"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#optimal-parameter-choice",
    "href": "lectures/lecture-12/lecture-12.html#optimal-parameter-choice",
    "title": "The main topics for today",
    "section": "Optimal parameter choice",
    "text": "Optimal parameter choice\n\nThe choise of \\tau that minimizes \\|I - \\tau A\\|_2 for A = A^* &gt; 0 is (prove it!)\n\n\n  \\tau_\\mathrm{opt} = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}.\n\nwhere \\lambda_{\\min} is the minimal eigenvalue, and \\lambda_{\\max} is the maximal eigenvalue of the matrix A.\n\nSo, to find optimal parameter, we need to know the bounds of the spectrum of the matrix A, and we can compute it by using power method."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#condition-number-and-convergence-speed",
    "href": "lectures/lecture-12/lecture-12.html#condition-number-and-convergence-speed",
    "title": "The main topics for today",
    "section": "Condition number and convergence speed",
    "text": "Condition number and convergence speed\nEven with the optimal parameter choice, the error at the next step satisfies\n\\|e_{k+1}\\|_2 \\leq q \\|e_k\\|_2 , \\quad\\rightarrow \\quad \\|e_k\\|_2 \\leq q^{k} \\|e_0\\|_2,\nwhere\n\n   q = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{\\mathrm{cond}(A) - 1}{\\mathrm{cond}(A)+1},\n\n\\mathrm{cond}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \\quad \\text{for} \\quad A=A^*&gt;0\nis the condition number of A.\nLet us do some demo…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rc(\"text\", usetex=True)\nimport scipy as sp\nimport scipy.sparse\nimport scipy.sparse.linalg as spla\nimport scipy\nfrom scipy.sparse import csc_matrix\nn = 500\nex = np.ones(n);\nA = sp.sparse.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\nev1, vec = spla.eigsh(A, k=2, which='LA')\nev2, vec = spla.eigsh(A, k=2, which='SA')\nlam_max = ev1[0]\nlam_min = ev2[0]\n\ntau_opt = 2.0/(lam_max + lam_min)\n\nfig, ax = plt.subplots()\nplt.close(fig)\n\nniters = 1000\nx = np.zeros(n)\nres_richardson = []\nfor i in range(niters):\n    rr = A.dot(x) - rhs\n    x = x - tau_opt * rr\n    res_richardson.append(np.linalg.norm(rr))\n#Convergence of an ordinary Richardson (with optimal parameter)\nplt.semilogy(res_richardson)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nprint(\"Maximum eigenvalue = {}, minimum eigenvalue = {}\".format(lam_max, lam_min))\ncond_number = lam_max.real / lam_min.real\nprint(\"Condition number = {}\".format(cond_number))\n#print(np.array(res_richardson)[1:] / np.array(res_richardson)[:-1])\nprint(\"Theoretical factor: {}\".format((cond_number - 1) / (cond_number + 1)))\n\nMaximum eigenvalue = 3.99984271815512, minimum eigenvalue = 3.932084756989659e-05\nCondition number = 101723.207035276\nTheoretical factor: 0.9999803389964071\n\n\n\n\n\n\n\n\n\n\nThus, for ill-conditioned matrices the error of the simple iteration method decays very slowly.\nThis is another reason why condition number is so important:\n\nBesides the bound on the error in the solution, it also gives an estimate of the number of iterations for the iterative methods.\n\nMain questions for the iterative method is how to make the matrix better conditioned.\nThe answer is  use preconditioners. Preconditioners will be discussed in further lectures.\n\n\nConsider non-hermitian matrix A\nPossible cases of Richardson iteration behaviour: - convergence - divergence - almost stable trajectory\nQ: how can we identify our case before running iterative method?\n\n# B = np.random.randn(2, 2)\nB = np.array([[1, 2], [-1, 0]])\n# B = np.array([[0, 1], [-1, 0]])\nx_true = np.zeros(2)\nf = B.dot(x_true)\neigvals = np.linalg.eigvals(B)\nprint(\"Spectrum of the matrix = {}\".format(eigvals))\n\n# Run Richardson iteration\nx = np.array([0, -1])\ntau = 1e-2\nconv_x = [x]\nr = B.dot(x) - f\nconv_r = [np.linalg.norm(r)]\nnum_iter = 1000\nfor i in range(num_iter):\n    x = x - tau * r\n    conv_x.append(x)\n    r = B.dot(x) - f\n    conv_r.append(np.linalg.norm(r))\n\nSpectrum of the matrix = [0.5+1.32287566j 0.5-1.32287566j]\n\n\n\nplt.semilogy(conv_r)\nplt.xlabel(\"Number of iteration, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm\", fontsize=20)\n\nText(0, 0.5, 'Residual norm')\n\n\n\n\n\n\n\n\n\n\nplt.scatter([x[0] for x in conv_x], [x[1] for x in conv_x])\nplt.xlabel(\"$x$\", fontsize=20)\nplt.ylabel(\"$y$\", fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.title(\"$x_0 = (0, -1)$\", fontsize=20)\n\nText(0.5, 1.0, '$x_0 = (0, -1)$')"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#better-iterative-methods",
    "href": "lectures/lecture-12/lecture-12.html#better-iterative-methods",
    "title": "The main topics for today",
    "section": "Better iterative methods",
    "text": "Better iterative methods\nBut before preconditioners, we can use better iterative methods.\nThere is a whole zoo of iterative methods, but we need to know just few of them."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#attempt-1-the-steepest-descent-method",
    "href": "lectures/lecture-12/lecture-12.html#attempt-1-the-steepest-descent-method",
    "title": "The main topics for today",
    "section": "Attempt 1: The steepest descent method",
    "text": "Attempt 1: The steepest descent method\n\nSuppose we change \\tau every step, i.e. \n\n\n   x_{k+1} = x_k - \\tau_k (A x_k - f).\n\n\nA possible choice of \\tau_k is such that it minimizes norm of the current residual\n\n \\tau_k = \\arg\\min_{\\tau} \\|A(x_k - \\tau_k (A x_k - f)) - f\\|_2^2.\n\nThis problem can be solved analytically (derive this solution!)\n\n \\tau_k = \\frac{r_k^{\\top}r_k}{r_k^{\\top}Ar_k}, \\quad r_k = Ax_k - f \n\nThis method is called the steepest descent.\nHowever, it still converges similarly to the Richardson iteration."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#attempt-2-chebyshev-iteration",
    "href": "lectures/lecture-12/lecture-12.html#attempt-2-chebyshev-iteration",
    "title": "The main topics for today",
    "section": "Attempt 2: Chebyshev iteration",
    "text": "Attempt 2: Chebyshev iteration\nAnother way to find \\tau_k is to consider\ne_{k+1} = (I - \\tau_k A) e_k = (I - \\tau_k A) (I - \\tau_{k-1} A)  e_{k-1} = \\ldots = p(A) e_0, \nwhere p(A) is a matrix polynomial (simplest matrix function)\n\n   p(A) = (I - \\tau_k A) \\ldots (I - \\tau_0 A),\n\nand p(0) = 1."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#optimal-choice-of-time-steps",
    "href": "lectures/lecture-12/lecture-12.html#optimal-choice-of-time-steps",
    "title": "The main topics for today",
    "section": "Optimal choice of time steps",
    "text": "Optimal choice of time steps\nThe error is written as\ne_{k+1} = p(A) e_0, \nand hence\n\\|e_{k+1}\\| \\leq \\|p(A)\\| \\|e_0\\|, \nwhere p(0) = 1 and p(A) is a matrix polynomial.\nTo get better error reduction, we need to minimize\n\\Vert p(A) \\Vert\nover all possible polynomials p(x) of degree k+1 such that p(0)=1. We will use \\|\\cdot\\|_2."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros",
    "href": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros",
    "title": "The main topics for today",
    "section": "Polynomials least deviating from zeros",
    "text": "Polynomials least deviating from zeros\nImportant special case: A = A^* &gt; 0.\nThen A = U \\Lambda U^*,\nand\n\\Vert p(A) \\Vert_2 = \\Vert U p(\\Lambda) U^* \\Vert_2 = \\Vert p(\\Lambda) \\Vert_2 = \\max_i |p(\\lambda_i)| \\overset{!}{\\leq}\n\\max_{\\lambda_\\min \\leq \\lambda {\\leq} \\lambda_\\max} |p(\\lambda)|.\nThe latter inequality is the only approximation. Here we make a  crucial assumption  that we do not want to benefit from distribution of spectra between \\lambda_\\min and \\lambda_\\max.\nThus, we need to find a polynomial such that p(0) = 1, that has the least possible deviation from 0 on [\\lambda_\\min, \\lambda_\\max]."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros-2",
    "href": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros-2",
    "title": "The main topics for today",
    "section": "Polynomials least deviating from zeros (2)",
    "text": "Polynomials least deviating from zeros (2)\nWe can do the affine transformation of the interval [\\lambda_\\min, \\lambda_\\max] to the interval [-1, 1]:\n\n\\xi = \\frac{{\\lambda_\\max + \\lambda_\\min - (\\lambda_\\min-\\lambda_\\max)x}}{2}, \\quad x\\in [-1, 1].\n\nThe problem is then reduced to the problem of finding the polynomial least deviating from zero on an interval [-1, 1]."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#exact-solution-chebyshev-polynomials",
    "href": "lectures/lecture-12/lecture-12.html#exact-solution-chebyshev-polynomials",
    "title": "The main topics for today",
    "section": "Exact solution: Chebyshev polynomials",
    "text": "Exact solution: Chebyshev polynomials\nThe exact solution to this problem is given by the famous Chebyshev polynomials of the form\nT_n(x) =  \\cos (n \\arccos x)"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#what-do-you-need-to-know-about-chebyshev-polynomials",
    "href": "lectures/lecture-12/lecture-12.html#what-do-you-need-to-know-about-chebyshev-polynomials",
    "title": "The main topics for today",
    "section": "What do you need to know about Chebyshev polynomials",
    "text": "What do you need to know about Chebyshev polynomials\n\nThis is a polynomial!\nWe can express T_n from T_{n-1} and T_{n-2}:\n\nT_n(x) = 2x T_{n-1}(x) - T_{n-2}(x), \\quad T_0(x)=1, \\quad T_1(x)=x\n\n|T_n(x)| \\leq 1 on x \\in [-1, 1].\nIt has (n+1) alternation points, where the maximal absolute value is achieved (this is the sufficient and necessary condition for the optimality) (Chebyshev alternance theorem, no proof here).\nThe roots are just\n\nn \\arccos x_k = \\frac{\\pi}{2} + \\pi k, \\quad \\rightarrow\\quad x_k = \\cos \\frac{\\pi(2k + 1)}{2n}, \\; k = 0, \\ldots,n-1\nWe can plot them…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nx1 = np.linspace(-1, 1, 128)\nx2 = np.linspace(-1.1, 1.1, 128)\np = np.polynomial.Chebyshev((0, 0, 0, 0, 0, 0, 0, 0, 0, 1), (-1, 1)) #These are Chebyshev series, a proto of \"chebfun system\" in MATLAB\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(x1, p(x1))\nax1.set_title('Interval $x\\in[-1, 1]$')\nax2.plot(x2, p(x2))\nax2.set_title('Interval $x\\in[-1.1, 1.1]$')\n\nText(0.5, 1.0, 'Interval $x\\\\in[-1.1, 1.1]$')"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#convergence-of-the-chebyshev-accelerated-richardson-iteration",
    "href": "lectures/lecture-12/lecture-12.html#convergence-of-the-chebyshev-accelerated-richardson-iteration",
    "title": "The main topics for today",
    "section": "Convergence of the Chebyshev-accelerated Richardson iteration",
    "text": "Convergence of the Chebyshev-accelerated Richardson iteration\nNote that p(x) = (1-\\tau_n x)\\dots (1-\\tau_0 x), hence roots of p(x) are 1/\\tau_i and that we additionally need to map back from [-1,1] to [\\lambda_\\min, \\lambda_\\max]. This results into\n\\tau_i = \\frac{2}{\\lambda_\\max + \\lambda_\\min - (\\lambda_\\max - \\lambda_\\min)x_i}, \\quad x_i = \\cos \\frac{\\pi(2i + 1)}{2n}\\quad i=0,\\dots,n-1\nThe convergence (we only give the result without the proof) is now given by\n\n   e_{k+1} \\leq C q^k e_0, \\quad q = \\frac{\\sqrt{\\mathrm{cond}(A)}-1}{\\sqrt{\\mathrm{cond}(A)}+1},\n\nwhich is better than in the Richardson iteration.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 64\nex = np.ones(n);\n\nA = sp.sparse.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\nev1, vec = spla.eigsh(A, k=2, which='LA')\nev2, vec = spla.eigsh(A, k=2, which='SA')\nlam_max = ev1[0]\nlam_min = ev2[0]\n\nniters = 64\nroots = [np.cos((np.pi * (2 * i + 1)) / (2 * niters)) for i in range(niters)]\ntaus = [(lam_max + lam_min - (lam_min - lam_max) * r) / 2 for r in roots]\nx = np.zeros(n)\nr = A.dot(x) - rhs\nres_cheb = [np.linalg.norm(r)]\n\nprint(1/np.array(taus))\n\nfor i in range(niters):\n    x = x - 1.0 / taus[i] * r\n    r = A.dot(x) - rhs\n    res_cheb.append(np.linalg.norm(r))\n    \nplt.semilogy(res_richardson, label=\"Richardson\")\nplt.semilogy(res_cheb, label=\"Chebyshev\")\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n[2.50622630e-01 2.50924658e-01 2.51530169e-01 2.52442095e-01\n 2.53664865e-01 2.55204461e-01 2.57068471e-01 2.59266178e-01\n 2.61808653e-01 2.64708878e-01 2.67981890e-01 2.71644951e-01\n 2.75717745e-01 2.80222614e-01 2.85184826e-01 2.90632893e-01\n 2.96598938e-01 3.03119118e-01 3.10234126e-01 3.17989766e-01\n 3.26437629e-01 3.35635888e-01 3.45650225e-01 3.56554925e-01\n 3.68434169e-01 3.81383570e-01 3.95511991e-01 4.10943730e-01\n 4.27821135e-01 4.46307759e-01 4.66592186e-01 4.88892691e-01\n 5.13462953e-01 5.40599097e-01 5.70648426e-01 6.04020340e-01\n 6.41200067e-01 6.82766079e-01 7.29412363e-01 7.81977158e-01\n 8.41480389e-01 9.09172947e-01 9.86602291e-01 1.07570085e+00\n 1.17890673e+00 1.29933105e+00 1.44099335e+00 1.60915915e+00\n 1.81083297e+00 2.05549470e+00 2.35622630e+00 2.73148428e+00\n 3.20797673e+00 3.82550533e+00 4.64546321e+00 5.76650218e+00\n 7.35516474e+00 9.71018736e+00 1.34098577e+01 1.96891165e+01\n 3.15513749e+01 5.76947930e+01 1.29218671e+02 3.40581915e+02]\n\n\n\n\n\n\n\n\n\n\nWhat happened with great Chebyshev iterations?\n\nniters = 64\nroots = [np.cos((np.pi * (2 * i + 1)) / (2 * niters)) for i in range(niters)]\ntaus = [(lam_max + lam_min - (lam_min - lam_max) * r) / 2 for r in roots]\nx = np.zeros(n)\nr = A.dot(x) - rhs\nres_cheb_even = [np.linalg.norm(r)]\n#print(taus)\n\n# Implementation may be non-optimal if number of iterations is not power of two\ndef leb_shuffle_2n(n):\n    if n == 1:\n        return np.array([0,], dtype=int)\n    else:\n        prev = leb_shuffle_2n(n // 2)\n        ans = np.zeros(n, dtype=int)\n        ans[::2] = prev\n        ans[1::2] = n - 1 - prev\n        return ans\n\ngood_perm_even = leb_shuffle_2n(niters)\nprint(good_perm_even, len(good_perm_even))\n# good_perm_even = np.random.permutation([i for i in range(niters)])\nts = np.array(taus)[good_perm_even]\nplt.figure()\nplt.plot(1/ts)\nfor i in range(niters):\n    x = x - 1.0/taus[good_perm_even[i]] * r\n    r = A.dot(x) - rhs\n    res_cheb_even.append(np.linalg.norm(r))\nplt.figure()\n    \nplt.semilogy(res_richardson, label=\"Richardson\")\nplt.semilogy(res_cheb_even, label=\"Chebyshev\")\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n[ 0 63 31 32 15 48 16 47  7 56 24 39  8 55 23 40  3 60 28 35 12 51 19 44\n  4 59 27 36 11 52 20 43  1 62 30 33 14 49 17 46  6 57 25 38  9 54 22 41\n  2 61 29 34 13 50 18 45  5 58 26 37 10 53 21 42] 64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPermutation of roots of Chebyshev polynomial has crucial effect on convergence\nOn the optimal permutation you can read in paper (V. Lebedev, S. Finogenov 1971) (ru, en)\n\n\n\nChebfun project\n\nOpensource project for numerical computing (Python and Matlab interfaces)\nIt is based on numerical algorithms working with piecewise polynomial interpolants and Chebyshev polynomials\nThis project was initiated by Nick Trefethen and his student Zachary Battles in 2002, see paper on chebfun project in SISC\nChebfun toolbox focuses mostly on the following problems\n\nApproximation\nQuadrature\nODE\nPDE\nRootfinding\n1D global optimization"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#beyond-chebyshev",
    "href": "lectures/lecture-12/lecture-12.html#beyond-chebyshev",
    "title": "The main topics for today",
    "section": "Beyond Chebyshev",
    "text": "Beyond Chebyshev\n\nWe have made an important assumption about the spectrum: it is contained within an interval over the real line (and we need to know the bounds)\nIf the spectrum is contained within two intervals, and we know the bounds, we can also put the optimization problem for the optimal polynomial."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#spectrum-of-the-matrix-contained-in-multiple-segments",
    "href": "lectures/lecture-12/lecture-12.html#spectrum-of-the-matrix-contained-in-multiple-segments",
    "title": "The main topics for today",
    "section": "Spectrum of the matrix contained in multiple segments",
    "text": "Spectrum of the matrix contained in multiple segments\n\nFor the case of two segments the best polynomial is given by Zolotarev polynomials (expressed in terms of elliptic functions). Original paper was published in 1877, see details here\nFor the case of more than two segments the best polynomial can be expressed in terms of hyperelliptic functions"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#how-can-we-make-it-better",
    "href": "lectures/lecture-12/lecture-12.html#how-can-we-make-it-better",
    "title": "The main topics for today",
    "section": "How can we make it better",
    "text": "How can we make it better\n\nThe implementation of the Chebyshev acceleration requires the knowledge of the spectrum.\nIt only stores the previous vector x_k and computes the new correction vector\n\nr_k = A x_k - f.\n\nIt belongs to the class of two-term iterative methods, i.e. it approximates x_{k+1} using 2 vectors: x_k and r_k.\nIt appears that if we store more vectors, then we can go without the spectrum estimation (and better convergence in practice)!"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#crucial-point-krylov-subspace",
    "href": "lectures/lecture-12/lecture-12.html#crucial-point-krylov-subspace",
    "title": "The main topics for today",
    "section": "Crucial point: Krylov subspace",
    "text": "Crucial point: Krylov subspace\nThe Chebyshev method produces the approximation of the form\nx_{k+1} = x_0 + p(A) r_0,\ni.e. it lies in the Krylov subspace of the matrix which is defined as\n\n   \\mathcal{K}_k(A, r_0) = \\mathrm{Span}(r_0, Ar_0, A^2 r_0, \\ldots, A^{k-1}r_0 )\n\nThe most natural approach then is to find the vector in this linear subspace that minimizes certain norm of the error"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#idea-of-krylov-methods",
    "href": "lectures/lecture-12/lecture-12.html#idea-of-krylov-methods",
    "title": "The main topics for today",
    "section": "Idea of Krylov methods",
    "text": "Idea of Krylov methods\nThe idea is to minimize given functional: - Energy norm of error for systems with hermitian positive-definite matrices (CG method). - Residual norm for systems with general matrices (MINRES and GMRES methods). - Rayleigh quotient for eigenvalue problems (Lanczos method).\nTo make methods practical one has to 1. Orthogonalize vectors A^i r_0 of the Krylov subspace for stability (Lanczos process). 2. Derive recurrent formulas to decrease complexity.\nWe will consider these methods in details on the next lecture."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#take-home-message",
    "href": "lectures/lecture-12/lecture-12.html#take-home-message",
    "title": "The main topics for today",
    "section": "Take home message",
    "text": "Take home message\n\nMain idea of iterative methods\nRichardson iteration: hermitian and non-hermitian case\nChebyshev acceleration\nDefinition of Krylov subspace\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/general_info.html#about-the-course",
    "href": "lectures/general_info.html#about-the-course",
    "title": "",
    "section": "About the course",
    "text": "About the course\n\nThe course is about faster and more accurate computations (in scientific computing, in ML…)\nThree main problems:\n\nsolving linear systems\nfinding eigenvalues and eigenvectors\ncomputing matrix functions (matrix exponential especially)\n\nThe methods are different for small-scale and large-scale problems:\n\nusing matrix decompositions\niterative methods"
  },
  {
    "objectID": "lectures/general_info.html#learning-outcomes",
    "href": "lectures/general_info.html#learning-outcomes",
    "title": "",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nSolve medium-scale numerical linear algebra problems (solve linear systems, compute eigenvalues and eigenvectors, solve linear least squares) using matrix factorizations\nIterative methods for sparse/structured systems\nFind which methods are the most appropriate for the particular problem\nFind appropriate software"
  },
  {
    "objectID": "lectures/general_info.html#approximate-syllabus",
    "href": "lectures/general_info.html#approximate-syllabus",
    "title": "",
    "section": "(Approximate) Syllabus",
    "text": "(Approximate) Syllabus\n\nWeek 1: Intro and floating points arithmetics, matrices, vectors, norms, ranks.\nWeek 2: Matrix decompositions 1: SVD and its applications\nWeek 3: Matrix decompositions 2: Linear systems and LU, eigendecomposition\nWeek 4: Matrix decompositions 3: QR and Schur + project proposal deadline\nWeek 5: Sparse and structured matrices, iterative methods (part 1)\nWeek 6: Iterative methods (part 2), matrix functions and advanced topics\nWeek 7: Oral exam (two days)\nWeek 8: Application period and project presentations"
  },
  {
    "objectID": "lectures/general_info.html#lecture-access",
    "href": "lectures/general_info.html#lecture-access",
    "title": "",
    "section": "Lecture access",
    "text": "Lecture access\n\nLectures can be downloaded and viewed on GitHub: (link distributed soon)\nTelegram chat for the course (link distributed soon)"
  },
  {
    "objectID": "lectures/general_info.html#team",
    "href": "lectures/general_info.html#team",
    "title": "",
    "section": "Team",
    "text": "Team\nCourse instructor: Ivan Oseledets\nTAs: Will be distributed soon"
  },
  {
    "objectID": "lectures/general_info.html#how-do-we-grade",
    "href": "lectures/general_info.html#how-do-we-grade",
    "title": "",
    "section": "How do we grade",
    "text": "How do we grade\n\n50% homework. Includes 3 problem sets with coding in Python and theoretical problems.\n20% final exam - the format and rules will be announced later\n15% Midterm test – written test with simple problems.\n15% project. More details and policies about this activity will be presented later.\n10% Bonus. Solve bonus tasks from HW.\n\nTotal maximum is 110%."
  },
  {
    "objectID": "lectures/general_info.html#problem-sets",
    "href": "lectures/general_info.html#problem-sets",
    "title": "",
    "section": "Problem sets",
    "text": "Problem sets\n\nHomework is distributed in Jupyter notebooks\nProblem sets contain both theoretical and programming tasks\nNo hand-written solutions are accepted, only Markdown and \\LaTeX text in the single Jupyter Notebook file are Ok."
  },
  {
    "objectID": "lectures/general_info.html#problem-set-rules",
    "href": "lectures/general_info.html#problem-set-rules",
    "title": "",
    "section": "Problem set rules",
    "text": "Problem set rules\n\nSolutions must be submitted on Canvas before the deadline\nDeadlines are strict. After the deadline Canvas submission is closed. Only the last submission will be graded.\nDeadline for every problem set will be announced at the moment of publishing\nProblem sets will be checked for plagiarism. If noticed, the score will be divided by a number of similar works. You can use ChatGPT on them, but you are responsible for the quality and if we have suspicions, we will arrange inperson meetings about ‘understanding’ of them."
  },
  {
    "objectID": "lectures/general_info.html#attendance",
    "href": "lectures/general_info.html#attendance",
    "title": "",
    "section": "Attendance",
    "text": "Attendance\nAttendance is not strict, but do not disappoint us."
  },
  {
    "objectID": "lectures/general_info.html#exam",
    "href": "lectures/general_info.html#exam",
    "title": "",
    "section": "Exam",
    "text": "Exam\n\nClassical oral exam with a list of questions (questions from 2020)\nIf you fail exam, you get zero for the course\nThe list of questions such that if you can not answer any question from this list during the final exam, you will automatically get F for this course. It will be probably updated by the end of November."
  },
  {
    "objectID": "lectures/general_info.html#projects",
    "href": "lectures/general_info.html#projects",
    "title": "",
    "section": "Projects",
    "text": "Projects\n\nWe give you time (\\approx one week) and points (30% of your final grade) to test studied methods in real-world problems\nWe will provide some examples in the second part of this class\nFor inspiration you can checkout recent conference papers (e.g. here)\n2–5 students per team\nStart thinking about your project topics as early as possible!"
  },
  {
    "objectID": "lectures/general_info.html#grades",
    "href": "lectures/general_info.html#grades",
    "title": "",
    "section": "Grades",
    "text": "Grades\n\nA: 86 - 100 %\nB: 70 - 85 %\nC: 50 - 70 %\nD: 30 - 50 %\nE: 15 - 30 %\nF: 0 - 15 %\n\nBut they can be slightly adjusted."
  },
  {
    "objectID": "lectures/general_info.html#python",
    "href": "lectures/general_info.html#python",
    "title": "",
    "section": "Python",
    "text": "Python\n\nWe will use Python ecosystem for programming.\nPlease, follow the rules that will be given in the problem sets"
  },
  {
    "objectID": "lectures/general_info.html#materials",
    "href": "lectures/general_info.html#materials",
    "title": "",
    "section": "Materials",
    "text": "Materials\nOur materials: - Lecture notes are avaliable online - Matrix decomposition cheat sheet\nIf you have difficulties with basic linear algebra: - Cheat sheet with basics - Gilbert Strang book “Introduction to Linear Algebra” - Gilbert Strang has recorded lectures on YouTube\nComprehensive NLA books: - Gene H. Golub, Charles. F. Van Loan, “Matrix computations” (4th edition) - Lloyd N. Trefethen and David Bau III, “Numerical Linear Algebra” - Eugene. E. Tyrtyshnikov, “Brief introduction to numerical analysis” - James W. Demmel, “Numerical Linear Algebra”\n\nMany applications of linear algebra you can find in Introduction to Applied Linear Algebra by S. Boyd and L. Vandenberghe"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#предыдущая-лекция",
    "href": "lectures/lecture-3/lecture-3.html#предыдущая-лекция",
    "title": "",
    "section": "Предыдущая лекция",
    "text": "Предыдущая лекция\n\nПиковая производительность алгоритма\nСложность алгоритмов умножения матриц\nИдея блочного разбиения (почему это хорошо?)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сегодняшняя-лекция",
    "href": "lectures/lecture-3/lecture-3.html#сегодняшняя-лекция",
    "title": "",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\n\nРанг матрицы\nСкелетное разложение\nАппроксимация низкого ранга\nСингулярное разложение (SVD)\nПрименения SVD"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#матрицы-и-линейные-пространства",
    "href": "lectures/lecture-3/lecture-3.html#матрицы-и-линейные-пространства",
    "title": "",
    "section": "Матрицы и линейные пространства",
    "text": "Матрицы и линейные пространства\n\nМатрицу можно рассматривать как последовательность векторов, которые являются столбцами матрицы:\n\n A = [a_1, \\ldots, a_m], \nгде a_m \\in \\mathbb{C}^{n\\times 1}.\n\nПроизведение матрицы на вектор эквивалентно взятию линейной комбинации этих столбцов\n\n y =  Ax \\quad \\Longleftrightarrow \\quad y = a_1 x_1 + a_2 x_2 + \\ldots +a_m x_m. \n\nЭто частный случай блочной матричной нотации (столбцы также являются блоками), которую мы уже видели (разбиение на блоки для соответствия кэш-памяти, алгоритм Штрассена)."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#линейная-зависимость",
    "href": "lectures/lecture-3/lecture-3.html#линейная-зависимость",
    "title": "",
    "section": "Линейная зависимость",
    "text": "Линейная зависимость\nОпределение. Векторы a_i называются линейно зависимыми, если существуют одновременно ненулевые коэффициенты x_i такие, что\n\\sum_i a_i x_i = 0,\nили в матричной форме\n Ax = 0, \\quad \\Vert x \\Vert \\ne 0. \nВ этом случае мы говорим, что матрица A имеет нетривиальное нуль-пространство (или ядро), обозначаемое N(A) (или \\text{ker}(A)).\nВекторы, которые не являются линейно зависимыми, называются линейно независимыми."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#линейное-векторное-пространство",
    "href": "lectures/lecture-3/lecture-3.html#линейное-векторное-пространство",
    "title": "",
    "section": "Линейное (векторное) пространство",
    "text": "Линейное (векторное) пространство\nЛинейное пространство, натянутое на векторы \\{a_1, \\ldots, a_m\\}, определяется как множество всех возможных векторов вида\n \\mathcal{L}(a_1, \\ldots, a_m) = \\left\\{y: y = \\sum_{i=1}^m a_i x_i, \\, \\forall x_i, \\, i=1,\\dots, n \\right\\}, \nВ матричной форме линейное пространство - это множество всех y таких, что\ny = A x.\nЭто множество также называется областью значений (или образом) матрицы, обозначаемой \\text{range}(A) (или \\text{im}(A)) соответственно."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#размерность-линейного-пространства",
    "href": "lectures/lecture-3/lecture-3.html#размерность-линейного-пространства",
    "title": "",
    "section": "Размерность линейного пространства",
    "text": "Размерность линейного пространства\n\nРазмерность линейного пространства \\text{im}(A), обозначаемая как \\text{dim}\\, \\text{im} (A), это минимальное количество векторов, необходимых для представления каждого вектора из \\text{im} (A).\nРазмерность \\text{im}(A) имеет прямую связь с рангом матрицы."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#ранг-матрицы",
    "href": "lectures/lecture-3/lecture-3.html#ранг-матрицы",
    "title": "",
    "section": "Ранг матрицы",
    "text": "Ранг матрицы\n\nРанг матрицы A - это максимальное число линейно независимых столбцов в матрице A, или размерность пространства столбцов = \\text{dim} \\, \\text{im}(A).\nВы также можете использовать линейные комбинации строк для определения ранга, т.е. формально существуют два ранга: столбцовый ранг и строчный ранг матрицы.\n\nТеорема\nРазмерность пространства столбцов матрицы равна размерности пространства её строк.\nДоказательство\n\nВ матричной форме этот факт можно записать как \\mathrm{dim}\\ \\mathrm{im} (A) = \\mathrm{dim}\\ \\mathrm{im} (A^\\top).\nТаким образом, существует единственный ранг!"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#матрица-полного-ранга",
    "href": "lectures/lecture-3/lecture-3.html#матрица-полного-ранга",
    "title": "",
    "section": "Матрица полного ранга",
    "text": "Матрица полного ранга\n\nМатрица A \\in \\mathbb{R}^{m \\times n} называется матрицей полного ранга, если \\mathrm{rank}(A) = \\min(m, n).\n\nПредположим, у нас есть линейное пространство, порожденное n векторами. Пусть эти векторы случайны, с элементами из стандартного нормального распределения \\mathcal{N}(0, 1).\nВ: Какова вероятность того, что это подпространство имеет размерность m &lt; n?\nО: Случайная матрица имеет полный ранг с вероятностью 1."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#скелетное-разложение",
    "href": "lectures/lecture-3/lecture-3.html#скелетное-разложение",
    "title": "",
    "section": "Скелетное разложение",
    "text": "Скелетное разложение\nОчень полезным представлением для вычисления ранга матрицы является скелетное разложение, которое тесно связано с рангом. Это разложение объясняет, почему и как матрицы низкого ранга могут быть сжаты.\nЕго можно графически представить следующим образом:\n или в матричной форме\n A = C \\widehat{A}^{-1} R, \nгде C - это некоторые k=\\mathrm{rank}(A) столбцов матрицы A, R - некоторые k строк матрицы A, а \\widehat{A} - невырожденная подматрица на их пересечении.\n\nЗамечание\nМы еще формально не определили обратную матрицу, поэтому напомним:\n\nОбратной матрицей для матрицы P является матрица Q = P^{-1} такая, что P Q = QP = I.\nЕсли матрица квадратная и имеет полный ранг, то обратная матрица существует.\n\n\n\nДоказательство скелетного разложения\n\nПусть C\\in \\mathbb{C}^{n\\times k} - это k столбцов, основанных на невырожденной подматрице \\widehat{A}. Следовательно, они линейно независимы.\nВозьмем любой другой столбец a_i матрицы A. Тогда a_i можно представить как линейную комбинацию столбцов C, т.е. a_i = C x_i, где x_i - вектор коэффициентов.\na_i = C x_i - это n уравнений. Мы берем k уравнений из них, соответствующих строкам, содержащим \\widehat{A}, и получаем уравнение\n\n\\widehat{r}_i = \\widehat{A} x_i \\quad \\Longrightarrow \\quad x_i = \\widehat{A}^{-1} \\widehat r_i\nТаким образом, a_i = C\\widehat{A}^{-1} \\widehat r_i для каждого i и\nA = [a_1,\\dots, a_m] = C\\widehat{A}^{-1} R.\n\n\nБолее подробный взгляд на скелетное разложение\n\nЛюбая матрица ранга r может быть записана в форме\n\nA = C \\widehat{A}^{-1} R,\nгде C имеет размер n \\times r, R имеет размер r \\times m и \\widehat{A} имеет размер r \\times r, или\n A = UV, \nгде U и V не являются уникальными, например, U = C \\widehat{A}^{-1}, V=R.\n\nФорма A = U V является стандартной для скелетного разложения.\nТаким образом, каждая матрица ранга r может быть записана как произведение “тонкой” (“высокой”) матрицы U на “широкую” (“короткую”) матрицу V.\n\nВ индексной форме это\n a_{ij} = \\sum_{\\alpha=1}^r u_{i \\alpha} v_{\\alpha j}. \nДля ранга 1 имеем\n a_{ij} = u_i v_j, \nт.е. это разделение индексов, и матрица ранга r является суммой матриц ранга 1!\n\n\nХранение\nИнтересно отметить, что для матрицы ранга r\nA = U V\nможно хранить только U и V, что дает нам (n+m) r параметров, поэтому это может использоваться для сжатия. Мы также можем вычислять произведение матрицы на вектор Ax гораздо быстрее:\n\nУмножение y = Vx требует \\mathcal{O}(mr) операций.\nУмножение z = Uy требует \\mathcal{O}(nr) операций.\n\nТо же самое работает для сложения, поэлементного умножения и т.д. Для сложения:\n    A_1 + A_2 = U_1 V_1 + U_2 V_2 = [U_1|U_2] [V_1^\\top|V_2^\\top]^\\top \n\n#A fast matrix-by-vector product demo\nimport numpy as np\nn = 4096\nr = 10\nu = np.random.normal(size=(n, r))\nv = np.random.normal(size=(n, r))\na = u @ v.T\nx = np.random.normal(size=(n,))\nprint(n*n/(2*n*r))\n%timeit a @ x\n%timeit u @ (v.T @ x)\n\n204.8\n2.39 ms ± 114 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n29.3 μs ± 293 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#computing-matrix-rank",
    "href": "lectures/lecture-3/lecture-3.html#computing-matrix-rank",
    "title": "",
    "section": "Computing matrix rank",
    "text": "Computing matrix rank\nWe can also try to compute the matrix rank using the built-in np.linalg.matrix_rank function\n\n#Computing matrix rank\nimport numpy as np\nn = 50 \na = np.ones((n, n))\nprint('Rank of the matrix:', np.linalg.matrix_rank(a))\nb = a + 1e-7 * np.random.randn(*a.shape)\nprint('Rank of the matrix:', np.linalg.matrix_rank(b, tol=1e-7))\n\na = np.eye(2000)*0.1\nnp.linalg.det(a)\n\nRank of the matrix: 1\nRank of the matrix: 46\n\n\nnp.float64(0.0)\n\n\n Таким образом, малые возмущения могут существенно влиять на ранг! \n\nНеустойчивость ранга матрицы\nДля любой матрицы A ранга r с r &lt; \\min(m, n) существует матрица B такая, что ее ранг равен \\min(m, n) и\n \\Vert A - B \\Vert = \\epsilon. \nВопрос: Означает ли это, что численно ранг матрицы не имеет смысла? (То есть, малые возмущения приводят к полному рангу!)\nОтвет: Нет. Мы должны найти матрицу B такую, что \\|A-B\\| \\leq \\epsilon и B имеет минимальный ранг. Поэтому мы можем вычислить ранг только с заданной точностью \\epsilon. Одним из подходов к вычислению ранга матрицы r является SVD."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#аппроксимация-низкого-ранга",
    "href": "lectures/lecture-3/lecture-3.html#аппроксимация-низкого-ранга",
    "title": "",
    "section": "Аппроксимация низкого ранга",
    "text": "Аппроксимация низкого ранга\nВажной задачей во многих приложениях является нахождение аппроксимации низкого ранга для заданной матрицы с заданной точностью \\epsilon или рангом r.  Примеры: * анализ главных компонент * рекомендательные системы * метод наименьших квадратов * сжатие нейронных сетей\nЭти задачи могут быть решены с помощью SVD."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сингулярное-разложение",
    "href": "lectures/lecture-3/lecture-3.html#сингулярное-разложение",
    "title": "",
    "section": "Сингулярное разложение",
    "text": "Сингулярное разложение\nДля вычисления аппроксимации низкого ранга нам необходимо вычислить сингулярное разложение (SVD).\nТеорема Любая матрица A\\in \\mathbb{C}^{n\\times m} может быть представлена как произведение трех матриц:\n A = U \\Sigma V^*, \nгде - U - унитарная матрица размера n \\times K, - V - унитарная матрица размера m \\times K, K = \\min(m, n), - \\Sigma - диагональная матрица с неотрицательными элементами \\sigma_1 \\geq  \\ldots, \\geq \\sigma_K на диагонали. - Более того, если \\text{rank}(A) = r, то \\sigma_{r+1} = \\dots = \\sigma_K = 0.\n\nДоказательство\n\nМатрица A^*A является эрмитовой, следовательно, диагонализируема в унитарном базисе (будет обсуждаться далее в курсе).\nA^*A\\geq0 (неотрицательно определена), поэтому собственные значения неотрицательны. Следовательно, существует унитарная матрица V = [v_1, \\dots, v_n] такая, что\n\n V^* A^* A V = \\text{diag}(\\sigma_1^2,\\dots, \\sigma_n^2), \\quad \\sigma_1\\geq \\sigma_2\\geq \\dots \\geq \\sigma_n. \nПусть \\sigma_i = 0 для i&gt;r, где r - некоторое целое число.  Пусть V_r= [v_1, \\dots, v_r], \\Sigma_r = \\text{diag}(\\sigma_1, \\dots,\\sigma_r). Следовательно\n V^*_r A^* A V_r = \\Sigma_r^2 \\quad \\Longrightarrow \\quad (\\Sigma_r^{-1} V_r^* A^*) (A V_r\\Sigma_r^{-1} ) = I. \nВ результате, матрица U_r = A V_r\\Sigma_r^{-1} удовлетворяет U_r^* U_r = I и, следовательно, имеет ортогональные столбцы.  Добавим к U_r любые ортогональные столбцы, которые ортогональны столбцам в U_r, и обозначим эту матрицу как U. Тогда\n AV = U \\begin{bmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}\\quad \\Longrightarrow \\quad U^* A V = \\begin{bmatrix}\\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}.\n\nПоскольку умножение на невырожденные матрицы не меняет ранг A, мы имеем r = \\text{rank}(A).\nСледствие 1: A = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_\\alpha v_\\alpha^* или поэлементно a_{ij} = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_{i\\alpha} \\overline{v}_{j\\alpha}\nСледствие 2: \\text{ker}(A) = \\mathcal{L}\\{v_{r+1},\\dots,v_n\\}\n\\text{im}(A) = \\mathcal{L}\\{u_{1},\\dots,u_r\\}\n\\text{ker}(A^*) = \\mathcal{L}\\{u_{r+1},\\dots,u_n\\}\n\\text{im}(A^*) = \\mathcal{L}\\{v_{1},\\dots,v_r\\}"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#теорема-экарта-янга",
    "href": "lectures/lecture-3/lecture-3.html#теорема-экарта-янга",
    "title": "",
    "section": "Теорема Экарта-Янга",
    "text": "Теорема Экарта-Янга\nНаилучшее приближение низкого ранга может быть вычислено с помощью SVD.\nТеорема: Пусть r &lt; \\text{rank}(A), A_r = U_r \\Sigma_r V_r^*. Тогда\n \\min_{\\text{rank}(B)=r} \\|A - B\\|_2 = \\|A - A_r\\|_2 = \\sigma_{r+1}. \nТо же самое верно для \\|\\cdot\\|_F, но \\|A - A_r\\|_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{\\min (n,m)}^2}."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#доказательство-1",
    "href": "lectures/lecture-3/lecture-3.html#доказательство-1",
    "title": "",
    "section": "Доказательство",
    "text": "Доказательство\n\nПоскольку \\text{rank} (B) = r, то \\text{dim}~\\text{ker}~B = n-r.\nСледовательно, существует z\\not=0 такой, что z\\in \\text{ker}(B) \\cap \\mathcal{L}(v_1,\\dots,v_{r+1}) (так как \\text{dim}\\{v_1,\\dots,v_{r+1}\\} = r+1).\nЗафиксируем \\|z\\| = 1. Тогда,\n\n \\|A-B\\|_2^2 \\geq \\|(A-B)z\\|_2^2 = \\|Az\\|_2^2 = \\| U\\Sigma V^* z\\|^2_2= \\|\\Sigma V^* z\\|^2_2 = \\sum_{i=1}^{n} \\sigma_i^2 (v_i^*z)^2 =\\sum_{i=1}^{r+1} \\sigma_i^2 (v_i^*z)^2 \\geq \\sigma_{r+1}^2\\sum_{i=1}^{r+1} (v_i^*z)^2 = \\sigma_{r+1}^2 \nтак как \\sigma_1\\geq \\dots \\geq \\sigma_{r+1} и \\sum_{i=1}^{r+1} (v_i^*z)^2 = \\|V^*z\\|_2^2 = \\|z\\|_2^2 = 1."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#основной-результат-о-приближении-низкого-ранга",
    "href": "lectures/lecture-3/lecture-3.html#основной-результат-о-приближении-низкого-ранга",
    "title": "",
    "section": "Основной результат о приближении низкого ранга",
    "text": "Основной результат о приближении низкого ранга\nСледствие: вычисление наилучшего приближения ранга r эквивалентно установке \\sigma_{r+1}= 0, \\ldots, \\sigma_K = 0. Ошибка\n \\min_{A_r} \\Vert A - A_r \\Vert_2 = \\sigma_{r+1}, \\quad \\min_{A_r} \\Vert A - A_r \\Vert_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{K}^2} \nвот почему важно смотреть на скорость убывания сингулярных значений."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#вычисление-svd",
    "href": "lectures/lecture-3/lecture-3.html#вычисление-svd",
    "title": "",
    "section": "Вычисление SVD",
    "text": "Вычисление SVD\n\nАлгоритмы для вычисления SVD сложны и будут обсуждаться позже.\nНо для численных расчетов мы уже можем использовать NumPy, JAX или PyTorch!\n\nВернемся к предыдущему примеру\n\n#Computing matrix rank\nimport numpy as np\nn = 50 \na = np.ones((n, n))\nprint('Rank of the matrix:', np.linalg.matrix_rank(a))\nb = a + 1e-5 * np.random.randn(*a.shape)\nprint('Rank of the matrix:', np.linalg.matrix_rank(b, tol=1e-7))\n\nRank of the matrix: 1\nRank of the matrix: 50\n\n\n\nu, s, v = np.linalg.svd(b) #b = u@jnp.diag(s)@v \nprint(s/s[0])\nprint(s[1]/s[0])\nr = 1\nu1 = u[:, :r]\ns1 = s[:r]\nv1 = v[:r, :]\na1 = u1.dot(np.diag(s1).dot(v1))\nprint(np.linalg.norm(b - a1, 2)/s[0])\n\n[1.00000000e+00 2.67591794e-06 2.56509433e-06 2.44789428e-06\n 2.37455067e-06 2.29593994e-06 2.28815285e-06 2.23799065e-06\n 2.05935979e-06 2.03632846e-06 1.99639422e-06 1.91341245e-06\n 1.83976105e-06 1.81660880e-06 1.74749549e-06 1.73011845e-06\n 1.64795628e-06 1.58936685e-06 1.52344312e-06 1.46849377e-06\n 1.41976378e-06 1.36738146e-06 1.32423364e-06 1.25094325e-06\n 1.20823796e-06 1.16024742e-06 1.08756172e-06 9.81247385e-07\n 9.32616886e-07 9.20283514e-07 8.93963429e-07 8.53777806e-07\n 8.01564035e-07 7.62399924e-07 7.12964817e-07 6.77836503e-07\n 6.64178420e-07 5.63347485e-07 5.17310367e-07 4.72905832e-07\n 4.24444379e-07 3.68835674e-07 3.07482216e-07 2.90162960e-07\n 2.44707429e-07 2.04581052e-07 1.82253444e-07 8.90398610e-08\n 7.58352672e-08 1.85055347e-08]\n2.675917943198842e-06\n2.6759179431971288e-06\n\n\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rc(\"text\", usetex=False)\n\nn = 100\na = [[1.0/(i-j+0.5) for i in range(n)] for j in range(n)] #Hilbert matrix \nda = np.array(a)\nu, s, v = np.linalg.svd(a)\ns - np.pi\n#plt.semilogy(s/s[0])\n#print(s[50] - np.pi)\n#plt.plot(s[:30], 'x')\n#s\n#plt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\n#plt.xlabel(r\"Singular value index, $i$\", fontsize=24)\n# plt.grid(True)\n# plt.xticks(fontsize=26)\n# plt.yticks(fontsize=26)\n# #We have very good low-rank approximation of it!\n\narray([ 3.99680289e-15,  1.77635684e-15,  1.77635684e-15,  1.33226763e-15,\n        1.33226763e-15,  1.33226763e-15,  1.33226763e-15,  1.33226763e-15,\n        1.33226763e-15,  1.33226763e-15,  1.33226763e-15,  1.33226763e-15,\n        1.33226763e-15,  1.33226763e-15,  1.33226763e-15,  1.33226763e-15,\n        1.33226763e-15,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -4.44089210e-16, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -4.44089210e-16, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -4.44089210e-16, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -1.33226763e-15, -1.33226763e-15,\n       -1.77635684e-15, -4.44089210e-15, -6.08402217e-14, -5.76427794e-13,\n       -5.26201305e-12, -4.61763960e-11, -3.89348109e-10, -3.15097903e-09,\n       -2.44420360e-08, -1.81421391e-07, -1.28599019e-06, -8.68452531e-06,\n       -5.57096630e-05, -3.38178667e-04, -1.93260882e-03, -1.03148256e-02,\n       -5.06661203e-02, -2.21847627e-01, -8.08302651e-01, -2.19424535e+00])"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#линейный-факторный-анализ-и-низкий-ранг",
    "href": "lectures/lecture-3/lecture-3.html#линейный-факторный-анализ-и-низкий-ранг",
    "title": "",
    "section": "Линейный факторный анализ и низкий ранг",
    "text": "Линейный факторный анализ и низкий ранг\nРассмотрим линейную факторную модель,\ny = Ax, \nгде y - вектор длины n, а x - вектор длины r.\nДанные организованы в виде выборок: мы наблюдаем векторы\ny_1, \\ldots, y_T,\nно не знаем матрицу A, тогда факторная модель может быть записана как\n\n  Y = AX,\n\nгде Y имеет размер n \\times T, A имеет размер n \\times r и X имеет размер r \\times T.\n\nЭто в точности модель ранга r: она говорит нам, что векторы лежат в малом подпространстве.\nМы также можем использовать SVD для восстановления этого подпространства (но не независимых компонент).\nАнализ главных компонент может быть выполнен с помощью SVD, посмотрите реализацию в пакете sklearn."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#применения-svd",
    "href": "lectures/lecture-3/lecture-3.html#применения-svd",
    "title": "",
    "section": "Применения SVD",
    "text": "Применения SVD\n\nSVD чрезвычайно важен в вычислительной науке и инженерии.\nОн имеет много названий: Анализ главных компонент, Правильное ортогональное разложение, Эмпирические ортогональные функции\nТеперь мы рассмотрим сжатие плотных матриц и метод активных подпространств"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сжатие-плотных-матриц",
    "href": "lectures/lecture-3/lecture-3.html#сжатие-плотных-матриц",
    "title": "",
    "section": "Сжатие плотных матриц",
    "text": "Сжатие плотных матриц\nПлотные матрицы обычно требуют хранения N^2 элементов. Аппроксимация ранга-r может уменьшить это число до \\mathcal{O}(Nr)\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the style for a more beautiful plot\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 6))\n\n# Generate the matrix\nn = 256\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\n\n# Compute SVD\nu, s, v = np.linalg.svd(a[n//2:, :n//2])\n\n# Plot singular values\nplt.semilogy(s/s[0], linewidth=2.5, color='#1f77b4', marker='o', markersize=6, \n             markerfacecolor='white', markeredgewidth=1.5, markeredgecolor='#1f77b4')\n\n# Add labels and customize\nplt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\nplt.xlabel(r\"Singular value index, $i$\", fontsize=24)\nplt.grid(True, alpha=0.7)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\n\n# Add title and improve appearance\nplt.title(\"Normalized Singular Values\", fontsize=26, pad=20)\nplt.tight_layout()\n\n# Add a box around the plot\nplt.box(True)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сжатие-параметров-в-полносвязных-нейронных-сетях",
    "href": "lectures/lecture-3/lecture-3.html#сжатие-параметров-в-полносвязных-нейронных-сетях",
    "title": "",
    "section": "Сжатие параметров в полносвязных нейронных сетях",
    "text": "Сжатие параметров в полносвязных нейронных сетях\n\nОдним из основных строительных блоков современных глубоких нейронных сетей является полносвязный слой, также известный как линейный слой\nЭтот слой реализует действие линейной функции на входной вектор: f(x) = Wx + b, где W - обучаемая матрица, а b - обучаемый вектор смещения\nИ W, и b обновляются во время обучения сети в соответствии с некоторым методом оптимизации, например, SGD, Adam и т.д.\nОднако хранение обученных оптимальных параметров (W и b) может быть непозволительно затратным, если вы хотите перенести вашу обученную сеть на устройство с ограниченной памятью\nВ качестве возможного решения вы можете сжать матрицы W_i из i-го линейного слоя с помощью усеченного SVD на основе сингулярных значений!\nЧто вы получаете после такой аппроксимации W?\n\nэффективное использование памяти при хранении\nболее быстрый вывод\nумеренное снижение точности при решении целевой задачи, например, классификации изображений"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#take-home-message",
    "href": "lectures/lecture-3/lecture-3.html#take-home-message",
    "title": "",
    "section": "Take home message",
    "text": "Take home message\n\nMatrix rank definition\nSkeleton approximation and dyadic representation of a rank-r matrix\nSingular value decomposition and Eckart-Young theorem\nThree applications of SVD (linear factor analysis, dense matrix compression, active subspaces)."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#next-lecture",
    "href": "lectures/lecture-3/lecture-3.html#next-lecture",
    "title": "",
    "section": "Next lecture",
    "text": "Next lecture\n\nLinear systems\nInverse matrix\nCondition number\nLinear least squares\nPseudoinverse\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#краткий-обзор-предыдущей-лекции",
    "href": "lectures/lecture-5/lecture-5.html#краткий-обзор-предыдущей-лекции",
    "title": "Матричное разложение: форма Шура",
    "section": "Краткий обзор предыдущей лекции",
    "text": "Краткий обзор предыдущей лекции\n\nЛинейные системы\nМетод Гаусса\nLU-разложение\nЧисло обусловленности как мера прямой устойчивости задачи"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#сегодняшняя-лекция",
    "href": "lectures/lecture-5/lecture-5.html#сегодняшняя-лекция",
    "title": "Матричное разложение: форма Шура",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\nСегодня мы поговорим о: - Собственных векторах и их приложениях (PageRank) - Вычислении собственных векторов методом степенной итерации - Теореме Шура - Нормальных матрицах"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#что-такое-собственный-вектор",
    "href": "lectures/lecture-5/lecture-5.html#что-такое-собственный-вектор",
    "title": "Матричное разложение: форма Шура",
    "section": "Что такое собственный вектор?",
    "text": "Что такое собственный вектор?\n\nОпределение. Вектор x \\ne 0 называется собственным вектором квадратной матрицы A, если существует число \\lambda такое, что\n\n Ax = \\lambda x. \n\nЧисло \\lambda называется собственным значением. Также используется термин собственная пара.\nПоскольку A - \\lambda I должна иметь нетривиальное ядро, собственные значения являются корнями характеристического многочлена\n\n \\det (A - \\lambda I) = 0."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#разложение-по-собственным-значениям",
    "href": "lectures/lecture-5/lecture-5.html#разложение-по-собственным-значениям",
    "title": "Матричное разложение: форма Шура",
    "section": "Разложение по собственным значениям",
    "text": "Разложение по собственным значениям\nЕсли матрица A размера n\\times n имеет n собственных векторов s_i, i=1,\\dots,n:\n As_i = \\lambda_i s_i, \nто это можно записать как\n A S = S \\Lambda, \\quad\\text{где}\\quad S=(s_1,\\dots,s_n), \\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n), \nили эквивалентно\n A = S\\Lambda S^{-1}. \n\nЭто называется разложением по собственным значениям матрицы. Матрицы, которые могут быть представлены через разложение по собственным значениям, называются диагонализируемыми.\n\n\nСуществование\n\nКакие классы матриц диагонализируемы?\nПростой пример - это матрицы с различными собственными значениями.\nВ более общем случае, матрица диагонализируема тогда и только тогда, когда алгебраическая кратность каждого собственного значения (кратность собственного значения в характеристическом многочлене) равна его геометрической кратности (размерности собственного подпространства).\nДля наших целей наиболее важным классом диагонализируемых матриц является класс нормальных матриц:\n\nAA^* = A^* A.\n\nВы узнаете, как доказать, что нормальные матрицы диагонализируемы, через несколько слайдов (тема разложения Шура).\n\n\nПример\n\nВы можете просто проверить, что, например, матрица\n\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\nимеет одно собственное значение 1 кратности 2 (так как её характеристический многочлен равен p(\\lambda)=(1-\\lambda)^2), но только один собственный вектор \\begin{pmatrix} c \\\\ 0  \\end{pmatrix}, и, следовательно, матрица не диагонализируема."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#почему-собственные-векторы-и-собственные-значения-важны",
    "href": "lectures/lecture-5/lecture-5.html#почему-собственные-векторы-и-собственные-значения-важны",
    "title": "Матричное разложение: форма Шура",
    "section": "Почему собственные векторы и собственные значения важны?",
    "text": "Почему собственные векторы и собственные значения важны?\n\nСобственные векторы являются как важными вспомогательными инструментами, так и играют важную роль в приложениях.\n\nМожете привести несколько примеров?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#применения-собственных-значенийсобственных-векторов",
    "href": "lectures/lecture-5/lecture-5.html#применения-собственных-значенийсобственных-векторов",
    "title": "Матричное разложение: форма Шура",
    "section": "Применения собственных значений/собственных векторов",
    "text": "Применения собственных значений/собственных векторов\n\nТеория коммуникаций: теоретический предел количества передаваемой информации\nПроектирование мостов (машиностроение)\nПроектирование hi-fi аудиосистем\nКвантовая химия: весь наш микромир управляется уравнением Шрёдингера, которое является задачей на собственные значения:\n\n H \\psi = E \\psi, \n\nРедукция порядка моделей сложных систем\nАнализ графов (PageRank, кластеризация графов)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#eigenvalues-are-vibrational-frequencies",
    "href": "lectures/lecture-5/lecture-5.html#eigenvalues-are-vibrational-frequencies",
    "title": "Матричное разложение: форма Шура",
    "section": "Eigenvalues are vibrational frequencies",
    "text": "Eigenvalues are vibrational frequencies\nA typical computation of eigenvectors / eigenvectors is for studying\n\nVibrational computations of mechanical structures\nModel order reduction of complex systems\n\n\nfrom IPython.display import YouTubeVideo \nYouTubeVideo(\"VcCcMZo6J6w\")"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#google-pagerank",
    "href": "lectures/lecture-5/lecture-5.html#google-pagerank",
    "title": "Матричное разложение: форма Шура",
    "section": "Google PageRank",
    "text": "Google PageRank\n\nОдним из самых известных вычислений собственных векторов является Google PageRank.\nВ настоящее время он активно не используется Google, но был одной из основных особенностей на ранних этапах. Вопрос в том, как мы ранжируем веб-страницы, какая из них важна, а какая нет.\nВсё, что мы знаем о сети, это какая страница ссылается на какую. PageRank определяется рекурсивным определением.\nОбозначим через p_i важность i-й страницы.\nЗатем мы определяем эту важность как среднее значение всех важностей всех страниц, которые ссылаются на текущую страницу. Это даёт нам линейную систему\n\n p_i = \\sum_{j \\in N(i)} \\frac{p_j}{L(j)}, \nгде L(j) - количество исходящих ссылок на j-й странице, N(i) - все соседи i-й страницы. Это можно переписать как\n p = G p, \\quad G_{ij} = \\frac{1}{L(j)} \nили как задачу на собственные значения\n\n   Gp = 1 p,\n\nт.е. собственное значение 1 уже известно. Заметим, что G является левой стохастической матрицей, т.е. сумма элементов в каждом её столбце равна 1. Проверьте, что любая левая стохастическая матрица имеет максимальное собственное значение, равное 1."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#демонстрация",
    "href": "lectures/lecture-5/lecture-5.html#демонстрация",
    "title": "Матричное разложение: форма Шура",
    "section": "Демонстрация",
    "text": "Демонстрация\n\nМы можем вычислить PageRank, используя некоторые пакеты Python.\nМы будем использовать пакет networkx для работы с графами, который можно установить с помощью\nМы будем использовать простой пример сети карате-клуба Захари.\nЭти данные были собраны вручную в 1977 году и представляют собой классический набор данных социальной сети.\n\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport networkx as nx\nkn = nx.read_gml('karate.gml')\n#nx.write_gml(kn, 'karate2.gml')\nnx.draw_networkx(kn, node_color=\"red\") #Draw the graph\n\n\n\n\n\n\n\n\n\nТеперь мы можем вычислить PageRank, используя встроенную функцию NetworkX.\nМы также отображаем размер узлов больше, если их PageRank больше.\n\n\npr = nx.algorithms.link_analysis.pagerank(kn)\npr_vector = list(pr.values())\npr_vector = np.array(pr_vector) * 3000\nnx.draw_networkx(kn, node_size=pr_vector, node_color=\"red\", labels=None)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений",
    "href": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений",
    "title": "Матричное разложение: форма Шура",
    "section": "Вычисление собственных значений",
    "text": "Вычисление собственных значений\n\nКак вычислить собственные значения и собственные векторы?\n\nСуществует два типа задач на собственные значения:\n\nполная задача на собственные значения (требуются все собственные значения и собственные векторы)\nчастичные собственные значения (требуются минимальные/максимальные собственные значения, собственные значения в заданной области)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений-через-характеристические-уравнения",
    "href": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений-через-характеристические-уравнения",
    "title": "Матричное разложение: форма Шура",
    "section": "Вычисление собственных значений через характеристические уравнения",
    "text": "Вычисление собственных значений через характеристические уравнения\nЗадача на собственные значения имеет вид\n Ax = \\lambda x, \nили\n (A - \\lambda I) x = 0, \nследовательно, матрица A - \\lambda I имеет нетривиальное ядро и должна быть сингулярной.\nЭто означает, что определитель\n p(\\lambda) = \\det(A - \\lambda I) = 0. \n\nЭто уравнение называется характеристическим уравнением и является многочленом порядка n.\nМногочлен степени n имеет n комплексных корней!"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#вспомним-определение-определителя",
    "href": "lectures/lecture-5/lecture-5.html#вспомним-определение-определителя",
    "title": "Матричное разложение: форма Шура",
    "section": "Вспомним определение определителя",
    "text": "Вспомним определение определителя\nОпределитель квадратной матрицы A определяется как\n\\det A = \\sum_{\\sigma \\in S_n} \\mathrm{sgn}({\\sigma})\\prod^n_{i=1} a_{i, \\sigma_i},\nгде - S_n - множество всех перестановок чисел 1, \\ldots, n - \\mathrm{sgn} - знак перестановки ( (-1)^p, где p - число транспозиций, которые нужно сделать)."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#свойства-определителя",
    "href": "lectures/lecture-5/lecture-5.html#свойства-определителя",
    "title": "Матричное разложение: форма Шура",
    "section": "Свойства определителя",
    "text": "Свойства определителя\nОпределитель имеет много полезных свойств:\n1. \\det(AB) = \\det(A) \\det(B)\n2. Если одна строка представлена в виде суммы двух векторов, определитель является суммой двух определителей\n3. “Разложение по минорам”: мы можем разложить определитель по выбранной строке или столбцу.\n\nЕсли делать это через разложение по минорам, получаем экспоненциальную сложность по n.\nМожем ли мы достичь сложности \\mathcal{O}(n^3)?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#собственные-значения-и-характеристическое-уравнение",
    "href": "lectures/lecture-5/lecture-5.html#собственные-значения-и-характеристическое-уравнение",
    "title": "Матричное разложение: форма Шура",
    "section": "Собственные значения и характеристическое уравнение",
    "text": "Собственные значения и характеристическое уравнение\n\nТеперь вернемся к собственным значениям.\nХарактеристическое уравнение может быть использовано для вычисления собственных значений, что приводит к наивному алгоритму:\n\np(\\lambda) = \\det(A - \\lambda I)\n\nВычислить коэффициенты многочлена\nВычислить корни\n\nЭто хорошая идея?\nПоделитесь своим мнением\nМы можем сделать короткую демонстрацию этого\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn = 3\na = [[1.0 / (i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\nev = np.linalg.eigvals(a)\n# There is a special numpy function for characteristic polynomial\ncf = np.poly(a)\nev_roots = np.roots(cf)\n# print('Coefficients of the polynomial:', cf)\n# print('Polynomial roots:', ev_roots)\nplt.scatter(ev_roots.real, ev_roots.imag, marker='x', label='roots')\nb = a + 1e-3 * np.random.randn(n, n)\nev_b = np.linalg.eigvals(b)\nplt.scatter(ev_b.real, ev_b.imag, marker='o', label='Lapack')\n# plt.scatter(ev_roots.real, ev_roots.imag, marker='o', label='Brute force')\nplt.legend(loc='best')\nplt.xlabel('Real part')\nplt.ylabel('Imaginary part')\n\nText(0, 0.5, 'Imaginary part')\n\n\n\n\n\n\n\n\n\n\nМораль\n\nНе делайте этого, если у вас нет веской причины.\nПоиск корней многочлена очень плохо обусловлен (может быть намного лучше, но не с мономами \\{1,x,x^2,\\dots\\}!). Обратите внимание, что матрица Грама мономов\n\nh_{ij} = \\int_0^1 x^i x^j\\, dx = \\frac{1}{i+j+1},\nявляется матрицей Гильберта, которая имеет экспоненциальное убывание сингулярных значений. - Таким образом, мономы “почти” линейно зависимы."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#степенной-метод",
    "href": "lectures/lecture-5/lecture-5.html#степенной-метод",
    "title": "Матричное разложение: форма Шура",
    "section": "Степенной метод",
    "text": "Степенной метод\n\nНас часто интересует вычисление части спектра, например, наибольших или наименьших собственных значений.\nТакже интересно отметить, что для эрмитовых матриц (A = A^*) собственные значения всегда действительны (докажите это!).\nСтепенной метод является простейшим методом для вычисления наибольшего по модулю собственного значения.\nЭто также наш первый пример итерационного метода и метода Крылова."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#степенной-метод-1",
    "href": "lectures/lecture-5/lecture-5.html#степенной-метод-1",
    "title": "Матричное разложение: форма Шура",
    "section": "Степенной метод",
    "text": "Степенной метод\n\nЗадача на собственные значения\n\nAx = \\lambda x, \\quad \\Vert x \\Vert_2 = 1 \\ \\text{для устойчивости}.\nможет быть переписана как итерация неподвижной точки. - Эта итерация называется степенным методом и находит наибольшее по модулю собственное значение матрицы A.\nСтепенной метод имеет вид\n x_{k+1} = A x_k, \\quad x_{k+1} := \\frac{x_{k+1}}{\\Vert x_{k+1} \\Vert_2}\nи\n x_{k+1}\\to v_1,\nгде Av_1 = \\lambda_1 v_1 и \\lambda_1 - наибольшее собственное значение, а v_1 - соответствующий собственный вектор.\n\nНа (k+1)-й итерации приближение к \\lambda_1 можно найти как\n\n \\lambda^{(k+1)} = (Ax_{k+1}, x_{k+1}), \n\nЗаметим, что \\lambda^{(k+1)} не требуется для (k+2)-й итерации, но может быть полезно для измерения ошибки на каждой итерации: \\|Ax_{k+1} - \\lambda^{(k+1)}x_{k+1}\\|.\nСходимость является геометрической, но коэффициент сходимости равен q^k, где q = \\left|\\frac{\\lambda_{2}}{\\lambda_{1}}\\right| &lt; 1, для \\lambda_1&gt;\\lambda_2\\geq\\dots\\geq \\lambda_n и k - номер итерации.\nЭто означает, что сходимость может быть произвольно малой. Чтобы доказать это, достаточно рассмотреть диагональную матрицу 2 \\times 2."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#анализ-сходимости-для-aa",
    "href": "lectures/lecture-5/lecture-5.html#анализ-сходимости-для-aa",
    "title": "Матричное разложение: форма Шура",
    "section": "Анализ сходимости для A=A^*",
    "text": "Анализ сходимости для A=A^*\nДавайте более точно рассмотрим степенной метод, когда A является эрмитовой матрицей. Через два слайда вы узнаете, что каждая эрмитова матрица диагонализируема. Следовательно, существует ортонормированный базис из собственных векторов v_1,\\dots,v_n такой, что Av_i = \\lambda_i v_i. Разложим x_0 в сумму v_i с коэффициентами c_i:\n x_0 = c_1 v_1 + \\dots + c_n v_n. \nПоскольку v_i являются собственными векторами, мы имеем\n\n\\begin{split}\nx_1 &= \\frac{Ax_0}{\\|Ax_0\\|} = \\frac{c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n}{\\|c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n \\|}  \\\\\n&\\vdots\\\\\nx_k &= \\frac{Ax_{k-1}}{\\|Ax_{k-1}\\|} = \\frac{c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n}{\\|c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n \\|}\n\\end{split}\n\nТеперь вы видите, что\n\nx_k = \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\frac{ v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k}v_2 + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}v_n}{\\left\\|v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k}v_2 + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}v_n\\right\\|},\n\nчто сходится к v_1, поскольку \\left| \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\right| = 1 и \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k \\to 0, если |\\lambda_2|&lt;|\\lambda_1|."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#что-нужно-помнить-о-степенном-методе",
    "href": "lectures/lecture-5/lecture-5.html#что-нужно-помнить-о-степенном-методе",
    "title": "Матричное разложение: форма Шура",
    "section": "Что нужно помнить о степенном методе",
    "text": "Что нужно помнить о степенном методе\n\nСтепенной метод дает оценку наибольшего по модулю собственного значения или спектрального радиуса заданной матрицы\nОдин шаг требует одного умножения матрицы на вектор. Если матрица позволяет выполнять умножение на вектор за \\mathcal{O}(n) операций (например, если она разреженная), то степенной метод применим для больших n.\nСходимость может быть медленной\nЕсли требуется только грубая оценка, достаточно всего нескольких итераций\nВектор решения находится в подпространстве Крылова \\{x_0, Ax_0,\\dots,A^{k}x_0\\} и имеет вид \\mu A^k x_0, где \\mu - нормировочная константа."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#теорема-шура",
    "href": "lectures/lecture-5/lecture-5.html#теорема-шура",
    "title": "Матричное разложение: форма Шура",
    "section": "Теорема Шура",
    "text": "Теорема Шура\nТеорема: Каждая матрица A \\in \\mathbb{C}^{n \\times n} может быть представлена в форме Шура A = UTU^*, где U - унитарная, а T - верхнетреугольная матрица.\nСхема доказательства. 1. Каждая матрица имеет по крайней мере 1 ненулевой собственный вектор (возьмем корень характеристического многочлена, (A-\\lambda I) сингулярна, имеет нетривиальное нуль-пространство). Пусть\nAv_1 = \\lambda_1 v_1, \\quad \\Vert v_1 \\Vert_2 = 1\n\nПусть U_1 = [v_1,v_2,\\dots,v_n], где v_2,\\dots, v_n - любые векторы, ортогональные v_1. Тогда\n\n U^*_1 A U_1 = \\begin{pmatrix} \\lambda_1 & *  \\\\ 0 & A_2  \\end{pmatrix}, \nгде A_2 - матрица размера (n-1) \\times (n-1). Это называется блочно-треугольной формой. Теперь мы можем работать только с A_2 и так далее.\nПримечание: Поскольку в этом доказательстве нам нужны собственные векторы, это доказательство не является практическим алгоритмом."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#применение-теоремы-шура",
    "href": "lectures/lecture-5/lecture-5.html#применение-теоремы-шура",
    "title": "Матричное разложение: форма Шура",
    "section": "Применение теоремы Шура",
    "text": "Применение теоремы Шура\n\nВажное применение теоремы Шура: нормальные матрицы.\nОпределение. Матрица A называется нормальной матрицей, если\n\n AA^* = A^* A. \nВопрос: Примеры нормальных матриц?\nПримеры: эрмитовы матрицы, унитарные матрицы."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#нормальные-матрицы",
    "href": "lectures/lecture-5/lecture-5.html#нормальные-матрицы",
    "title": "Матричное разложение: форма Шура",
    "section": "Нормальные матрицы",
    "text": "Нормальные матрицы\nТеорема: A является нормальной матрицей тогда и только тогда, когда A = U \\Lambda U^*, где U - унитарная, а \\Lambda - диагональная матрица.\nСхема доказательства: - Одно направление очевидно (если разложение выполняется, то матрица нормальная). - Другое направление сложнее. Рассмотрим форму Шура матрицы A. Тогда AA^* = A^*A означает TT^* = T^* T. - Рассматривая элементы, мы сразу видим, что единственная верхнетреугольная матрица T, удовлетворяющая условию TT^* = T^* T, - это диагональная матрица!\n\nВажное следствие\nТаким образом, каждая нормальная матрица унитарно диагонализуема, что означает, что она может быть диагонализована унитарной матрицей U.\nДругими словами, каждая нормальная матрица имеет ортогональный базис из собственных векторов."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#как-мы-вычисляем-разложение-шура",
    "href": "lectures/lecture-5/lecture-5.html#как-мы-вычисляем-разложение-шура",
    "title": "Матричное разложение: форма Шура",
    "section": "Как мы вычисляем разложение Шура?",
    "text": "Как мы вычисляем разложение Шура?\n\nВсё хорошо, но как мы вычисляем форму Шура?\nЭто будет рассмотрено в следующей лекции."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#спектр-и-псевдоспектр",
    "href": "lectures/lecture-5/lecture-5.html#спектр-и-псевдоспектр",
    "title": "Матричное разложение: форма Шура",
    "section": "Спектр и псевдоспектр",
    "text": "Спектр и псевдоспектр\n\nДля линейных динамических систем, заданных матрицей A, спектр может многое рассказать о системе (например, о стабильности, …)\nОднако для ненормальных матриц спектр может быть неустойчивым относительно малых возмущений.\nДля измерения таких возмущений было разработано понятие псевдоспектра."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#псевдоспектр",
    "href": "lectures/lecture-5/lecture-5.html#псевдоспектр",
    "title": "Матричное разложение: форма Шура",
    "section": "Псевдоспектр",
    "text": "Псевдоспектр\nМы рассматриваем объединение всех возможных собственных значений всех возмущений матрицы A.\n\\Lambda_{\\epsilon}(A) = \\{ \\lambda \\in \\mathbb{C}: \\exists E, x \\ne 0: (A + E) x = \\lambda x, \\quad \\Vert E \\Vert_2 \\leq \\epsilon. \\}\n\nДля малых E и нормальных матриц A это будут окружности вокруг собственных значений, для ненормальных матриц структура может быть совершенно иной. Подробнее: http://www.cs.ox.ac.uk/pseudospectra/"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#summary-of-todays-lecture",
    "href": "lectures/lecture-5/lecture-5.html#summary-of-todays-lecture",
    "title": "Матричное разложение: форма Шура",
    "section": "Summary of todays lecture",
    "text": "Summary of todays lecture\n\nEigenvalues, eigenvectors\nGershgorin theorem\nPower method\nSchur theorem\nNormal matrices\nSome advanced topics"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#next-lecture",
    "href": "lectures/lecture-5/lecture-5.html#next-lecture",
    "title": "Матричное разложение: форма Шура",
    "section": "Next lecture",
    "text": "Next lecture\n\nReview of the considered matrix decompositions\nPractical way to compute QR decomposition\nAlmost practical method for computing eigenvalues and eigenvectors\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#recap-of-the-previous-part",
    "href": "lectures/lecture-11/lecture-11.html#recap-of-the-previous-part",
    "title": "Questions?",
    "section": "Recap of the previous part",
    "text": "Recap of the previous part\n\nDistributed memory for huge dense matrices\nSparse matrix formats (COO, CSR, CSC)\nMatrix-by-vector product\nInefficiency of sparse matrix processing\nApproaches to reduce cache misses"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#plan-for-this-week-how-to-solve-large-sparse-linear-systems",
    "href": "lectures/lecture-11/lecture-11.html#plan-for-this-week-how-to-solve-large-sparse-linear-systems",
    "title": "Questions?",
    "section": "Plan for this week: how to solve large sparse linear systems",
    "text": "Plan for this week: how to solve large sparse linear systems\n\nDirect methods (start today and continue in the next lecture)\n\nLU decomposition\nNumber of reordering techniques to minimize fill-in\n\nKrylov methods"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#plan-for-today",
    "href": "lectures/lecture-11/lecture-11.html#plan-for-today",
    "title": "Questions?",
    "section": "Plan for today",
    "text": "Plan for today\nSparse direct solvers:\n\nLU decomposition of sparse matrices\nfill-in of L and U factors\nnested dissection\nspectral clustering in details"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#direct-methods-for-sparse-matrix-lu-decomposition",
    "href": "lectures/lecture-11/lecture-11.html#direct-methods-for-sparse-matrix-lu-decomposition",
    "title": "Questions?",
    "section": "Direct methods for sparse matrix: LU decomposition",
    "text": "Direct methods for sparse matrix: LU decomposition\n\nWhy sparse linear systems can be solved faster, what is the technique?\nIn the LU factorization of the matrix A the factors L and U can be also sparse:\n\nA = L U\n\nAnd solving linear systems with sparse triangular matrices is very easy.\n\n Note that the inverse matrix to sparse matrix is not sparse! \n\nimport numpy as np\nimport scipy.sparse as spsp\nn = 4\nex = np.ones(n);\n#a = spsp.spdiags(np.vstack((ex,  np.random.rand(n), np.random.rand(n))), [-1, 0, 1], n, n, 'csr'); \na = spsp.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \n\n#a = spsp.spdiags(np.vstack((-ex,  2*ex, -ex), np.random.rand(n)), [-1, 0, 1], n, n, 'csr'); \n\nb = np.array(np.linalg.inv(a.toarray()))\nprint(a.toarray())\nprint(b)\nnp.linalg.svd(b[:3, 4:])[1]\n\n[[ 2. -1.  0.  0.]\n [-1.  2. -1.  0.]\n [ 0. -1.  2. -1.]\n [ 0.  0. -1.  2.]]\n[[0.8 0.6 0.4 0.2]\n [0.6 1.2 0.8 0.4]\n [0.4 0.8 1.2 0.6]\n [0.2 0.4 0.6 0.8]]\n\n\narray([], dtype=float64)"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#and-the-factors",
    "href": "lectures/lecture-11/lecture-11.html#and-the-factors",
    "title": "Questions?",
    "section": "And the factors…",
    "text": "And the factors…\nL and U are typically sparse. In the tridiagonal case they are even bidiagonal!\n\nfrom scipy.sparse.linalg import splu\nT = splu(a.tocsc())#, permc_spec=\"NATURAL\")\nprint(T.L.toarray())\n\n[[ 1.   0.   0.   0. ]\n [ 0.   1.   0.   0. ]\n [-0.5 -0.5  1.   0. ]\n [ 0.  -0.5 -0.5  1. ]]\n\n\nInteresting to note that splu without permc_spec will produce permutations which will not yield the bidiagonal factor:\n\nfrom scipy.sparse.linalg import splu\nT = splu(a.tocsc())\nprint(T.L.todense())\nprint(T.perm_c)\n\n[[ 1.          0.          0.          0.          0.          0.\n   0.        ]\n [ 0.          1.          0.          0.          0.          0.\n   0.        ]\n [ 0.          0.          1.          0.          0.          0.\n   0.        ]\n [ 0.          0.          0.          1.          0.          0.\n   0.        ]\n [ 0.          0.          0.          0.          1.          0.\n   0.        ]\n [ 0.          0.          0.          0.          0.19991324  1.\n   0.        ]\n [ 0.81628124  0.14222853 -0.47418756  0.43359091 -0.00845846  0.23571107\n   1.        ]]\n[0 1 2 3 5 4 6]"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#d-case",
    "href": "lectures/lecture-11/lecture-11.html#d-case",
    "title": "Questions?",
    "section": "2D-case",
    "text": "2D-case\nFrom a matrix that comes as a discretization of a two-dimensional problem everything is much worse:\n\nimport scipy as sp\nimport scipy.sparse as spsp\nimport scipy.sparse.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 128\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = spsp.csc_matrix(A)\nT = spsp.linalg.splu(A)#, permc_spec=\"NATURAL\")\nplt.spy(T.L, marker='.', color='k', markersize=8)\nT.L\n#plt.spy(A, marker='.', color='k', markersize=8)\n\n&lt;Compressed Sparse Column sparse matrix of dtype 'float64'\n    with 610620 stored elements and shape (16384, 16384)&gt;\n\n\n\n\n\n\n\n\n\nFor correct permutation in 2D case the number of nonzeros in L factor grows as \\mathcal{O}(N \\log N). But complexity is \\mathcal{O}(N^{3/2})."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#main-challenge-how-to-make-factors-l-and-u-as-sparse-as-possible",
    "href": "lectures/lecture-11/lecture-11.html#main-challenge-how-to-make-factors-l-and-u-as-sparse-as-possible",
    "title": "Questions?",
    "section": "Main challenge: how to make factors L and U as sparse as possible?",
    "text": "Main challenge: how to make factors L and U as sparse as possible?"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#main-tool-to-analyze-factors-sparsity-graph-theory",
    "href": "lectures/lecture-11/lecture-11.html#main-tool-to-analyze-factors-sparsity-graph-theory",
    "title": "Questions?",
    "section": "Main tool to analyze factors sparsity: graph theory",
    "text": "Main tool to analyze factors sparsity: graph theory\n\nThe number of nonzeros in LU decomposition has a deep connection to the graph theory.\nnetworkx package can be used to visualize graphs, given only the adjacency matrix.\nIt may even recover to some extend the graph structure.\n\n\nimport networkx as nx\nimport scipy as sp\nn = 3\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(sp.sparse.kron(lp1, e), e) + sp.sparse.kron(sp.sparse.kron(e, lp1), e) + sp.sparse.kron(sp.sparse.kron(e, e), lp1)\nA = spsp.csc_matrix(A)\nG = nx.Graph(A)\nnx.draw(G, pos=nx.spectral_layout(G))#, node_size=1)"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#what-is-fill-in",
    "href": "lectures/lecture-11/lecture-11.html#what-is-fill-in",
    "title": "Questions?",
    "section": "What is fill-in?",
    "text": "What is fill-in?\n\nThe fill-in of a matrix are those entries which change from an initial zero to a nonzero value during the execution of an algorithm.\nThe fill-in is different for different permutations. So, before factorization we need to find reordering which produces the smallest fill-in.\n\nExample\nA = \\begin{bmatrix} * & * & * & * & *\\\\ * & * & 0 & 0 & 0 \\\\ * & 0  & * & 0 & 0 \\\\ * & 0 & 0& * & 0 \\\\ * & 0 & 0& 0 & * \\end{bmatrix} \nIf we eliminate elements from the top to the bottom, then we will obtain dense matrix. However, we could maintain sparsity if elimination was done from the bottom to the top."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#example-of-dense-factors-after-lu",
    "href": "lectures/lecture-11/lecture-11.html#example-of-dense-factors-after-lu",
    "title": "Questions?",
    "section": "Example of dense factors after LU",
    "text": "Example of dense factors after LU\nGiven matrix A=A^*&gt;0 we calculate its Cholesky decomposition A = LL^*.\nFactor L can be dense even if A is sparse:\n\n\\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} =\n\\begin{bmatrix} * &  &  &  \\\\ * & * &  &  \\\\ * & * & * &  \\\\ * & * & * & * \\end{bmatrix}\n\\begin{bmatrix} * & * & * & * \\\\  & * & * & * \\\\  &  & * & * \\\\  &  &  & * \\end{bmatrix}\n\nHow to make factors sparse, i.e. to minimize the fill-in?"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#why-permutations-can-reduce-fill-in-here-the-example",
    "href": "lectures/lecture-11/lecture-11.html#why-permutations-can-reduce-fill-in-here-the-example",
    "title": "Questions?",
    "section": "Why permutations can reduce fill-in? Here the example…",
    "text": "Why permutations can reduce fill-in? Here the example…\nWe need to find a permutation of indices so that factors are sparse, i.e. we build Cholesky factorisation of PAP^\\top, where P is a permutation matrix.\nFor the example from the previous slide\n\nP \\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} P^\\top =\n\\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\ * & * & * & * \\end{bmatrix} =\n\\begin{bmatrix} * &  &  &  \\\\  & * &  &  \\\\  &  & * &  \\\\ * & * & * & * \\end{bmatrix}\n\\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\  & &  & * \\end{bmatrix}\n\nwhere\n\nP = \\begin{bmatrix}  &  &  & 1 \\\\  &  & 1 &  \\\\  & 1 &  &  \\\\ 1 &  &  &  \\end{bmatrix}\n\n\nArrowhead form of the matrix gives sparse factors in LU decomposition\n\n\nimport numpy as np\nimport scipy.sparse as spsp\nimport scipy.sparse.linalg as spsplin\nimport scipy.linalg as splin\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nA = spsp.coo_matrix((np.random.randn(10), ([0, 0, 0, 0, 1, 1, 2, 2, 3, 3], \n                                           [0, 1, 2, 3, 0, 1, 0, 2, 0, 3])))\nprint(\"Original matrix\")\nplt.spy(A)\nplt.show()\nlu = spsplin.splu(A.tocsc(), permc_spec=\"NATURAL\")\nprint(\"L factor\")\nplt.spy(lu.L)\nplt.show()\nprint(\"U factor\")\nplt.spy(lu.U)\nplt.show()\nprint(\"Column permutation:\", lu.perm_c)\nprint(\"Row permutation:\", lu.perm_r)\n\nOriginal matrix\n\n\n\n\n\n\n\n\n\nL factor\n\n\n\n\n\n\n\n\n\nU factor\n\n\n\n\n\n\n\n\n\nColumn permutation: [0 1 2 3]\nRow permutation: [1 3 2 0]"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#block-version-of-appropriate-sparsity-pattern-arrowhead-structure",
    "href": "lectures/lecture-11/lecture-11.html#block-version-of-appropriate-sparsity-pattern-arrowhead-structure",
    "title": "Questions?",
    "section": "Block version of appropriate sparsity pattern (arrowhead structure)",
    "text": "Block version of appropriate sparsity pattern (arrowhead structure)\n\nPAP^\\top = \\begin{bmatrix} A_{11} &  & A_{13} \\\\  & A_{22} & A_{23} \\\\ A_{31} & A_{32} & A_{33}\\end{bmatrix}\n\nthen\n\nPAP^\\top = \\begin{bmatrix} A_{11} & 0 & 0 \\\\ 0 & A_{22} & 0 \\\\ A_{31} & A_{32} & A_{33} - A_{31}A_{11}^{-1} A_{13} - A_{32}A_{22}^{-1}A_{23} \\end{bmatrix} \\begin{bmatrix}  I & 0 & A_{11}^{-1}A_{13} \\\\ 0 & I & A_{22}^{-1}A_{23} \\\\ 0 & 0 & I\\end{bmatrix}\n\n\nBlock $ A_{33} - A_{31}A_{11}^{-1} A_{13} - A_{32}A_{22}^{-1}A_{23}$ is Schur complement for block diagonal matrix \\begin{bmatrix} A_{11} & 0 \\\\ 0 & A_{22} \\end{bmatrix}\nWe reduce problem to solving smaller linear systems with A_{11} and A_{22}"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#what-we-can-do-to-minimize-fill-in",
    "href": "lectures/lecture-11/lecture-11.html#what-we-can-do-to-minimize-fill-in",
    "title": "Questions?",
    "section": "What we can do to minimize fill-in?",
    "text": "What we can do to minimize fill-in?\n\nReordering the rows and the columns of the sparse matrix in order to reduce the number of nonzeros in L and U factors is called fill-in minimization.\nUnfortunately, this paper by Rose and Tarjan in 1975 proves that fill-in minimization problem is NP-complete.\nBut, many heuristics exist:\n\nMarkowitz pivoting - order by the product of nonzeros in column and row and stability constraint\nMinimum degree ordering - order by the degree of the vertex\nCuthill–McKee algorithm (and reverse Cuthill-McKee) - reorder to minimize the bandwidth (does not exploit graph representation).\nNested dissection: split the graph into two with minimal number of vertices on the separator (set of vertices removed after we separate the graph into two distinct connected graphs).  Complexity of the algorithm depends on the size of the graph separator. For 1D Laplacian separator contains only 1 vertex, in 2D - \\sqrt{N} vertices.\n\n\n\nHow can we find permutation?\n\nKey idea comes from graph theory\nSparse matrix can be treated as an adjacency matrix of a certain graph: the vertices (i, j) are connected, if the corresponding matrix element is non-zero.\n\n\n\nExample\nGraphs of \\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} and \\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\ * & * & * & * \\end{bmatrix} have the following form:\n and \n\nWhy the second ordering is better than the first one?"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#fill-in-upper-bound-minimization-markowitz-pivoting",
    "href": "lectures/lecture-11/lecture-11.html#fill-in-upper-bound-minimization-markowitz-pivoting",
    "title": "Questions?",
    "section": "Fill-in upper bound minimization: Markowitz pivoting",
    "text": "Fill-in upper bound minimization: Markowitz pivoting\n\nGeneral purpose approach to ordering the elements of sparse matrix that will be eliminated\nThe Markowitz merit for every non-zero element with indices (i, j) is computed as (r_i - 1)(c_j - 1), where r_i is number of nonzeros elements in the i-th row and c_j is a number of non-zero elements in the j-th column\nThis value is an upper abound on the fill-in after eliminating the (i, j) element. Why?\nWe can order elements with respect to these values, select the one with minimum value, eliminate it and update matrix. What about stability?\nThen re-compute these values and repeat the procedure\nThis method gives us the permutations of rows and columns and sparse factors\nMain drawback is efficient supporting number of nnz in every row and column after matrix update without complete re-calculating\nMore details can be found in book Direct Methods for Sparse Matrices by I. S. Duff, A. M. Erisman, and J. K. Reid\nDe facto standard approach in solving Linear programming (LP) problems and their MILP modifications"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#what-if-consider-only-neighbours-we-get-minimal-degree-ordering",
    "href": "lectures/lecture-11/lecture-11.html#what-if-consider-only-neighbours-we-get-minimal-degree-ordering",
    "title": "Questions?",
    "section": "What if consider only neighbours? We get minimal degree ordering!",
    "text": "What if consider only neighbours? We get minimal degree ordering!\n\nThe idea is to eliminate rows and/or columns with fewer non-zeros, update fill-in and then repeat. How it relates to Markowitz pivoting?\nEfficient implementation is an issue (adding/removing elements).\nCurrent champion is “approximate minimal degree” by Amestoy, Davis, Duff.\nIt is suboptimal even for 2D PDE problems\nSciPy sparse package uses minimal ordering approach for different matrices (A^{\\top}A, A + A^{\\top})"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#but-in-these-methods-we-ignore-the-knowledge-of-good-structure-for-sparse-lu-lets-exploit-it-explicitly-in-the-method",
    "href": "lectures/lecture-11/lecture-11.html#but-in-these-methods-we-ignore-the-knowledge-of-good-structure-for-sparse-lu-lets-exploit-it-explicitly-in-the-method",
    "title": "Questions?",
    "section": "But in these methods we ignore the knowledge of good structure for sparse LU! Let’s exploit it explicitly in the method!",
    "text": "But in these methods we ignore the knowledge of good structure for sparse LU! Let’s exploit it explicitly in the method!"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#how-to-formalize-reduction-to-block-arrowhead-form",
    "href": "lectures/lecture-11/lecture-11.html#how-to-formalize-reduction-to-block-arrowhead-form",
    "title": "Questions?",
    "section": "How to formalize reduction to block arrowhead form?",
    "text": "How to formalize reduction to block arrowhead form?\nDefinition. A separator in a graph G is a set S of vertices whose removal leaves at least two connected components.\nSeparator S gives the following ordering for an N-vertex graph G: - Find a separator S, whose removal leaves connected components T_1, T_2, \\ldots, T_k - Number the vertices of S from N − |S| + 1 to N - Recursively, number the vertices of each component: T_1 from 1 to |T_1|, T_2 from |T_1| + 1 to |T_1| + |T_2|, etc - If a component is small enough, enumeration in this component is arbitrarily\n\nSeparator and block arrowhead structure: example\nSeparator for the 2D Laplacian matrix\n\nA_{2D} = I \\otimes A_{1D} + A_{1D} \\otimes I, \\quad A_{1D} = \\mathrm{tridiag}(-1, 2, -1),\n\nis as follows\n \nOnce we have enumerated first indices in \\alpha, then in \\beta and separators indices in \\sigma we get the following matrix\n\nPAP^\\top = \\begin{bmatrix} A_{\\alpha\\alpha} &  & A_{\\alpha\\sigma} \\\\  & A_{\\beta\\beta} & A_{\\beta\\sigma} \\\\ A_{\\sigma\\alpha} & A_{\\sigma\\beta} & A_{\\sigma\\sigma}\\end{bmatrix}\n\nwhich has arrowhrad structure.\n\nThus, the problem of finding permutation was reduced to the problem of finding graph separator!"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#the-method-of-recursive-reduction-to-block-arrowhead-structure-nested-dissection",
    "href": "lectures/lecture-11/lecture-11.html#the-method-of-recursive-reduction-to-block-arrowhead-structure-nested-dissection",
    "title": "Questions?",
    "section": "The method of recursive reduction to block arrowhead structure – Nested dissection",
    "text": "The method of recursive reduction to block arrowhead structure – Nested dissection\n\nFor blocks A_{\\alpha\\alpha}, A_{\\beta\\beta} we continue splitting recursively.\nWhen the recursion is done, we need to eliminate blocks A_{\\sigma\\alpha} and A_{\\sigma\\beta}.\nThis makes block in the position of A_{\\sigma\\sigma}\\in\\mathbb{R}^{n\\times n} dense.\n\nCalculation of Cholesky of this block costs \\mathcal{O}(n^3) = \\mathcal{O}(N^{3/2}), where N = n^2 is the total number of nodes.\nSo, the complexity is \\mathcal{O}(N^{3/2})"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#packages-for-nested-dissection",
    "href": "lectures/lecture-11/lecture-11.html#packages-for-nested-dissection",
    "title": "Questions?",
    "section": "Packages for nested dissection",
    "text": "Packages for nested dissection\n\nMUltifrontal Massively Parallel sparse direct Solver (MUMPS)\nPardiso\nUmfpack as part of SuiteSparse\n\nAll of them have interfaces for C/C++, Fortran and Matlab\n\nNested dissection summary\n\nEnumeration: find a separator.\nDivide-and-conquer paradigm\nRecursively process two subsets of vertices after separation\nIn theory, nested dissection gives optimal complexity.\nIn practice, it beats others only for very large problems."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#separators-in-practice",
    "href": "lectures/lecture-11/lecture-11.html#separators-in-practice",
    "title": "Questions?",
    "section": "Separators in practice",
    "text": "Separators in practice\n\nComputing separators is not a trivial task.\nGraph partitioning heuristics has been an active research area for many years, often motivated by partitioning for parallel computation.\n\nExisting approaches:\n\nSpectral partitioning (uses eigenvectors of Laplacian matrix of graph) - more details below\nGeometric partitioning (for meshes with specified vertex coordinates) review and analysis\nIterative-swapping ((Kernighan-Lin, 1970), (Fiduccia-Matheysses, 1982)\nBreadth-first search (Lipton, Tarjan 1979)\nMultilevel recursive bisection (heuristic, currently most practical) (review and paper). Package for such kind of partitioning is called METIS, written in C, and available here"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#one-of-the-ways-to-construct-separators-spectral-graph-partitioning",
    "href": "lectures/lecture-11/lecture-11.html#one-of-the-ways-to-construct-separators-spectral-graph-partitioning",
    "title": "Questions?",
    "section": "One of the ways to construct separators – spectral graph partitioning",
    "text": "One of the ways to construct separators – spectral graph partitioning\n\nThe idea of spectral partitioning goes back to Miroslav Fiedler, who studied connectivity of graphs (paper).\nWe need to split the vertices into two sets.\nConsider +1/-1 labeling of vertices and the cost\n\nE_c(x) = \\sum_{j} \\sum_{i \\in N(j)} (x_i - x_j)^2, \\quad N(j) \\text{ denotes set of neighbours of a node } j. \n\nWe need a balanced partition, thus\n\n\\sum_i x_i =  0 \\quad \\Longleftrightarrow \\quad x^\\top e = 0, \\quad e = \\begin{bmatrix}1 & \\dots & 1\\end{bmatrix}^\\top,\nand since we have +1/-1 labels, we have\n\\sum_i x^2_i = n \\quad \\Longleftrightarrow \\quad \\|x\\|_2^2 = n."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#graph-laplacian",
    "href": "lectures/lecture-11/lecture-11.html#graph-laplacian",
    "title": "Questions?",
    "section": "Graph Laplacian",
    "text": "Graph Laplacian\nCost E_c can be written as (check why)\nE_c = (Lx, x)\nwhere L is the graph Laplacian, which is defined as a symmetric matrix with\nL_{ii} = \\mbox{degree of node $i$},\nL_{ij} = -1, \\quad \\mbox{if $i \\ne j$  and there is an edge},\nand 0 otherwise.\n\nRows of L sum to zero, thus there is an eigenvalue 0 and gives trivial eigenvector of all ones.\nEigenvalues are non-negative (why?)."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#partitioning-as-an-optimization-problem",
    "href": "lectures/lecture-11/lecture-11.html#partitioning-as-an-optimization-problem",
    "title": "Questions?",
    "section": "Partitioning as an optimization problem",
    "text": "Partitioning as an optimization problem\n\nMinimization of E_c with the mentioned constraints leads to a partitioning that tries to minimize number of edges in a separator, while keeping the partition balanced.\nWe now relax the integer quadratic programming to the continuous quadratic programming\n\nE_c(x) = (Lx, x)\\to \\min_{\\substack{x^\\top e =0, \\\\ \\|x\\|_2^2 = n}}"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator",
    "href": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator",
    "title": "Questions?",
    "section": "From Fiedler vector to separator",
    "text": "From Fiedler vector to separator\n\nThe solution to the minimization problem is given by the eigenvector (called Fiedler vector) corresponding to the second smallest eigenvalue of the graph Laplacian. Indeed,\n\n\n    \\min_{\\substack{x^\\top e =0, \\\\ \\|x\\|_2^2 = n}} (Lx, x) = n \\cdot \\min_{{x^\\top e =0}} \\frac{(Lx, x)}{(x, x)} = n \\cdot \\min_{{x^\\top e =0}} R(x), \\quad R(x) \\text{ is the Rayleigh quotient}\n\n\nSince e is the eigenvector, corresponding to the smallest eigenvalue, on the space x^\\top e =0 we get the second minimal eigevalue.\nThe sign x_i indicates the partitioning.\nIn computations, we need to find out, how to find this second minimal eigenvalue –– we at least know about power method, but it finds the largest. We will discuss iterative methods for eigenvalue problems later in our course.\nThis is the main goal of the iterative methods for large-scale linear problems, and can be achieved via few matrix-by-vector products.\n\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport networkx as nx\nkn = nx.read_gml('karate.gml')\nprint(\"Number of vertices = {}\".format(kn.number_of_nodes()))\nprint(\"Number of edges = {}\".format(kn.number_of_edges()))\nnx.draw_networkx(kn, node_color=\"red\") #Draw the graph\n\nNumber of vertices = 34\nNumber of edges = 78\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as spsplin\nLaplacian = nx.laplacian_matrix(kn).asfptype()\nplt.spy(Laplacian, markersize=5)\nplt.title(\"Graph laplacian\")\nplt.axis(\"off\")\nplt.show()\neigval, eigvec = spsplin.eigsh(Laplacian, k=3, which=\"SM\")\nprint(\"The 2 smallest eigenvalues =\", eigval)\n\n\n\n\n\n\n\n\nThe 2 smallest eigenvalues = [2.52665709e-17 4.68525227e-01 9.09247664e-01]\n\n\n\n#plt.scatter(np.arange(len(eigvec[:, 1])), np.sign(eigvec[:, 1]))\nplt.scatter(np.arange(len(eigvec[:, 1])), eigvec[:, 1])\n\nplt.show()\nprint(\"Sum of elements in Fiedler vector = {}\".format(np.sum(eigvec[:, 1].real)))\n\n\n\n\n\n\n\n\nSum of elements in Fiedler vector = -8.493206138382448e-15\n\n\n\nnx.draw_networkx(kn, node_color=np.sign(eigvec[:, 1]))\n\n\n\n\n\n\n\n\n\nSummary on demo\n\nHere we call SciPy sparse function to find fixed number of eigenvalues (and eigenvectors) that are smallest (other options are possible)\nDetails of the underlying method we will discuss soon\nFiedler vector gives simple separation of the graph\nTo separate graph on more than two parts you should use eigenvectors of laplacian as feature vectors and run some clustering algorithm, e.g. k-means"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator-1",
    "href": "lectures/lecture-11/lecture-11.html#from-fiedler-vector-to-separator-1",
    "title": "Questions?",
    "section": "From Fiedler vector to separator",
    "text": "From Fiedler vector to separator\n\nElements of eigenvector v corresponding to the second smallest eigenvalue of the Laplacian indicate the partitioning of vertices\nIf we select some small positive \\tau &gt;0, then we can split the vertices in three groups\n\nv_i &lt; -\\tau\nv_i \\in [-\\tau, \\tau]\nv_i &gt; \\tau\n\nAfter that the separator is composed with the vertices corresponding to elements of v such that v_i \\in [-\\tau, \\tau]\nThe size of separator can be tuned by the magnitude of \\tau\nThe distribution of elements in v is important to identify the size of separator\n\n\nFiedler vector and algebraic connectivity of a graph\nDefinition. The algebraic connectivity of a graph is the second-smallest eigenvalue of the Laplacian matrix.\nClaim. The algebraic connectivity of a graph is greater than 0 if and only if a graph is a connected graph."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#practical-problems",
    "href": "lectures/lecture-11/lecture-11.html#practical-problems",
    "title": "Questions?",
    "section": "Practical problems",
    "text": "Practical problems\nComputing bisection recursively is expensive.\nAs an alternative, one typically computes multilevel bisection that consists of 3 phases.\n\nGraph coarsening: From a given graph, we join vertices into larger nodes, and get sequences of graphs G_1, \\ldots, G_m.\nAt the coarse level, we do high-quality bisection\nThen, we do uncoarsening: we propagate the splitting from G_k to G_{k-1} and improve the quality of the split by local optimization algorithms (refinement)."
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#practical-problems-2",
    "href": "lectures/lecture-11/lecture-11.html#practical-problems-2",
    "title": "Questions?",
    "section": "Practical problems (2)",
    "text": "Practical problems (2)\n\nOnce the permutation has been computed, we need to implement the elimination, making use of efficient computational kernels.\nIf in the elemination we will be able to get the elements into blocks, we will be able to use BLAS-3 computations.\nIt is done by supernodal data structures:\nIf adjacent rows have the same sparsity structure, they can be stored in blocks:\nAlso, we can use such structure in efficient computations!\n\nDetails in this survey"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#take-home-message",
    "href": "lectures/lecture-11/lecture-11.html#take-home-message",
    "title": "Questions?",
    "section": "Take home message",
    "text": "Take home message\n\nSparse matrices & graphs ordering\nOrdering is important for LU fill-in: more details in the next lecture\nMarkowitz pivoting and minimal degree ordering\nOrderings from SciPy sparse package\nSeparators and how do they help in fill-in minimization\nNested dissection idea\nFiedler vector and spectral bipartitioning"
  },
  {
    "objectID": "lectures/lecture-11/lecture-11.html#plan-for-the-next-lecture",
    "href": "lectures/lecture-11/lecture-11.html#plan-for-the-next-lecture",
    "title": "Questions?",
    "section": "Plan for the next lecture",
    "text": "Plan for the next lecture\n\nBasic iterative methods for solving large linear systems\nConvergence\nAcceleration"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#recap-of-the-previous-lecture",
    "href": "lectures/lecture-7/lecture-7.html#recap-of-the-previous-lecture",
    "title": "",
    "section": "Recap of the previous lecture",
    "text": "Recap of the previous lecture\n\nEigenvectors and eigenvalues\nCharacteristic polynomial and why it is a bad idea\nPower method to find leading (maximum absolute value) eigenvalue and eigenvector\nGershgorin theorem\nSchur theorem: A = U T U^*\nNormal matrices: A^* A = A A^*\nAdvanced topic: pseudospectrum"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#today-lecture",
    "href": "lectures/lecture-7/lecture-7.html#today-lecture",
    "title": "",
    "section": "Today lecture",
    "text": "Today lecture\n\nToday we will talk about matrix factorizations as general tool\nBasic matrix factorizations in numerical linear algebra:\n\nLU decomposition and Gaussian elimination — already covered\nQR decomposition and Gram-Schmidt algorithm\nSchur decomposition and QR-algorithm\nMethods for computing SVD decomposition in the second part\n\nWe already introduced QR decomposition some time ago, but now we are going to discuss it in more details."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#general-concept-of-matrix-factorization",
    "href": "lectures/lecture-7/lecture-7.html#general-concept-of-matrix-factorization",
    "title": "",
    "section": "General concept of matrix factorization",
    "text": "General concept of matrix factorization\n\nIn numerical linear algebra we need to solve different tasks, for example:\n\nSolve linear systems Ax = f\nCompute eigenvalues / eigenvectors\nCompute singular values / singular vectors\nCompute inverses, even sometimes determinants\nCompute matrix functions like \\exp(A), \\cos(A) (these are not elementwise functions)\n\nIn order to do this, we represent the matrix as a sum and/or product of matrices with simpler structure, such that we can solve mentioned tasks faster / in a more stable form.\nWhat is a simpler structure?"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#what-is-a-simpler-structure",
    "href": "lectures/lecture-7/lecture-7.html#what-is-a-simpler-structure",
    "title": "",
    "section": "What is a simpler structure",
    "text": "What is a simpler structure\n\nWe already encountered several classes of matrices with structure.\nFor dense matrices the most important classes are\n\nunitary matrices\nupper/lower triangular matrices\ndiagonal matrices"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#other-classes-of-structured-matrices",
    "href": "lectures/lecture-7/lecture-7.html#other-classes-of-structured-matrices",
    "title": "",
    "section": "Other classes of structured matrices",
    "text": "Other classes of structured matrices\n\nFor sparse matrices the sparse constraints are often included in the factorizations.\nFor Toeplitz matrices an important class of matrices is the class of matrices with low displacement rank, which is based on the low-rank matrices.\nThe class of low-rank matrices and block low-rank matrices appears in many applications."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#plan",
    "href": "lectures/lecture-7/lecture-7.html#plan",
    "title": "",
    "section": "Plan",
    "text": "Plan\nThe plan for today’s lecture is to discuss the decompositions one-by-one and point out: - How to compute a particular decomposition - When the decomposition exists - What is done in real life (LAPACK)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#decompositions-we-want-to-discuss-today",
    "href": "lectures/lecture-7/lecture-7.html#decompositions-we-want-to-discuss-today",
    "title": "",
    "section": "Decompositions we want to discuss today",
    "text": "Decompositions we want to discuss today\n\nLU factorization & Cholesky factorization — quick reminder, already done.\nQR decomposition and Gram-Schmidt algorithm\nOne slide about the SVD (more details in the second part of today lecture)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#plu-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#plu-decomposition",
    "title": "",
    "section": "PLU decomposition",
    "text": "PLU decomposition\n\nAny nonsingular matrix can be factored as\n\n A = P L U, \nwhere P is a permutation matrix, L is a lower triangular matrix, U is an upper triangular.\n\nMain goal of the LU decomposition is to solve linear systems, because\n\n A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, \nand this reduces to the solution of two linear systems\n L y = f,  \\quad U x = y \nwith lower and upper triangular matrices respectively.\nQ: what is the complexity of solving these linear systems?"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#positive-definite-matrices-and-cholesky-decomposition-reminder",
    "href": "lectures/lecture-7/lecture-7.html#positive-definite-matrices-and-cholesky-decomposition-reminder",
    "title": "",
    "section": "Positive definite matrices and Cholesky decomposition, reminder",
    "text": "Positive definite matrices and Cholesky decomposition, reminder\nIf the matrix is Hermitian positive definite, i.e. \n A = A^*, \\quad (Ax, x) &gt; 0, \\quad x \\ne 0, \nthen it can be factored as\n A = RR^*, \nwhere R is lower triangular.\nWe will need this for the QR decomposition."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition",
    "title": "",
    "section": "QR decomposition",
    "text": "QR decomposition\n\nThe next decomposition: QR decomposition.\nAgain from the name it is clear that a matrix is represented as a product\n\n\n    A = Q R,\n\nwhere Q is an column orthogonal (unitary) matrix and R is upper triangular.\n\nThe matrix sizes: Q is n \\times m, R is m \\times m if n\\geq m. See our poster for visualization of QR decomposition\nQR decomposition is defined for any rectangular matrix."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-applications",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-applications",
    "title": "",
    "section": "QR decomposition: applications",
    "text": "QR decomposition: applications\nThis decomposition plays a crucial role in many problems: - Computing orthogonal bases in a linear space - Used in the preprocessing step for the SVD - QR-algorithm for the computation of eigenvectors and eigenvalues (one of the 10 most important algorithms of the 20th century) is based on the QR decomposition - Solving overdetermined systems of linear equations (linear least-squares problem)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-and-least-squares-reminder",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-and-least-squares-reminder",
    "title": "",
    "section": "QR decomposition and least squares, reminder",
    "text": "QR decomposition and least squares, reminder\n\nSuppose we need to solve\n\n \\Vert A x - b \\Vert_2 \\rightarrow \\min_x, \nwhere A is n \\times m, n \\geq m.\n\nThen we factorize\n\n A = Q R, \nand use equation for pseudo-inverse matrix in the case of the full rank matrix A:\n x = A^{\\dagger}b = (A^*A)^{-1}A^*b = ((QR)^*(QR))^{-1}(QR)^*b = (R^*Q^*QR)^{-1}R^*Q^*b = R^{-1}Q^*b. \nthus x can be recovered from\nR x = Q^*b\n\nNote that this is a square system of linear equations with lower triangular matrix. What is the complexity of solving this system?"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#existence-of-qr-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#existence-of-qr-decomposition",
    "title": "",
    "section": "Existence of QR decomposition",
    "text": "Existence of QR decomposition\nTheorem. Every rectangular n \\times m matrix has a QR decomposition.\nThere are several ways to prove it and compute it:\n\nTheoretical: using the Gram matrices and Cholesky factorization\nGeometrical: using the Gram-Schmidt orthogonalization\nPractical: using Householder/Givens transformations\n\n\nProof using Cholesky decomposition\nIf we have the representation of the form\nA = QR,\nthen A^* A = ( Q R)^* (QR)  = R^* (Q^* Q) R = R^* R, the matrix A^* A is called Gram matrix, and its elements are scalar products of the columns of A.\n\n\nProof using Cholesky decomposition (full column rank)\n\nAssume that A has full column rank. Then, it is simple to show that A^* A is positive definite:\n\n (A^* A y, y) = (Ay, Ay) = \\Vert Ay \\Vert^2  &gt; 0, \\quad y\\not = 0. \n\nTherefore, A^* A = R^* R always exists.\nThen the matrix Q = A R^{-1} is unitary:\n\n (A R^{-1})^* (AR^{-1})= R^{-*} A^* A R^{-1} = R^{-*} R^* R R^{-1} = I. \n\n\nProof using Cholesky decomposition (rank-deficient case)\n\nWhen an n \\times m matrix does not have full column rank, it is said to be rank-deficient.\nThe QR decomposition, however, also exists.\nFor any rank-deficient matrix there is a sequence of full-column rank matrices A_k such that A_k \\rightarrow A (why?).\nEach A_k can be decomposed as A_k = Q_k R_k.\nThe set of all unitary matrices is compact, thus there exists a converging subsequence Q_{n_k} \\rightarrow Q (why?), and Q^* A_k \\rightarrow Q^* A = R, which is triangular."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#stability-of-qr-decomposition-via-cholesky-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#stability-of-qr-decomposition-via-cholesky-decomposition",
    "title": "",
    "section": "Stability of QR decomposition via Cholesky decomposition",
    "text": "Stability of QR decomposition via Cholesky decomposition\n\nSo, the simplest way to compute QR decomposition is then\n\nA^* A = R^* R,\nand\nQ = A R^{-1}.\n\nIt is a bad idea for numerical stability. Let us do some demo (for a submatrix of the Hilbert matrix).\n\n\nimport numpy as np\nn = 10\nr = 5\na = [[1.0 / (i + j + 0.5) for i in range(r)] for j in range(n)]\na = np.array(a)\nq, Rmat = np.linalg.qr(a)\ne = np.eye(r)\nprint('Built-in QR orth', np.linalg.norm(np.dot(q.T, q) - e))\ngram_matrix = a.T.dot(a)\nRmat1 = np.linalg.cholesky(gram_matrix)\n#q1 = np.dot(a, np.linalg.inv(Rmat1.T))\nq1 = np.linalg.solve(Rmat1, a.T).T\nprint('Via Cholesky:', np.linalg.norm(np.dot(q1.T, q1) - e))\n\nBuilt-in QR orth 6.764143318685874e-16\nVia Cholesky: 1.528398708004441e-09"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#second-way-gram-schmidt-orthogonalization",
    "href": "lectures/lecture-7/lecture-7.html#second-way-gram-schmidt-orthogonalization",
    "title": "",
    "section": "Second way: Gram-Schmidt orthogonalization",
    "text": "Second way: Gram-Schmidt orthogonalization\n\nQR decomposition is a mathematical way of writing down the Gram-Schmidt orthogonalization process.\n\nGiven a sequence of vectors a_1, \\ldots, a_m we want to find orthogonal basis q_1, \\ldots, q_m such that every a_i is a linear combination of such vectors.\n\nGram-Schmidt: 1. q_1 := a_1/\\Vert a_1 \\Vert 2. q_2 := a_2 - (a_2, q_1) q_1, \\quad q_2 := q_2/\\Vert q_2 \\Vert 3. q_3 := a_3 - (a_3, q_1) q_1 - (a_3, q_2) q_2, \\quad q_3 := q_3/\\Vert q_3 \\Vert 4. And so on\nNote that the transformation from Q to A has triangular structure, since from the k-th vector we subtract only the previous ones. It follows from the fact that the product of triangular matrices is a triangular matrix."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#modified-gram-schmidt",
    "href": "lectures/lecture-7/lecture-7.html#modified-gram-schmidt",
    "title": "",
    "section": "Modified Gram-Schmidt",
    "text": "Modified Gram-Schmidt\n\nGram-Schmidt can be very unstable (i.e., the produced vectors will be not orthogonal, especially if q_k has small norm).\nThis is called loss of orthogonality.\nThere is a remedy, called modified Gram-Schmidt method. Instead of doing\n\nq_k := a_k - (a_k, q_1) q_1 - \\ldots - (a_k, q_{k-1}) q_{k-1}\nwe do it step-by-step. First we set q_k := a_k and orthogonalize sequentially:\n\n   q_k := q_k - (q_k, q_1)q_1, \\quad q_k := q_{k} - (q_k,q_2)q_2, \\ldots\n\n\nIn exact arithmetic, it is the same. In floating point it is absolutely different!\nNote that the complexity is \\mathcal{O}(nm^2) operations\n\n\nn = 100 \na = np.random.rand(n)\nb = a + 1e-9*np.random.randn(n)\na = a/np.linalg.norm(a)\nc = b - np.dot(b, a)*a\nc = c/np.linalg.norm(c)\nprint(np.dot(c, a))\nc = c - np.dot(c, a)*a\nc = c/np.linalg.norm(c)\nprint(np.dot(c, a))\n\n1.576475053416404e-07\n-2.42861286636753e-17"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-the-almost-practical-way",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-the-almost-practical-way",
    "title": "",
    "section": "QR decomposition: the (almost) practical way",
    "text": "QR decomposition: the (almost) practical way\n\nIf A = QR, then\n\n R = Q^* A, \nand we need to find a certain orthogonal matrix Q that brings a matrix into upper triangular form.\n- For simplicity, we will look for an n \\times n matrix such that\n Q^* A = \\begin{bmatrix} * & * & *  \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ & 0_{(n-m) \\times m} \\end{bmatrix} \n\nWe will do it column-by-column.\n\nFirst, we find a Householder matrix H_1 = (I - 2 uu^{\\top}) such that (we illustrate on a 4 \\times 3 matrix)\n\n H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & * & * \\\\ 0 & * & * \\end{bmatrix} \nThen,\n H_2 H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ 0 & 0 & * \\end{bmatrix}, \nwhere\n H_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & H'_2, \\end{bmatrix} \nand H'_2 is a 3 \\times 3 Householder matrix.\nFinally,\n H_3 H_2 H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ 0 & 0 & 0 \\end{bmatrix}, \nYou can try to implement it yourself, it is simple."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-decomposition-real-life",
    "href": "lectures/lecture-7/lecture-7.html#qr-decomposition-real-life",
    "title": "",
    "section": "QR decomposition: real life",
    "text": "QR decomposition: real life\n\nIn reality, since this is a dense matrix factorization, you should implement the algorithm in terms of blocks (why?).\nInstead of using Householder transformation, we use block Householder transformation of the form\n\nH = I - 2UU^*, \nwhere U^* U = I.\n\nThis allows us to use BLAS-3 operations."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#rank-revealing-qr-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#rank-revealing-qr-decomposition",
    "title": "",
    "section": "Rank-revealing QR-decomposition",
    "text": "Rank-revealing QR-decomposition\n\nThe QR-decomposition can be also used to compute the (numerical) rank of the matrix, see Rank-Revealing QR Factorizations and the Singular Value Decomposition, Y. P. Hong; C.-T. Pan\nIt is done via so-called rank-revealing factorization.\nIt is based on the representation\n\nP A = Q R,\nwhere P is the permutation matrix (it permutes columns), and R has the block form\nR = \\begin{bmatrix} R_{11} & R_{12} \\\\ 0 & R_{22}\\end{bmatrix}.\n\nThe goal is to find P such that the norm of R_{22} is small, so you can find the numerical rank by looking at it.\nAn estimate is \\sigma_{r+1} \\leq \\Vert R_{22} \\Vert_2 (check why)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#summary",
    "href": "lectures/lecture-7/lecture-7.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nLU and QR decompositions can be computed using direct methods in finite amount of operations.\nWhat about Schur form and SVD?\nThey can not be computed by direct methods (why?) they can only be computed by iterative methods.\nAlthough iterative methods still have the same \\mathcal{O}(n^3) complexity in floating point arithmetic thanks to fast convergence."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#schur-form",
    "href": "lectures/lecture-7/lecture-7.html#schur-form",
    "title": "",
    "section": "Schur form",
    "text": "Schur form\n\nRecall that every matrix can be written in the Schur form\n\nA = Q T Q^*,\nwith upper triangular T and unitary Q and this decomposition gives eigenvalues of the matrix (they are on the diagonal of T).\n\nThe first and the main algorithm for computing the Schur form is the QR algorithm."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#qr-algorithm",
    "title": "",
    "section": "QR algorithm",
    "text": "QR algorithm\n\nThe QR algorithm was independently proposed in 1961 by Kublanovskaya and Francis.\n Do not mix QR algorithm and QR decomposition! \nQR decomposition is the representation of a matrix, whereas QR algorithm uses QR decomposition to compute the eigenvalues!"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#way-to-qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#way-to-qr-algorithm",
    "title": "",
    "section": "Way to QR algorithm",
    "text": "Way to QR algorithm\n\nConsider the equation\n\nA = Q T Q^*,\nand rewrite it in the form\n Q T = A Q. \n\nOn the left we can see QR factorization of the matrix AQ.\nWe can use this to derive fixed-point iteration for the Schur form, also known as QR algorithm."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#derivation-of-the-qr-algorithm-as-fixed-point-iteration",
    "href": "lectures/lecture-7/lecture-7.html#derivation-of-the-qr-algorithm-as-fixed-point-iteration",
    "title": "",
    "section": "Derivation of the QR algorithm as fixed-point iteration",
    "text": "Derivation of the QR algorithm as fixed-point iteration\nWe can write down the iterative process\n\n    Q_{k+1} R_{k+1} = A Q_k, \\quad Q_{k+1}^* A = R_{k+1} Q^*_k\n\nIntroduce\nA_k = Q^* _k A Q_k = Q^*_k Q_{k+1} R_{k+1} = \\widehat{Q}_k R_{k+1}\nand the new approximation reads\nA_{k+1} = Q^*_{k+1} A Q_{k+1} = ( Q_{k+1}^* A = R_{k+1} Q^*_k)  = R_{k+1} \\widehat{Q}_k.\nSo we arrive at the standard form of the QR algorithm.\nThe final formulas are then written in the classical QRRQ-form:\n\nStart from A_0 = A.\nCompute QR factorization of A_k = Q_k R_k.\nSet A_{k+1} = R_k Q_k.\n\nIterate until A_k is triangular enough (e.g. norm of subdiagonal part is small enough)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#what-about-the-convergence-and-complexity",
    "href": "lectures/lecture-7/lecture-7.html#what-about-the-convergence-and-complexity",
    "title": "",
    "section": "What about the convergence and complexity",
    "text": "What about the convergence and complexity\nStatement\nMatrices A_k are unitary similar to A\nA_k = Q^*_{k-1} A_{k-1} Q_{k-1} = (Q_{k-1} \\ldots Q_1)^* A (Q_{k-1} \\ldots Q_1)\nand the product of unitary matrices is a unitary matrix.\n\nComplexity of each step is \\mathcal{O}(n^3), if a general QR decomposition is done.\nOur hope is that A_k will be very close to the triangular matrix for suffiently large k.\n\n\nimport jax.numpy as jnp\nn = 40\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\nniters = 10000\nfor k in range(niters):\n    q, rmat = np.linalg.qr(a)\n    a = rmat.dot(q)\nprint('Leading 3x3 block of a:')\nprint(a[:4, :4])\n\nLeading 3x3 block of a:\n[[ 3.08733278e-01  3.08447671e+00  1.39093779e-02 -9.15979870e-02]\n [-3.05714157e+00  2.90031089e-01  1.49436447e-01 -4.41658920e-02]\n [-2.25478613e-21 -1.27296314e-20  5.50815465e-01  3.04169557e+00]\n [-1.45411659e-20 -7.68894785e-21 -3.00877077e+00  5.09624202e-01]]"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#convergence-and-complexity-of-the-qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#convergence-and-complexity-of-the-qr-algorithm",
    "title": "",
    "section": "Convergence and complexity of the QR algorithm",
    "text": "Convergence and complexity of the QR algorithm\n\nThe convergence of the QR algorithm is from the largest eigenvalues to the smallest.\nAt least 2-3 iterations is needed for an eigenvalue.\nEach step is one QR factorization and one matrix-by-matrix product, as a result \\mathcal{O}(n^3) complexity.\n\nQ: does it mean \\mathcal{O}(n^4) complexity totally?\nA: fortunately, not.\n\nWe can speedup the QR algorithm by using shifts, since A_k - \\lambda I has the same Schur vectors.\nWe will discuss these details later"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#convergence-of-qr-algorithm",
    "href": "lectures/lecture-7/lecture-7.html#convergence-of-qr-algorithm",
    "title": "",
    "section": "Convergence of QR algorithm",
    "text": "Convergence of QR algorithm\nDoes QR algorithm always convergence?\nProvide an example."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#counterexample",
    "href": "lectures/lecture-7/lecture-7.html#counterexample",
    "title": "",
    "section": "Counterexample",
    "text": "Counterexample\nFor a matrix A = \\begin{bmatrix} 0 & 1 \\\\\n                                  1 & 0 \\end{bmatrix}\nwe have A_k = A."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#connection-to-orthogonal-iteration",
    "href": "lectures/lecture-7/lecture-7.html#connection-to-orthogonal-iteration",
    "title": "",
    "section": "Connection to orthogonal iteration",
    "text": "Connection to orthogonal iteration\nIn the previous lecture, we considered power iteration, which is A^k v – approximation of the eigenvector.\nThe QR algorithm computes (implicitly) QR-factorization of the matrix A^k:\nA^k = A \\cdot \\ldots \\cdot A = Q_1 R_1 Q_1 R_1 \\ldots = Q_1 Q_2 R_2 Q_2 R_2 \\ldots (R_2 R_1) = \\ldots (Q_1 Q_2 \\ldots Q_k) (R_k \\ldots R_1)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#a-few-words-about-the-svd",
    "href": "lectures/lecture-7/lecture-7.html#a-few-words-about-the-svd",
    "title": "",
    "section": "A few words about the SVD",
    "text": "A few words about the SVD\n\nLast but not least: the singular value decomposition of matrix.\n\nA = U \\Sigma V^*.\n\nWe can compute it via eigendecomposition of\n\nA^* A = V^* \\Sigma^2 V,\nand/or\nAA^* = U^* \\Sigma^2 U\nwith QR algorithm, but it is a bad idea (c.f. Gram matrices).\n\nWe will discuss methods for computing SVD later."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#bonus-cs-cosine-sine-decomposition",
    "href": "lectures/lecture-7/lecture-7.html#bonus-cs-cosine-sine-decomposition",
    "title": "",
    "section": "Bonus: CS (Cosine-Sine) decomposition",
    "text": "Bonus: CS (Cosine-Sine) decomposition\n\nLet Q be square unitary matrix with even number of rows and it is splitted in four equal size blocks\n\n Q = \\begin{bmatrix} Q_{11} & Q_{12} \\\\ Q_{21} & Q_{22} \\end{bmatrix} \n\nThen there exist unitary matrices U_1, U_2, V_1, V_2 such that\n\n \\begin{bmatrix} U_1^* & 0 \\\\ 0 & U_2^* \\end{bmatrix} \\begin{bmatrix} Q_{11} & Q_{12} \\\\ Q_{21} & Q_{22} \\end{bmatrix} \\begin{bmatrix} V_1 & 0 \\\\ 0 & V_2 \\end{bmatrix} = \\begin{bmatrix} C & S \\\\ -S & C \\end{bmatrix}, \nwhere C = \\mathrm{diag}(c) and S = \\mathrm{diag}(s) such that c_i \\geq 0, s_i \\geq 0 and c_i^2 + s_i^2 = 1\n\nQ: how many SVD do we have inside the CS decomposition?\nThe case of rectangular matrix with orthonormal columns\n\n \\begin{bmatrix} U_1^* & 0 \\\\ 0 & U_2^* \\end{bmatrix} \\begin{bmatrix} Q_{1} \\\\ Q_{2} \\end{bmatrix} V = \\begin{bmatrix} C \\\\ S \\end{bmatrix} \n\nThe algorithm for computing this decomposition is presented here\nThis decomposition naturally arises in the problem of finding distances and angles between subspaces"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#summary-1",
    "href": "lectures/lecture-7/lecture-7.html#summary-1",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nQR decomposition and Gram-Schmidt algorithm, reduction to a simpler form by Householder transformations\nSchur decomposition and QR algorithm"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#next-steps",
    "href": "lectures/lecture-7/lecture-7.html#next-steps",
    "title": "",
    "section": "Next steps",
    "text": "Next steps\n\nEfficient implementation of QR algorithm and its convergence\nEfficient computation of the SVD: 4 algorithms\nMore applications of the SVD\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#recap-of-the-previous-part",
    "href": "lectures/lecture-8/lecture-8.html#recap-of-the-previous-part",
    "title": "Questions?",
    "section": "Recap of the previous part",
    "text": "Recap of the previous part\n\nQR decomposition and Gram-Schmidt algorithm\nSchur decomposition and QR-algorithm (basic)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#plan-for-today",
    "href": "lectures/lecture-8/lecture-8.html#plan-for-today",
    "title": "Questions?",
    "section": "Plan for today",
    "text": "Plan for today\nToday we will talk about:\n\nAlgorithms for the symmetric eigenvalue problems\n\nQR algorithm (in more details)\nDivide-and-Conquer\nbisection\n\nAlgorithms for SVD computation"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#schur-form-computation",
    "href": "lectures/lecture-8/lecture-8.html#schur-form-computation",
    "title": "Questions?",
    "section": "Schur form computation",
    "text": "Schur form computation\n\nRecall that we are trying to avoid \\mathcal{O}(n^3) complexity for each iteration.\nThe idea is to make a matrix have a simpler structure so that each step of QR algorithm becomes cheaper.\nIn case of a general matrix we can use the Hessenberg form."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#hessenberg-form",
    "href": "lectures/lecture-8/lecture-8.html#hessenberg-form",
    "title": "Questions?",
    "section": "Hessenberg form",
    "text": "Hessenberg form\nThe matrix A is said to be in the Hessenberg form, if\na_{ij} = 0, \\quad \\mbox{if } i \\geq j+2.\nH = \\begin{bmatrix} * & * & * & * & * \\\\ * & * & * & * & * \\\\ 0 & * & * & * & *\\\\ 0 & 0 & * & * & *\\\\ 0 & 0 & 0 & * & * \\\\ \\end{bmatrix}."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#reduction-any-matrix-to-hessenberg-form",
    "href": "lectures/lecture-8/lecture-8.html#reduction-any-matrix-to-hessenberg-form",
    "title": "Questions?",
    "section": "Reduction any matrix to Hessenberg form",
    "text": "Reduction any matrix to Hessenberg form\n\nBy applying Householder reflections we can reduce any matrix to the Hessenberg form\n\nU^* A U = H\n\nThe only difference with Schur decomposition is that we have to map the first column to the vector with two non-zeros, and the first element is not changed.\nThe computational cost of such reduction is \\mathcal{O}(n^3) operations.\nIn a Hessenberg form, computation of one iteration of the QR algorithm costs \\mathcal{O}(n^2) operations (e.g. using Givens rotations, how?), and the Hessenberg form is preserved by the QR iteration (check why)."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#symmetric-hermitian-case",
    "href": "lectures/lecture-8/lecture-8.html#symmetric-hermitian-case",
    "title": "Questions?",
    "section": "Symmetric (Hermitian) case",
    "text": "Symmetric (Hermitian) case\n\nIn the symmetric case, we have A = A^*, then H = H^* and the upper Hessenberg form becomes tridiagonal matrix.\nFrom now on we will talk about the case of symmetric tridiagonal form.\nAny symmetric (Hermitian) matrix can be reduced to the tridiagonal form by Householder reflections.\nKey point is that tridiagonal form is preserved by the QR algorithm, and the cost of one step can be reduced to \\mathcal{O}(n)!"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#qr-algorithm-iterations",
    "href": "lectures/lecture-8/lecture-8.html#qr-algorithm-iterations",
    "title": "Questions?",
    "section": "QR algorithm: iterations",
    "text": "QR algorithm: iterations\n\nThe iterations of the QR algorithm have the following form:\n\nA_k = Q_k R_k, \\quad A_{k+1} = R_k Q_k.\n\nIf A_0 = A is  tridiagonal symmetric matrix , this form is preserved by the QR algorithm.\n\nLet us see..\n\n%matplotlib inline\nimport jax.numpy as jnp\nimport jax\nimport matplotlib.pyplot as plt\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n#Generate a random tridiagonal matrix\n\nn = 20\nd = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n\nsub_diag = jax.random.normal(jax.random.PRNGKey(1), (n-1,))\n\nmat = jnp.diag(d) + jnp.diag(sub_diag, -1) + jnp.diag(sub_diag, 1)\nmat1 = jnp.abs(mat)\nmat1 = mat1/jnp.max(mat1.flatten())\nplt.spy(mat)\nq, r = jnp.linalg.qr(mat)\nplt.figure()\nb = r.dot(q)\n# b[abs(b) &lt;= 1e-12] = 0\nb = b.at[abs(b) &lt;= 1e-12].set(0.0)\n#b = jax.ops.index_update(b, jax.ops.index[abs(b) &lt;= 1e-12], 0)\nplt.spy(b)\n#plt.figure()\n#plt.imshow(np.abs(r.dot(q)))\nb[0, :]\n\nArray([-1.47168069, -0.75157939,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ],      dtype=float64)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#tridiagonal-form",
    "href": "lectures/lecture-8/lecture-8.html#tridiagonal-form",
    "title": "Questions?",
    "section": "Tridiagonal form",
    "text": "Tridiagonal form\n\nIn the tridiagonal form, you do not have to compute the Q matrix: you only have to compute the triadiagonal part that appears after the iterations\n\nA_k = Q_k R_k, \\quad A_{k+1}  = R_k Q_k,\nin the case when A_k = A^*_k and is also tridiagonal.\n\nSuch matrix is defined by \\mathcal{O}(n) parameters; computation of the QR is more complicated, but it is possible to compute A_{k+1} directly without computing Q_k.\nThis is called implicit QR-step."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#theorem-on-implicit-qr-iteration",
    "href": "lectures/lecture-8/lecture-8.html#theorem-on-implicit-qr-iteration",
    "title": "Questions?",
    "section": "Theorem on implicit QR iteration",
    "text": "Theorem on implicit QR iteration\nAll the implicit QR algorithms are based on the following theorem:\nLet\nQ^* A Q = H\nbe an irreducible upper Hessenberg matrix. Then, the first column of the matrix Q defines all of its other columns. It can be found from the equation\nA Q = Q H."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#convergence-of-the-qr-algorithm",
    "href": "lectures/lecture-8/lecture-8.html#convergence-of-the-qr-algorithm",
    "title": "Questions?",
    "section": "Convergence of the QR-algorithm",
    "text": "Convergence of the QR-algorithm\n\nThe convergence of the QR-algorithm is a very delicate issue (see E. E. Tyrtyshnikov, “Brief introduction to numerical analysis” for details).\n\nSummary. If we have a decomposition of the form\nA = X \\Lambda X^{-1}, \\quad A = \\begin{bmatrix}A_{11} & A_{12} \\\\ A_{21} & A_{22}\\end{bmatrix}\nand\n\n\\Lambda = \\begin{bmatrix} \\Lambda_1 & 0 \\\\\n0 & \\Lambda_2 \\end{bmatrix}, \\quad \\lambda(\\Lambda_1)=\\{\\lambda_1,\\dots,\\lambda_m\\}, \\ \\lambda(\\Lambda_2)=\\{\\lambda_{m+1},\\dots,\\lambda_r\\},\n\nand there is a gap between the eigenvalues of \\Lambda_1 and \\Lambda_2 (|\\lambda_1|\\geq \\dots \\geq |\\lambda_m| &gt; |\\lambda_{m+1}| \\geq\\dots \\geq |\\lambda_r| &gt;0), then the A^{(k)}_{21} block of A_k in the QR-iteration goes to zero with\n\\Vert A^{(k)}_{21} \\Vert \\leq  C q^k, \\quad q = \\left| \\frac{\\lambda_{m+1}}{\\lambda_{m}}  \\right |,\nwhere m is the size of \\Lambda_1.\nSo we need to increase the gap! It can be done by the QR algorithm with shifts."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#qr-algorithm-with-shifts",
    "href": "lectures/lecture-8/lecture-8.html#qr-algorithm-with-shifts",
    "title": "Questions?",
    "section": "QR-algorithm with shifts",
    "text": "QR-algorithm with shifts\nA_{k} - s_k I = Q_k R_k, \\quad A_{k+1} = R_k Q_k + s_k I\nThe convergence rate for a shifted version is then\n\\left| \\frac{\\lambda_{m+1} - s_k}{\\lambda_{m} - s_k}  \\right |,\nwhere \\lambda_m is the m-th largest eigenvalue of the matrix in modulus. - If the shift is close to the eigenvalue, then the convergence speed is better. - There are different strategies to choose shifts. - Introducing shifts is a general strategy to improve convergence of iterative methods of finding eigenvalues. - In next slides we will illustrate how to choose shift on a simpler algorithm."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#shifts-and-power-method",
    "href": "lectures/lecture-8/lecture-8.html#shifts-and-power-method",
    "title": "Questions?",
    "section": "Shifts and power method",
    "text": "Shifts and power method\n\nRemember the power method for the computation of the eigenvalues.\n\nx_{k+1} := A x_k, \\quad x_{k+1} := \\frac{x_{k+1}}{\\Vert x_{k+1} \\Vert}.\n\nIt converges to the eigenvector corresponding to the largest eigenvalue in modulus.\nThe convergence can be very slow.\nLet us try to use shifting strategy. If we shift the matrix as\n\n  A := A - \\lambda_k I\nand the corresponding eigenvalue becomes small (but we need large). - That is not what we wanted!"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#inverse-iteration-and-rayleigh-quotient-iteration",
    "href": "lectures/lecture-8/lecture-8.html#inverse-iteration-and-rayleigh-quotient-iteration",
    "title": "Questions?",
    "section": "Inverse iteration and Rayleigh quotient iteration",
    "text": "Inverse iteration and Rayleigh quotient iteration\n\nTo make a small eigenvalue large, we need to invert the matrix, and that gives us inverse iteration\n\nx_{k+1} = (A - \\lambda I)^{-1} x_k,\nwhere \\lambda is the shift which is approximation to the eigenvalue that we want.\n\nAs it was for the power method, the convergence is linear.\nTo accelerate convergence one can use the Rayleigh quotient iteration (inverse iteration with adaptive shifts) which is given by the selection of the adaptive shift:\n\nx_{k+1} = (A - \\lambda_k I)^{-1} x_k,\n\\lambda_k = \\frac{(Ax_k, x_k)}{(x_k, x_k)}\n\nIn the symmetric case A = A^* the convergence is locally cubic and locally quadratic otherwise."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-1",
    "href": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-1",
    "title": "Questions?",
    "section": "Singular values and eigenvalues (1)",
    "text": "Singular values and eigenvalues (1)\n\nNow let us talk about singular values and eigenvalues.\nSVD\n\nA = U \\Sigma V^*\nexists for any matrix.\n\nIt can be also viewed as a reduction of a given matrix to the diagonal form by means of two-sided unitary transformations:\n\n\\Sigma = U^* A V.\n\nBy two-sided Householder transformation we can reduce any matrix to the bidiagonal form B (how?)."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-2",
    "href": "lectures/lecture-8/lecture-8.html#singular-values-and-eigenvalues-2",
    "title": "Questions?",
    "section": "Singular values and eigenvalues (2)",
    "text": "Singular values and eigenvalues (2)\n\nImplicit QR algorithm (with shifts) gives the way of computing the eigenvalues (and Schur form).\nBut we cannot apply QR algorithm directly to the bidiagonal matrix, as it is not diagonalizable in general case.\nHowever, the problem of the computation of the SVD can be reduced to the symmetric eigenvalue problem in two ways:\n\n\nWork with the tridiagonal matrix\n\nT = B^* B\n\nWork with the extended matrix\n\nT = \\begin{bmatrix} 0 & B \\\\ B^* & 0 \\end{bmatrix}\n\nThe case 1 is OK if you do not form T directly!\nThus, the problem of computing singular values can be reduced to the problem of the computation of the eigenvalues of symmetric tridiagonal matrix."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#algorithms-for-the-sev-symmetric-eigenvalue-problem",
    "href": "lectures/lecture-8/lecture-8.html#algorithms-for-the-sev-symmetric-eigenvalue-problem",
    "title": "Questions?",
    "section": "Algorithms for the SEV (symmetric eigenvalue problem)",
    "text": "Algorithms for the SEV (symmetric eigenvalue problem)\nDone: - QR algorithm: the “gold standard” of the eigenvalue computations - RQI-iteration: Rayleigh quotient iteration is implicitly performed at each step of the QR algorithm\nNext slides: - Divide-and-conquer - Bisection method - Jacobi method"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#divide-and-conquer",
    "href": "lectures/lecture-8/lecture-8.html#divide-and-conquer",
    "title": "Questions?",
    "section": "Divide-and-conquer",
    "text": "Divide-and-conquer\n\nSuppose we have a tridiagonal matrix, and we split it into two blocks:\n\nT = \\begin{bmatrix} T'_1 & B \\\\ B^{\\top} & T'_2 \\end{bmatrix}\n\nWe can write the matrix T as\n\nT = \\begin{bmatrix} T_1 & 0 \\\\ 0 & T_2 \\end{bmatrix} + b_m v v^*\nwhere vv^* is rank 1 matrix, v = (0,\\dots,0,1,1,0,\\dots,0)^T.\n\nSuppose we have decomposed T_1 and T_2 already:\n\nT_1 = Q_1 \\Lambda_1 Q^*_1, \\quad T_2 = Q_2 \\Lambda_2 Q^*_2\n\nThen (check),\n\n\\begin{bmatrix} Q^*_1 & 0 \\\\ 0 & Q^*_2 \\end{bmatrix} T\\begin{bmatrix} Q_1 & 0 \\\\ 0 & Q_2 \\end{bmatrix} = D + \\rho u u^{*}, \\quad D = \\begin{bmatrix} \\Lambda_1 & 0 \\\\ 0 & \\Lambda_2\\end{bmatrix}\n\nI.e. we have reduced the problem to the problem of the computation of the eigenvalues of  diagonal plus low-rank matrix"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#diagonal-plus-low-rank-matrix",
    "href": "lectures/lecture-8/lecture-8.html#diagonal-plus-low-rank-matrix",
    "title": "Questions?",
    "section": "Diagonal-plus-low-rank matrix",
    "text": "Diagonal-plus-low-rank matrix\nIt is tricky to compute the eigenvalues of the matrix\nD + \\rho u u^* \nThe characteristic polynomial has the form\n\\det(D + \\rho uu^* - \\lambda I) = \\det(D - \\lambda I)\\det(I + \\rho (D - \\lambda I)^{-1} uu^*) = 0.\nThen (prove!!)\n\\det(I + \\rho (D - \\lambda I)^{-1} uu^*) = 1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{d_i - \\lambda} = 0\nHint: find \\det(I + w u^*) using the fact that \\text{det}(C) = \\prod_{i=1}^n\\lambda_i(C) and \\text{trace}(C) = \\sum_{i=1}^n \\lambda_i."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#characteristic-equation",
    "href": "lectures/lecture-8/lecture-8.html#characteristic-equation",
    "title": "Questions?",
    "section": "Characteristic equation",
    "text": "Characteristic equation\n1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{d_i - \\lambda} = 0\nHow to find the roots?\n\nimport jax.numpy as jnp\n\nlm = jnp.array([1, 2, 3, 4])\nM = len(lm)\nD = jnp.array(lm)\na = jnp.min(lm)\nb = jnp.max(lm)\nt = jnp.linspace(-1, 6, 1000)\nu = 0.5 * jnp.ones(M)\nrho = 1\ndef fun(lam):\n    return 1 + rho * jnp.sum(u**2/(D - lam))\nres = [fun(lam) for lam in t]\nplt.figure(figsize=(10,8))\nplt.plot(t, res, 'k')\n#plt.plot(jnp.zeros_like(t))\nplt.ylim([-6, 6])\nplt.tight_layout()\nplt.yticks(fontsize=24)\nplt.xticks(fontsize=24)\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nThe function has only one root at [d_i, d_{i+1}]\nWe have proved, by the way, the Cauchy interlacing theorem (what happens to the eigenvalues under rank-1 perturbation)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#how-to-find-the-root",
    "href": "lectures/lecture-8/lecture-8.html#how-to-find-the-root",
    "title": "Questions?",
    "section": "How to find the root",
    "text": "How to find the root\n\nA Newton method will fail (draw a picture with a tangent line).\nNote that Newton method is just approximation of a function f(\\lambda) by a linear function.\nMuch better approximation is the hyperbola:\n\nf(\\lambda) \\approx c_0 + \\frac{c_1}{d_i - \\lambda} + \\frac{c_2}{d_{i+1} - \\lambda}.\n\nTo fit the coefficients, we have to evaluate f(\\lambda) and f'(\\lambda) in the particular point.\nAfter that, the approximation can be recovered from solving quadratic equation"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#important-issues",
    "href": "lectures/lecture-8/lecture-8.html#important-issues",
    "title": "Questions?",
    "section": "Important issues",
    "text": "Important issues\n\nFirst, stability: this method was abandoned for a long time due to instability of the computation of the eigenvectors.\nIn the recursion, we need to compute the eigenvectors of the D + \\rho uu^* matrix.\nThe exact expression for the eigenvectors is just (let us check!)\n\n(D - \\alpha_i I)^{-1}u, where \\alpha_i is the computed root.\n\nThe reason of instability:\n\nif \\alpha_i and \\alpha_{i+1} are close, then the corresponding eigenvectors are collinear, but they have to be orthogonal\nif \\alpha_i and \\alpha_{i+1} are close, then they close to d_i, therefore matrices D - \\alpha_i I and D - \\alpha_{i+1} I are close to singular"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#loewner-theorem",
    "href": "lectures/lecture-8/lecture-8.html#loewner-theorem",
    "title": "Questions?",
    "section": "Loewner theorem",
    "text": "Loewner theorem\n\nThe solution came is to use a strange Loewner theorem:\n\nIf \\alpha_i and d_i satisfy the interlacing theorem\nd_n &lt; \\alpha_n &lt; \\ldots &lt; d_{i+1} &lt; \\alpha_{i+1} \\ldots\nThen there exists a vector \\widehat{u} such that \\alpha_i are exact eigenvalues of the matrix\n\\widehat{D} = D + \\widehat{u} \\widehat{u}^*.\n\nSo, you first compute the eigenvalues, then compute \\widehat{u} and only then the eigenvectors."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#divide-and-conquer-and-the-fast-multipole-method",
    "href": "lectures/lecture-8/lecture-8.html#divide-and-conquer-and-the-fast-multipole-method",
    "title": "Questions?",
    "section": "Divide and conquer and the Fast Multipole Method",
    "text": "Divide and conquer and the Fast Multipole Method\nIn the computations of divide and conquer we have to evaluate the sums of the form\nf(\\lambda) = 1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{(d_i - \\lambda)},\nand have to do it at least for n points.\n\nThe complexity is then \\mathcal{O}(n^2), as for the QR algorithm.\nCan we make it \\mathcal{O}(n \\log n)?\nThe answer is yes, but we have to replace the computations by the approximate ones by the help of Fast Multipole Method.\n\nLets explain a little bit…"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#few-more-algorithms",
    "href": "lectures/lecture-8/lecture-8.html#few-more-algorithms",
    "title": "Questions?",
    "section": "Few more algorithms",
    "text": "Few more algorithms\n\nAbsolutely different approach is based on the bisection.\nGiven a matrix A its inertia is defined as a triple\n\n(\\nu, \\zeta, \\pi),\nwhere \\nu is the number of negative, \\zeta - zero and \\pi - positive eigenvalues.\n\nIf X is non-singular, then\n\nInertia(A) = Inertia(X^* A X)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#bisection-via-gaussian-elimination",
    "href": "lectures/lecture-8/lecture-8.html#bisection-via-gaussian-elimination",
    "title": "Questions?",
    "section": "Bisection via Gaussian elimination",
    "text": "Bisection via Gaussian elimination\n\nGiven z we can do the Gaussian elimination:\n\nA - zI = L D L^*,\nand inertia of the diagonal matrix is trivial to compute.\n\nThus, if we want to find all the eigenvalues in the interval a, b\nUsing inertia, we can easily count the number of eigenvalues in an interval.\nIllustration: if Inertia(A)=(5,0,2) and after shift Inertia(A-zI)=(4,0,3), z\\in[a,b] then we know that \\lambda(A)\\in[a,z]."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-method",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-method",
    "title": "Questions?",
    "section": "Jacobi method",
    "text": "Jacobi method\n\nRecall what a Jacobi (Givens rotations) are\nIn a plane they correspong to a 2 \\times 2 orthogonal matrix of the form\n\n\\begin{pmatrix} \\cos \\phi & \\sin \\phi \\\\ -\\sin \\phi & \\cos \\phi \\end{pmatrix},\nand in the n-dimensional case we select two variables i and j and rotate."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-method-cont.",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-method-cont.",
    "title": "Questions?",
    "section": "Jacobi method (cont.)",
    "text": "Jacobi method (cont.)\n\nThe idea of the Jacobi method is to minimize sum of squares of off-diagonal elements:\n\n\\Gamma(A) = \\mathrm{off}( U^* A U), \\quad \\mathrm{off}^2(X) = \\sum_{i \\ne j} \\left|X_{ij}\\right|^2 = \\|X \\|^2_F - \\sum\\limits_{i=1}^n x^2_{ii}.\nby applying succesive Jacobi rotations U to zero off-diagonal elements.\n\nWhen the “pivot” is chosen, it is easy to eliminate it.\nThe main question is then what is the order of sweeps we have to make (i.e. in which order to eliminate).\nIf we always eliminate the largest off-diagonal elements the method has quadratic convergence.\nIn practice, a cyclic order (i.e., (1, 2), (1, 3), \\ldots, (2, 3), \\ldots) is used."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-method-convergence",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-method-convergence",
    "title": "Questions?",
    "section": "Jacobi method: convergence",
    "text": "Jacobi method: convergence\n\nTo show convergence, we firstly show that\n\n \\text{off}(B) &lt; \\text{off}(A), \nwhere B = U^*AU.\n\nIn this case we use the unitary invariance of Frobenius norm and denote by p and q the indices that is changed after rotation:\n\n$ ^2(A) = ^2(B) = |B|^2_F - {i=1}^n b^2{ii} = | A |^2_F - {i p, q} b^2{ii} - (b^2_{pp} + b^2_{qq}) = | A |^2_F - {i p, q} a^2{ii} - (a^2_{pp} + 2a^2_{pq} + a^2_{qq}) = | A |^2_F - {i =1}^n a^2{ii} - 2a^2_{pq} = ^2(A) - 2a^2_{pq} &lt; ^2(A)$\n\nWe show that the ‘’size’’ of off-diagonal elements decreases after Jacobi rotation.\nIf we always select the largest off-diagonal element a_{pq} = \\gamma to eliminate (pivot), then we have\n\n |a_{ij}| \\leq \\gamma, \nthus\n \\text{off}(A)^2 \\leq 2 N \\gamma^2, \nwhere 2N = n(n-1) is the number of off-diagonal elements.\n\nOr rewrite this inequality in the form\n\n2\\gamma^2 \\geq \\frac{\\text{off}^2(A)}{N}.\nNow we use relations \\Gamma^2(A) = \\text{off}^2(A) - 2\\gamma^2 \\leq \\text{off}^2(A) - \\dfrac{\\text{off}^2(A)}{N} and get\n \\Gamma(A) \\leq \\sqrt{\\left(1 - \\frac{1}{N}\\right)} \\text{off}(A). \n\nAften N steps we have the factor\n\n\\left(1 - \\frac{1}{N}\\right)^{\\frac{N}{2}} \\approx e^{-\\frac{1}{2}},\ni.e. linear convergence. However, the convergence is locally quadratic (given without proof here)."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#jacobi-summary",
    "href": "lectures/lecture-8/lecture-8.html#jacobi-summary",
    "title": "Questions?",
    "section": "Jacobi: summary",
    "text": "Jacobi: summary\nJacobi method was the first numerical method for the eigenvalues, proposed in 1846.\n\nLarge constant\nVery accurate (high relative error for small eigenvalues)\nGood parallel capabilities"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#summary-for-this-part",
    "href": "lectures/lecture-8/lecture-8.html#summary-for-this-part",
    "title": "Questions?",
    "section": "Summary for this part",
    "text": "Summary for this part\n\nMany algorithms for the computation of the SEV solution:\n\nQR\nDivide-and-conquer\nBisection\nJacobi"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#next-lecture",
    "href": "lectures/lecture-8/lecture-8.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nWe start sparse and/or structured NLA."
  },
  {
    "objectID": "files/qr_exercise.html",
    "href": "files/qr_exercise.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\n\nn = 100\na = np.random.rand(n)\nb = a + 1e-9 * np.random.randn(n)\n\n# Сначала нормируем вектор a\na = a / np.linalg.norm(a)\n\n# Убираем из b проекцию на a (попытка сделать c ортогональным к a)\nc = b - np.dot(b, a) * a\nc = c / np.linalg.norm(c)\n\n# Скалярное произведение c и a пока не идеально мало:\nprint(np.dot(c, a))\n\n# Делаем повторную ортогонализацию:\n# Снова вычитаем проекцию c на a\nc = c - np.dot(c, a) * a\nc = c / np.linalg.norm(c)\n\n# Теперь скалярное произведение гораздо меньше!\nprint(np.dot(c, a))\n\n-2.7671064037337878e-08\n-6.591949208711867e-17"
  },
  {
    "objectID": "files/qr_exercise.html#упражнение",
    "href": "files/qr_exercise.html#упражнение",
    "title": "",
    "section": "Упражнение",
    "text": "Упражнение\nВ этом упражнении мы реализуем классический и модифицированный процессы Грама-Шмидта для построения QR-разложения матрицы.\nВход: матрица A \\in \\mathbb{R}^{n \\times n} Выход: матрицы Q и R такие, что A = QR, при этом Q ортогональная, а R верхнетреугольная. Мы сфокусируемся на матрице Q, потому что имея её, мы можем легко получить R по формуле R = Q^T A.\n\nЧисленная неустойчивость. Классический процесс Грама-Шмидта (CGS) может приводить к большим погрешностям при вычислении ортонормированной системы векторов, особенно при плохой обусловленности исходной матрицы. В машинной арифметике возникает потеря ортогональности.\nМодифицированный процесс (MGS). Идея заключается в том, чтобы последовательно вычитать проекции на уже построенные ортонормированные вектора, шаг за шагом. Формально, если q_1, \\dots, q_{k-1} уже построены, то для k-го вектора: \nq_k := a_k, \\quad\nq_k := q_k - (q_k,\\,q_1)\\,q_1, \\quad\nq_k := q_k - (q_k,\\,q_2)\\,q_2, \\quad \\dots\n В точной арифметике это эквивалентно классическому процессу, но в машинной арифметике MGS, как правило, даёт более устойчивое решение.\nСложность. Оба алгоритма имеют асимптотическую сложность порядка O(n^2 m), где m \\times n — размерность исходной матрицы. Для квадратной матрицы n \\times n сложность O(n^3).\nQR-разложение. По готовой ортонормированной матрице Q можно найти верхнетреугольную R, например, умножив Q^T A. Тогда A = Q R.\n\n\nimport numpy as np\n\ndef hilbert_matrix(n):\n    H = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(n):\n            H[i, j] = 1.0 / (i + j + 1)\n    return H\n\ndef generate_matrix(n, type='random'):\n    if type == 'random':\n        A = np.random.randn(n, n)\n    elif type == 'hilbert':\n        A = hilbert_matrix(n)\n    return A\n\ndef classical_gs(A):\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    for k in range(n):\n        # Берём исходный столбец\n        v = A[:, k].copy()\n        \n        # Считаем все скалярные произведения \\alpha_j = (q_j, v) заранее\n        # (здесь q_j уже ортонормированы из предыдущих шагов)\n        alphas = np.array([np.dot(Q[:, j], v) for j in range(k)])\n        \n        # Вычитаем сумму \\sum_j \\alpha_j * q_j\n        for j in range(k):\n            v -= alphas[j] * Q[:, j]\n            ### YOUR CODE HERE\n        \n         ### YOUR CODE HERE\n        Q[:, k] = np.zeros_like(v) # WRONG!\n    return Q\n\ndef modified_gs(A):\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    for k in range(n):\n        v = A[:, k].copy()\n        # В МОДИФИЦИРОВАННОМ алгоритме на каждом шаге j\n        # берём текущее \"свежее\" v\n        ### YOUR CODE HERE\n        for j in range(k):\n            pass\n        Q[:, k] = np.zeros_like(v) # WRONG!\n    return Q\n\n\n\n# Пример: проверяем ортогонализацию на матрице Гильберта\nn = 10\nA = generate_matrix(n, type='hilbert')\n\nQ_cgs = classical_gs(A)\nQ_mgs = modified_gs(A)\n\n# Оцениваем \"ошибку ортогональности\" как ||Q^T Q - I|| по Фробениусу\nerr_cgs = np.linalg.norm(Q_cgs.T @ Q_cgs - np.eye(n), ord='fro')\nerr_mgs = np.linalg.norm(Q_mgs.T @ Q_mgs - np.eye(n), ord='fro')\n\n# Фробениусова норма единичной матрицы I (это sqrt(n))\nnorm_I_fro = np.linalg.norm(np.eye(n), ord='fro')\n\nrel_cgs_fro = err_cgs / norm_I_fro\nrel_mgs_fro = err_mgs / norm_I_fro\n\nprint(\"Абсолютная ошибка (CGS) =\", err_cgs)\nprint(\"Абсолютная ошибка (MGS) =\", err_mgs)\nprint(\"Относительная ошибка (CGS) =\", rel_cgs_fro)\nprint(\"Относительная ошибка (MGS) =\", rel_mgs_fro)\n\nАбсолютная ошибка (CGS) = 3.4653383412436143\nАбсолютная ошибка (MGS) = 8.763548618214397e-05\nОтносительная ошибка (CGS) = 1.095836202143963\nОтносительная ошибка (MGS) = 2.7712774019178854e-05"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html",
    "href": "seminars/seminar-2/seminar-2.html",
    "title": "Matrix Norms Calculation",
    "section": "",
    "text": "Matrix norms, unitary matrices\nimport numpy as np"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#householder-martices",
    "href": "seminars/seminar-2/seminar-2.html#householder-martices",
    "title": "Matrix Norms Calculation",
    "section": "Householder Martices",
    "text": "Householder Martices\nHouseholder matrix is the matrix of the form: H \\equiv H(v) = I - 2 vv^*, where v is an n \\times 1 column and v^* v = 1.\nIt is also a reflection:  Hx = x - 2(v^* x) v\nAttention! If it does not work, remember about vector norm"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#build-your-own-from-a-vector",
    "href": "seminars/seminar-2/seminar-2.html#build-your-own-from-a-vector",
    "title": "Matrix Norms Calculation",
    "section": "Build your own from a vector",
    "text": "Build your own from a vector\n\ndef build_householder(v):\n    # v - vector of size n\n    v = v/np.linalg.norm(v)\n    a = np.identity(v.size)\n    \n    H = a - 2*np.outer(v, v)\n    \n    return H\n\n\nv = np.random.normal(size=(3))\nprint(v)\nh = build_householder(v)\nprint(h)"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#see-how-it-reflects-vectors",
    "href": "seminars/seminar-2/seminar-2.html#see-how-it-reflects-vectors",
    "title": "Matrix Norms Calculation",
    "section": "See how it reflects vectors",
    "text": "See how it reflects vectors\n\nv = np.random.normal(size=(3))\nh = build_householder(v)\n\nx = np.random.normal(size=(3))\nprint(x)\nprint(np.matmul(h, x))\n\n\nv = np.array([0,  1,  -1])\nh = build_householder(v)\nx = np.array([0,  1,  0])\nprint(np.round(h , decimals=2))\nprint(x)\nprint(np.round(h @ x.T, decimals=2))"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#optional-check-that-it-indeed-is-also-a-reflection",
    "href": "seminars/seminar-2/seminar-2.html#optional-check-that-it-indeed-is-also-a-reflection",
    "title": "Matrix Norms Calculation",
    "section": "Optional: check that it indeed is also a reflection",
    "text": "Optional: check that it indeed is also a reflection\n Hx = x - 2(v^* x) v\n\nv = np.random.normal(size=(3))\nv = v/np.linalg.norm(v)\nh = build_householder(v)\n\nx = np.random.normal(size=(3))\n\nv = np.array([0,  1,  -1])\nv = v/np.linalg.norm(v)\nh = build_householder(v)\nx = np.array([0,  1,  0])\n\nhx = h @ x.T\n\nreflected =  x - 2 * np.dot(v.T,x) * v\n\nprint(\"initial vector: \", x)\nprint(\"transofrmed by matrix: \", hx)\nprint(\"reflected by vector: \", reflected)"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#check-unitarity",
    "href": "seminars/seminar-2/seminar-2.html#check-unitarity",
    "title": "Matrix Norms Calculation",
    "section": "Check unitarity",
    "text": "Check unitarity\n\nUse numpy tools to check if the matrix is unitary using formula U* x U = I\n\nn = 3\n\n\nv = np.random.normal(size=(n))\nh = build_householder(v)\n\nprint(np.round(h @ h, decimals=2))\n\n\ndef householder_transform(A):\n    \"\"\"\n    Transforms the matrix A into an upper triangular matrix using Householder reflections.\n    \n    Parameters:\n        A (numpy.ndarray): The matrix to be transformed.\n    \n    Returns:\n        R (numpy.ndarray): The upper triangular matrix after applying Householder transformations.\n    \"\"\"\n    A = A.copy()\n    m, n = A.shape\n    for j in range(min(m, n)):\n        # Create the vector x for the current column\n        x = A[j:, j]\n        \n        # Calculate the norm of x and the Householder vector v\n        norm_x = np.linalg.norm(x)\n        if norm_x == 0:\n            continue\n        sign = -1 if x[0] &lt; 0 else 1\n        v = x.copy()\n        v[0] += sign * norm_x  # Adjust the first element of v for the reflection\n        v /= np.linalg.norm(v)  # Normalize v\n        \n        # Apply the Householder transformation to A[j:, j:]\n        A[j:, j:] -= 2 * np.outer(v, v @ A[j:, j:])\n    \n    return A\n\n# Example matrix\nA = np.array([\n    [4, 1, -2, 2],\n    [1, 2, 0, 1],\n    [-2, 0, 3, -2],\n    [2, 1, -2, -1]\n], dtype=float)\n\nR = householder_transform(A)\nprint(\"Upper triangular matrix R:\\n\", R)\n\n\n\nBonus task: check that it also preserves the norm. You can check it for your own custom norm if you created one!\n \\frac{\\Vert x - \\widehat{x} \\Vert}{\\Vert x \\Vert} \\leq \\varepsilon. \n \\frac{\\Vert y - \\widehat{y} \\Vert}{\\Vert y \\Vert } = \\frac{\\Vert U ( x - \\widehat{x}) \\Vert}{\\Vert U  x\\Vert}  \\leq \\varepsilon. \n\nv = np.random.normal(size=(n))\nv = v/np.linalg.norm(v)\nh = build_householder(v)\nepsilon = 0.001\n\nx = np.random.normal(size=(n))\nx_hat = x + epsilon * np.random.normal(size=(n)) # approximaton of x\ny = x - x_hat                                    # error of approximation\n\ninitial_error = np.linalg.norm(y)/np.linalg.norm(x)\ntransformed_error = np.linalg.norm(h @ y.T)/np.linalg.norm(h @ x.T)\n        \nprint(\"initial error:     \", initial_error)\nprint(\"transformed error: \", transformed_error)"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html",
    "href": "seminars/seminar-1/seminar-1.html",
    "title": "",
    "section": "",
    "text": "0.1+0.2\n\n0.30000000000000004\n0.4+0.5\n\n0.9\nФиксированная и плавающая точка, векторные нормы и понятие устойчивости алгоритмов."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#почему-0.1-0.2-0.3",
    "href": "seminars/seminar-1/seminar-1.html#почему-0.1-0.2-0.3",
    "title": "",
    "section": "🧐 Почему 0.1 + 0.2 != 0.3?",
    "text": "🧐 Почему 0.1 + 0.2 != 0.3?\nЧисла с плавающей запятой в Python хранятся в формате IEEE 754, который использует бинарное представление чисел. Однако не все десятичные дроби можно точно представить в двоичной системе. Например:\n\nДесятичное 0.1 в двоичном формате представляется бесконечной дробью:\n0.0001100110011001100110011001100110011... (повторяется бесконечно)\nТо же самое касается 0.2:\n0.001100110011001100110011001100110011... (тоже бесконечная дробь)\n\nТак как компьютер работает с ограниченной точностью, он усекает эти дроби, оставляя только конечное число битов. Это приводит к небольшим ошибкам округления.\n\n🧮 Доказательство в Python\nЕсли вывести точное двоичное представление 0.1, 0.2 и 0.3, можно увидеть разницу:\n\nfrom decimal import Decimal\n\nprint(Decimal(0.1))  # 0.1000000000000000055511151231257827021181583404541015625\nprint(Decimal(0.2))  # 0.200000000000000011102230246251565404236316680908203125\nprint(Decimal(0.3))  # 0.299999999999999988897769753748434595763683319091796875\n\n0.1000000000000000055511151231257827021181583404541015625\n0.200000000000000011102230246251565404236316680908203125\n0.299999999999999988897769753748434595763683319091796875\n\n\nТеперь сложим 0.1 + 0.2:\n\nprint(Decimal(0.1) + Decimal(0.2))  \n\n0.3000000000000000166533453694\n\n\nА 0.3 на самом деле:\n\nprint(Decimal(0.3))  \n\n0.299999999999999988897769753748434595763683319091796875\n\n\nОни не равны из-за разницы в последних разрядах!\n\n\n🤔 Как правильно сравнивать?\nПоскольку числа с плавающей запятой содержат небольшие ошибки округления, их нельзя сравнивать напрямую с ==. Вместо этого используют погрешность (epsilon):\n\n(0.1+0.2)==0.3\n\nFalse\n\n\n\nimport math\n\na = 0.1 + 0.2\nb = 0.3\n\nprint(math.isclose(a, b, rel_tol=1e-9)) \n\nTrue\n\n\nФункция math.isclose() проверяет, находятся ли два числа достаточно близко друг к другу с учетом заданной относительной ошибки."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#числа-с-фиксированной-точкой",
    "href": "seminars/seminar-1/seminar-1.html#числа-с-фиксированной-точкой",
    "title": "",
    "section": "Числа с фиксированной точкой",
    "text": "Числа с фиксированной точкой\nЧисло с фиксированной точкой состоит из 1-битного знака, m-битного целого и n-битного дробного числа: \n\\text{decimal} =\n(-1)^{\\text{sign}} \\times\n\\Big(\n\\sum_{i=0}^{m-1} \\text{integer}[i] \\cdot base^{m-1-i} +\n\\sum_{i=0}^{n-1} \\text{fractional}[i] \\cdot base^{-i-1}\n\\Big)\n\n\nдиапазон [-2^m + 2^{-n}, 2^m - 2^{-n}]\nразрешение 2^{-n}\nобщее количество бит m + n + 1\n\n\ndef binary_fixed_point_to_decimal(x, m=8, n=8):\n    \"\"\"\n    x - binary string of size 1 + m + n\n    m - size of an integer part\n    n - sze of a fractional part\n    \"\"\"\n    sign_part, integer_part, fractional_part = x[0], x[1:m+1], x[m+1:m+n+1]\n    sign_value = (-1) ** int(sign_part)\n    integer_value = sum([\n        int(v) * 2 ** i\n        for i, v in enumerate(integer_part[::-1])\n    ])\n    fractional_value = sum([\n        int(v) * 2 ** -(i + 1)\n        for i, v in enumerate(fractional_part)\n    ])\n    return sign_value * (integer_value + fractional_value)\n\nm, n = 8, 8\nx = '00000010100100000'\nprint(binary_fixed_point_to_decimal(x, m, n) == 5.125)\n\nTrue\n\n\n\nx = '11111111111111111' # Insert a string corresponding to a minimal possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == -(2 ** m - 2 ** (-n)))\n\nTrue\n\n\n\nx = '01111111111111111' # Insert a string corresponding to a maximal possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == 2 ** m - 2 ** (-n))\n\nTrue\n\n\n\nx = '00000000000000001' # Insert a string corresponding to an absolute minimal but nonzero possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == 2 ** (-n))\n\nTrue\n\n\nОсновной недостаток чисел с фиксированной точкой — ограниченный диапазон и разрешение. Например, для 8-битного формата: - диапазон [-128, 127] - разрешение 2^{-7} = 0.0078125\nЭто означает, что для представления очень больших или очень маленьких чисел потребуется больше бит.\nДля 32-битного формата: - диапазон [-2^{31}, 2^{31} - 1] - разрешение 2^{-23} \\approx 1.192 \\times 10^{-7}\nЭто означает, что для представления очень больших или очень маленьких чисел потребуется больше бит."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#числа-с-плавающей-точкой",
    "href": "seminars/seminar-1/seminar-1.html#числа-с-плавающей-точкой",
    "title": "",
    "section": "Числа с плавающей точкой",
    "text": "Числа с плавающей точкой\nЧисла с плавающей точкой состоят из 1-битного знака, m-битного экспоненты и n-битного мантиссы:\n\n\\text{decimal} =\n(-1)^{\\text{sign}} \\times\nbase^{\\Big(\\sum_{i=0}^{m-1} \\text{exponent}[i] \\cdot base^{m-1-i} - (2^{m-1} - 1)\\Big)}\n\\times\n\\Big(1 + \\sum_{i=0}^{n-1} \\text{mantissa}[i] \\cdot base^{-i-1}\\Big)\n\n\nзначения экспоненты, равные 0 и 1, зарезервированы для специальных чисел: NaN, бесконечность и т.д.\nобщее количество бит m + n + 1\n\nHalf (float16) vs Single (float32) vs and Double (float32) Точность\n\nfloat16 - 16 bit total: 1 for a sign, m = 5 for exponent and n = 10 for mantissa\nfloat32 - 32 bits total: 1 for a sign, m = 8 for exponent and n = 23 for mantissa\nfloat64 - 64 bits total: 1 for a sign, m = 11 for exponent and n = 52 for mantissa\n\n\ndef binary_floating_point_to_decimal(x, m=8, n=23):\n    \"\"\"\n    x - binary string of size 1 + m + n\n    m - size of an exponent part\n    n - sze of a mantissa part\n    \"\"\"\n    sign_part, exponent_part, mantissa_part = x[0], x[1:m+1], x[m+1:n+m+1]\n    sign_value = (-1) ** int(sign_part)\n\n    mantissa_value = 1\n    for i, v in enumerate(mantissa_part):\n        mantissa_value += int(v) * (2 ** -(i + 1))\n\n    exponent_value = 0\n    for i, v in enumerate(exponent_part):\n        exponent_value += int(v) * 2 ** i\n    exponent_value -= (2 ** (m - 1) - 1)\n        \n    return sign_value * (2 ** exponent_value) * mantissa_value\n\nm, n = 8, 23\nx = '01000000101001000000000000000000'\nprint(binary_floating_point_to_decimal(x, m, n) == 5.125)\n\nTrue"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#ошибки-округления",
    "href": "seminars/seminar-1/seminar-1.html#ошибки-округления",
    "title": "",
    "section": "Ошибки округления",
    "text": "Ошибки округления\nИз-за того, что представления с плавающей точкой являются лишь приближениями к действительным числам, могут возникать ошибки округления.\nНапример, рассмотрим простой алгоритм суммирования, где x_i — числа с плавающей точкой:\n\nf(x) = x_1 + x_2 + ... + x_n\n\nРеализуйте простой алгоритм из лекции (добавляя по одному):\n[!] Установите n равным 1000 и все x_i равными 0.1.\n\ntotal = 0.0\nfor _ in range(100000):\n    total += 0.1\n\nprint(\"Expected result: 10000.0\")\nprint(f\"Actual result:\", total)\n\nExpected result: 10000.0\nActual result: 10000.000000018848\n\n\nРеализуйте алгоритм Кахана из лекции и проверьте возникающую ошибку.\n[!] Установите n равным 1000 и все x_i равными 0.1.\n\ns = 0\nc = 0\nfor i in range(1000):\n    y = 0.1 - c\n    t = s + y\n    c = (t - s) - y\n    s = t\n\nprint(\"Expected result: 100.0\")\nprint(f\"Actual result:\", s)\n\nExpected result: 100.0\nActual result: 100.0\n\n\nОбъяснение: значение 0.1 не может быть точно представлено в двоичной системе, поэтому оно становится приближением. Когда это приближение добавляется по одному, малые ошибки округления накапливаются, что приводит к конечному результату, немного меньшему, чем 100.0."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#векторы-и-векторные-нормы",
    "href": "seminars/seminar-1/seminar-1.html#векторы-и-векторные-нормы",
    "title": "",
    "section": "Векторы и векторные нормы",
    "text": "Векторы и векторные нормы\nВ NLA мы обычно работаем не с числами, а с векторами, которые являются просто массивами чисел размера n.\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])\n\nprint(f'Size of the x vector is {len(x)}')\nprint(f'Type of the vector elements is {type(x[0])}')\n\nSize of the x vector is 5\nType of the vector elements is &lt;class 'numpy.int64'&gt;\n\n\nКак видите, этот вектор содержит только целые значения. Теперь преобразуйте их в тип float32.\n\nx = x.astype(np.float32)\nprint(f'Type of the vector elements is {type(x[0])}')\n\nType of the vector elements is &lt;class 'numpy.float32'&gt;\n\n\nДля измерения малости вектора используется его норма \\|x\\|.\nСамый важный класс норм — p-нормы: \n\\|x\\|_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}\n Примеры p-норм: - Манхэттенское расстояние или L_1 норма - когда p=1 - Евклидова норма или L_2 норма - когда p=2 - Бесконечная норма, или норма Чебышева - когда p=+\\infty: $ |x|_{} = _i | x_i|$\nПосчитайте нормы для вектора x:\nПодсказка: используйте np.linalg.norm\n\nprint('L1 norm:', np.linalg.norm(x, 1))\nprint('L2 norm:', np.linalg.norm(x, 2))\nprint('Chebyshev norm:', np.linalg.norm(x, np.inf))\n\nL1 norm: 15.0\nL2 norm: 7.4161983\nChebyshev norm: 5.0\n\n\nЕдиничный диск для p-нормы — это множество точек таких, что \\|x\\|_p = 1.\nВизуализируйте единичный диск для следующих p-норм: p \\in (0.25, 0.75, 1.0, 2.0, 5.0, \\infty)\nПодсказка: y = \\pm (1 - |x|^p)^{1/p}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef unit_disk(p):\n    x = np.linspace(-1, 1, 201)\n    y = (1 - np.abs(x) ** p) ** (1 / p)\n    x = np.hstack([x, x[1:][::-1], x[0]])\n    y = np.hstack([y, -y[1:][::-1], y[0]])\n    return x, y\n\nplt.figure(figsize=(4, 4))\nplt.axis('equal')\nfor p in (0.25, 0.5, 1.0, 2.0, 5.0, np.inf):\n    x, y = unit_disk(p)\n    plt.plot(x, y, label=f'$p$={p}')\nplt.legend(loc=1)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#устойчивость",
    "href": "seminars/seminar-1/seminar-1.html#устойчивость",
    "title": "",
    "section": "Устойчивость",
    "text": "Устойчивость\nПредположим, у нас есть вектор x, функция f(x), и алгоритм \\text{alg}(x) для приближения функции. Тогда алгоритм называется устойчивым в прямом направлении, если для некоторого малого \\varepsilon\n\n\\|\\text{alg}(x) - f(x)\\|  \\leq \\varepsilon\n\n[Задание] Проверьте суммирующие алгоритмы, упомянутые ранее (простой и Кахана) на устойчивость в прямом направлении.\nПусть x_i = 0.1 и n = 100. \nf(x) = \\sum_{i=1}^{100} x_i, \\;\\;\nx_i = 0.1\n Запишите ошибку, возникающую в каждом шаге суммирования: \n\\text{error}[i] = |0.1 \\cdot i - \\text{alg}(x)|\n\n\nfrom matplotlib import pyplot as plt\nN = 10000\n\n# Naive\ntotal = 0.0\nerror_naive = []\nfor i in range(N):\n    total += 0.1\n    refer = (i + 1) / 10\n    error_naive.append(np.abs(refer - total))\n\n# Kahan\ns = 0\nc = 0\nerror_kahan = []\nfor i in range(N):\n    y = 0.1 - c\n    t = s + y\n    c = (t - s) - y\n    s = t\n    error_kahan.append(np.abs(c))\n\nplt.figure(figsize=(8, 4))\nplt.title(r'Forward stability of summation algorithms $\\varepsilon(n)$')\nplt.plot(error_naive, label='Naive')\nplt.plot(error_kahan, label='Kahan')\nplt.ylabel(r'$\\varepsilon$', rotation=0)\nplt.xlabel(r'$n$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWhat do you see?"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#простая-но-очень-важная-идея-умножения-матриц",
    "href": "seminars/seminar-1/seminar-1.html#простая-но-очень-важная-идея-умножения-матриц",
    "title": "",
    "section": "Простая, но очень важная идея умножения матриц",
    "text": "Простая, но очень важная идея умножения матриц\nПредположим, у вас есть следующее выражение: \nb = A_1 A_2 A_3 x,\n где A_1, A_2, A_3 \\in \\mathbb{R}^{3 \\times 3} - случайные квадратные плотные матрицы, а x \\in \\mathbb{R}^n - вектор. Вам нужно вычислить b. Какой способ лучше всего использовать?\nA_1 A_2 A_3 x (слева направо) \\left(A_1 \\left(A_2 \\left(A_3 x\\right)\\right)\\right) (справа налево) Не имеет значения Результаты первых двух вариантов не будут одинаковыми.\n\nimport numpy as np\n\n# Function to create a random square matrix of size n\ndef create_random_matrix(n):\n    return np.random.rand(n, n)\n\n# Define the size of the matrices\nn = 200\n\n# Create a list of 3 random matrices\nmatrices = [create_random_matrix(n) for _ in range(3)]\ny = np.random.rand(n, 1)  # y is a vector\n\n# Function to compute the expression in a given order\ndef compute_expression(matrices, y, reverse=False):\n    result = y\n    if reverse:\n        # Start with y and multiply with each matrix from right to left\n        for matrix in reversed(matrices):\n            result = matrix @ result\n    else:\n        # Start with the first matrix and multiply each next matrix from left to right\n        result = matrices[0]\n        for matrix in matrices[1:]:\n            result = result @ matrix\n        # Finally multiply with y\n        result = result @ y\n    return result\n\n# Time the expression from left to right\nprint(\"💎 From left to right\")\n%timeit compute_expression(matrices, y)\n\n# Time the expression from right to left\nprint(\"💎 From right to left\")\n%timeit compute_expression(matrices, y, reverse=True)\n\n💎 From left to right\n2.23 ms ± 636 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n💎 From right to left\n56.3 µs ± 3.95 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7 (unsolved).html",
    "href": "seminars/seminar-7/seminar-7 (unsolved).html",
    "title": "Recall determinant definition",
    "section": "",
    "text": "import numpy as np\nimport itertools\nfrom scipy.linalg import lu\nimport time\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7 (unsolved).html#determinant-of-triangular-matrix",
    "href": "seminars/seminar-7/seminar-7 (unsolved).html#determinant-of-triangular-matrix",
    "title": "Recall determinant definition",
    "section": "Determinant of triangular matrix",
    "text": "Determinant of triangular matrix\nDeterminant of an upper-(lower-)triangular matrix M is equal to \\prod_{i=1}^{n}M_{i,i}.\nProof: row expansion"
  },
  {
    "objectID": "projects.html#выбор-темы-проекта-и-команды",
    "href": "projects.html#выбор-темы-проекта-и-команды",
    "title": "",
    "section": "Выбор темы проекта и команды",
    "text": "Выбор темы проекта и команды\n🗓 15 марта. 🦄 2 балла\nВыбрать команду для проекта. Размер команды - 3-5 человек. К этому моменту необходимо лично согласовать с преподавателем тему проекта и высокоуровневое описание процесса работы над ним. Шаблон \\LaTeX для работы представлен здесь, выполнять этапы проекта надо в нём. Рекомендуется использовать VSCode с расширением LaTeX Workshop. Так же доступна версия в overleaf. Допускается выполнение проекта в Typst, если вы умеете, но требования ниже будут предъявляться точно такие же (например, корректное цитирование). В дальнейшем необходимо использовать данный шаблон, дополняя его по мере продвижения в проекте. Все материалы и ссылки по теме удобно собирать в notion, чтобы при нашем обсуждении они были под рукой. * оценка следа\n\nВозможные темы\nГлавное требование к теме проекта - вам должно быть прикольно его делать, тема должна вас живо интересовать. Второе требование - он должен быть связан с методами оптимизации (хотя бы как-то 🙂). Тему проекта необходимо придумать/ найти/ выбрать самостоятельно.\n\nВзять недавно опубликованную статью на конференции NeurIPS, ICML, ICLR. Детально разобраться. Воспроизвести. Попробовать на других данных/моделях/методах. Возможно, предложить посмотреть/протестировать что-нибудь новое.\n\n\n\nИзучение методов оптимизации в непрерывном времени\n\nСтатья\nСтатья\n\nГрадиентный спуск можно рассматривать как дискретизацию Эйлера обыкновенного дифференциального уравнения градиентного потока. Оказывается, ускоренным методам тоже можно поставить в соответствие их непрерывные аналоги. В рамках проекта предлагается изучить где особенно полезны могут быть такие аналогии, доказать сходимость некоторых ускоренных методов в выпуклом случае, изучить сходимость методов для случая бесконечной дисперсии.\nСоздание питон библиотеки - черного ящика для бенчмаркинга методов оптимизации с разными запусками, единым интерфейсом, построением графиков.\nПоиграться с диффузионными моделями\nСсылка.\nДиффузионные модели сегодня часто используются на практике для задач генерации из распределения данных (чаще всего, изображений). В рамках проекта предлагается рассмотреть какую-нибудь простую диффузионную модель, разобраться в спейифике обучения таких моделей с точки зрения оптимизации и проделать несколько своих численных экспериментов.\nПостроение оптимального криптовалютного портфеля\nВ рамках проекта предлагается изучить стратегии портфельного инвестирования и сравнить некоторые из них в реальных условиях на рынке криптовалют с помощью программы на языке Python. Нужно будет сформулировать задачу, граничные условия и критерии для сравнения. Можно начать с портфельной теории Марковица и посмотреть, можно ли использовать недавние достижения в NLP для того, чтобы существенно это улучшить.\nЕще идеи проектов\nВ качестве вдохновения можно посмотреть на лучшие проекты студентов прошлых лет:\n\nЛучшие проекты по оптимизации 2018\nЛучшие проекты по оптимизации 2019\nЛучшие проекты по оптимизации 2020\n\n\n\n\nФормат сдачи\nЗагрузить в свой notion в соответствующее поле pdf с названием темы и абстрактом. Убедитесь, что на этом этапе вы удалили остальные пункты из шаблона выше. Пдфка должна выглядеть как-то так:\n\n\n\nExample of this stage of the project\n\n\n\n\nКритерии оценивания\n\n0 баллов - файл не загружен вовремя/ файл загружен в другом формате\n1 балл - файл загружен вовремя, тема не согласована\n2 балла - файл загружен вовремя, тема согласована"
  },
  {
    "objectID": "projects.html#описание-решаемой-задачи",
    "href": "projects.html#описание-решаемой-задачи",
    "title": "",
    "section": "Описание решаемой задачи",
    "text": "Описание решаемой задачи\n🗓 22 марта. 🦄 2 балла\nНа этом этапе необходимо четко сформулировать решаемую задачу. С большой вероятностью, в этом пункте нужно написать задачу оптимизации или другую решаемую математическую задачу.\nОбратите внимание, что если в качестве проекта вы работаете с существующей статьей вы сначала должны написать свою задачу в рамках проекта - разобраться в чём-то, воспроизвести, повторить численные результаты, придумать другие численные эксперименты с другими моделями. А после этого необходимо так же привести в наиболее простом виде решаемую задачу из статьи. На этом этапе и далее важно не присваивать себе чужие заслуги. Постарайтесь избегать формулировок вида: мы решаем задачу (вместо этого можно написать авторы статьи решают задачу), мы хотим исследовать проблему/ предложить метод оптимизации/ проанализировать сходимость, устойчивость (вместо этого авторы статьи предлагают/ исследуют и т.д.)\n\nФормат сдачи\nЗагрузить в свой notion в соответствующее поле обновленный pdf с добавленной постановкой задачи с учётом фидбека по предыдущим пунктам.\n\n\nКритерии оценивания\n\n0 баллов - файл не загружен вовремя/ файл загружен в другом формате\n1 балл - файл загружен вовремя, решаемая задача сформулирована не чётко, не понятно; решаемая задача неадекватна студенческому проекту (например, для обучения модели нам нужно 1000 GPU и 355 GPU-years или в рамках проекта мы хотим придумать метод минимизации выпуклой гладкой функции с оракулом 1 порядка, который сходится квадратично по значению функции); формулировка задачи для того, чтобы её понять требует большого количества времени чтения профильной литературы по теме; формулировка задачи слишком поверхностна или наивна.\n2 балла - файл загружен вовремя, решаемая проблема ясна и сформулирована чётко."
  },
  {
    "objectID": "projects.html#литературный-обзор",
    "href": "projects.html#литературный-обзор",
    "title": "",
    "section": "Литературный обзор",
    "text": "Литературный обзор\n🗓 5 апреля. 🦄 8 баллов\nНа этом этапе необходимо понять научный ландшафт вокруг постановки задачи. Для этого постарайтесь, чтобы после литературного обзора были ясны ответы на следующие вопросы:\n\nКакие результаты были достигнуты в похожих формулировках?\nЕсть ли более простые формулировки и результаты в схожей тематике?\nНа какие результаты необходимо существенно опираться?\nКакие исследования обуславливают актуальность рассматриваемой задачи?\nС какими источниками необходимо ознакомиться для существенного понимания задачи?\nЕсть ли в открытом доступе код для воспроизведение экспериментов для рассматриваемой задачи?\n\nМожно использовать поиск в интернете, поиск по google scholar, поиск по perplexity.ai, поиск по ссылкам в статье. В идеале ссылаться на рецензируемые опубликованные статьи монографии. Однако, при необходимости, можно ссылаться на статьи на arxiv, блогпосты и другие источники, существенные и авторитетные для задачи. Цитирование необходимо делать с помощью bibtex. Пример откуда его брать приведён ниже:\n\n\n\nЗдесь показан один из способов найти bibtex для цитирования\n\n\n\nФормат сдачи\nЗагрузить в свой notion в соответствующее поле обновленный pdf с добавленным литературным обзором с учётом фидбека по предыдущим пунктам.\n\n\nКритерии оценивания\nБаллы будут сниматься в следующих случаях (список не полный):\n\nМенее 7 релевантных источников по теме.\nРабота с источниками была проведена поверхностно, ссылки добавлены ради ссылок, а не ради сути.\nВ результате литературного обзора совершенно не понятно, какое место занимает проект на научном ландшафте."
  },
  {
    "objectID": "projects.html#project-proposal",
    "href": "projects.html#project-proposal",
    "title": "",
    "section": "Project proposal",
    "text": "Project proposal\n🗓 19 апреля. 🦄 8 баллов\nСобираем воедино всё, что было раньше, планируем и проводим фазу прототипирования. На мой взгляд - это важнейший этап проекта. Тут нужно очень четко определить куда и как двигаться, какие тропы уже пройдены другими людьми. Обратите внимание на следующие аспекты:\n\nНазвание проекта.\nAbstract (краткое описание проекта в один абзац).\nОписание проекта (обратите внимание на конкретность постановки задачи и её реалистичность).\nOutcomes - опишите, что конкретно будет выходом Вашего проекта (код, теорема, численные эксперименты, телеграм бот, веб сайт, приложение, рассказ).\nЛитературный обзор.\nДетальный план работ. Ясно, что в процессе выполнения проекта он будет меняться, однако наличие плана здесь лучше его отсутствия. Здесь вы должны написать что вы будете показывать к стадиям в следующем семестре:\n\nНачальная фаза проекта\nMid-term project review\nПредзащита\nЗащита\n\nМетрики качества. По возможности, приведите формальные и измеряемые показатели, по которым можно оценивать Ваше решение проект - это могут быть конкретные метрики качества алгоритмов, соц. опрос, логическое доказательство и т.д. Основная задача этого пункта - договориться на берегу о том, как мы сможем объективно оценить работу, проведенную в проекте. Обратите внимание, что результат проекта может быть “отрицательным” в том смысле, что мы собрались исследовать применение метода к какому-то классу задач и у нас не получилось. Это абсолютно нормально, тогда нужно будет просто описать этот процесс (мы попробовали и не вышло, но зато вот такое вот интересное наблюдали).\nОтчёт о фазе прототипирования. На этом этапе необходимо максимально широкими мазками приступить к работе над проектом. Если есть существующий код - нужно его запустить, представить результаты ваших экспериментов, показать проблемы, с которыми вы столкнулись. Попытаться предпринять первые шаги к решению проекта. Сделать что-нибудь с наскока. Совсем идеально показать какой-нибудь прототип (если это применимо к проекту).\n\n\nФормат сдачи\nЗагрузить в свой notion в соответствующее поле обновленный pdf с добавленным литературным обзором с учётом фидбека по предыдущим пунктам.\n\n\nКритерии оценивания\nБаллы будут сниматься в следующих случаях (список не полный):\n\nНе учтены комментариипо предыдущим пунктам, если они былми.\nПлан работ не реалистичный, очень поверхностный. Обратите внимание, что тяжело уверенно планировать творческие задачи (доказать теорему). Здесь лучше писать чуть более специфично (например, попробовать доказать/ обобщить доказательство из другого источника).\nНет метрик качества.\nНет литературного обзора.\nНе написан чёткий выход (outcomes) проекта.\nНет отчёта о фазе прототипирования.\nИзображения в proposal низкого качества плохо подписаны."
  },
  {
    "objectID": "projects.html#начальная-фаза",
    "href": "projects.html#начальная-фаза",
    "title": "",
    "section": "Начальная фаза",
    "text": "Начальная фаза\n🗓 26 апреля. 🦄 8 баллов\nПостер в латехе с разделением на разделы, объяснением - что в каком разделе где будет. Для удобства приведен 📝 \\LaTeX шаблон с 📜 примером."
  },
  {
    "objectID": "projects.html#предзащита",
    "href": "projects.html#предзащита",
    "title": "",
    "section": "Предзащита",
    "text": "Предзащита\n🗓 13 мая. 🦄 10 баллов"
  },
  {
    "objectID": "projects.html#publishing-plan",
    "href": "projects.html#publishing-plan",
    "title": "",
    "section": "Publishing plan",
    "text": "Publishing plan\n🗓 15 мая. 🦄 4 балла На основании результатов проделанной работы мы совместно решаем каким образом необходимо опубликовать проделанный труд. Это может быть: * статья в журнале * статья на конференции * статья для летней школы * доклад на конференции * публикация на вашем сайте * статья в блоге * видео на YouTubeканале и т.д.\nВ частности, этот план содержит конкретные даты, выбранный журнал, куда это будет подаваться."
  },
  {
    "objectID": "projects.html#публичная-защита",
    "href": "projects.html#публичная-защита",
    "title": "",
    "section": "Публичная защита",
    "text": "Публичная защита\n🗓 21 мая. 🦄 10 баллов\nК защите должен быть готов постер в латехе с результатами проекта. Подведение итогов. Здесь в первую очередь оценивается выступление студента. Оно должно быть понятным, структурированным, интересным.\nВсе дедлайны понимаются как 23:59:59 по Московскому времени."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Вычислительная линейная алгебра",
    "section": "",
    "text": "Вычислительная линейная алгебра\n\nКурс для 1-го курса бакалавриата ФПМИ МФТИ в рамках программы “AI360”. 1 лекция + 1 семинар в неделю.\nОценка за курс складывается из двух компонент в равном соотношении:\n\nТесты, проводимые на семинарах.\nОценка за проект.\n\n\nYour browser does not support the video tag.\n\n\n\n\n\n\n\n\n№\nЛекция\nСеминар\n\n\n\n\n1\nАрифметика чисел с плавающей точкой. Векторные нормы. Устойчивость\nЧисла с плавающей точкой, основы линейной алгебры - векторы, матрицы🎥 Видео💿 Скачать\n\n\n2\nИерархия памяти, умножение матриц, алгоритм Штрассена\nНормы матриц, унитарные матрицы✍️ Записи🎥 Видео💿 Скачать\n\n\n3\nРанг матрицы, скелетное разложение, SVD\nРанг метрица, скелетное разложение, SVD, LoRA ✍️ Записи🎥 Видео💿 Скачать\n\n\n4\nЛинейные системы\nSVD, PCA 👨‍💻 Eigenfaces ✍️ Записи🎥 Видео 💿 Скачать\n\n\n5\nСобственные значения и собственные векторы\nЛинейные системы, Собственные значения и собственные векторы ✍️ Записи🎥 Видео💿 Скачать\n\n\n6\nОбзор разложений матриц. Вычисление QR-разложения и разложения Шура\n👨‍💻 Упражнение PageRank\n\n\n7\nПроблема собственных значений симметричных матриц и SVD\n\n\n\n8\nРандомизированная линейная алгебра\nРандомизированное SVD, рандомизированное умножение матриц\n\n\n9\nОт плотной к разреженной линейной алгебре\nПрактика с разреженными линейными системами\n\n\n10\nВведение в итерационные методы\nГрадиентный спуск + многочлены Чебышёва + метод тяжелого шарика\n\n\n11\nВеликие итерационные методы\nГрам-Шмидт + идея метода сопряженных градиентов\n\n\n12\nИтерационные методы и предобуславливатели\nShampoo\n\n\n13\nСтруктурированные матрицы, БПФ, свертки, матрицы Тёплица\nПрактика с матрично-векторными операциями\n\n\n\n\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Иван Валерьевич Оселедец\n                    \n                    Лектор\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Максимович Меркулов\n                    \n                    Семинарист\n                  \n                \n              \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7.html",
    "href": "seminars/seminar-7/seminar-7.html",
    "title": "Recall determinant definition",
    "section": "",
    "text": "import numpy as np\nimport itertools\nfrom scipy.linalg import lu\nimport time\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7.html#determinant-of-triangular-matrix",
    "href": "seminars/seminar-7/seminar-7.html#determinant-of-triangular-matrix",
    "title": "Recall determinant definition",
    "section": "Determinant of triangular matrix",
    "text": "Determinant of triangular matrix\nDeterminant of an upper-(lower-)triangular matrix M is equal to \\prod_{i=1}^{n}M_{i,i}.\nProof: row expansion"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html",
    "href": "seminars/seminar-4/seminar-4.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#four-fundamental-subspaces",
    "href": "seminars/seminar-4/seminar-4.html#four-fundamental-subspaces",
    "title": "",
    "section": "Four Fundamental Subspaces",
    "text": "Four Fundamental Subspaces\nLet A = \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix}\n\nFind the column space \\text{col}(A) and null space \\text{null}(A) of A.\nDetermine if \\vec{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\in \\text{col}(A).\nDetermine if \\vec{u} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\text{null}(A).\n\n\ndef swap(matrix, row1, row2):\n    \"\"\"Swap two rows in a matrix.\"\"\"\n    matrix[[row1, row2]] = matrix[[row2, row1]]\n    return matrix\n\ndef scale(matrix, row, scalar):\n    \"\"\"Multiply all entries in a specified row by a scalar.\"\"\"\n    matrix[row] *= scalar\n    return matrix\n\ndef replace(matrix, row1, row2, scalar):\n    \"\"\"Replace a row by itself plus a scalar multiple of another row.\"\"\"\n    matrix[row1] += scalar * matrix[row2]\n    return matrix\n\n\nM = np.array([[1, -1, 0], [2, 1, 0], [-1, 3, 0]], dtype=float)\n\n\nM1 = replace(M.copy(), 1, 0, -2)\nM2 = replace(M1, 2, 0, 1)\nM3 = scale(M2, 1, 1/3)\nM4 = replace(M3, 2, 1, -2)\n\nprint(\"Reduced Matrix M4:\")\nprint(M4)\n\n\nM_augm = np.array([[1, -1, 1], [2, 1, 2], [-1, 3, 1]], dtype=float)\nM\n\n\n# Row Reduction Example\nM1 = replace(M_augm.copy(), 1, 0, -2)\nM2 = replace(M1, 2, 0, 1)\nM3 = scale(M2, 1, 1/3)\nM4 = replace(M3, 2, 1, -2)\nM4\n\nA = \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & -1 \\\\ 0 & 3 \\\\ 0 & 2 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \n\\mathrm{im}(A) = \\mathrm{col}(A) = \\mathrm{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 3 \\end{bmatrix} \\right\\}\n\\mathrm{im}(A^T) =  \\mathrm{col}(A^T) = \\mathrm{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\right\\}\n\\mathrm{ker}(A) = \\mathrm{null}(A) = \\{ 0 \\}\n\\mathrm{ker}(A^T) = \\mathrm{null}(A^T) =  \\mathrm{span}\\left\\{ \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right\\}"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#skeleton-decomposition",
    "href": "seminars/seminar-4/seminar-4.html#skeleton-decomposition",
    "title": "",
    "section": "Skeleton decomposition",
    "text": "Skeleton decomposition\nA = \\sum_{\\alpha = 1}^r U_{\\alpha} V_{\\alpha}^T\n##Low-rank Approximation\n\nimg = Image.open('./sk_campus_img.jpg').convert('L')\nimg_array = np.array(img)\noriginal_shape = img_array.shape\n\nplt.figure(figsize=(8, 4))\nplt.imshow(img_array, cmap='gray')\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n\nU, S, Vt = np.linalg.svd(img_array, full_matrices=False)  # economy SVD\nU.shape, S.shape, Vt.shape\n\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.semilogy(S / S[0])\nplt.xlabel(r\"$k$\")\nplt.ylabel(r\"$\\sigma_k / \\sigma_1$\")\nplt.title(r\"$\\sigma_k / \\sigma_1$\")\nplt.grid()\n\n\ncumulative_energy = np.cumsum(S**2) / np.sum(S**2)\nplt.subplot(1, 2, 2)\nplt.plot(cumulative_energy)\nplt.xlabel(r\"$k$\")\nplt.ylabel(r\"Cumulative Energy\")\nplt.title(r\"$(\\sum_{i=1}^k \\sigma_i) / \\sum_{i=0}^n \\sigma_i)$\")\nplt.grid()\n\nplt.show()\n\n\ndef reconstruct_image(k):\n    return (U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :])\n\n\nranks = [5, 20, 50, 100, original_shape[1]]\nplt.figure(figsize=(15, 5))\n\nfor i, rank in enumerate(ranks, 1):\n    plt.subplot(1, len(ranks), i)\n    recon_img = reconstruct_image(rank)\n    plt.imshow(recon_img, cmap='gray')\n    plt.title(f'Rank {rank}') if rank!= original_shape[1] else plt.title(f'Original Image')\n    plt.axis('off')\n\nplt.suptitle(\"Low-Rank Approximations of Image\")\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#svd-and-its-applications",
    "href": "seminars/seminar-4/seminar-4.html#svd-and-its-applications",
    "title": "",
    "section": "SVD and its Applications",
    "text": "SVD and its Applications\nSingular Value Decomposition (SVD) is a versatile tool in numerical linear algebra, implemented in many programming languages, typically relying on the LAPACK (Linear Algebra Package) library written in Fortran for its underlying computations.\n\nGeometric interpretation\n\ndef plot_transformed_circle_and_vectors(A, plot_singular_vectors=False, singular_values=None, singular_vectors=None,\n                                        circle_color='black', vector_colors=['blue', 'deeppink'],\n                                        singular_vector_colors=['red', 'green'],\n                                        singular_labels=[r'$\\sigma_1 u_1$', r'$\\sigma_2 u_2$'],\n                                        label_offset=0.2, xlim=(-8, 8), ylim=(-8, 8)):\n    theta = np.linspace(0, 2 * np.pi, 300)\n    unit_circle = np.vstack((np.cos(theta), np.sin(theta)))\n\n    transformed_circle = A @ unit_circle\n\n    plt.plot(transformed_circle[0, :], transformed_circle[1, :], color=circle_color, alpha=0.5)\n\n    e1_transformed = A @ np.array([1, 0])\n    e2_transformed = A @ np.array([0, 1])\n\n    for i, vec in enumerate([e1_transformed, e2_transformed]):\n        color = vector_colors[i]\n        plt.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color=color)\n\n    if plot_singular_vectors and singular_values is not None and singular_vectors is not None:\n        for i, (sigma, vec) in enumerate(zip(singular_values, singular_vectors.T)):\n            vec_scaled = sigma * vec\n            color = singular_vector_colors[i]\n            label = singular_labels[i]\n            plt.quiver(0, 0, vec_scaled[0], vec_scaled[1], angles='xy', scale_units='xy', scale=1, color=color)\n            plt.text(vec_scaled[0] * (1 + label_offset), vec_scaled[1] * (1 + label_offset), label, color=color, fontsize=12)\n\n    plt.axvline(x=0, color='black', lw=1)\n    plt.axhline(y=0, color='black', lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n\n\nA = np.array([[1, 3], [4, 5]])\n\nU, D, Vt = np.linalg.svd(A)\n\nprint('Unit circle (before transformation):')\nplot_transformed_circle_and_vectors(np.eye(2), xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\nplt.show()\n\nprint('1st rotation by V (right singular vectors):')\nplot_transformed_circle_and_vectors(Vt.T, xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\nplt.show()\n\nprint('Scaling by D:')\nscaling_matrix = np.diag(D) @ Vt\nplot_transformed_circle_and_vectors(scaling_matrix, xlim=(-8, 8), ylim=(-8, 8))\nplt.show()\n\nprint('2nd rotation by U (final transformation by A):')\nfinal_transformation = U @ np.diag(D) @ Vt\nplot_transformed_circle_and_vectors(final_transformation, xlim=(-8, 8), ylim=(-8, 8))\nplt.show()\n\n\nA = np.array([[1, 3], [4, 5]])\n\nU, D, Vt = np.linalg.svd(A)\n\nprint(\"Transformed unit circle, basis vectors, and singular vectors:\")\nplot_transformed_circle_and_vectors(A,\n                                    plot_singular_vectors=True,\n                                    singular_values=D,\n                                    singular_vectors=U,\n                                    singular_labels=[r'$\\sigma_1 u_1$', r'$\\sigma_2 u_2$'])\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#applications-in-image-reconstruction",
    "href": "seminars/seminar-4/seminar-4.html#applications-in-image-reconstruction",
    "title": "",
    "section": "Applications in Image Reconstruction",
    "text": "Applications in Image Reconstruction\nMatrix completion, commonly used for filling missing data, can be applied to image and recommendation systems. A well-known example is movie recommendation systems, where a ratings matrix is often only partially filled, as users have not rated every movie. To provide accurate recommendations, we aim to predict these missing ratings.\nThis task is feasible because user ratings tend to follow patterns, meaning the ratings matrix is often low-rank; only a limited amount of information is needed to approximate it well.\nA similar approach applies to images, where pixel values often depend on neighboring pixels, making low-rank approximations effective for reconstructing images with missing or corrupted data."
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#svd-in-facial-recognition-eigenfaces",
    "href": "seminars/seminar-4/seminar-4.html#svd-in-facial-recognition-eigenfaces",
    "title": "",
    "section": "SVD in Facial Recognition: Eigenfaces",
    "text": "SVD in Facial Recognition: Eigenfaces\nThe “Eigenfaces for Recognition” paper introduced a novel approach to facial recognition. Unlike earlier methods that focused on detecting individual features (e.g., eyes or nose), Eigenfaces uses SVD to extract and encode essential information from face images. This encoding allows for efficient comparisons between faces by compressing the most relevant facial information into a low-dimensional representation. This method paved the way for data-driven approaches in face recognition, relying on similarities within the encoded space rather than feature-by-feature comparison.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\n\nfrom sklearn.datasets import fetch_lfw_people\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom sklearn.datasets import fetch_lfw_people\n\n\nlfw_dataset = fetch_lfw_people(min_faces_per_person=100)\nX, y, target_names = lfw_dataset.data, lfw_dataset.target, lfw_dataset.target_names\nh, w = lfw_dataset.images.shape[1:3]\nprint(f\"Number of samples: {X.shape[0]}, Image size: {h}x{w}, Unique classes: {len(target_names)}\")\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n\nU, S, VT = np.linalg.svd(X_train, full_matrices=False)\n\nnum_components = 100\nface_space = VT[:num_components, :]\n\nX_train_transformed = X_train @ face_space.T\nX_test_transformed = X_test @ face_space.T\n\nplt.figure(figsize=(8, 4))\nplt.semilogy(np.arange(len(S)), S / S[0], marker=\"o\")\nplt.title(\"$\\sigma_k / \\sigma_1$\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"$\\sigma_k / \\sigma_1$\")\nplt.grid()\nplt.show()\n\n\ndef plot_reconstructed_images(original, transformed, face_space, h, w, index=0):\n    reconstructed = transformed[index] @ face_space\n    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n    ax[0].imshow(original[index].reshape(h, w), cmap=\"gray\")\n    ax[0].set_title(\"Original Image\")\n    ax[1].imshow(reconstructed.reshape(h, w), cmap=\"gray\")\n    ax[1].set_title(\"Reconstructed Image\")\n    plt.show()\n\nplot_reconstructed_images(X_train, X_train_transformed, face_space, h, w, index=0)\n\nclf_knn = KNeighborsClassifier().fit(X_train_transformed, y_train)\nclf_mlp = MLPClassifier(hidden_layer_sizes=(1024,), batch_size=256, early_stopping=True).fit(X_train_transformed, y_train)\n\ny_pred_knn = clf_knn.predict(X_test_transformed)\ny_pred_mlp = clf_mlp.predict(X_test_transformed)\nprint(\"k-NN Classifier Report:\\n\", classification_report(y_test, y_pred_knn, target_names=target_names))\nprint(\"MLP Classifier Report:\\n\", classification_report(y_test, y_pred_mlp, target_names=target_names))\n\nIt might seem discouraging, but it’s worthwhile to check if the data is imbalanced and go through the steps again (exercise)."
  },
  {
    "objectID": "seminars/seminar-11/seminar-11.html",
    "href": "seminars/seminar-11/seminar-11.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport scipy.sparse\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nfrom sklearn.utils.extmath import randomized_svd\nThe best low-rank approximation can be computed by SVD.\nTheorem: Let r &lt; \\text{rank}(A), A_r = U_r \\Sigma_r V_r^*. Then\n\\min_{\\text{rank}(B)=r} \\|A - B\\|_2 = \\|A - A_r\\|_2 = \\sigma_{r+1}.\nThe same holds for \\|\\cdot\\|_F, but \\|A - A_r\\|_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{\\min (n,m)}^2}."
  },
  {
    "objectID": "seminars/seminar-11/seminar-11.html#low-rank-and-sparse-decomposition",
    "href": "seminars/seminar-11/seminar-11.html#low-rank-and-sparse-decomposition",
    "title": "",
    "section": "Low-rank and sparse decomposition",
    "text": "Low-rank and sparse decomposition\nA_r = U_r \\Sigma_r V_r^* S = A - A_r Ax = A_r x + Sx = U_r \\Sigma_r V_r^*x + Sx\nFor A: n \\times n and rank truncation r&lt;n:\nComplexity of Ax: \\mathcal{O}(n^2)\nComplexity of A_rx: \\mathcal{O}(nr)\nComplexity of Sx: \\mathcal{O}(nnz(S))\nIt becomes effective with:\nr&lt;&lt;n\nnnz(S)&lt;&lt;n^2\n\ndef decompose_matrix_with_sparse_correction_optimized(A, rank, threshold=1e-3):\n    U, sigma, Vt = randomized_svd(A, n_components=rank, n_iter=5, random_state=None)\n    U_r = U[:, :rank]\n    Sigma_r = sigma[:rank]\n    Vt_r = Vt[:rank, :]\n    B = (U_r * Sigma_r) @ Vt_r\n    S_dense = A - B\n    S_dense[np.abs(S_dense) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_dense)\n    return U_r, Sigma_r, Vt_r, S_sparse\n\ndef optimized_multiply(U, Sigma, Vt, S, x):\n    temp = Vt @ x\n    temp = Sigma * temp\n    Bx = U @ temp\n    Sx = S @ x\n    return Bx + Sx\n\n\n# Parameters\nsizes = np.arange(200, 4000, 200)\nthreshold = 0.6\nmax_rank = 0.05\n\n\nmses = []\nexact_times = []\napprox_times = []\nsvd_times = []\n\n\nfor i, n in enumerate(sizes):\n    A = np.random.rand(n, n)\n    x = np.random.rand(n)\n    rank = int(n*max_rank)\n    start_time = time.time()\n    U_r, Sigma_r, Vt_r, S_sparse = decompose_matrix_with_sparse_correction_optimized(A, rank, threshold)\n\n    svd_times.append(time.time() - start_time)\n\n    # Measure time for exact multiplication\n    start_time = time.time()\n    exact_result = A @ x\n    exact_times.append(time.time() - start_time)\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(sizes),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(sizes, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Size Grows\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(sizes, exact_times, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(sizes, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nn = 3000\nthreshold = 0.5\nmax_ranks = np.arange(0.0, 1.0, 0.05)\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\n\nU, Sigma, Vt, S = decompose_matrix_with_sparse_correction_optimized(A, n, 0)\n\n\n\nfor i, max_rank in enumerate(max_ranks):\n    rank = max(int(n*max_rank), 1)\n    U_r = U[:, :rank]\n    Sigma_r = Sigma[:rank]\n    Vt_r = Vt[:rank, :]\n    B = (U_r * Sigma_r) @ Vt_r\n    S_dense = A - B\n    S_dense[np.abs(S_dense) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_dense)\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(max_ranks, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Rank is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(max_ranks, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(max_ranks, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nn = 3200\nthresholds = np.arange(0.0, 1.0, 0.05)\nrank = 200\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\nU_r, Sigma_r, Vt_r, S = decompose_matrix_with_sparse_correction_optimized(A, rank, 0)\n\nfor i, threshold in enumerate(thresholds):\n    S_sparse = S.toarray()\n    S_sparse[np.abs(S_sparse) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_sparse)\n\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Density is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(thresholds, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\nU_r, Sigma_r, Vt_r, S = decompose_matrix_with_sparse_correction_optimized(A, rank, 0)\n\nfor i, threshold in enumerate(thresholds):\n    S_sparse = S.toarray()\n    S_sparse[np.abs(S_sparse) &lt; threshold] = 0\n    #S_sparse = scipy.sparse.csr_matrix(S_sparse)\n\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks))\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Density is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(thresholds, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html",
    "href": "seminars/seminar-5/Seminar-5.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\nimport scipy"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html#outline",
    "href": "seminars/seminar-5/Seminar-5.html#outline",
    "title": "",
    "section": "Outline",
    "text": "Outline\n\nLU decomposition as a sum of rank one matrices\nLU Decomposition with Pivoting\nOverdetermined Linear Systems (Least Squares) 3.1 Geometry of Normal Equation  3.2 Pseudoinverse  3.3 Pseudoinverse via SVD  3.4 QR approach  3.5 Linear Regression\n\nIn many applications, we encounter overdetermined systems of linear equations, where there are more equations than unknowns, represented as\n Ax = b. \nThese systems often lack an exact solution.\nTo address this, we can use the least squares method, which finds an approximate solution by minimizing the residual error:\n x = \\arg\\min_x \\| Ax - b \\|_2"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html#overdetermined-linear-systems",
    "href": "seminars/seminar-5/Seminar-5.html#overdetermined-linear-systems",
    "title": "",
    "section": "Overdetermined Linear Systems",
    "text": "Overdetermined Linear Systems\nMany applications lead to unsolvable linear equations Ax = b, where the number of equations is greater, than the number of unknowns.\nThe least squares method chooses the solution as\n x = \\arg\\min_x \\| Ax - b \\|_2\n\nThe Normal Equation\nThe solution to the least squares problem satisfies the normal equation:\n A^T A x = A^T b \nWhen $ A $ has full column rank, the matrix $ A^T A $ is positive definite, allowing us to solve this equation efficiently with Cholesky decomposition:\n A^T A = R^T R \nwhere $ R $ is an upper triangular matrix (and $ R^T $ is lower triangular). This leads to the following set of equations:\n\n\\begin{align}\n    R^T y &= A^T b \\\\\n    R x &= y\n\\end{align}\n\nHowever, solving the normal equations directly can be numerically unstable, especially for larger problems. This approach is generally safe for small problems, but for stability, other methods are recommended. If the pivots in Gaussian elimination are small, LU decomposition may fail, whereas Cholesky decomposition remains stable in these cases.\n\n# Cholesky Decomposition Method\ndef leastsq_chol(A, b):\n    R = scipy.linalg.cholesky(A.T @ A)\n    w = scipy.linalg.solve_triangular(R, A.T @ b, trans='T')\n    return scipy.linalg.solve_triangular(R, w)\n\n\n\nThe Pseudoinverse\nIn linear algebra, not all matrices have an inverse, particularly when a system of equations has no solution or many solutions. The Moore-Penrose pseudoinverse offers a way to find an approximate solution that minimizes error, even when a unique solution doesn’t exist.\nIn the lecture, the pseudoinverse is defined as:\n A^{\\dagger} = \\lim_{\\alpha \\to 0} (A^T A + \\alpha I)^{-1} A^T \nAlternatively,\n A^{\\dagger} = \\lim_{\\alpha \\to 0} A^T (A A^T + \\alpha I)^{-1} \nThese limits exist even if $ (A^T A)^{-1} $ or $ (A AT){-1} $ do not. Later, we will see how this relates to Tikhonov regularization.\nIf $ A $ has full column rank, the pseudoinverse simplifies to:\n A^{\\dagger} = (A^T A)^{-1} A^T \n\n\nComputing the Pseudoinverse Using SVD\nAnother way to compute the pseudoinverse is through the Singular Value Decomposition (SVD) $ A = U V^T $:\n A^{\\dagger} = V \\Sigma^{\\dagger} U^T \nIn this approach, we invert the diagonal entries of $ $ where possible.\nWith the pseudoinverse $ A^{} $, we can write the solution to $ Ax = b $ as:\n x = A^{\\dagger} b \n\n\nExample: Solving a System Using Pseudoinverse\nLet’s now consider the toy matrix and system of equations:\n \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x = \\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} \n\n# Pseudo-inverse Method using Matrix Inversion\ndef leastsq_pinv(A, b):\n    return np.linalg.inv(A.T @ A) @ A.T @ b\n\n\n# Pseudo-inverse Method using SVD\ndef leastsq_pinv_svd(A, b):\n    U, S, Vh = scipy.linalg.svd(A)\n    S_plus = np.zeros(A.shape).T\n    S_plus[:S.shape[0], :S.shape[0]] = np.linalg.inv(np.diag(S))\n    return Vh.T @ S_plus @ U.T @ b\n\nLet’s now return to our toy matrix and consider the system\n\\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x =\\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} \n\nA = np.array([[1., -1.], [2., 1.], [-1., 3.]])\nb = np.array([[0.], [3.], [2.]])\n\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\n\nA_pinv = np.linalg.pinv(A)\nx_sol = A_pinv @ b\nprint(\"\\n Solution by using pseudoinverse: \\n\", x_sol)\n\n\nx_1 = np.linspace(-3, 3, 100)\nx_2_1 = x_1\nx_2_2 = 3. - 2. * x_1\nx_2_3 =(2. - x_1) / 3\n\nplt.plot(x_1, x_2_1)\nplt.plot(x_1, x_2_2)\nplt.plot(x_1, x_2_3)\nplt.scatter(x_sol[0], x_sol[1], c='r')\n\nplt.xlim(0., 2.)\nplt.ylim(-0.5, 2.)\nplt.show()\n\n\n\nQR Decomposition for Solving Least Squares Problem\nIf $ A $ has full column rank, a QR decomposition exists for $ A $:\n A = QR \nThis allows us to rewrite the normal equations as:\n R^T Q^T Q R x = R^T Q^T b \nSince $ Q^T Q = I $, this further simplifies to:\n R x = Q^T b. \n\n# QR Decomposition Method\ndef leastsq_qr(A, b):\n    Q, R = scipy.linalg.qr(A, mode='economic')\n    return scipy.linalg.solve_triangular(R, Q.T @ b)\n\n\n\nLinear Regression\n\ndef SyntheticData(x, corr_col=False, noise=0.1, num_points=100):\n    A = np.random.randn(num_points, len(x) - 1)\n    A = np.hstack((A, np.ones((num_points, 1))))\n    if corr_col and len(x) &gt; 2:\n        A[:, 2] = A[:, 1] + np.random.rand(num_points) * noise * 1e-4\n    noise = np.random.randn(num_points, 1) * noise\n    b = A @ x.reshape((-1, 1)) + noise\n    return A, b\n\n\nx_true = np.array([2.0, 1.0])\n\nA, b = SyntheticData(x_true, num_points=1000)\n\nx_chol = leastsq_chol(A, b)\nx_pinv = leastsq_pinv(A, b)\nx_svd = leastsq_pinv_svd(A, b)\nx_qr = leastsq_qr(A, b)\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\nprint(\"True coefficients:\\n\", x_true)\nprint(\"Cholesky solution:\\n\", x_chol.flatten())\nprint(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\nprint(\"SVD solution:\\n\", x_svd.flatten())\nprint(\"QR solution:\\n\", x_qr.flatten())\n\nplt.figure(figsize=(10, 6))\nplt.scatter(A[:, 0], b, label='Data Points', color='blue', alpha=0.5)\n\nplt.plot(A[:, 0], A @ x_true, label='True Line', color='green')\nplt.plot(A[:, 0], A @ x_chol, label='Cholesky Fit', color='red')\nplt.plot(A[:, 0], A @ x_pinv, label='Pseudo-inverse Fit', color='purple')\nplt.plot(A[:, 0], A @ x_svd, label='SVD Fit', color='orange')\nplt.plot(A[:, 0], A @ x_qr, label='QR Fit', color='cyan')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('1D Linear Regression')\nplt.show()\n\nFinally, let’s consider the case where $ A $ does not have full column rank.\n\nx_true = np.array([2.0, 3.0, -1.0, 1.0])\n\nA, b = SyntheticData(x_true, corr_col=True, num_points=1000)\n\nx_chol = leastsq_chol(A, b)\nx_pinv = leastsq_pinv(A, b)\nx_svd = leastsq_pinv_svd(A, b)\nx_qr = leastsq_qr(A, b)\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\nprint(\"True coefficients:\\n\", x_true)\nprint(\"Cholesky solution:\\n\", x_chol.flatten())\nprint(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\nprint(\"SVD solution:\\n\", x_svd.flatten())\nprint(\"QR solution:\\n\", x_qr.flatten())\n\nIn such cases Tikhonov regularization is more effective for finding stable solutions.\nExercise Show that the problem\n \\min \\|x'\\|_2 \\quad \\text{s.t. } x' = \\arg\\min_x \\| Ax - b \\|_2^2 \nis equivalent to the following regularization problem:\n \\min_x \\| Ax - b \\|_2^2 + \\lambda \\|x\\|_2^2. \nNote that the analytical solution is available in this case as well:\n x^* = (A^T A + \\lambda I)^{-1}  X^T b.\nThe analytical solutions described earlier involve inverting the matrix A^T A (or A^T A + \\lambda I), which is computationally expensive. This brings us to iterative methods, which are generally more efficient and have become the primary approach for numerous applications.\nGradient descent is one of the most widely used optimization methods. It is important to note that the objective function (e.g., the loss function, which is the residual in our case: \\mathcal{L}(x) = \\|Ax - b\\|_2^2) should be differentiable with respect to the unknown $ x $. Using gradient descent, the weight vector at each step can be updated as follows:\n x^{k+1} = x^k - \\beta_k \\nabla \\mathcal{L}(x^k) \nExercise Compute the gradient of \\mathcal{L}_{\\lambda}(x) = \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2.\nTo further improve efficiency, one could use stochastic gradient descent (SGD), which computes the gradient over a randomly selected subset of data points, rather than the full dataset A.\nThese ideas will be explored further in the second homework assignment… stay tuned!"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html",
    "href": "files/Eigenfaces_exercise.html",
    "title": "",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#load-the-dataset",
    "href": "files/Eigenfaces_exercise.html#load-the-dataset",
    "title": "",
    "section": "Load the dataset",
    "text": "Load the dataset\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_lfw_people #lol\nfrom sklearn.preprocessing import StandardScaler\n\ndataset     = fetch_lfw_people(min_faces_per_person=100, resize=0.4)\nA           = dataset['data']\nlabels      = dataset['target']\nclasses     = dataset['target_names']\nlabel_names = np.array([classes[label] for label in labels])\nprint('🤖: Dataset contains {} points in {}-dimensional space'.format(*A.shape))\n\ndef plot_gallery(dataset, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    n_samples, h, w = dataset.images.shape\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    images, titles = dataset[\"images\"], dataset[\"target\"]\n    titles = [dataset[\"target_names\"][title] for title in titles]\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\nplot_gallery(dataset)\n\n🤖: Dataset contains 1140 points in 1850-dimensional space"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-normalize-the-data-to-have-zero-mean",
    "href": "files/Eigenfaces_exercise.html#task-normalize-the-data-to-have-zero-mean",
    "title": "",
    "section": "Task: normalize the data to have zero mean",
    "text": "Task: normalize the data to have zero mean\n\n### YOU CODE HERE\nA_std = ..."
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-calculate-svd-of-normalized-matrix",
    "href": "files/Eigenfaces_exercise.html#task-calculate-svd-of-normalized-matrix",
    "title": "",
    "section": "Task: Calculate SVD of normalized matrix",
    "text": "Task: Calculate SVD of normalized matrix\n\nA_{std} = U \\Sigma V^\\top\n\n\n### YOU CODE HERE\nu, sigmas, vt = ..."
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-plot-eigenfaces",
    "href": "files/Eigenfaces_exercise.html#task-plot-eigenfaces",
    "title": "",
    "section": "Task: plot eigenfaces",
    "text": "Task: plot eigenfaces\n\ndef plot_eigenfaces(dataset=dataset, u=u, sigmas=sigmas, vt=vt, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    n_samples, h, w = dataset.images.shape\n    ### YOU CODE HERE\n    projections = ...\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        \n        plt.imshow(projections[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(f\"Eigenface #{i+1}\", size=12)\n        plt.xticks(())\n        plt.yticks(())\n      \nplot_eigenfaces()"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-plot-reconstructions",
    "href": "files/Eigenfaces_exercise.html#task-plot-reconstructions",
    "title": "",
    "section": "Task: plot reconstructions",
    "text": "Task: plot reconstructions\n\ndef plot_projections(rank = 20, dataset=dataset, u=u, sigmas=sigmas, vt=vt, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    n_samples, h, w = dataset.images.shape\n    ### YOU CODE HERE\n    projections = ...\n    reconstructions = ...\n\n    images, titles = dataset[\"images\"], dataset[\"target\"]\n    titles = [dataset[\"target_names\"][title] for title in titles]\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    print(f\"Rank {rank} compression\")\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        ### YOU CODE HERE\n        plt.imshow(reconstructions[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(f\"{titles[i]}\", size=12)\n        plt.xticks(())\n        plt.yticks(())\n      \nplot_projections()\n\nRank 20 compression"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#plot-cumulative-variance-by-each-individual-component-graph",
    "href": "files/Eigenfaces_exercise.html#plot-cumulative-variance-by-each-individual-component-graph",
    "title": "",
    "section": "Plot cumulative variance by each individual component graph",
    "text": "Plot cumulative variance by each individual component graph\n\n### YOUR CODE HERE\ntotal_variance = ...\nvariance_explained = [(i / total_variance)*100 for i in sorted(sigmas, reverse=True)]\ncumulative_variance_explained = np.cumsum(variance_explained)\n\n### YOUR CODE HERE\nn_sigmas = ...\nxs = [0.5 + i for i in range(n_sigmas)]\nplt.bar(xs, variance_explained, alpha=0.5, align='center',\n        label='Individual explained variance')\nplt.step(xs, cumulative_variance_explained, where='mid',\n         label='Cumulative explained variance')\nplt.ylabel('Explained variance')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\n# plt.xticks(np.arange(A_std.shape[1]+1))\nplt.show()"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#представление-чисел-с-фиксированной-точкой",
    "href": "lectures/lecture-1/lecture-1.html#представление-чисел-с-фиксированной-точкой",
    "title": "Представление чисел",
    "section": "Представление чисел с фиксированной точкой",
    "text": "Представление чисел с фиксированной точкой\n\nЧисла с фиксированной точкой - простейший способ представления вещественных чисел в цифровом виде\n\nТакже известен как формат Qm.n, где:\n\nm бит для целой части\nn бит для дробной части\n\n\nОсновные свойства:\n\nДиапазон: [-(2^m), 2^m - 2^{-n}]\nРазрешение: 2^{-n} (наименьшая представимая разница)\nХранение: всего m + n + 1 бит (включая знаковый бит)\n\nОграничения:\n\nФиксированный диапазон представимых чисел\nКомпромисс между диапазоном (m) и точностью (n)\nНеэффективно для представления очень больших или очень малых чисел"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-формула",
    "href": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-формула",
    "title": "Представление чисел",
    "section": "Числа с плавающей точкой: формула",
    "text": "Числа с плавающей точкой: формула\nf = (-1)^s 2^{(p-b)} \\left( 1 + \\frac{d_1}{2} + \\frac{d_2}{2^2}  + \\ldots + \\frac{d_m}{2^m}\\right),\nгде s \\in \\{0, 1\\} - знаковый бит, d_i \\in \\{0, 1\\} - m-битная мантисса, p \\in \\mathbb{Z}; 0 \\leq p \\leq 2^e, e - e-битная экспонента, обычно определяемая как 2^e - 1\nМожно представить как равномерную m-битную сетку между двумя последовательными степенями числа 2."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#простые-примеры",
    "href": "lectures/lecture-1/lecture-1.html#простые-примеры",
    "title": "Представление чисел",
    "section": "Простые примеры",
    "text": "Простые примеры\nСуществует много способов записать число в научной нотации, но всегда есть единственное нормализованное представление с ровно одной ненулевой цифрой слева от десятичной точки.\nFor example: - 0.232 \\times 10^3 = 23.2 \\times 10^1 = 2.32 \\times 10^2 = \\ldots - 01001 = 1.001 \\times 2^3 = \\ldots\nПример 1: Каково нормализованное представление числа 00101101.101?\n0.0001101001110 = 1.110100111 \\times 2^{-4}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#умножение-более-подробно",
    "href": "lectures/lecture-1/lecture-1.html#умножение-более-подробно",
    "title": "Представление чисел",
    "section": "Умножение более подробно",
    "text": "Умножение более подробно\nРассмотрим два числа с плавающей точкой x, y, у которых экспоненты и дробные части равны x_e, y_e и x_m, y_m соответственно. Результат обычного умножения чисел с плавающей точкой будет:\n\n\\operatorname{Mul}(x, y)=\\left(1+x_m\\right) \\cdot 2^{x_e} \\cdot\\left(1+y_m\\right) \\cdot 2^{y_e}=\\left(1+x_m+y_m+x_m \\cdot y_m\\right) \\cdot 2^{x_e+y_e}\n\nНедавние статьи: Addition is all you need пытаются доказать, что мы можем заменить это умножение сложением и все равно получить обучаемые нейронные сети (требует проверки)."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#фиксированная-и-плавающая-точка",
    "href": "lectures/lecture-1/lecture-1.html#фиксированная-и-плавающая-точка",
    "title": "Представление чисел",
    "section": "Фиксированная и плавающая точка",
    "text": "Фиксированная и плавающая точка\nВ: Каковы преимущества/недостатки чисел с фиксированной и плавающей точкой?\nA: In most cases, they work just fine.\n\nHowever, fixed point represents numbers within specified range and controls absolute accuracy.\nFloating point represent numbers with relative accuracy, and is suitable for the case when numbers in the computations have varying scale (i.e., 10^{-1} and 10^{5}).\nIn practice, if speed is of no concern, use float32 or float64 (на самом деле, нет!)"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-на-логарифмической-шкале",
    "href": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-на-логарифмической-шкале",
    "title": "Представление чисел",
    "section": "Числа с плавающей точкой на логарифмической шкале",
    "text": "Числа с плавающей точкой на логарифмической шкале\nВизуализируем, как числа с плавающей точкой распределены на вещественной прямой:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define parameters for a small floating-point system\nb = 2  # base\ne_max = 2  # maximum exponent\nmantissa_bits = 2  # number of bits for mantissa\n\n# Generate all possible combinations of exponent and mantissa\nexponents = range(-e_max, e_max + 1)\nmantissas = np.linspace(0, 1 - 2**(-mantissa_bits), 2**mantissa_bits)\n\n# Calculate floating-point numbers\nfp_numbers = []\nfor e in exponents:\n    for m in mantissas:\n        fp_numbers.append((1 + m) * b**(e))\n\n# Sort the numbers for proper visualization\nfp_numbers.sort()\n\n# Create figure\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the floating-point numbers\nax.scatter(fp_numbers, [1] * len(fp_numbers), marker='|', s=100, color='blue')\nax.set_ylim(0.5, 1.5)\nax.set_yticks([])  # Remove y-axis ticks\n\nax.set_title(r'Distribution of Floating-Point Numbers (base=$%d$, max_exp=$%d$, mantissa_bits=$%d$)' % (b, e_max, mantissa_bits))\nax.set_xlabel('Value')\n\n# Add text explanation\nplt.figtext(0.1, -0.05, \n    r\"Base (b) = $%d$, Max exponent = $%d$, Mantissa bits = $%d$\" % (b, e_max, mantissa_bits) + \"\\n\" +\n    r\"Numbers are of the form $(1 + x_m) \\cdot 2^e$, where $x_m$ is the fraction and $e$ is the exponent.\",\n    wrap=True)\n#plt.tight_layout()\n#plt.show()\n\nText(0.1, -0.05, 'Base (b) = $2$, Max exponent = $2$, Mantissa bits = $2$\\nNumbers are of the form $(1 + x_m) \\\\cdot 2^e$, where $x_m$ is the fraction and $e$ is the exponent.')"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#ieee-754",
    "href": "lectures/lecture-1/lecture-1.html#ieee-754",
    "title": "Представление чисел",
    "section": "IEEE 754",
    "text": "IEEE 754\nВ современных компьютерах представление чисел с плавающей точкой контролируется стандартом IEEE 754, который был опубликован в 1985 году, и до этого момента разные компьютеры по-разному работали с числами с плавающей точкой.\nIEEE 754 имеет: - Представление чисел с плавающей точкой (как описано выше), (-1)^s \\times c \\times b^q. - Две бесконечности, +\\infty и -\\infty - Два нуля: +0 и -0 - Два вида NaN: тихий NaN (qNaN) и сигнальный NaN (sNaN) - qNaN не вызывает исключение на уровне модуля операций с плавающей точкой (FPU), пока вы не проверите результат вычислений - значение sNaN вызывает исключение от FPU при использовании соответствующей переменной. Этот тип NaN может быть полезен для целей инициализации - C++11 предлагает стандартный интерфейс для создания различных NaN - Правила округления - Правила для \\frac{0}{0}, \\frac{1}{-0}, \\ldots\nВозможные значения определяются с помощью - основания b - точности p - количество цифр - максимально возможного значения e_{\\max}\nи имеют следующие ограничения - $ 0 c b^p - 1$ - 1 - e_{\\max} \\leq q + p - 1 \\leq e_{\\max}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#одинарная-и-двойная-точность",
    "href": "lectures/lecture-1/lecture-1.html#одинарная-и-двойная-точность",
    "title": "Представление чисел",
    "section": "Одинарная и двойная точность",
    "text": "Одинарная и двойная точность\nДва стандартных формата, называемые binary32 и binary64 (также называемые форматами одинарной и двойной точности). В последнее время формат binary16 играет важную роль в обучении глубоких нейронных сетей.\n\n\n\nНазвание\nОбщее название\nОснование\nРазряды\nEmin\nEmax\n\n\n\n\nbinary16\nполовинная точность\n2\n11\n-14\n+ 15\n\n\nbinary32\nодинарная точность\n2\n24\n-126\n+ 127\n\n\nbinary64\nдвойная точность\n2\n53\n-1022\n+1023"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#примеры",
    "href": "lectures/lecture-1/lecture-1.html#примеры",
    "title": "Представление чисел",
    "section": "Примеры",
    "text": "Примеры\n\nДля числа +0\n\nsign равен 0\nexponent равен 00000000000\nfraction состоит из нулей\n\nДля числа -0\n\nsign равен 1\nexponent равен 00000000000\nfraction состоит из нулей\n\nДля +бесконечности\n\nsign равен 0\nexponent равен 11111111111\nfraction состоит из нулей\n\n\nВопрос: что насчет -бесконечности и NaN?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#точность-и-память",
    "href": "lectures/lecture-1/lecture-1.html#точность-и-память",
    "title": "Представление чисел",
    "section": "Точность и память",
    "text": "Точность и память\nОтносительная точность одинарной точности составляет 10^{-7}-10^{-8}, в то время как для двойной точности - 10^{-14}-10^{-16}.\n Важное замечание 1:  float16 занимает 2 байта, float32 занимает 4 байта, float64, или двойная точность, занимает 8 байт.\n Важное замечание 2:  Только эти два типа чисел с плавающей точкой поддерживаются аппаратно (float32 и float64) + на GPU/TPU поддерживаются различные типы с плавающей точкой.\n Важное замечание 3:  Вам следует использовать двойную точность в вычислительной науке и инженерии, и float32/float16 на GPU/в науке о данных.\nСейчас для больших моделей float16 становится все более надежным."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#как-формат-представления-чисел-влияет-на-обучение-нейронных-сетей-нс",
    "href": "lectures/lecture-1/lecture-1.html#как-формат-представления-чисел-влияет-на-обучение-нейронных-сетей-нс",
    "title": "Представление чисел",
    "section": "Как формат представления чисел влияет на обучение нейронных сетей (НС)?",
    "text": "Как формат представления чисел влияет на обучение нейронных сетей (НС)?\n\nВеса в слоях (полносвязных, сверточных, функциях активации) могут храниться с разной точностью\nВажно повышать энергоэффективность устройств, используемых для обучения НС\nПроект DeepFloat от Facebook демонстрирует, как переработать операции с плавающей точкой для обеспечения эффективности при обучении НС, подробнее см. в этой статье\nВлияние представления вещественных чисел на градиенты функций активации\nОбычно первая цифра равна единице\nСубнормальные числа имеют первую цифру 0 для представления нулей и чисел, близких к нулю\nСубнормальные числа заполняют промежуток между положительными и отрицательными числами\nУ них есть проблемы с производительностью, часто по умолчанию обнуляются\n\n\n\nИ на кривых обучения\n\n\nГрафики взяты из этой статьи"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#bfloat16-brain-floating-point",
    "href": "lectures/lecture-1/lecture-1.html#bfloat16-brain-floating-point",
    "title": "Представление чисел",
    "section": "bfloat16 (Brain Floating Point)",
    "text": "bfloat16 (Brain Floating Point)\n\nЭтот формат занимает 16 бит\n\n1 бит для sign\n8 бит для exponent\n7 бит для fraction\n\n\nУсеченный формат одинарной точности из стандарта IEEE\nВ чем разница между float32 и float16?\nЭтот формат используется в Intel FPGA, Google TPU, Xeon CPUs и других платформах"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#tensor-float-от-nvidia-блог-пост-об-этом-формате",
    "href": "lectures/lecture-1/lecture-1.html#tensor-float-от-nvidia-блог-пост-об-этом-формате",
    "title": "Представление чисел",
    "section": "Tensor Float от Nvidia (блог пост об этом формате)",
    "text": "Tensor Float от Nvidia (блог пост об этом формате)\n\nСравнение с другими форматами\n\n\n\nРезультаты\n\n\n\nPyTorch и Tensorflow с поддержкой этого формата доступны в Nvidia NCG"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#альтернатива-стандарту-ieee-754",
    "href": "lectures/lecture-1/lecture-1.html#альтернатива-стандарту-ieee-754",
    "title": "Представление чисел",
    "section": "Альтернатива стандарту IEEE 754",
    "text": "Альтернатива стандарту IEEE 754\nПроблемы в IEEE 754: - переполнение в бесконечность или ноль - множество различных NaN - невидимые ошибки округления - точность либо очень высокая, либо очень низкая - subnormal numbers – числа между 0 и минимальным возможным представимым числом, т.е. significand начинается с нуля\nКонцепция posits может заменить числа с плавающей точкой, см. эту статью\n\n\nпредставляют числа с некоторой точностью, но обеспечивают пределы изменения\nнет переполнений!\nпример представления числа\n\n\n\nДемонстрация точности деления\n\nimport random\nimport numpy as np\n\nc = np.float32(0.925924589693)\nprint(c)\na = np.float32(1.786875867457e-2)\nb = np.float32(c / a)\nprint('{0:10.16f}'.format(b))\nprint(abs(a * b - c)/abs(c))\n\n0.9259246\n51.8180694580078125\n0.0\n\n\n\n\nКвадратный корень\n\na = np.float64(1e-100)\nb = np.sqrt(a)\nprint(b.dtype)\nprint('{0:10.64f}'.format(abs(b * b - a)/abs(a)))\n\nfloat64\n0.0000000000000000000000000000000000000000000000000000000000000000\n\n\n\n\nДемонстрация точности экспоненты\n\na = np.float32(0.000001)\nb = np.exp(a)\nprint(b.dtype)\nprint((np.log(b) - a)/a)\n\nfloat32\n-0.046326134"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#еще-примерчики",
    "href": "lectures/lecture-1/lecture-1.html#еще-примерчики",
    "title": "Представление чисел",
    "section": "Еще примерчики",
    "text": "Еще примерчики\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create x points in the range [-2e-15, 2e-15]\nx = np.linspace(-2e-15, 2e-15, 100)\n\n# Compute log(1+x)/x - 1 using method (a) and (b), being careful about x=0\ny_a = np.zeros_like(x)\ny_b = np.zeros_like(x)\nnonzero = x != 0\ny_a[nonzero] = (np.log(x[nonzero]+1))/x[nonzero] - 1  # (a)\ny_b[nonzero] = (np.log(x[nonzero]+1))/((1+x[nonzero])-1) - 1  # (b\n\n# Create the plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n\n# Plot for method (a)\nax1.plot(x, y_a, 'b-')\nax1.grid(True)\nax1.set_xlabel('x')\nax1.set_ylabel('log(1+x)/x - 1')\nax1.set_title('Method (a): (log(1+x))/x - 1')\n\n# Plot for method (b)\nax2.plot(x, y_b, 'r-')\nax2.grid(True)\nax2.set_xlabel('x')\nax2.set_ylabel('log(1+x)/x - 1')\nax2.set_title('Method (b): (log(1+x))/((1+x)-1) - 1')\n\n# Adjust layout and add a main title\nplt.tight_layout()\nfig.suptitle('Comparison of two methods for computing log(1+x)/x - 1 in double precision', fontsize=16)\nplt.subplots_adjust(top=0.88)\n\nplt.show()\n\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_10237/1439480773.py:12: RuntimeWarning: invalid value encountered in divide\n  y_b[nonzero] = (np.log(x[nonzero]+1))/((1+x[nonzero])-1) - 1  # (b"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#итоги-демо",
    "href": "lectures/lecture-1/lecture-1.html#итоги-демо",
    "title": "Представление чисел",
    "section": "Итоги демо",
    "text": "Итоги демо\n\nДля некоторых значений обратные функции дают точные ответы\nОтносительная accuracy должна сохраняться благодаря IEEE standard\nНе выполняется для многих современных GPU\nБольше деталей про адаптацию IEEE 754 standard для GPU можно найти здесь"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#потеря-значимых-цифр",
    "href": "lectures/lecture-1/lecture-1.html#потеря-значимых-цифр",
    "title": "Представление чисел",
    "section": "Потеря значимых цифр",
    "text": "Потеря значимых цифр\n\nМногие операции приводят к потере цифр loss of significance\nНапример, плохая идея вычитать два больших близких числа, разность будет иметь меньше правильных цифр\nЭто связано с алгоритмами и их свойствами (forward/backward stability), которые мы обсудим позже"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#summation-algorithm",
    "href": "lectures/lecture-1/lecture-1.html#summation-algorithm",
    "title": "Представление чисел",
    "section": "Summation algorithm",
    "text": "Summation algorithm\nОднако ошибки округления могут зависеть от алгоритма.\n\nРассмотрим простейшую задачу: даны n чисел с плавающей точкой x_1, \\ldots, x_n\nВычислить их сумму\n\nS = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.\n\nПростейший алгоритм - складывать числа одно за другим\nКакова реальная ошибка такого алгоритма?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#наивный-алгоритм",
    "href": "lectures/lecture-1/lecture-1.html#наивный-алгоритм",
    "title": "Представление чисел",
    "section": "Наивный алгоритм",
    "text": "Наивный алгоритм\ny_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.\n\nWorst-case ошибка пропорциональна \\mathcal{O}(n), в то время как mean-squared ошибка равна \\mathcal{O}(\\sqrt{n}).\nKahan algorithm дает границу worst-case ошибки \\mathcal{O}(1) (т.е. независимо от n).\n Можете ли вы найти алгоритм лучше?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#суммирование-кахана",
    "href": "lectures/lecture-1/lecture-1.html#суммирование-кахана",
    "title": "Представление чисел",
    "section": "Суммирование Кахана",
    "text": "Суммирование Кахана\nСледующий алгоритм дает ошибку 2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2), где \\varepsilon - это machine precision.\n\nПричина потери значимости при суммировании заключается в операциях с числами разного порядка\nОсновная идея Kahan summation состоит в том, чтобы отслеживать маленькие ошибки и аккумулировать их в отдельной переменной\nЭтот подход называется compensated summation\n\n\nimport math\nimport numpy as np\nfrom numba import jit as numba_jit\n\nn = 10 ** 6\nsm = 1e-10\nx = np.ones(n, dtype=np.float32) * sm\nx[0] = 1.0\ntrue_sum = 1.0 + (n - 1)*sm\napprox_sum = np.sum(x)\nmath_fsum = math.fsum(x)\n\n@numba_jit(nopython=True)\ndef kahan_sum_numba(x):\n    s = np.float32(0.0)\n    c = np.float32(0.0)\n    for i in range(len(x)):\n        y = x[i] - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\ndef simple_sum(x):\n    s = 0.0\n    for i in range(len(x)):\n        s += x[i]\n    return s\n\nk_sum_numba = kahan_sum_numba(x)\nsimple_sum_result = simple_sum(x)\n\nprint('Error in np sum: {0:3.1e}'.format(approx_sum - true_sum))\nprint('Error in simple sum: {0:3.1e}'.format(simple_sum_result - true_sum))\nprint('Error in Kahan sum Numba: {0:3.1e}'.format(k_sum_numba - true_sum))\nprint('Error in math fsum: {0:3.1e}'.format(math_fsum - true_sum))\n\nError in np sum: 1.7e-06\nError in simple sum: -1.0e-04\nError in Kahan sum Numba: 1.7e-08\nError in math fsum: 1.3e-12"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#итого",
    "href": "lectures/lecture-1/lecture-1.html#итого",
    "title": "Представление чисел",
    "section": "Итого",
    "text": "Итого\n\nНеобходимо быть очень осторожным с числами с плавающей точкой, так как они могут давать неверные результаты из-за ошибок округления.\nДля многих стандартных алгоритмов стабильность хорошо изучена и проблемы могут быть легко обнаружены."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#векторы",
    "href": "lectures/lecture-1/lecture-1.html#векторы",
    "title": "Представление чисел",
    "section": "Векторы",
    "text": "Векторы\n\nВ численной линейной алгебре мы обычно работаем не с числами, а с векторами\nНапомним, что вектор в фиксированном базисе размера n может быть представлен как одномерный массив с n числами\nКак правило, он рассматривается как матрица размера n \\times 1 (вектор-столбец)\n\nПример: Многочлены степени \\leq n образуют линейное пространство. Многочлен $ x^3 - 2x^2 + 1$ может быть представлен как вектор \\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix} в базисе \\{x^3, x^2, x, 1\\}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#норма-вектора",
    "href": "lectures/lecture-1/lecture-1.html#норма-вектора",
    "title": "Представление чисел",
    "section": "Норма вектора",
    "text": "Норма вектора\n\nВекторы обычно предоставляют (приближенное) описание физического (или какого-либо другого) объекта\nОдин из главных вопросов - насколько точным является приближение (1%, 10%)\nПриемлемое представление, конечно, зависит от конкретного применения. Например:\n\nВ дифференциальных уравнениях в частных производных точности 10^{-5} - 10^{-10} являются типичным случаем\nВ приложениях, основанных на данных, иногда ошибка в 80\\% допустима, так как полезный сигнал искажен сильным шумом"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#расстояния-и-нормы",
    "href": "lectures/lecture-1/lecture-1.html#расстояния-и-нормы",
    "title": "Представление чисел",
    "section": "Расстояния и нормы",
    "text": "Расстояния и нормы\n\nНорма - это количественная мера малости вектора, которая обычно обозначается как \\Vert x \\Vert.\n\nНорма должна удовлетворять определенным свойствам:\n\n\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (неравенство треугольника)\nЕсли \\Vert x \\Vert = 0, то x = 0\n\nРасстояние между двумя векторами определяется как\n d(x, y) = \\Vert x - y \\Vert. \nСтандартные нормы Наиболее известной и широко используемой нормой является евклидова норма:\n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\nкоторая соответствует расстоянию в нашей реальной жизни. Если векторы имеют комплексные элементы, мы используем их модуль."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#p-норма",
    "href": "lectures/lecture-1/lecture-1.html#p-норма",
    "title": "Представление чисел",
    "section": "p-норма",
    "text": "p-норма\nЕвклидова норма, или 2-норма, является подклассом важного класса p-норм:\n \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}. \nСуществуют два очень важных частных случая: - Норма Чебышева, определяется как элемент с максимальным абсолютным значением:\n \\Vert x \\Vert_{\\infty} = \\max_i | x_i| \n\n\nL_1 норма (или манхэттенское расстояние), которая определяется как сумма модулей элементов x:\n\n \\Vert x \\Vert_1 = \\sum_i |x_i|"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#эквивалентность-норм",
    "href": "lectures/lecture-1/lecture-1.html#эквивалентность-норм",
    "title": "Представление чисел",
    "section": "Эквивалентность норм",
    "text": "Эквивалентность норм\nВсе нормы эквивалентны в том смысле, что\n C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_* \nдля некоторых положительных констант C_1(n), C_2(n), x \\in \\mathbb{R}^n для любой пары норм \\Vert \\cdot \\Vert_* и \\Vert \\cdot \\Vert_{**}. Эквивалентность норм по существу означает, что если вектор мал в одной норме, то он мал и в другой норме. Однако константы могут быть большими."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#вычисление-норм-в-python",
    "href": "lectures/lecture-1/lecture-1.html#вычисление-норм-в-python",
    "title": "Представление чисел",
    "section": "Вычисление норм в Python",
    "text": "Вычисление норм в Python\nПакет NumPy содержит все необходимое для вычисления норм: функция np.linalg.norm\n\nimport numpy as np\nn = 100\na = np.random.randn(n)\nb = a + 1e-5 * np.random.normal((n,))\nprint('Relative error in L1 norm:', np.linalg.norm(a - b, 1) / np.linalg.norm(b, 1))\nprint('Relative error in L2 norm:', np.linalg.norm(a - b) / np.linalg.norm(b))\nprint('Relative error in Chebyshev norm:', np.linalg.norm(a - b, np.inf) / np.linalg.norm(b, np.inf))\n\nRelative error in L1 norm: 0.0013743710119635821\nRelative error in L2 norm: 0.0010294800327283588\nRelative error in Chebyshev norm: 0.0004017056903068766"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#единичные-шары-в-различных-нормах",
    "href": "lectures/lecture-1/lecture-1.html#единичные-шары-в-различных-нормах",
    "title": "Представление чисел",
    "section": "Единичные шары в различных нормах",
    "text": "Единичные шары в различных нормах\n\nЕдиничный шар - это множество точек, таких что \\Vert x \\Vert \\leq 1\nДля евклидовой нормы единичный шар является обычным кругом\nДля других норм единичные шары выглядят совершенно по-разному\n\n\n\nimport matplotlib.pyplot as plt\np = 0.5 # Which norm do we use\nM = 10000 # Number of sampling points\nb = []\nfor i in range(M):\n    a = np.random.randn(2, 1)\n    if np.linalg.norm(a[:, 0], p) &lt;= 1:\n        b.append(a[:, 0])\nb = np.array(b)\nplt.plot(b[:, 0], b[:, 1], '.')\nplt.axis('equal')\nplt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))\n\nText(0.5, 1.0, 'Unit disk in the p-th norm, $p=0.5$')"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#что-такое-устойчивый-алгоритм",
    "href": "lectures/lecture-1/lecture-1.html#что-такое-устойчивый-алгоритм",
    "title": "Представление чисел",
    "section": "Что такое устойчивый алгоритм?",
    "text": "Что такое устойчивый алгоритм?\nИ мы завершаем лекцию понятием устойчивости.\n\nПусть x - объект (например, вектор)\nПусть f(x) - функция (функционал), которую вы хотите вычислить\n\nУ вас также есть численный алгоритм alg(x), который фактически вычисляет приближение к f(x).\nАлгоритм называется устойчивым, если\n\\Vert \\text{alg}(x) - f(x) \\Vert  \\leq \\varepsilon \nАлгоритм называется обратно устойчивым, если для любого x существует близкий вектор x + \\delta x такой, что\n\\text{alg}(x) = f(x + \\delta x)\nи \\Vert \\delta x \\Vert мало."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#классический-пример",
    "href": "lectures/lecture-1/lecture-1.html#классический-пример",
    "title": "Представление чисел",
    "section": "Классический пример",
    "text": "Классический пример\nКлассическим примером является решение систем линейных уравнений с помощью метода Гаусса, который похож на LU-разложение (подробнее позже)\nРассмотрим матрицу Гильберта с элементами\nA = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.\nИ рассмотрим линейную систему\nAx = f.\n\nimport numpy as np\nn = 100\na = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] # Hilbert matrix\nA = np.array(a)\n#rhs =  np.random.randn(n)\nrhs = np.ones(n)\nsol = np.linalg.solve(A, rhs)\nprint(np.linalg.norm(A @ sol - rhs)/np.linalg.norm(rhs))\nplt.plot(sol)\n\n1.3614802098870779e-07\n\n\n\n\n\n\n\n\n\n\nrhs = np.ones(n)\nsol = np.linalg.solve(A, rhs)\nprint(np.linalg.norm(A @ sol - rhs)/np.linalg.norm(rhs))\n#plt.plot(sol)\n\n0.0018351191"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#больше-примеров-неустойчивости",
    "href": "lectures/lecture-1/lecture-1.html#больше-примеров-неустойчивости",
    "title": "Представление чисел",
    "section": "Больше примеров неустойчивости",
    "text": "Больше примеров неустойчивости\nКак вычислить следующие функции численно устойчивым способом?\n\n\\log(1 - \\tanh^2(x))\n\\text{SoftMax}(x)_j = \\dfrac{e^{x_j}}{\\sum\\limits_{i=1}^n e^{x_i}}\n\n\nu = 30\neps = 1e-6\nprint(\"Исходная функция:\", np.log(1 - np.tanh(u)**2))\neps_add = np.log(1 - np.tanh(u)**2 + eps)\nprint(\"Попытка улучшить стабильность добавлением малой константы:\", eps_add)\nprint(\"Использование более численно стабильной формы:\", np.log(4) - 2 * np.log(np.exp(-u) + np.exp(u)))\n\nИсходная функция: -inf\nПопытка улучшить стабильность добавлением малой константы: -13.815510557964274\nИспользование более численно стабильной формы: -58.61370563888011\n\n\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_10237/1002619719.py:3: RuntimeWarning: divide by zero encountered in log\n  print(\"Исходная функция:\", np.log(1 - np.tanh(u)**2))\n\n\n\nn = 5\nx = np.random.normal(size=(n,))\nx[0] = 1000\nprint(np.exp(x) / np.sum(np.exp(x)))\nprint(np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x))))\n\n[nan  0.  0.  0.  0.]\n[1. 0. 0. 0. 0.]\n\n\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_7697/1311588733.py:4: RuntimeWarning: overflow encountered in exp\n  print(np.exp(x) / np.sum(np.exp(x)))\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_7697/1311588733.py:4: RuntimeWarning: invalid value encountered in divide\n  print(np.exp(x) / np.sum(np.exp(x)))"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#основные-выводы",
    "href": "lectures/lecture-1/lecture-1.html#основные-выводы",
    "title": "Представление чисел",
    "section": "Основные выводы",
    "text": "Основные выводы\n\nЧисла с плавающей точкой (двойная, одинарная точность, количество байт), ошибки округления\nНормы как меры малости, используются для вычисления точности\nНормы: 1, p и евклидова\nПрямая/обратная ошибка (и устойчивость алгоритмов) (в следующих лекциях)"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#brief-recap-of-the-previous-lecture",
    "href": "lectures/lecture-9/lecture-9.html#brief-recap-of-the-previous-lecture",
    "title": "Questions?",
    "section": "Brief recap of the previous lecture",
    "text": "Brief recap of the previous lecture\nSVD and algorithms for its computations: divide-and-conquer, QR, Jacobi, bisection."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#todays-lecture",
    "href": "lectures/lecture-9/lecture-9.html#todays-lecture",
    "title": "Questions?",
    "section": "Todays lecture",
    "text": "Todays lecture\nToday, we will do a brief dive into the randomized NLA.\nA good read is (https://arxiv.org/pdf/2002.01387.pdf)"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#random-numbers",
    "href": "lectures/lecture-9/lecture-9.html#random-numbers",
    "title": "Questions?",
    "section": "Random numbers",
    "text": "Random numbers\nAll the computations that we considered up to today were deterministic.\nHowever, reduction of complexity can be done by using randomized (stochastic) computation.\nExample: randomized matrix multiplication."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#checking-matrix-equality",
    "href": "lectures/lecture-9/lecture-9.html#checking-matrix-equality",
    "title": "Questions?",
    "section": "Checking matrix equality",
    "text": "Checking matrix equality\nWe can check, if $ A B = C$ in \\mathcal{O}(n^2) operations.\nHow?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#freivalds-algorithm",
    "href": "lectures/lecture-9/lecture-9.html#freivalds-algorithm",
    "title": "Questions?",
    "section": "Freivalds algorithm",
    "text": "Freivalds algorithm\nChecks by multiplying by random vectors!\nComplexity is k n^2, probability is of failure is \\frac{1}{2^k}."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#matrix-multiplication",
    "href": "lectures/lecture-9/lecture-9.html#matrix-multiplication",
    "title": "Questions?",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nBut can we multiply matrices faster using randomization ideas?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#randomized-matrix-multiplication",
    "href": "lectures/lecture-9/lecture-9.html#randomized-matrix-multiplication",
    "title": "Questions?",
    "section": "Randomized matrix multiplication",
    "text": "Randomized matrix multiplication\n\nWe know that matrix multiplication AB costs O(mnp) for matrices m \\times p and p \\times n\nWe can construct approximation of this product by sampling rows and columns of the multipliers\n\nQ: how to sample them?\nA: generate probabilities from their norms!\n\nSo the final approximation expression\n\n AB \\approx \\sum_{t=1}^k \\frac{1}{kp_{i_t}} A^{(i_t)} B_{(i_t)}, \nwhere A^{(i_t)} is a column of A and B_{(i_t)} is a row of B\n\nComplexity reduction from O(mnp) to O(mnk)\n\n\nimport numpy as np\n\nn = 1\np = 10000\nm = 1\nA = np.random.randn(n, p)\nB = np.random.randn(p, m)\nC = A @ A.T\n\ndef randomized_matmul(A, B, k):\n    p1 = A.shape[1]\n    p = np.linalg.norm(A, axis=0) * np.linalg.norm(B, axis=1)\n    p = p\n    p = p.ravel() / p.sum()\n    n = A.shape[1]\n    p = np.ones(p1)\n    p = p/p.sum()\n    idx = np.random.choice(np.arange(n), (k,), False, p)\n    #d = 1 / np.sqrt(k * p[idx])\n    d = 1.0/np.sqrt(k)#np.sqrt(p1)/np.sqrt(k*p[idx])\n    A_sketched = A[:, idx]*np.sqrt(p1)/np.sqrt(k)#* d[None, :]\n    B_sketched = B[idx, :]*np.sqrt(p1)/np.sqrt(k) #* d[:, None]\n    C = A_sketched @ B_sketched\n    print(d)\n    return C\n\ndef randomized_matmul_topk(A, B, K):\n    \n    norm_mult = np.linalg.norm(A,axis=0) * np.linalg.norm(B,axis=1)\n    top_k_idx = np.sort(np.argsort(norm_mult)[::-1][:K])\n    \n    A_top_k_cols = A[:, top_k_idx]\n    B_top_k_rows = B[top_k_idx, :]\n\n    C_approx = A_top_k_cols @ B_top_k_rows\n    return C_approx\n\nnum_items = 3000\nC_appr_samples = randomized_matmul(A, B, num_items)\nprint(C_appr_samples, 'appr')\nprint(C, 'true')\nC_appr_topk = randomized_matmul_topk(A, B, num_items)\nprint(np.linalg.norm(C_appr_topk - C, 2) / np.linalg.norm(C, 2))\nprint(np.linalg.norm(C_appr_samples - C, 2) / np.linalg.norm(C, 2))\n\n0.018257418583505537\n[[-209.68265641]] appr\n[[10065.73675927]] true\n1.012091041179466\n1.020831327246555"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#approximation-error",
    "href": "lectures/lecture-9/lecture-9.html#approximation-error",
    "title": "Questions?",
    "section": "Approximation error",
    "text": "Approximation error\n \\mathbb{E} [\\|AB - CR\\|^2_F] = \\frac{1}{k} \\left(\\sum_{i=1}^n \\| A^{(i)} \\|_2 \\| B_{(i)} \\|_2\\right)^2   - \\frac{1}{k}\\|AB\\|_F^2 \n\nOther sampling probabilities are possible\nUse approximation  AB \\approx ASD(SD)^\\top B  = ACC^{\\top}B can replace sampling and scaling with another matrix that\n\nreduces the dimension\nsufficiently accurately approximates\n\n\nQ: what matrices can be used?\n\nStochastic trace estimator\nMany problems can be written in the form of the trace estimation:\n\\mathrm{Tr}(A) = \\sum_{i} A_{ii}.\nCan we compute the trace of the matrix if we only have access to matrix-by-vector products?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#two-estimators",
    "href": "lectures/lecture-9/lecture-9.html#two-estimators",
    "title": "Questions?",
    "section": "Two estimators",
    "text": "Two estimators\nThe randomized trace estimators can be computed from the following formula:\n\\mathrm{Tr}(A) = E_w w^* A w, \\quad E ww^* = 1\nIn order to sample, we pick k independent samples of w_k, get random variable X_k and average the results.\nGirard trace estimator: Sample w \\sim N(0, 1)\nThen, \\mathrm{Var} X_k = \\frac{2}{k} \\sum_{i, j=1}^n \\vert A_{ij} \\vert^2 = \\frac{2}{k} \\Vert A \\Vert^2_F\nHutchinson trace estimator: Let w be a Rademacher random vector (i.e., elements are sampled from the uniform distribution.\nIt gives the minimal variance estimator."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#intdim",
    "href": "lectures/lecture-9/lecture-9.html#intdim",
    "title": "Questions?",
    "section": "Intdim",
    "text": "Intdim\nThe variance of the trace can be estimated in terms of intrinsic dimension (intdim) for symmetric positive definite matrices.\nIt is defined as \\mathrm{intdim}(A) = \\frac{\\mathrm{Tr}(A)}{\\Vert A \\Vert_F}. It is easy to show that\n1 \\leq \\mathrm{intdim}(A) \\leq ?.\nThen, the probability of the large deviation can be estimated as\nP( \\vert \\overline{X}_k - \\mathrm{Tr}(A) \\vert \\geq t \\mathrm{Tr}(A)) \\leq \\frac{2}{k \\mathrm{intdim}(A) t^2}"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#better-bounds-for-spd-matrices",
    "href": "lectures/lecture-9/lecture-9.html#better-bounds-for-spd-matrices",
    "title": "Questions?",
    "section": "Better bounds for SPD matrices",
    "text": "Better bounds for SPD matrices\nIf A is SPD, then\nP(\\overline{X}_k \\geq \\tau \\mathrm{Tr}(A) ) \\leq \\exp\\left(-1/2 \\mathrm{intdim}(A) (\\sqrt{\\tau} - 1)^2)\\right) \nSimilar inequality holds for the lower bound.\nThis estimate is much better.\nAn interesting (and often mislooked) property of stochastic estimator is that it comes with a stochastic variance estimate (from samples!)\nWarning: we still need \\varepsilon^{-2} samples to get to the accuracy \\varepsilon when using independent samples.\n\nDistances between languages (original paper)"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#where-do-stochastic-methods-also-help",
    "href": "lectures/lecture-9/lecture-9.html#where-do-stochastic-methods-also-help",
    "title": "Questions?",
    "section": "Where do stochastic methods also help?",
    "text": "Where do stochastic methods also help?\n\nSVD\nLinear systems"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#randomized-svd-halko-et-al-2011",
    "href": "lectures/lecture-9/lecture-9.html#randomized-svd-halko-et-al-2011",
    "title": "Questions?",
    "section": "Randomized SVD (Halko et al, 2011)",
    "text": "Randomized SVD (Halko et al, 2011)\n\nProblem statement reminder\n\n A \\approx U\\Sigma V^\\top, \nwhere A is of size m \\times n, U is of size m \\times k and V is of size n \\times k.\n\nWe have already known that the complexity of rank-k approximation is O(mnk)\nHow can we reduce this complexity?\nAssume we know orthogonal matrix Q of size m \\times k such that\n\nA \\approx Q Q^{\\top}A \n\nIn other words, columns of Q represent orthogonal basis in the column space of matrix A\nThen the following deterministic steps can give the factors U, \\Sigma and V corresponding of SVD of matrix A\n\nForm k \\times n matrix B = Q^{\\top}A\nCompute SVD of small matrix B = \\hat{U}\\Sigma V^{\\top}\nUpdate left singular vectors U = Q\\hat{U}\n\nIf k \\ll \\min(m, n) then these steps can be performed fast\nIf Q forms exact basis in column space of A, then U, \\Sigma and V are also exact!\nSo, how to compose matrix Q?\n\n\nRandomized approximation of basis in column space of A\n\nThe main approach\n\nGenerate k + p Gaussian vectors of size m and form matrix G\nCompute Y = AG\nCompute QR decomposition of Y and use the resulting matrix Q as an approximation of the basis\n\nParameter p is called oversampling parameter and is needed to improve approximation of the leading k left singular vectors later\nComputing of Y can be done in parallel\nHere we need only matvec function for matrix A rather than its elements as a 2D array - black-box concept!\nInstead of Gaussian random matrix one can use more structured but still random matrix that can be multiplied by A fast\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn = 1000\nk = 100\nm = 200\n# Lowrank matrix\nA = np.random.randn(n, k)\nB = np.random.randn(k, m)\nA = A @ B\n\n# Random matrix\n# A = np.random.randn(n, m)\n\ndef randomized_svd(A, rank, p):\n    m, n = A.shape\n    G = np.random.randn(n, rank + p)\n    Y = A @ G\n    Q, _ = np.linalg.qr(Y)\n    B = Q.T @ A\n    u, S, V = np.linalg.svd(B)\n    U = Q @ u\n    return U, S, V\n\nrank = 100\np = 5\nU, S, V = randomized_svd(A, rank, p)\nprint(\"Error from randomized SVD\", np.linalg.norm(A - U[:, :rank] * S[None, :rank] @ V[:rank, :]))\nplt.semilogy(S[:rank] / S[0], label=\"Random SVD\")\nu, s, v = np.linalg.svd(A)\nprint(\"Error from exact SVD\", np.linalg.norm(A - u[:, :rank] * s[None, :rank] @ v[:rank, :]))\nplt.semilogy(s[:rank] / s[0], label=\"Exact SVD\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\nError from randomized SVD 1.7704601563939492e-11\nError from exact SVD 1.195330542835496e-11\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as spsplin\n# More details about Facebook package for computing randomized SVD is here: https://research.fb.com/blog/2014/09/fast-randomized-svd/ \nimport fbpca\nn = 1000\nm = 200\nA = np.random.randn(n, m)\nk = 10\np = 10\n%timeit spsplin.svds(A, k=k)\n%timeit randomized_svd(A, k, p)\n%timeit fbpca.pca(A, k=k, raw=False) \n\n60.5 ms ± 11.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n8.07 ms ± 3.32 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n3.09 ms ± 177 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nConvergence theorem\nThe averaged error of the presented algorithm, where k is target rank and p is oversampling parameter, is the following - in Frobenius norm\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_F \\leq \\left( 1 + \\frac{k}{p-1} \\right)^{1/2}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^2_j \\right)^{1/2}  \n\nin spectral norm\n\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_2 \\leq \\left( 1 + \\sqrt{\\frac{k}{p-1}} \\right)\\sigma_{k+1} + \\frac{e\\sqrt{k+p}}{p}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^2_j \\right)^{1/2} \nThe expectation is taken w.r.t. random matrix G generated in the method described above.\nCompare these upper bounds with Eckart-Young theorem. Are these bounds good?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#accuracy-enhanced-randomized-svd",
    "href": "lectures/lecture-9/lecture-9.html#accuracy-enhanced-randomized-svd",
    "title": "Questions?",
    "section": "Accuracy enhanced randomized SVD",
    "text": "Accuracy enhanced randomized SVD\n\nMain idea: power iteration\nIf A = U \\Sigma V^\\top, then $A^{(q)} = (AA{})qA = U {2q+1}V$, where q some small natural number, e.g. 1 or 2\nThen we sample from A^{(q)}, not from A\n\n Y = (AA^{\\top})^qAG \\qquad Q, R = \\mathtt{qr}(Y) \n\nThe main reason: if singular values of A decays slowly, the singular values of A^{(q)} will decay faster\n\n\nn = 1000\nm = 200\nA = np.random.randn(n, m)\ns = np.linalg.svd(A, compute_uv=False)\nAq = A @ A.T @ A\nsq = np.linalg.svd(Aq, compute_uv=False)\nplt.semilogy(s / s[0], label=\"$A$\")\nplt.semilogy(sq / sq[0], label=\"$A^{(1)}$\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\n\n\n\n\n\n\n\n\nLoss of accuracy with rounding errors\n\nCompose A^{(q)} naively leads to condition number grows and loss of accuracy\n\nQ: how can we battle with this issue?\nA: sequential orthogonalization!\n\ndef more_accurate_randomized_svd(A, rank, p, q):\n    m, n = A.shape\n    G = np.random.randn(n, rank + p)\n    Y = A @ G\n    Q, _ = np.linalg.qr(Y)\n    for i in range(q):\n        W = A.T @ Q\n        W, _ = np.linalg.qr(W)\n        Q = A @ W\n        Q, _ = np.linalg.qr(Q)\n    B = Q.T @ A\n    u, S, V = np.linalg.svd(B)\n    U = Q @ u\n    return U, S, V\n\nn = 1000\nm = 200\nA = np.random.randn(n, m)\n\nrank = 100\np = 20\nU, S, V = randomized_svd(A, rank, p)\nprint(\"Error from randomized SVD\", np.linalg.norm(A - U[:, :rank] * S[None, :rank] @ V[:rank, :]))\nplt.semilogy(S[:rank] / S[0], label=\"Random SVD\")\n\nUq, Sq, Vq = more_accurate_randomized_svd(A, rank, p, 5)\nprint(\"Error from more accurate randomized SVD\", np.linalg.norm(A - Uq[:, :rank] * Sq[None, :rank] @ Vq[:rank, :]))\nplt.semilogy(Sq[:rank] / Sq[0], label=\"Accurate random SVD\")\n\nu, s, v = np.linalg.svd(A)\nprint(\"Error from exact SVD\", np.linalg.norm(A - u[:, :rank] * s[None, :rank] @ v[:rank, :]))\nplt.semilogy(s[:rank] / s[0], label=\"Exact SVD\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\nError from randomized SVD 286.99760873015225\nError from more accurate randomized SVD 250.2388642432797\nError from exact SVD 249.3503301291079\n\n\n\n\n\n\n\n\n\n\n%timeit spsplin.svds(A, k=k)\n%timeit fbpca.pca(A, k=k, raw=False)\n%timeit randomized_svd(A, k, p) \n%timeit more_accurate_randomized_svd(A, k, p, 1)\n%timeit more_accurate_randomized_svd(A, k, p, 2)\n%timeit more_accurate_randomized_svd(A, k, p, 5)\n\n347 ms ± 60.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n82.3 ms ± 6.93 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n68.7 ms ± 4.99 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n118 ms ± 6.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n176 ms ± 13.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n352 ms ± 43.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\nConvergence theorem\nThe presented above method provides the following upper bound\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_2 \\leq \\left[\\left( 1 + \\sqrt{\\frac{k}{p-1}} \\right)\\sigma^{2q+1}_{k+1} + \\frac{e\\sqrt{k+p}}{p}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^{2(2q+1)}_j \\right)^{1/2}\\right]^{1/(2q+1)} \nConsider the worst case, where no lowrank structure exists in the given matrix.\nQ: what is the degree of suboptimality w.r.t. Eckart-Young theorem?\n\n\nSummary on randomized SVD\n\nEfficient method to get approximate SVD\nSimple to implement\nIt can be extended to one-pass method, where matrix A is needed only to construct Q\nIt requires only matvec with target matrix"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#kaczmarz-method-to-solve-linear-systems",
    "href": "lectures/lecture-9/lecture-9.html#kaczmarz-method-to-solve-linear-systems",
    "title": "Questions?",
    "section": "Kaczmarz method to solve linear systems",
    "text": "Kaczmarz method to solve linear systems\n\nWe have already discussed how to solve overdetermined linear systems Ax = f in the least-squares manner\n\npseudoinverse matrix\nQR decomposition\n\nOne more approach is based on iterative projections a.k.a. Kaczmarz method or algebraic reconstruction technique in compoutational tomography domain\nInstead of solving all equations, pick one randomly, which reads\n\na^{\\top}_i x = f_i,\nand given an approximation x_k try to find x_{k+1} as\nx_{k+1} = \\arg \\min_x \\frac12 \\Vert x - x_k \\Vert^2_2, \\quad \\mbox{s.t.} \\quad  a^{\\top}_i x = f_i.\n\nA simple analysis gives\n\nx_{k+1} = x_k - \\frac{(a_i, x_k) - f_i}{(a_i, a_i)} a_i. \n\nA cheap update, but the analysis is quite complicated.\nYou can recognize in this method stochastic gradient descent with specific step size equal to \\frac{1}{\\|a_i\\|_2^2} for every sample"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#convergence-theorem-2",
    "href": "lectures/lecture-9/lecture-9.html#convergence-theorem-2",
    "title": "Questions?",
    "section": "Convergence theorem",
    "text": "Convergence theorem\n\nAssume we generate i according to the distribution over the all available indices proportional to norms of the rows, i.e. \\mathbb{P}[i = k] = \\frac{\\|a_k\\|_2^2}{\\| A \\|^2_F}. This method is called Randomized Kaczmarz method (RKM)\nWhy sampling strategy is important here?\nInvestigation of the best sampling is provided here\nIf the overdetermined linear system is consistent, then\n\n \\mathbb{E}[\\|x_{k+1} - x^*\\|^2_2] \\leq \\left(1 - \\frac{1}{\\kappa^2_F(A)}\\right) \\mathbb{E}[\\|x_{k} - x^*\\|^2_2], \nwhere \\kappa_F(A) = \\frac{\\| A \\|_F}{\\sigma_{\\min}(A)} and \\sigma_{\\min}(A) is a minimal non-zero singular value of A. This result was presented in (Strohmer and Vershynin, 2009)\n\nIf the overdetermined linear system is inconsistent, then\n\n \\mathbb{E}[\\|x_{k+1} - x^*\\|^2_2] \\leq \\left(1 - \\frac{1}{\\kappa^2_F(A)}\\right) \\mathbb{E}[\\|x_{k} - x^*\\|^2_2] + \\frac{\\|r^*\\|_2^2}{\\| A \\|^2_F}, \nwhere r^* = Ax^* - f\n\nInconsistent overdetermined linear system\n\nIt was shown in (Needell, 2010) that RKM does not converge to A^{\\dagger}f\nTo address this issue Randomized extended Kaczmarz method was proposed in (A Zouzias, N Freris, 2013)\nThe main idea is to use two steps of RKM:\n\nthe first step is for system A^\\top z = 0 starting from z_k\n\n z^{k+1} = z^{k} - \\frac{a^\\top_{:, j} z^k}{\\| a_{:, j} \\|_2^2}a_{:, j}  \n\nthe second step is for system Ax = f - z_{k+1} starting from x_k\n\nx^{k+1} = x^k - \\frac{a_{i,:}x_k - f_i + z^{k+1}_i}{\\|a_{i,:}\\|_2^2}a^{\\top}_{i,:} \n\nHere a_{:, j} denotes the j-th column of A and a_{i, :} denotes the i-th row of A\n\nIf z^0 \\in f + \\mathrm{range}(A) and x^0 \\in \\mathrm{range}(A^\\top), then REK converges exponentially to A^{\\dagger}f"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#sampling-and-sketching",
    "href": "lectures/lecture-9/lecture-9.html#sampling-and-sketching",
    "title": "Questions?",
    "section": "Sampling and sketching",
    "text": "Sampling and sketching\n\nSampling of a particular row can be considered as a particular case of more general approach called sketching\nIdea: replace matrix A with another matrix SA, where matrix SA has significantly smaller number of rows but preserves some important properties of matrix A\nPossible choices:\n\nrandom projection\nrandom row selection\n\nExample: linear least squares problem \\|Ax - b\\|_2^2 \\to \\min_x transforms to \\| (SA)y - Sb \\|_2^2 \\to \\min_y and we expect that x \\approx y\nBlendenpick solver is based on that idea and outperforms LAPACK routine\nMore details see in Sketching as a Tool for Numerical Linear Algebra by D. Woodruff"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#coherence",
    "href": "lectures/lecture-9/lecture-9.html#coherence",
    "title": "Questions?",
    "section": "Coherence",
    "text": "Coherence\nThe key idea is the coherence of the matrix.\nLet A be n \\times r and U be an orthogonal matrix whose columns form the basis of the column space of A.\nThen, coherence is defined as\n\\mu(A) = \\max \\Vert U_{i, *} \\Vert^2\nCoherence is always smaller than 1 and bigger than \\frac{r}{n}, it has nothing to do with the condition number.\nWhat does it mean?"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#coherence-1",
    "href": "lectures/lecture-9/lecture-9.html#coherence-1",
    "title": "Questions?",
    "section": "Coherence",
    "text": "Coherence\nSmall coherence means, that sampling rows uniformly gives a good preconditioner (will be covered later in the course, and why it is important)\nOne can do S A = QR, and look at the condition number of AR^{-1}.\n\nSummary on randomized methods in solving linear systems\n\nEasy to use family of methods\nEspecially useful in problems with streaming data\nExisting theoretical bounds for convergence\nMany interpretations in different domains (SGD in deep learning, ART in computational tomography)\n\n\n\nSummary on randomized matmul\n\nSimple method to get approximation of result\nCan be used if the high accuracy is not crucial\nEspecially useful for large dense matrices"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#next-lecture",
    "href": "lectures/lecture-9/lecture-9.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nWe start sparse and/or structured NLA."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#recap-of-the-previous-lecture",
    "href": "lectures/lecture-10/lecture-10.html#recap-of-the-previous-lecture",
    "title": "Questions?",
    "section": "Recap of the previous lecture",
    "text": "Recap of the previous lecture\n\nRandomized matmul\nHutchinson trace estimator\nRandomized SVD\nKarcmarz"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#plan-of-todays-lecture",
    "href": "lectures/lecture-10/lecture-10.html#plan-of-todays-lecture",
    "title": "Questions?",
    "section": "Plan of todays lecture",
    "text": "Plan of todays lecture\n\nSimple topic in parallel computing in NLA (on the matvec example)\nSparse matrix part (with a separate plan)"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#large-scale-dense-matrices",
    "href": "lectures/lecture-10/lecture-10.html#large-scale-dense-matrices",
    "title": "Questions?",
    "section": "Large scale dense matrices",
    "text": "Large scale dense matrices\n\nIf the size of the dense matrix is huge, then it can not be stored in memory\nPossible options\n\nThis matrix is structured, e.g. block Toeplitz with Toeptitz blocks (next lectures). Then the compressed storage is possible\nFor unstructured dense matrices distributed memory helps\nMPI for processing distributed storing matrices\n\n\n\nDistributed memory and MPI\n\nSplit matrix into blocks and store them on different machines\nEvery machine has its own address space and can not damage data on other machines\nIn this case machines communicate with each other to aggregate results of computations\nMPI (Message Passing Interface) is a standard for parallel computing in distributed memory\n\n\n\nExample: matrix-by-vector product\n\nAssume you want to compute Ax and matrix A can not be stored in available memory\nThen you can split it on blocks and distribute blocks on separate machines\nPossible strategies\n\n1D blocking splits only rows on blocks\n2D blocking splits both rows and columns\n\n\n\n1D blocking scheme\n\n\n\nTotal time of computing matvec with 1D blocking\n\nEach machine has $n / p $ complete rows and n / p elements of vector\nTotal operations are n^2 / p\nTotal time for sending and writing data are t_s \\log p + t_w n, where t_s time unit for sending and t_w time unit for writing\n\n\n\n2D blocking scheme\n\n\n\nTotal time of computing matvec with 2D blocking\n\nEach machine has $n / $ size block and n / \\sqrt{p} elements of vector\nTotal operations are n^2 / p\nTotal time for sending and writing data are approximately t_s \\log p + t_w (n/\\sqrt{p}) \\log p, where t_s time unit for sending and t_w time unit for writing\n\n\n\n\nPackages supported distributed storage\n\nScaLAPACK\nTrilinos\n\nIn Python you can use mpi4py for parallel programming of your algorithm.\n\nPyTorch supports distributed training and data storage, see details here\n\n\n\nSummary on large unstructered matrix processing\n\nDistributed manner of storage\nMPI\nPackages that use parallel computations\nDifferent blocking strategies"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#sparse-matrices-intro",
    "href": "lectures/lecture-10/lecture-10.html#sparse-matrices-intro",
    "title": "Questions?",
    "section": "Sparse matrices intro",
    "text": "Sparse matrices intro\n\nFor dense linear algebra problems, we are limited by the memory to store the full matrix, it is N^2 parameters.\nThe class of sparse matrices where most of the elements are zero, allows us at least to store such matrices.\n\nThe question if we can:\n\nsolve linear systems\nsolve eigenvalue problems\n\nwith sparse matrices"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#plan-for-the-next-part-of-the-lecture",
    "href": "lectures/lecture-10/lecture-10.html#plan-for-the-next-part-of-the-lecture",
    "title": "Questions?",
    "section": "Plan for the next part of the lecture",
    "text": "Plan for the next part of the lecture\nNow we will talk about sparse matrices, where they arise, how we store them, how we operate with them.\n\nFormats: list of lists and compressed sparse row format, relation to graphs\nMatrix-by-vector product\nParallell processing of sparse matrices\nFast direct solvers for Gaussian elimination (start)"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#applications-of-sparse-matrices",
    "href": "lectures/lecture-10/lecture-10.html#applications-of-sparse-matrices",
    "title": "Questions?",
    "section": "Applications of sparse matrices",
    "text": "Applications of sparse matrices\nSparse matrices arise in:\n\npartial differential equations (PDE), mathematical modelling\ngraphs mining, e.g. social networks analysis\nrecommender systems\nwherever relations between objects are “sparse”.\n\n\nSparse matrices are ubiquitous in PDE\nThe simplest partial differential equation (PDE), called\nLaplace equation:\n\n   \\Delta T = \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} = f(x,y), \\quad x,y\\in \\Omega\\equiv[0,1]^2,\n\n\n    T_{\\partial\\Omega} = 0.\n\n\nDiscretization\n\\frac{\\partial^2 T}{\\partial x^2} \\approx \\frac{T(x+h) + T(x-h) - 2T(x)}{h^2} + \\mathcal{O}(h^2),\nsame for \\frac{\\partial^2 T}{\\partial y^2}, and we get a linear system.\nFirst, let us consider one-dimensional case:\nAfter the discretization of the one-dimensional Laplace equation with Dirichlet boundary conditions we have\n\\frac{u_{i+1} + u_{i-1} - 2u_i}{h^2} = f_i,\\quad i=1,\\dots,N-1\n u_{0} = u_N = 0 or in the matrix form\n A u = f, and (for n = 5) A=-\\frac{1}{h^2}\\begin{bmatrix} 2 & -1 & 0 & 0 & 0\\\\ -1 & 2 & -1 & 0 &0 \\\\ 0 & -1 & 2& -1 & 0 \\\\ 0 & 0 & -1 & 2  &-1\\\\ 0 & 0 & 0 & -1 & 2 \\end{bmatrix}\nThe matrix is triadiagonal and sparse\n(and also Toeplitz: all elements on the diagonal are the same)\n\n\nBlock structure in 2D\nIn two dimensions, we get equation of the form\n-\\frac{4u_{ij} -u_{(i-1)j} - u_{(i+1)j} - u_{i(j-1)}-u_{i(j+1)}}{h^2} = f_{ij},\nor in the Kronecker product form\n\\Delta_{2D} = \\Delta_{1D} \\otimes I + I \\otimes \\Delta_{1D},\nwhere \\Delta_{1D} is a 1D Laplacian, and \\otimes is a Kronecker product of matrices.\nFor matrices A\\in\\mathbb{R}^{n\\times m} and B\\in\\mathbb{R}^{l\\times k} its Kronecker product is defined as a block matrix of the form\n\n   A\\otimes B = \\begin{bmatrix}a_{11}B & \\dots & a_{1m}B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n1}B & \\dots & a_{nm}B\\end{bmatrix}\\in\\mathbb{R}^{nl\\times mk}.\n\nIn the block matrix form the 2D-Laplace matrix can be written in the following form:\nA = -\\frac{1}{h^2}\\begin{bmatrix} \\Delta_1 + 2I & -I & 0 & 0 & 0\\\\ -I & \\Delta_1 + 2I  & -I & 0 &0 \\\\ 0 & -I & \\Delta_1 + 2I & -I & 0 \\\\ 0 & 0 & -I & \\Delta_1 + 2I   &-I\\\\ 0 & 0 & 0 & -I & \\Delta_1 + 2I \\end{bmatrix}\n\nShort list of Kronecker product properties\n\nIt is bilinear\n(A\\otimes B) (C\\otimes D) = AC \\otimes BD\nLet \\mathrm{vec}(X) be vectorization of matrix X columnwise. Then \\mathrm{vec}(AXB) = (B^T \\otimes A) \\mathrm{vec}(X).\n\n\n\n\n\nSparse matrices help in computational graph theory\n\nGraphs are represented with adjacency matrix, which is usually sparse\nNumerical solution of graph theory problems are based on processing of this sparse matrix\n\nCommunity detection and graph clustering\nLearning to rank\nRandom walks\nOthers\n\nExample: probably the largest publicly available hyperlink graph consists of 3.5 billion web pages and 128 billion hyperlinks, more details see here\nMore medium scale graphs to test your algorithms are available in Stanford Large Network Dataset Collection\n\n\n\nSuiteSpare matrix collection (formerly Florida sparse matrix collection)\nMore sparse matrices you can find in SuiteSparse matrix collection which contains all sorts of matrices for different applications.\n\nfrom IPython.display import IFrame\nIFrame(\"http://yifanhu.net/GALLERY/GRAPHS/search.html\", width=700, height=450)\n\n\n        \n        \n\n\n\n\nSparse matrices and deep learning\n\nDNN has a lot of parameters\nSome of them may be redundant\nHow to prune the parameters without significantly accuracy reduction?\nSparse variational dropout method leads to significantly sparse filters in DNN almost without accuracy decreasing: the idea of pruning"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#sparse-matrix-construction",
    "href": "lectures/lecture-10/lecture-10.html#sparse-matrix-construction",
    "title": "Questions?",
    "section": "Sparse matrix: construction",
    "text": "Sparse matrix: construction\n\nWe can create sparse matrix using scipy.sparse package (actually this is not the best sparse matrix package)\nWe can go to really large sizes (at least, to store this matrix in the memory)\n\nPlease note the following functions - Create sparse matrices with given diagonals spdiags - Kronecker product of sparse matrices kron - There is also overloaded arithmectics for sparse matrices\n\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nfrom scipy.sparse import csc_matrix, csr_matrix\nimport matplotlib.pyplot as plt\nimport scipy.linalg\nimport scipy.sparse.linalg\n%matplotlib inline\nn = 128\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csc_matrix(A)\nplt.spy(A, aspect='equal', marker='.', markersize=5)\n\n\n\n\n\n\n\n\n\nSparsity pattern\n\nThe spy command plots the sparsity pattern of the matrix: the (i, j) pixel is drawn, if the corresponding matrix element is non-zero.\nSparsity pattern is really important for the understanding the complexity of the sparse linear algebra algorithms.\nOften, only the sparsity pattern is needed for the analysis of “how complex” the matrix is.\n\n\n\nSparse matrix: definition\n\nThe definition of “sparse matrix” is that the number of non-zero elements is much less than the total number of elements.\nYou can do the basic linear algebra operations (like solving linear systems at the first place) faster, than if working for with the full matrix."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#what-we-need-to-find-out-to-see-how-it-actually-works",
    "href": "lectures/lecture-10/lecture-10.html#what-we-need-to-find-out-to-see-how-it-actually-works",
    "title": "Questions?",
    "section": "What we need to find out to see how it actually works",
    "text": "What we need to find out to see how it actually works\n\nQuestion 1: How to store the sparse matrix in memory?\nQuestion 2: How to multiply sparse matrix by vector fast?\nQuestion 3: How to solve linear systems with sparse matrices fast?\n\n\nSparse matrix storage\nThere are many storage formats, important ones:\n\nCOO (Coordinate format)\nLIL (Lists of lists)\nCSR (compressed sparse row)\nCSC (compressed sparse column)\nBlock variants\n\nIn scipy there are constructors for each of these formats, e.g. \nscipy.sparse.lil_matrix(A).\n\nCoordinate format (COO)\nThe simplest format is to use coordinate format to represent the sparse matrix as positions and values of non-zero elements.\ni, j, val\nwhere i, j are integer array of indices, val is the real array of matrix elements.  So we need to store 3\\cdot nnz elements, where nnz denotes number of nonzeroes in the matrix.\nQ: What is good and what is bad in this format?\n\n\nMain disadvantages\n\nIt is not optimal in storage (why?)\nIt is not optimal for matrix-by-vector product (why?)\nIt is not optimal for removing elements as you must make nnz operations to find one element (this is good in LIL format)\n\nFirst two disadvantages are solved by compressed sparse row (CSR) format.\n\n\nCompressed sparse row (CSR)\nIn the CSR format a matrix is stored as 3 different arrays:\nia, ja, sa\nwhere:\n\nia (row start) is an integer array of length n+1\nja (column indices) is an integer array of length nnz\nsa (values) is an real-value array of length nnz\n\n\nSo, we got 2\\cdot{\\bf nnz} + n+1 elements.\n\n\n\nSparse matrices in PyTorch and Tensorflow\n\nPyTorch supports sparse matrices stored in COO format\nIncompletre backward operation for these matrices, see summary here\nTensorflow also supports sparse matrices in COO format\nThe list of supported operations is here and gradient support is also limited\n\n\n\nCSR helps in sparse matrix by vector product (SpMV)\n\n   for i in range(n):\n        \n        for k in range(ia[i]:ia[i+1]):\n            \n            y[i] += sa[k] * x[ja[k]]\nLet us do a short timing test\n\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nimport scipy.sparse.linalg\nfrom scipy.sparse import csc_matrix, csr_matrix, coo_matrix\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 1024\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\nB = coo_matrix(A)\n%timeit A.dot(rhs)\n%timeit B.dot(rhs)\n\n3.24 ms ± 74.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n19.7 ms ± 379 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nAs you see, CSR is faster, and for more unstructured patterns the gain will be larger.\n\n\nSparse matrices and efficiency\n\nSparse matrices give complexity reduction.\n\nBut they are not very good for parallel/GPU implementation.\n\nThey do not give maximal efficiency due to random data access.\n\nTypically, peak efficiency of 10\\%-15\\% is considered good.\n\n\n\nRecall how we measure efficiency of linear algebra operations\nThe standard way to measure the efficiency of linear algebra operations on a particular computing architecture is to use flops (number of floating point operations per second)\nWe can measure peak efficiency of an ordinary matrix-by-vector product.\n\nimport numpy as np\nimport time\nn = 4000\nk = 1400\na = np.random.randn(n, n)\nv = np.random.randn(n, k)\nt = time.time()\nnp.dot(a, v)\nt = time.time() - t\nprint('Time: {0: 3.1e}, Efficiency: {1: 3.1e} Gflops'.\\\n      format(t,  ((k*2 * n ** 2)/t) / 10 ** 9))\n\nTime:  8.8e-02, Efficiency:  5.1e+02 Gflops\n\n\n\nn = 4000000\nk = 10\nex = np.ones(n)\na = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.random.randn(n, k)\nt = time.time()\na.dot(rhs)\nt = time.time() - t\nprint('Time: {0: 3.1e}, Efficiency: {1: 3.1e} Gflops'.\\\n      format(t,  (3 * n * k) / t / 10 ** 9))\n\nTime:  9.5e-02, Efficiency:  1.3e+00 Gflops\n\n\n\n\nRandom data access and cache misses\n\nInitially all matrix and vector entries are stored in RAM (Random Access Memory)\nIf you want to compute matvec, the part of matrix and vector elements are moved to cache (fast and small capacity memory, see lecture about Strassen algorithm)\nAfter that, CPU takes data from cache to proccess it and then returns result in cache, too\nIf CPU needs some data that is not already in cache, this situation is called cache miss\nIf cache miss happens, the required data is moved from RAM to cache\n\nQ: what if cache does not have free space?\n\nThe larger number of cache misses, the slower computations\n\n\nCSR sparse matrix by vector product\n\n   for i in range(n):\n        \n        for k in range(ia[i]:ia[i+1]):\n            \n            y[i] += sa[k] * x[ja[k]]\n            \n\nWhat part of operands is strongly led cache misses?\n\nHow this issue can be solved?\n\n\n\n\nReordering reduces cache misses\n\nIf ja stores sequential elements, then they will be moved to cache altogether and number of cache misses decreases\nThis happens when sparse matrix is banded or at least block diagonal\nWe can convert given sparse matrix to banded or block diagonal with permutations\nLet P be row permutation matrix and Q be column permutation matrix\nA_1 = PAQ is a matrix, which has less bandwith than A\ny = Ax \\to \\tilde{y} = A_1 \\tilde{x}, where \\tilde{x} = Q^{\\top}x and \\tilde{y} = Py\nSeparated block diagonal form is a cache-oblivious format for sparse matrix by vector product\nIt can be extended for 2D, where separated not only rows, but also columns\n\n\nExample\n\nSBD in 1D \n\n\n\n\nSparse transpose matrix-by-vector product\n\nIn some cases it is important to compute not only Ax for sparse A, but also A^{\\top}x\nMort details will be discussed in the lecture about Krylov methods for non-symmetric linear systems\nTransposing is computationally expensive\nHere is proposed compressed sparse block format of storage appropriate for this case\n\n\nCompressed sparse block (CSB)\n\nSplit matrix in blocks\nStore block indices and indices of data inside each block\nThus, feasible number of bits to store indices\nOrdering of the blocks and inside blocks is impoprtant for parallel implementation\nSwitching between blockrow to blockcolumn makes this format appropriate to transpose matrix by vector product"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#solve-linear-systems-with-sparse-matrices",
    "href": "lectures/lecture-10/lecture-10.html#solve-linear-systems-with-sparse-matrices",
    "title": "Questions?",
    "section": "Solve linear systems with sparse matrices",
    "text": "Solve linear systems with sparse matrices\n\nDirect methods\n\nLU decomposition\nNumber of reordering techniques to minimize fill-in\n\nKrylov methods\n\nLet us start with small demo of solving sparse linear system…\n\nn = 1024\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\nsol = sp.sparse.linalg.spsolve(A, rhs)\n_, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(sol)\nax1.set_title('Not reshaped solution')\nax2.contourf(sol.reshape((n, n), order='f'))\nax2.set_title('Reshaped solution')\n\nText(0.5, 1.0, 'Reshaped solution')"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#take-home-message",
    "href": "lectures/lecture-10/lecture-10.html#take-home-message",
    "title": "Questions?",
    "section": "Take home message",
    "text": "Take home message\n\nAbout parallel matrix-by-vector product and different blocking.\nCSR format for storage\nCache and parallel issues in sparse matrix processing\nReordering and blocking as a way to solve these issues"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#краткое-содержание-предыдущих-лекцийсеминаров",
    "href": "lectures/lecture-2/lecture-2.html#краткое-содержание-предыдущих-лекцийсеминаров",
    "title": "Visualization",
    "section": "Краткое содержание предыдущих лекций/семинаров",
    "text": "Краткое содержание предыдущих лекций/семинаров\n\nАрифметика с плавающей точкой и связанные с ней проблемы\nУстойчивые алгоритмы: обратная и прямая устойчивость\nВажнейшие матричные нормы: спектральная и норма Фробениуса\nУнитарные матрицы сохраняют эти нормы\nСуществуют два “базовых” класса унитарных матриц: матрицы Хаусхолдера и Гивенса"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#примеры-пиковой-производительности",
    "href": "lectures/lecture-2/lecture-2.html#примеры-пиковой-производительности",
    "title": "Visualization",
    "section": "Примеры пиковой производительности",
    "text": "Примеры пиковой производительности\nFlops –– операции с плавающей точкой в секунду.\nGiga = 2^{30} \\approx 10^9,\nTera = 2^{40} \\approx 10^{12},\nPeta = 2^{50} \\approx 10^{15},\nExa = 2^{60} \\approx 10^{18}\nКакова пиковая производительность:\n\nСовременного CPU\nСовременного GPU\nСамого мощного суперкомпьютера в мире?\n\n\nТактовая частота процессора и производительность в флопс\nFLOPS = сокеты * (ядра на сокет) * (количество тактов в секунду) * (количество операций с плавающей точкой за такт).\n\nОбычно количество сокетов = 1\nКоличество ядер обычно 2 или 4\n\nКоличество тактов в секунду - это знакомая тактовая частота\nКоличество операций с плавающей точкой за такт зависит от конкретного процессора\n\n\nСовременный CPU (Intel Core i7) –– 400 Гфлопс\nСовременный GPU Nvidia DGX H100 – зависит от точности!\nСамый мощный суперкомпьютер в мире –– 1.102 Экзафлопс/с –– пиковая производительность"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#умножение-матрицы-на-вектор-matvec",
    "href": "lectures/lecture-2/lecture-2.html#умножение-матрицы-на-вектор-matvec",
    "title": "Visualization",
    "section": "Умножение матрицы на вектор (matvec)",
    "text": "Умножение матрицы на вектор (matvec)\nУмножение матрицы A размера n\\times n на вектор x размера n\\times 1 (y=Ax):\n\ny_{i} = \\sum_{j=1}^n a_{ij} x_j\n\nтребует n^2 умножений и n(n-1) сложений. Таким образом, общая сложность составляет 2n^2 - n =  \\mathcal{O}(n^2)"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#насколько-плохо-mathcalon2",
    "href": "lectures/lecture-2/lecture-2.html#насколько-плохо-mathcalon2",
    "title": "Visualization",
    "section": "Насколько плохо \\mathcal{O}(n^2)?",
    "text": "Насколько плохо \\mathcal{O}(n^2)?\n\nПусть A - матрица попарного гравитационного взаимодействия между планетами в галактике.\nЧисло планет в средней галактике составляет 10^{11}, поэтому размер этой матрицы 10^{11} \\times 10^{11}.\nЧтобы моделировать эволюцию во времени, мы должны умножать эту матрицу на вектор на каждом временном шаге.\nСамые мощные суперкомпьютеры выполняют около 10^{16} операций с плавающей точкой в секунду (флопс), поэтому время, необходимое для умножения матрицы A на вектор, составляет примерно\n\n\\begin{align*}\n\\frac{(10^{11})^2 \\text{ операций}}{10^{16} \\text{ флопс}} = 10^6 \\text{ сек} \\approx 11.5 \\text{ дней}\n\\end{align*}\nдля одного временного шага. Если бы мы могли умножать со сложностью \\mathcal{O}(n), мы бы получили\n\\begin{align*}\n\\frac{10^{11} \\text{ операций}}{10^{16} \\text{ флопс}} = 10^{-5} \\text{ сек}.\n\\end{align*}\nВот видео на YouTube, которое иллюстрирует столкновение двух галактик, смоделированное алгоритмом со сложностью \\mathcal{O}(n \\log n):\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"7HF5Oy8IMoM\")"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#можем-ли-мы-побить-mathcalon2",
    "href": "lectures/lecture-2/lecture-2.html#можем-ли-мы-побить-mathcalon2",
    "title": "Visualization",
    "section": "Можем ли мы побить \\mathcal{O}(n^2)?",
    "text": "Можем ли мы побить \\mathcal{O}(n^2)?\n\nВ общем случае НЕТ.\nДело в том, что у нас есть входные данные размера \\mathcal{O}(n^2), поэтому нет способа быть быстрее для произвольной матрицы.\nК счастью, мы можем быть быстрее для определенных типов матриц. Вот несколько примеров:\n\nПростейший пример - матрица, состоящая из всех единиц, которую можно легко умножить, используя только n-1 сложений. Эта матрица имеет ранг один. В более общем случае мы можем быстро умножать матрицы малого ранга (или матрицы, имеющие блоки малого ранга)\nРазреженные матрицы (содержат \\mathcal{O}(n) ненулевых элементов)\nСтруктурированные матрицы:\n\nФурье\nЦиркулянтные\nТеплицевы\nГанкелевы"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#произведение-матриц",
    "href": "lectures/lecture-2/lecture-2.html#произведение-матриц",
    "title": "Visualization",
    "section": "Произведение матриц",
    "text": "Произведение матриц\nРассмотрим композицию двух линейных операторов:\n\ny = Bx\nz = Ay\n\nТогда z = Ay = A B x = C x, где C - это произведение матриц."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#произведение-матрицы-на-матрицу-мм-классика",
    "href": "lectures/lecture-2/lecture-2.html#произведение-матрицы-на-матрицу-мм-классика",
    "title": "Visualization",
    "section": "Произведение матрицы на матрицу (ММ): классика",
    "text": "Произведение матрицы на матрицу (ММ): классика\nОпределение. Произведение матрицы A размера n \\times k и матрицы B размера k \\times m - это матрица C размера n \\times m с элементами \n   c_{ij} = \\sum_{s=1}^k a_{is} b_{sj}, \\quad i = 1, \\ldots, n, \\quad j = 1, \\ldots, m\n\nДля m=k=n сложность наивного алгоритма составляет 2n^3 - n^2 = \\mathcal{O}(n^3)."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#обсуждение-матричного-умножения",
    "href": "lectures/lecture-2/lecture-2.html#обсуждение-матричного-умножения",
    "title": "Visualization",
    "section": "Обсуждение матричного умножения",
    "text": "Обсуждение матричного умножения\n\nПроизведение матриц является основой почти всех эффективных алгоритмов в численной линейной алгебре.\nПо сути, все алгоритмы плотной численной линейной алгебры сводятся к последовательности матричных произведений.\nЭффективная реализация матричного умножения снижает сложность численных алгоритмов на тот же коэффициент.\nОднако реализация матричного умножения совсем не проста!"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#эффективная-реализация-матричного-умножения",
    "href": "lectures/lecture-2/lecture-2.html#эффективная-реализация-матричного-умножения",
    "title": "Visualization",
    "section": "Эффективная реализация матричного умножения",
    "text": "Эффективная реализация матричного умножения\nВ1: Легко ли умножать матрицу на матрицу наиболее эффективным способом?"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#ответ-нет-это-непросто",
    "href": "lectures/lecture-2/lecture-2.html#ответ-нет-это-непросто",
    "title": "Visualization",
    "section": "Ответ: нет, это непросто",
    "text": "Ответ: нет, это непросто\nЕсли вы хотите сделать это максимально быстро, используя имеющиеся компьютеры."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#демонстрация",
    "href": "lectures/lecture-2/lecture-2.html#демонстрация",
    "title": "Visualization",
    "section": "Демонстрация",
    "text": "Демонстрация\nДавайте сделаем короткую демонстрацию и сравним процедуру np.dot(), которая в моем случае использует MKL, с написанной вручную подпрограммой умножения матриц на Python, а также с ее версией на numba.\n\nimport numpy as np\ndef matmul(a, b):\n    n = a.shape[0] # размер первой размерности матрицы a\n    k = a.shape[1] # размер второй размерности матрицы a\n    m = b.shape[1] # размер второй размерности матрицы b\n    c = np.zeros((n, m)) # создаем матрицу нулей размера n x m\n    for i in range(n): # проходим по строкам результирующей матрицы\n        for j in range(m): # проходим по столбцам результирующей матрицы\n            for s in range(k): # суммируем произведения элементов\n                c[i, j] += a[i, s] * b[s, j]\n                \n    return c # возвращаем результат умножения матриц\n\n\nimport numpy as np\nfrom numba import jit # Компилятор \"на лету\" для Python, см. http://numba.pydata.org\n\n@jit(nopython=True)\ndef numba_matmul(a, b):\n    n = a.shape[0]\n    k = a.shape[1]\n    m = b.shape[1]\n    c = np.zeros((n, m))\n    for i in range(n):\n        for j in range(m):\n            for s in range(k):\n                c[i, j] += a[i, s] * b[s, j]\n    return c\n\nТеперь мы просто сравним время вычислений.\nПопробуйте угадать ответ.\n\nn = 100\na = np.random.randn(n, n)\nb = np.random.randn(n, n)\n\n%timeit matmul(a, b)\n%timeit numba_matmul(a, b)\n%timeit a @ b\n\n265 ms ± 854 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n572 μs ± 997 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n10.5 μs ± 69.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nIs this answer correct for any dimensions of matrices?\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndim_range = [10*i for i in range(1, 11)]\ntime_range_matmul = []\ntime_range_numba_matmul = []\ntime_range_np = []\nfor n in dim_range:\n    print(\"Dimension = {}\".format(n))\n    a = np.random.randn(n, n)\n    b = np.random.randn(n, n)\n\n    t = %timeit -o -q matmul(a, b)\n    time_range_matmul.append(t.best)\n    t = %timeit -o -q numba_matmul(a, b)\n    time_range_numba_matmul.append(t.best)\n    t = %timeit -o -q np.dot(a, b)\n    time_range_np.append(t.best)\n\nDimension = 10\nDimension = 20\nDimension = 30\nDimension = 40\nDimension = 50\nDimension = 60\nDimension = 70\nDimension = 80\nDimension = 90\nDimension = 100\n\n\n\nplt.plot(dim_range, time_range_matmul, label=\"Matmul\")\nplt.plot(dim_range, time_range_numba_matmul, label=\"Matmul Numba\")\nplt.plot(dim_range, time_range_np, label=\"Numpy\")\nplt.legend(fontsize=18)\nplt.xlabel(\"Dimension\", fontsize=18)\nplt.ylabel(\"Time\", fontsize=18)\nplt.yscale(\"log\")"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#почему-наивная-реализация-медленная",
    "href": "lectures/lecture-2/lecture-2.html#почему-наивная-реализация-медленная",
    "title": "Visualization",
    "section": "Почему наивная реализация медленная?",
    "text": "Почему наивная реализация медленная?\nОна медленная из-за двух проблем:\n\nОна не использует преимущества быстрой памяти (кэша) и архитектуры памяти в целом\nОна не использует доступные возможности параллелизации (особенно важно для GPU)"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#архитектура-памяти",
    "href": "lectures/lecture-2/lecture-2.html#архитектура-памяти",
    "title": "Visualization",
    "section": "Архитектура памяти",
    "text": "Архитектура памяти\n\n\nБыстрая память маленькая\nБольшая память медленная"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#типичные-характеристики-иерархии-памяти",
    "href": "lectures/lecture-2/lecture-2.html#типичные-характеристики-иерархии-памяти",
    "title": "Visualization",
    "section": "Типичные характеристики иерархии памяти",
    "text": "Типичные характеристики иерархии памяти\n\n\n\n\n\n\n\n\n\nТип памяти\nРазмер\nВремя доступа\nПримечания\n\n\n\n\nРегистры CPU\nНесколько КБ\n&lt;1 нс\nСамые быстрые, прямой доступ CPU\n\n\nКэш L1\n32-64 КБ\n1-4 нс\nРазделен на кэш инструкций и данных\n\n\nКэш L2\n256 КБ - 1 МБ\n4-10 нс\nЕдиный кэш\n\n\nКэш L3\n2-32 МБ\n10-20 нс\nОбщий между ядрами CPU\n\n\nОперативная память (RAM)\n8-32 ГБ\n100 нс\nОсновная системная память\n\n\nSSD\n256 ГБ - 2 ТБ\n10-100 мкс\nБыстрое вторичное хранилище\n\n\nЖесткий диск\n1-10 ТБ\n5-10 мс\nСамый медленный, но самый большой\n\n\n\nКлючевые наблюдения: - Время доступа увеличивается примерно в 10 раз на каждом уровне - Размер увеличивается в 10-100 раз на каждом уровне - Эффективное использование более быстрых уровней памяти критично для производительности"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#кэш-линии-и-когерентность-кэша",
    "href": "lectures/lecture-2/lecture-2.html#кэш-линии-и-когерентность-кэша",
    "title": "Visualization",
    "section": "Кэш-линии и когерентность кэша",
    "text": "Кэш-линии и когерентность кэша\n\nКэш-память организована в кэш-линии - блоки фиксированного размера (обычно 64 байта)\nКогда CPU нужны данные, он загружает всю кэш-линию, содержащую эти данные\nЭто эффективно при последовательном доступе к памяти (пространственная локальность)\n\nКогерентность кэша обеспечивает: - Все ядра CPU видят согласованное представление памяти - Когда одно ядро изменяет данные, другие ядра получают уведомление - Предотвращает состояния гонки и несогласованность данных\nПочему это важно для матричных операций: - Последовательный доступ к строкам/столбцам матрицы влияет на использование кэш-линий - Плохое использование кэш-линий = больше кэш-промахов = более низкая производительность\n- Многопоточный код требует когерентных кэшей для корректности"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#архитектура-памяти-gpu",
    "href": "lectures/lecture-2/lecture-2.html#архитектура-памяти-gpu",
    "title": "Visualization",
    "section": "Архитектура памяти GPU",
    "text": "Архитектура памяти GPU\nСовременные GPU имеют иную иерархию памяти по сравнению с CPU:\n\n\n\n\n\n\n\n\n\nТип памяти\nРазмер\nВремя доступа\nПримечания\n\n\n\n\nРегистры\n~4 МБ на SM\n~1 такт\nСамые быстрые, на поток/блок\n\n\nРазделяемая память/Кэш L1\n64-256 КБ на SM\n~20-30 тактов\nОбщая в пределах блока потоков\n\n\nКэш L2\n512КБ - 60МБ\n~200 тактов\nОбщий для всего GPU\n\n\nГлобальная память (VRAM)\n16-80 ГБ\n~400-600 тактов\nОсновная память GPU\n\n\nСистемная память (RAM)\n8-128 ГБ\n&gt;1000 тактов\nПамять CPU, доступ через PCIe\n\n\n\nКлючевые отличия от CPU: - Гораздо более параллельный доступ (тысячи потоков) - Больший файл регистров, но меньшие кэши - Более высокая пропускная способность памяти, но выше латентность - Критически важен объединенный доступ к памяти\nШаблоны доступа к памяти: - Объединенный: потоки в варпе обращаются к последовательной памяти = быстро - С шагом/случайный: потоки обращаются к разрозненной памяти = медленно - Конфликты банков разделяемой памяти могут ограничивать пропускную способность\nЛучшие практики: - Использовать разделяемую память для часто используемых данных - Обеспечивать объединенный доступ к глобальной памяти - Минимизировать передачу данных между CPU и GPU\nПримечание: Последний GPU NVIDIA H100 может иметь до 80ГБ памяти HBM3 VRAM"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#как-сделать-алгоритмы-более-вычислительно-интенсивными",
    "href": "lectures/lecture-2/lecture-2.html#как-сделать-алгоритмы-более-вычислительно-интенсивными",
    "title": "Visualization",
    "section": "Как сделать алгоритмы более вычислительно интенсивными",
    "text": "Как сделать алгоритмы более вычислительно интенсивными\nРеализация в NLA: использовать блочные версии алгоритмов. \nЭтот подход является основой BLAS (Basic Linear Algebra Subroutines), написанной много лет назад на Фортране и до сих пор управляющей вычислительным миром."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#разделим-матрицу-на-блоки-для-иллюстрации-рассмотрим-разделение-на-блочную-матрицу-2-times-2",
    "href": "lectures/lecture-2/lecture-2.html#разделим-матрицу-на-блоки-для-иллюстрации-рассмотрим-разделение-на-блочную-матрицу-2-times-2",
    "title": "Visualization",
    "section": "Разделим матрицу на блоки! Для иллюстрации рассмотрим разделение на блочную матрицу 2 \\times 2:",
    "text": "Разделим матрицу на блоки! Для иллюстрации рассмотрим разделение на блочную матрицу 2 \\times 2:\n\n   A = \\begin{bmatrix}\n         A_{11} & A_{12} \\\\\n         A_{21} & A_{22}\n        \\end{bmatrix}, \\quad B = \\begin{bmatrix}\n         B_{11} & B_{12} \\\\\n         B_{21} & B_{22}\n        \\end{bmatrix}\nТогда,\nAB = \\begin{bmatrix}A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\\\\n            A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\end{bmatrix}.\nЕсли A_{11}, B_{11} и их произведение помещаются в кэш-память (которая составляет 20 Мб (L3) для современного процессора Intel), тогда мы загружаем их в память только один раз."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#blas",
    "href": "lectures/lecture-2/lecture-2.html#blas",
    "title": "Visualization",
    "section": "BLAS",
    "text": "BLAS\nBLAS имеет три уровня: 1. BLAS-1, операции типа c = a + b 2. BLAS-2, операции типа умножения матрицы на вектор 3. BLAS-3, умножение матрицы на матрицу\nВ чем принципиальные различия между ними?\nОсновное различие заключается в соотношении количества операций и входных данных!\n\nBLAS-1: \\mathcal{O}(n) данных, \\mathcal{O}(n) операций\nBLAS-2: \\mathcal{O}(n^2) данных, \\mathcal{O}(n^2) операций\n\nBLAS-3: \\mathcal{O}(n^2) данных, \\mathcal{O}(n^3) операций"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#почему-blas-так-важна-и-актуальна",
    "href": "lectures/lecture-2/lecture-2.html#почему-blas-так-важна-и-актуальна",
    "title": "Visualization",
    "section": "Почему BLAS так важна и актуальна?",
    "text": "Почему BLAS так важна и актуальна?\n\nСовременная реализация базовых операций линейной алгебры\nПредоставляет стандартные имена для операций в любых новых реализациях (например, ATLAS, OpenBLAS, MKL). Вы можете вызвать функцию умножения матрицы на матрицу (GEMM), связать свой код с любой реализацией BLAS, и он будет работать корректно\nФормулировка новых алгоритмов в терминах операций BLAS\nСуществуют обертки для большинства популярных языков"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#пакеты-связанные-с-blas",
    "href": "lectures/lecture-2/lecture-2.html#пакеты-связанные-с-blas",
    "title": "Visualization",
    "section": "Пакеты, связанные с BLAS",
    "text": "Пакеты, связанные с BLAS\n\nATLAS - Автоматически настраиваемое программное обеспечение для линейной алгебры. Оно автоматически адаптируется под конкретную архитектуру системы.\nLAPACK - Пакет линейной алгебры. Предоставляет операции линейной алгебры высокого уровня (например, факторизации матриц), основанные на вызовах подпрограмм BLAS.\nIntel MKL - Математическая библиотека ядра. Предоставляет реализацию BLAS и LAPACK, оптимизированную для процессоров Intel. Доступна в дистрибутиве Anaconda Python:"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#более-быстрые-алгоритмы-умножения-матриц",
    "href": "lectures/lecture-2/lecture-2.html#более-быстрые-алгоритмы-умножения-матриц",
    "title": "Visualization",
    "section": "Более быстрые алгоритмы умножения матриц",
    "text": "Более быстрые алгоритмы умножения матриц\nНапомним, что умножение матрицы на матрицу требует \\mathcal{O}(n^3) операций. Однако хранение требует \\mathcal{O}(n^2).\nВопрос: возможно ли уменьшить количество операций до \\mathcal{O}(n^2)?\nОтвет: поиск алгоритма умножения матриц со сложностью \\mathcal{O}(n^2) все еще не завершен.\n\nАлгоритм Штрассена дает \\mathcal{O}(n^{2.807\\dots}) –– иногда используется на практике\nТекущий мировой рекорд \\mathcal{O}(n^{2.37\\dots}) –– большая константа, непрактичен, основан на алгоритме Копперсмита-Винограда.\nОн улучшил предыдущий рекорд (Уильямс 2012) на 3\\cdot 10^{-7}\nВ статьях до сих пор изучается умножение матриц 3 \\times 3 и интерпретируется с разных сторон (Heule, et. al. 2019)\n\nРассмотрим алгоритм Штрассена подробнее."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#наивное-умножение",
    "href": "lectures/lecture-2/lecture-2.html#наивное-умножение",
    "title": "Visualization",
    "section": "Наивное умножение",
    "text": "Наивное умножение\nПусть A и B - две матрицы размера 2\\times 2. Наивное умножение C = AB\n\n\\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22}  \\end{bmatrix}  =\n\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}  \\end{bmatrix}\n\\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22}  \\end{bmatrix} =\n\\begin{bmatrix}\na_{11}b_{11} + a_{12}b_{21} & a_{11}b_{21} + a_{12}b_{22} \\\\\na_{21}b_{11} + a_{22}b_{21} & a_{21}b_{21} + a_{22}b_{22}\n\\end{bmatrix}\n\nсодержит 8 умножений и 4 сложения."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#алгоритм-штрассена",
    "href": "lectures/lecture-2/lecture-2.html#алгоритм-штрассена",
    "title": "Visualization",
    "section": "Алгоритм Штрассена",
    "text": "Алгоритм Штрассена\nВ работе Gaussian elimination is not optimal (1969) Штрассен обнаружил, что можно вычислить C используя 18 сложений и только 7 умножений: \n\\begin{split}\nc_{11} &= f_1 + f_4 - f_5 + f_7, \\\\\nc_{12} &= f_3 + f_5, \\\\\nc_{21} &= f_2 + f_4, \\\\\nc_{22} &= f_1 - f_2 + f_3 + f_6,\n\\end{split}\n где \n\\begin{split}\nf_1 &= (a_{11} + a_{22}) (b_{11} + b_{22}), \\\\\nf_2 &= (a_{21} + a_{22}) b_{11}, \\\\\nf_3 &= a_{11} (b_{12} - b_{22}), \\\\\nf_4 &= a_{22} (b_{21} - b_{11}), \\\\\nf_5 &= (a_{11} + a_{12}) b_{22}, \\\\\nf_6 &= (a_{21} - a_{11}) (b_{11} + b_{12}), \\\\\nf_7 &= (a_{12} - a_{22}) (b_{21} + b_{22}).\n\\end{split}\n\nК счастью, эти формулы работают даже если a_{ij} и b_{ij}, i,j=1,2 являются блочными матрицами.\nТаким образом, алгоритм Штрассена выглядит следующим образом. - Сначала мы разделяем матрицы A и B размера n\\times n, n=2^d на 4 блока размера \\frac{n}{2}\\times \\frac{n}{2} - Затем мы вычисляем умножения в описанных формулах рекурсивно\nЭто снова приводит нас к идее разделяй и властвуй."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#пример-алгоритма-штрассена",
    "href": "lectures/lecture-2/lecture-2.html#пример-алгоритма-штрассена",
    "title": "Visualization",
    "section": "Пример алгоритма Штрассена",
    "text": "Пример алгоритма Штрассена\nДавайте перемножим две матрицы 2x2, используя метод Штрассена:\nA = \\begin{bmatrix} 2 & 3 \\\\ 4 & 1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 7 \\\\ 6 & 8 \\end{bmatrix}\nВычислим 7 произведений от f_1 до f_7:\n\\begin{align*}\nf_1 &= (2 + 1)(5 + 8) = 3 \\cdot 13 = 39 \\\\\nf_2 &= (4 + 1)(5) = 5 \\cdot 5 = 25 \\\\\nf_3 &= (2)(7 - 8) = 2 \\cdot (-1) = -2 \\\\\nf_4 &= (1)(6 - 5) = 1 \\cdot 1 = 1 \\\\\nf_5 &= (2 + 3)(8) = 5 \\cdot 8 = 40 \\\\\nf_6 &= (4 - 2)(5 + 7) = 2 \\cdot 12 = 24 \\\\\nf_7 &= (3 - 1)(6 + 8) = 2 \\cdot 14 = 28\n\\end{align*}\nТеперь вычислим элементы результирующей матрицы C:\n\\begin{align*}\nc_{11} &= f_1 + f_4 - f_5 + f_7 = 39 + 1 - 40 + 28 = 28 \\\\\nc_{12} &= f_3 + f_5 = -2 + 40 = 38 \\\\\nc_{21} &= f_2 + f_4 = 25 + 1 = 26 \\\\\nc_{22} &= f_1 - f_2 + f_3 + f_6 = 39 - 25 - 2 + 24 = 36\n\\end{align*}\nТаким образом:\nC = \\begin{bmatrix} 28 & 38 \\\\ 26 & 36 \\end{bmatrix}\nВы можете проверить, что это равно результату стандартного матричного умножения!"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#сложность-алгоритма-штрассена",
    "href": "lectures/lecture-2/lecture-2.html#сложность-алгоритма-штрассена",
    "title": "Visualization",
    "section": "Сложность алгоритма Штрассена",
    "text": "Сложность алгоритма Штрассена\n\nКоличество умножений\nПодсчет количества умножений - тривиальная задача. Обозначим через M(n) количество умножений, используемых для перемножения 2 матриц размера n\\times n с использованием концепции “разделяй и властвуй”. Тогда для наивного алгоритма количество умножений равно\n M_\\text{naive}(n) = 8 M_\\text{naive}\\left(\\frac{n}{2} \\right) = 8^2 M_\\text{naive}\\left(\\frac{n}{4} \\right)\n= \\dots = 8^{d-1} M(2) = 8^{d} M(1) = 8^{d} = 8^{\\log_2 n} = n^{\\log_2 8} = n^3 \nТаким образом, даже при использовании идеи “разделяй и властвуй” мы не можем быть лучше чем n^3.\nДавайте посчитаем количество умножений для алгоритма Штрассена:\n M_\\text{strassen}(n) = 7 M_\\text{strassen}\\left(\\frac{n}{2} \\right) = 7^2 M_\\text{strassen}\\left(\\frac{n}{4} \\right)\n= \\dots = 7^{d-1} M(1) = 7^{d} = 7^{\\log_2 n} = n^{\\log_2 7} \n\n\nКоличество сложений\nНет смысла оценивать количество сложений A(n) для наивного алгоритма, так как мы уже получили n^3 умножений. Для алгоритма Штрассена имеем:\n A_\\text{strassen}(n) = 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 \nпоскольку на первом уровне нам нужно сложить матрицы размера \\frac{n}{2}\\times \\frac{n}{2} 18 раз, а затем углубиться для каждого из 7 умножений. Таким образом,\n\n \\begin{split}\nA_\\text{strassen}(n) =& 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 = 7 \\left(7 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 18 \\left( \\frac{n}{4} \\right)^2 \\right) + 18 \\left( \\frac{n}{2} \\right)^2 =\n7^2 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 7\\cdot 18 \\left( \\frac{n}{4} \\right)^2 +  18 \\left( \\frac{n}{2} \\right)^2 = \\\\\n=& \\dots = 18 \\sum_{k=1}^d 7^{k-1} \\left( \\frac{n}{2^k} \\right)^2 = \\frac{18}{4} n^2 \\sum_{k=1}^d \\left(\\frac{7}{4} \\right)^{k-1} = \\frac{18}{4} n^2 \\frac{\\left(\\frac{7}{4} \\right)^d - 1}{\\frac{7}{4} - 1} = 6 n^2 \\left( \\left(\\frac{7}{4} \\right)^d - 1\\right) \\leqslant 6 n^2 \\left(\\frac{7}{4} \\right)^d = 6 n^{\\log_2 7}\n\\end{split}\n \n(поскольку 4^d = n^2 и 7^d = n^{\\log_2 7}).\nАсимптотическое поведение A(n) также может быть найдено из основной теоремы.\n\n\nTotal complexity\nTotal complexity is M_\\text{strassen}(n) + A_\\text{strassen}(n)= 7 n^{\\log_2 7}. Strassen algorithm becomes faster when\n\\begin{align*}\n2n^3 &&gt; 7 n^{\\log_2 7}, \\\\\nn &&gt; 667,\n\\end{align*}\nso it is not a good idea to get to the bottom level of recursion."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#alphatensor",
    "href": "lectures/lecture-2/lecture-2.html#alphatensor",
    "title": "Visualization",
    "section": "AlphaTensor",
    "text": "AlphaTensor\nНедавняя статья AlphaTensor показала, как современное глубокое обучение с подкреплением может быть использовано для получения новых разложений тензоров."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#интерпретация-с-точки-зрения-обучения-с-подкреплением",
    "href": "lectures/lecture-2/lecture-2.html#интерпретация-с-точки-зрения-обучения-с-подкреплением",
    "title": "Visualization",
    "section": "Интерпретация с точки зрения обучения с подкреплением",
    "text": "Интерпретация с точки зрения обучения с подкреплением\nВ обучении с подкреплением агент учится выполнять действия на основе состояния и вознаграждения.\nВ данном случае состоянием является тензор.\nДействие - это вычитание тензора ранга один.\nЕсли в конце получается ненулевой результат, вы получаете вознаграждение.\nЗатем вы выполняете миллионы различных действий и закрепляете хорошие результаты."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#избранные-результаты",
    "href": "lectures/lecture-2/lecture-2.html#избранные-результаты",
    "title": "Visualization",
    "section": "Избранные результаты",
    "text": "Избранные результаты\n\nЛучшие ранги для определенных размеров матриц\nНовые варианты алгоритма Штрассена 4x4, которые работают быстрее на реальном оборудовании (но только для этого конкретного оборудования!)\nУлучшенное произведение антисимметричной матрицы на вектор"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#итоги-части-про-матричное-умножение",
    "href": "lectures/lecture-2/lecture-2.html#итоги-части-про-матричное-умножение",
    "title": "Visualization",
    "section": "Итоги части про матричное умножение",
    "text": "Итоги части про матричное умножение\n\nМатричное умножение - это основа численной линейной алгебры. Для достижения высокой эффективности необходимо мыслить блоками\nВсе это связано с иерархией компьютерной памяти\nКонцепция блочных алгоритмов"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#предыдущая-лекция",
    "href": "lectures/lecture-4/lecture-4.html#предыдущая-лекция",
    "title": "Итоги",
    "section": "Предыдущая лекция",
    "text": "Предыдущая лекция\n\nРанг матрицы\nСкелетное разложение\nНизкоранговая аппроксимация\nСингулярное разложение (SVD)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#сегодняшняя-лекция",
    "href": "lectures/lecture-4/lecture-4.html#сегодняшняя-лекция",
    "title": "Итоги",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\n\nЛинейные системы\nОбратная матрица\nЧисло обусловленности\nМетод Гаусса"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#линейные-системы",
    "title": "Итоги",
    "section": "Линейные системы",
    "text": "Линейные системы\n\nЛинейные системы уравнений являются базовым инструментом в численной линейной алгебре.\nОни встречаются в:\n\nЗадачах линейной регрессии\nДискретизации дифференциальных/интегральных уравнений в частных производных\nЛинеаризации задач нелинейной регрессии\nОптимизации (например, методы Гаусса-Ньютона и Ньютона-Рафсона, условия Каруша-Куна-Такера)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-уравнения-и-матрицы",
    "href": "lectures/lecture-4/lecture-4.html#линейные-уравнения-и-матрицы",
    "title": "Итоги",
    "section": "Линейные уравнения и матрицы",
    "text": "Линейные уравнения и матрицы\n\nИз школы мы знаем о линейных уравнениях.\nСистема линейных уравнений может быть записана в форме\n\n\\begin{align*}\n    &2 x + 3 y = 5\\quad &\\longrightarrow \\quad &2x + 3 y + 0 z = 5\\\\\n    &2 x + 3z = 5\\quad &\\longrightarrow\\quad &2 x + 0 y + 3 z = 5\\\\\n    &x + y = 2\\quad &\\longrightarrow\\quad  & 1 x + 1 y + 0 z = 2\\\\\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#матричная-форма",
    "href": "lectures/lecture-4/lecture-4.html#матричная-форма",
    "title": "Итоги",
    "section": "Матричная форма",
    "text": "Матричная форма\n\n\\begin{pmatrix}\n2 & 3 & 0 \\\\\n2 & 0 & 3 \\\\\n1 & 1 & 0 \\\\\n\\end{pmatrix}\\begin{pmatrix}\nx \\\\\ny \\\\\nz\n\\end{pmatrix} =\n\\begin{pmatrix}\n5 \\\\\n5 \\\\\n2\n\\end{pmatrix}\n\nили просто\n A u = f,  \nгде A - матрица размера 3 \\times 3, а f - правая часть"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#переопределенные-и-недоопределенные-системы-линейных-уравнений",
    "href": "lectures/lecture-4/lecture-4.html#переопределенные-и-недоопределенные-системы-линейных-уравнений",
    "title": "Итоги",
    "section": "Переопределенные и недоопределенные системы линейных уравнений",
    "text": "Переопределенные и недоопределенные системы линейных уравнений\nЕсли система Au = f имеет:\n\nбольше уравнений, чем неизвестных, она называется переопределенной системой (как правило, не имеет решения)\nменьше уравнений, чем неизвестных, она называется недоопределенной системой (решение не единственно, для получения единственного решения необходимы дополнительные предположения)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#существование-решений",
    "href": "lectures/lecture-4/lecture-4.html#существование-решений",
    "title": "Итоги",
    "section": "Существование решений",
    "text": "Существование решений\nРешение системы линейных уравнений с квадратной матрицей A\nA u = f\nсуществует тогда и только тогда, когда * \\det A \\ne 0\nили\n\nматрица A имеет полный ранг."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#масштабы-линейных-систем",
    "href": "lectures/lecture-4/lecture-4.html#масштабы-линейных-систем",
    "title": "Итоги",
    "section": "Масштабы линейных систем",
    "text": "Масштабы линейных систем\nВ разных приложениях типичный размер линейных систем может быть разным.\n\nМалые: n \\leq 10^4 (полная матрица может храниться в памяти, плотная матрица)\nСредние: n = 10^4 - 10^6 (обычно разреженная или структурированная матрица)\nБольшие: n = 10^8 - 10^9 (обычно разреженная матрица + параллельные вычисления)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-большими",
    "href": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-большими",
    "title": "Итоги",
    "section": "Линейные системы могут быть большими",
    "text": "Линейные системы могут быть большими\n\nМы берем непрерывную задачу, дискретизируем ее на сетке с N элементами и получаем линейную систему с матрицей N\\times N.\nПример сетки вокруг самолета A319 (взято с сайта GMSH). \n\nОсновная сложность в том, что эти системы большие: миллионы или миллиарды неизвестных!"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-структурированными",
    "href": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-структурированными",
    "title": "Итоги",
    "section": "Линейные системы могут быть структурированными",
    "text": "Линейные системы могут быть структурированными\n\nХранение N^2 элементов матрицы непозволительно даже для N = 100000.\n\nВ: как работать с такими матрицами?\nО: к счастью, эти матрицы структурированные и требуют хранения только \\mathcal{O}(N) параметров.\n\nНаиболее распространенной структурой являются разреженные матрицы: такие матрицы имеют только \\mathcal{O}(N) ненулевых элементов!\nПример (одна из известных матриц для n = 5):\n\n\n  \\begin{pmatrix}\n  2 & -1 & 0 & 0 & 0 \\\\\n  -1 & 2 & -1 & 0 & 0 \\\\\n  0 & -1 & 2 & -1 & 0 \\\\\n  0 & 0 &-1& 2 & -1  \\\\\n  0 & 0 & 0 & -1 & 2 \\\\\n  \\end{pmatrix}\n\n\nПо крайней мере такие матрицы можно хранить\nТакже можно быстро умножать такую матрицу на вектор\nНо как решать линейные системы?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#основные-вопросы-о-линейных-системах",
    "href": "lectures/lecture-4/lecture-4.html#основные-вопросы-о-линейных-системах",
    "title": "Итоги",
    "section": "Основные вопросы о линейных системах",
    "text": "Основные вопросы о линейных системах\n\nКакую точность мы получаем в решении (из-за ошибок округления)?\nКак мы вычисляем решение? (LU-разложение, метод Гаусса)\nКакова сложность решения линейных систем?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы",
    "title": "Итоги",
    "section": "Как решать линейные системы?",
    "text": "Как решать линейные системы?\nВажно: забудьте про определители и правило Крамера (оно хорошо работает только для матриц 2 \\times 2)!"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы-1",
    "href": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы-1",
    "title": "Итоги",
    "section": "Как решать линейные системы?",
    "text": "Как решать линейные системы?\nОсновной инструмент - исключение переменных.\n\\begin{align*}\n    &2 y + 3 x = 5 \\quad&\\longrightarrow \\quad &y = 5/2 -  3/2 x \\\\\n    &2 x + 3z = 5 \\quad&\\longrightarrow\\quad &z = 5/3 - 2/3 x\\\\\n    &z + y = 2 \\quad&\\longrightarrow\\quad  & 5/2 + 5/3 - (3/2 + 2/3) x = 2,\\\\\n\\end{align*}\nи так находится x (и все предыдущие).\nЭтот процесс называется методом Гаусса и является одним из наиболее широко используемых алгоритмов."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#метод-гаусса",
    "href": "lectures/lecture-4/lecture-4.html#метод-гаусса",
    "title": "Итоги",
    "section": "Метод Гаусса",
    "text": "Метод Гаусса\nМетод Гаусса состоит из двух шагов: 1. Прямой ход 2. Обратный ход"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#прямой-проход",
    "href": "lectures/lecture-4/lecture-4.html#прямой-проход",
    "title": "Итоги",
    "section": "Прямой проход",
    "text": "Прямой проход\n\nНа прямом проходе мы исключаем x_1:\n\n\n   x_1 = f_1 - (a_{12} x_2 + \\ldots + a_{1n} x_n)/a_{11},\n\nи затем подставляем это в уравнения 2, \\ldots, n.\n\nЗатем мы исключаем x_2 и так далее из второго уравнения.\nВажно, что ведущие элементы (на которые мы делим) не равны 0."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#обратный-проход",
    "href": "lectures/lecture-4/lecture-4.html#обратный-проход",
    "title": "Итоги",
    "section": "Обратный проход",
    "text": "Обратный проход\nНа обратном проходе: - решаем уравнение для x_n - подставляем его в уравнение для x_{n-1} и так далее, пока не вычислим все x_i, i=1,\\ldots, n."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#метод-гаусса-и-lu-разложение",
    "href": "lectures/lecture-4/lecture-4.html#метод-гаусса-и-lu-разложение",
    "title": "Итоги",
    "section": "Метод Гаусса и LU-разложение",
    "text": "Метод Гаусса и LU-разложение\n\nМетод Гаусса - это вычисление одного из важнейших матричных разложений: LU-разложения.\n\nОпределение: LU-разложением квадратной матрицы A называется представление\nA = LU,\nгде - L - нижнетреугольная матрица (элементы строго над диагональю равны нулю) - U - верхнетреугольная матрица (элементы строго под диагональю равны нулю)\nЭто разложение не единственно, поэтому обычно требуют, чтобы матрица L имела единицы на диагонали.\nОсновная цель LU-разложения - это решение линейной системы, потому что\n A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, \nи это сводится к решению двух линейных систем прямой проход\n L y = f, \nи обратный проход\n U x = y. \nСуществует ли LU-разложение всегда?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#сложность-метода-гауссаlu-разложения",
    "href": "lectures/lecture-4/lecture-4.html#сложность-метода-гауссаlu-разложения",
    "title": "Итоги",
    "section": "Сложность метода Гаусса/LU-разложения",
    "text": "Сложность метода Гаусса/LU-разложения\n\nКаждый шаг исключения требует \\mathcal{O}(n^2) операций.\nТаким образом, стоимость наивного алгоритма составляет \\mathcal{O}(n^3).\n\nПодумайте: может ли алгоритм Штрассена помочь здесь?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#блочное-lu-разложение",
    "href": "lectures/lecture-4/lecture-4.html#блочное-lu-разложение",
    "title": "Итоги",
    "section": "Блочное LU-разложение",
    "text": "Блочное LU-разложение\nМы можем попробовать вычислить блочную версию LU-разложения:\n\\begin{pmatrix} A_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{pmatrix} = \\begin{pmatrix} L_{11} & 0 \\\\\nL_{21} & L_{22}\n\\end{pmatrix} \\begin{pmatrix} U_{11} & U_{12} \\\\\n0 & U_{22}\n\\end{pmatrix} \n\nЕсть две основные операции: вычисление LU-разложения половинных матриц + произведение матриц."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#существование-lu-разложения",
    "href": "lectures/lecture-4/lecture-4.html#существование-lu-разложения",
    "title": "Итоги",
    "section": "Существование LU-разложения",
    "text": "Существование LU-разложения\n\nАлгоритм LU-разложения не даст сбой, если мы не делим на ноль на каждом шаге метода Гаусса.\n\nВопрос: когда это так, для какого класса матриц?\nОтвет: это верно для строго регулярных матриц."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#строго-регулярные-матрицы-и-lu-разложение",
    "href": "lectures/lecture-4/lecture-4.html#строго-регулярные-матрицы-и-lu-разложение",
    "title": "Итоги",
    "section": "Строго регулярные матрицы и LU-разложение",
    "text": "Строго регулярные матрицы и LU-разложение\n\nОпределение. Матрица A называется строго регулярной, если все её ведущие главные миноры (т.е. подматрицы, состоящие из первых k строк и k столбцов) невырождены.\nВ этом случае всегда существует LU-разложение. Обратное также верно (проверьте!).\n\nСледствие: Если L - унитреугольная матрица (единицы на диагонали), то LU-разложение единственно. \nДоказательство: Действительно, L_1 U_1 = L_2 U_2 означает L_2^{-1} L_1 = U_2 U_1^{-1}. L_2^{-1} L_1 - нижнетреугольная матрица с единицами на диагонали. U_2 U_1^{-1} - верхнетреугольная матрица. Следовательно, L_2^{-1} L_1 = U_2 U_1^{-1} = I и L_1 = L_2, U_1 = U_2."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#lu-разложение-для-эрмитовых-положительно-определенных-матриц-разложение-холецкого",
    "href": "lectures/lecture-4/lecture-4.html#lu-разложение-для-эрмитовых-положительно-определенных-матриц-разложение-холецкого",
    "title": "Итоги",
    "section": "LU-разложение для эрмитовых положительно определенных матриц (разложение Холецкого)",
    "text": "LU-разложение для эрмитовых положительно определенных матриц (разложение Холецкого)\n\nСтрого регулярные матрицы имеют LU-разложение.\nВажным подклассом строго регулярных матриц является класс эрмитовых положительно определенных матриц\n\nОпределение. Матрица A называется  положительно определенной , если для любого x: \\Vert x \\Vert \\ne 0 выполняется\n\n(x, Ax) &gt; 0.\n - если это выполняется для x \\in \\mathbb{C}^n, то матрица A должна быть эрмитовой - если это выполняется для x \\in \\mathbb{R}^n, то матрица A может быть несимметричной\n\nУтверждение: Эрмитова положительно определенная матрица A является строго регулярной и имеет разложение Холецкого вида\n\nA = RR^*,\nгде R - нижнетреугольная матрица.\n\nДавайте попробуем доказать этот факт (на доске).\nИногда это называют “квадратным корнем” из матрицы.\n\nВычисление LU-разложения\n\nВо многих случаях вычисление LU-разложения один раз - хорошая идея!\nПосле того, как разложение найдено (это требует \\mathcal{O}(n^3) операций), решение линейных систем с L и U требует только \\mathcal{O}(n^2) операций.\n\nПроверьте:\n\nРешение линейных систем с треугольными матрицами простое (почему?).\nКак мы вычисляем множители L и U?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#когда-lu-разложение-не-работает",
    "href": "lectures/lecture-4/lecture-4.html#когда-lu-разложение-не-работает",
    "title": "Итоги",
    "section": "Когда LU-разложение не работает",
    "text": "Когда LU-разложение не работает\n\nЧто происходит, если матрица не является строго регулярной (или ведущие элементы в методе Гаусса очень малы?).\nСуществует классический пример матрицы 2 \\times 2 с плохим LU-разложением.\nРассмотрим матрицу\n\n\n    A = \\begin{pmatrix}\n    \\varepsilon & 1 \\\\\n    1 & 1\n    \\end{pmatrix}\n\n\nЕсли \\varepsilon достаточно мал, мы можем потерпеть неудачу. В отличие от этого, разложение Холецкого всегда стабильно.\n\nДавайте рассмотрим демонстрацию.\n\nimport numpy as np\n\neps = 9.9e-15\na = [[eps, 1],[1.0,  1]]\na = np.array(a)\na0 = a.copy()\nn = a.shape[0]\nL = np.zeros((n, n))\nU = np.zeros((n, n))\nfor k in range(n): #Eliminate one row\n    L[k, k] = 1.0\n    for i in range(k+1, n):\n        L[i, k] = a[i, k]/a[k, k]\n        for j in range(k+1, n):\n            a[i, j] -= L[i, k]*a[k, j]\n    for j in range(k, n):\n        U[k, j] = a[k, j]\nprint('L * U - A:\\n', np.dot(L, U) - a0)\nL\n\nL * U - A:\n [[0. 0.]\n [0. 0.]]\n\n\narray([[1.00000000e+00, 0.00000000e+00],\n       [1.01010101e+14, 1.00000000e+00]])"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#концепция-выбора-ведущего-элемента",
    "href": "lectures/lecture-4/lecture-4.html#концепция-выбора-ведущего-элемента",
    "title": "Итоги",
    "section": "Концепция выбора ведущего элемента",
    "text": "Концепция выбора ведущего элемента\n\nМы можем выполнять выбор ведущего элемента, т.е. переставлять строки и столбцы для максимизации A_{kk}, на который мы делим.\nПростейшая, но эффективная стратегия - это выбор ведущей строки: на каждом шаге выбираем индекс с максимальным по модулю значением и ставим его на диагональ.\nЭто дает нам разложение\n\nA = P L U,\nгде P - это матрица перестановок.\nВопрос: Что делает выбор ведущей строки хорошим?\nОтвет: Он хорош тем, что\n | L_{ij}|&lt;1, \nно элементы U могут вырасти до 2^n! (на практике это встречается очень редко).\n\nМожете ли вы придумать матрицу, где элементы U растут максимально возможным образом?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#устойчивость-линейных-систем",
    "href": "lectures/lecture-4/lecture-4.html#устойчивость-линейных-систем",
    "title": "Итоги",
    "section": "Устойчивость линейных систем",
    "text": "Устойчивость линейных систем\n\nСуществует фундаментальная проблема решения линейных систем, которая не зависит от используемого алгоритма.\nОна возникает, когда элементы матрицы представлены в виде чисел с плавающей точкой или присутствует измерительный шум.\n\nПроиллюстрируем эту проблему на следующем примере.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 40\na = [[1.0/(i + j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\nrhs = np.random.normal(size=(n,)) #Right-hand side\nx = np.linalg.solve(a, rhs) #This function computes LU-factorization and solves linear system\n\n#And check if everything is fine\ner = np.linalg.norm(np.dot(a, x) - rhs) / np.linalg.norm(rhs)\nprint(er)\nplt.plot(x)\nplt.grid(True)\n\n19.87968164147078\n\n\n\n\n\n\n\n\n\n\nКак видите, ошибка растет с увеличением n, и нам нужно выяснить почему.\nВажный момент заключается в том, что это не проблема алгоритма: это проблема представления матрицы в памяти.\nОшибка возникает в момент, когда элементы матрицы вычисляются приближенно."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#вопросы-по-демонстрации",
    "href": "lectures/lecture-4/lecture-4.html#вопросы-по-демонстрации",
    "title": "Итоги",
    "section": "Вопросы по демонстрации",
    "text": "Вопросы по демонстрации\n\nВ чем была проблема в предыдущем примере?\nПочему ошибка растет так быстро?\nИ здесь мы подходим к одному из основных понятий численной линейной алгебры: понятию числа обусловленности матрицы.\n\nНо прежде чем говорить об этом, нам нужно определить обратную матрицу."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#обратная-матрица-определение",
    "href": "lectures/lecture-4/lecture-4.html#обратная-матрица-определение",
    "title": "Итоги",
    "section": "Обратная матрица: определение",
    "text": "Обратная матрица: определение\n\nОбратная матрица к матрице A определяется как матрица X, обозначаемая A^{-1}, такая что\n\n AX = XA = I, \nгде I - единичная матрица (т.е. I_{ij} = 0 при i \\ne j и 1 в противном случае). - Вычисление обратной матрицы связано с решением линейных систем. Действительно, i-й столбец произведения дает\n A x_i = e_i,\nгде e_i - это i-й столбец единичной матрицы. - Таким образом, мы можем применить метод Гаусса для решения этой системы. Более того, если в этом процессе нет деления на ноль (и опорные элементы не зависят от правой части), то систему можно решить."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#обратная-матрица-и-линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#обратная-матрица-и-линейные-системы",
    "title": "Итоги",
    "section": "Обратная матрица и линейные системы",
    "text": "Обратная матрица и линейные системы\nЕсли мы вычислили A^{-1}, то решение линейной системы\nAx = f\nпросто x = A^{-1} f.\nДействительно,\n A(A^{-1} f) = (AA^{-1})f = I f = f."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#ряд-неймана",
    "href": "lectures/lecture-4/lecture-4.html#ряд-неймана",
    "title": "Итоги",
    "section": "Ряд Неймана",
    "text": "Ряд Неймана\n\nЧтобы изучить, почему могут возникать такие большие ошибки в решении (см. пример выше с матрицей Гильберта), нам нужен важный вспомогательный результат.\n\nРяд Неймана:\nЕсли для матрицы F выполняется условие \\Vert F \\Vert &lt; 1, то матрица (I - F) обратима и\n(I - F)^{-1} = I + F + F^2 + F^3 + \\ldots = \\sum_{k=0}^{\\infty} F^k.\nЗаметим, что это матричная версия геометрической прогрессии.\nВопрос: какая норма здесь рассматривается? Какая норма является “наилучшей” в данном случае?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#доказательство",
    "href": "lectures/lecture-4/lecture-4.html#доказательство",
    "title": "Итоги",
    "section": "Доказательство",
    "text": "Доказательство\nДоказательство конструктивно. Сначала докажем, что ряд \\sum_{k=0}^{\\infty} F^k сходится.\nКак и в скалярном случае, имеем\n (I - F) \\sum_{k=0}^N F^k = (I - F^{N+1}) \\rightarrow I, \\quad N \\to +\\infty \nДействительно,\n \\| (I - F^{N+1}) - I\\| = \\|F^{N+1}\\| \\leqslant \\|F\\|^{N+1} \\to 0, \\quad N\\to +\\infty. \nМы также можем оценить норму обратной матрицы:\n \\left\\Vert \\sum_{k=0}^N F^k \\right\\Vert \\leq \\sum_{k=0}^N \\Vert F \\Vert^k \\Vert I \\Vert \\leq \\frac{\\Vert I \\Vert}{1 - \\Vert F \\Vert}"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#малое-возмущение-обратной-матрицы",
    "href": "lectures/lecture-4/lecture-4.html#малое-возмущение-обратной-матрицы",
    "title": "Итоги",
    "section": "Малое возмущение обратной матрицы",
    "text": "Малое возмущение обратной матрицы\n\nИспользуя этот результат, мы можем оценить, как возмущение матрицы влияет на обратную матрицу.\nПредположим, что возмущение E мало в том смысле, что \\Vert A^{-1} E \\Vert &lt; 1.\nТогда\n\n(A + E)^{-1} = \\sum_{k=0}^{\\infty} (-A^{-1} E)^k A^{-1}\nи более того,\n \\frac{\\Vert (A + E)^{-1} - A^{-1} \\Vert}{\\Vert A^{-1} \\Vert} \\leq \\frac{\\Vert A^{-1} \\Vert \\Vert E \\Vert \\Vert I \\Vert}{1 - \\Vert A^{-1} E \\Vert}. \nКак видите, норма обратной матрицы входит в оценку."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#число-обусловленности-линейной-системы",
    "href": "lectures/lecture-4/lecture-4.html#число-обусловленности-линейной-системы",
    "title": "Итоги",
    "section": "Число обусловленности линейной системы",
    "text": "Число обусловленности линейной системы\nРассмотрим возмущенную линейную систему:\n (A + \\Delta A) \\widehat{x} = f + \\Delta f."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#оценки",
    "href": "lectures/lecture-4/lecture-4.html#оценки",
    "title": "Итоги",
    "section": "Оценки",
    "text": "Оценки\n\n\\begin{split}\n\\widehat{x} - x &= (A + \\Delta A)^{-1} (f + \\Delta f) - A^{-1} f =\\\\\n&= \\left((A + \\Delta A)^{-1} - A^{-1}\\right)f + (A + \\Delta A)^{-1} \\Delta f = \\\\\n& = \\Big[ \\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k - I \\Big]A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k \\Big]A^{-1} \\Delta f  \\\\\n&= \\Big[\\sum_{k=1}^{\\infty} (-A^{-1} \\Delta A)^k\\Big] A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k \\Big] A^{-1} \\Delta f,\n\\end{split}\n\nследовательно\n\n\\begin{split}\n\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq\n&\\frac{1}{\\|A^{-1}f\\|} \\Big[ \\frac{\\|A^{-1}\\|\\|\\Delta A\\|}{1 - \\|A^{-1}\\Delta A\\|}\\|A^{-1}f\\| + \\frac{1}{1 - \\|A^{-1} \\Delta A\\|} \\|A^{-1} \\Delta f\\|  \\Big] \\\\\n\\leq & \\frac{\\|A\\|\\|A^{-1}\\|}{1 - \\|A^{-1}\\Delta A\\|} \\frac{\\|\\Delta A\\|}{\\|A\\|} + \\frac{\\|A^{-1}\\|}{1 - \\|A^{-1}\\Delta A\\|} \\frac{\\|\\Delta f\\|}{\\|A^{-1}f\\|}\\\\\n\\end{split}\n\nЗаметим, что \\|AA^{-1}f\\| \\leq \\|A\\|\\|A^{-1}f\\|, поэтому \\| A^{-1} f \\| \\geq \\frac{\\|f\\|}{\\|A\\|}\nТеперь мы готовы получить окончательную оценку\n\n\\begin{split}\n\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq\n&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A^{-1}\\Delta A\\|} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) \\leq \\\\\n\\leq\n&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A\\|\\|A^{-1}\\|\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) = \\\\\n= &\\frac{\\mathrm{cond}(A)}{1 - \\mathrm{cond}(A)\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big)\n\\end{split}\n\nКлючевую роль играет число обусловленности \\mathrm{cond}(A) = \\Vert A \\Vert \\Vert A^{-1} \\Vert."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#число-обусловленности",
    "href": "lectures/lecture-4/lecture-4.html#число-обусловленности",
    "title": "Итоги",
    "section": "Число обусловленности",
    "text": "Число обусловленности\n\nЧем больше число обусловленности, тем меньше цифр мы можем восстановить. Заметим, что число обусловленности различно для разных норм.\nЗаметим, что если \\Delta A = 0, то\n\n \\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq \\mathrm{cond}(A) \\frac{\\|\\Delta f\\|}{\\|f\\|} \n\nСпектральная норма матрицы равна наибольшему сингулярному числу, а сингулярные числа обратной матрицы равны обратным значениям сингулярных чисел.\nТаким образом, число обусловленности в спектральной норме равно отношению наибольшего сингулярного числа к наименьшему.\n\n \\mathrm{cond}_2 (A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#матрица-гильберта-снова",
    "href": "lectures/lecture-4/lecture-4.html#матрица-гильберта-снова",
    "title": "Итоги",
    "section": "Матрица Гильберта (снова)",
    "text": "Матрица Гильберта (снова)\n\nМы также можем попробовать проверить, насколько точна оценка как с единицами в правой части, так и со случайным вектором в правой части.\nРезультаты разительно отличаются\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn = 30\na = [[1.0/(i + j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\n#rhs = np.random.normal(size=n)\nrhs = np.ones(n) #Right-hand side\n#rhs = np.random.randn(n)\n#rhs = np.arange(n)\n#rhs = (-1)**rhs\nf = np.linalg.solve(a, rhs)\n\n#And check if everything is fine\ner = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\ncn = np.linalg.cond(a, 2)\nprint('Error:', er, 'Log Condition number:', np.log10(cn))\n\nu1, s1, v1 = np.linalg.svd(a)\ncf = u1.T@rhs\nplt.plot(u1[:, 20])\n#cf\n#cf/s1\n\nError: 2.2843047554427222e-08 Log Condition number: 19.020186116767867\n\n\n\n\n\n\n\n\n\nА теперь со случайной правой частью…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nn = 100\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\n#rhs = np.random.randn(n) #Right-hand side\n#f = np.linalg.solve(a, rhs)\n\n#And check if everything is fine\n#er = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\n#cn = np.linalg.cond(a)\n#print('Error:', er, 'Condition number:', cn)\n\nu, s, v = np.linalg.svd(a)\n#rhs = np.random.randn(n)\nrhs = np.ones((n,))\nplt.plot(u[:, 0])\nplt.grid(True)\nplt.xlabel(\"Index of vector elements\", fontsize=20)\nplt.ylabel(\"Elements of vector\", fontsize=20)\nplt.xticks(fontsize=18)\n_ = plt.yticks(fontsize=18)\n\n\n\n\n\n\n\n\nМожете ли вы объяснить это?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#переопределенные-линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#переопределенные-линейные-системы",
    "title": "Итоги",
    "section": "Переопределенные линейные системы",
    "text": "Переопределенные линейные системы\n\nВажный класс задач - это переопределенные линейные системы, когда количество уравнений больше, чем количество неизвестных.\nПростейший пример, который вы все знаете - это линейная аппроксимация, аппроксимация набора 2D точек прямой линией.\nТогда типичный способ - это минимизировать невязку (метод наименьших квадратов)\n\n\\Vert A x - b \\Vert_2 \\rightarrow \\min"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#переопределенная-система-и-матрица-грама",
    "href": "lectures/lecture-4/lecture-4.html#переопределенная-система-и-матрица-грама",
    "title": "Итоги",
    "section": "Переопределенная система и матрица Грама",
    "text": "Переопределенная система и матрица Грама\nУсловие оптимальности - это 0\\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right), где \\nabla обозначает градиент. Следовательно,\n 0 \\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right) = 2(A^*A x - A^*b) = 0. \nТаким образом,\n \\quad A^* A x = A^* b \n\nМатрица A^* A называется матрицей Грама, а система называется нормальным уравнением.\nЭто не лучший способ решения, так как число обусловленности A^* A является квадратом числа обусловленности A (проверьте почему)."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#псевдообратная-матрица",
    "href": "lectures/lecture-4/lecture-4.html#псевдообратная-матрица",
    "title": "Итоги",
    "section": "Псевдообратная матрица",
    "text": "Псевдообратная матрица\n\nМатрица A^* A может быть вырожденной в общем случае.\nПоэтому нам нужно ввести понятие псевдообратной матрицы A^{\\dagger} такой, что решение задачи наименьших квадратов может быть формально записано как\n\nx = A^{\\dagger} b.\n\nМатрица A^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* называется псевдообратной матрицей Мура-Пенроуза для матрицы A.\nЕсли матрица A имеет полный ранг по столбцам, то A^* A невырождена и мы получаем\n\nA^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^*. \n\nЕсли матрица A квадратная и невырожденная, мы получаем стандартную обратную матрицу A:\n\nA^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^* = A^{-1} A^{-*} A^* = A^{-1}\n\nЕсли A имеет линейно зависимые столбцы, то A^\\dagger b дает решение, которое имеет минимальную евклидову норму"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#вычисление-псевдообратной-матрицы-через-svd",
    "href": "lectures/lecture-4/lecture-4.html#вычисление-псевдообратной-матрицы-через-svd",
    "title": "Итоги",
    "section": "Вычисление псевдообратной матрицы через SVD",
    "text": "Вычисление псевдообратной матрицы через SVD\nПусть A = U \\Sigma V^* - SVD разложение матрицы A. Тогда,\nA^{\\dagger} = V \\Sigma^{\\dagger} U^*,\nгде \\Sigma^{\\dagger} состоит из обратных значений ненулевых сингулярных чисел матрицы A. Действительно,\n\\begin{align*}\nA^{\\dagger} &= \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = \\lim_{\\alpha \\rightarrow 0}( \\alpha VV^* + V \\Sigma^2 V^*)^{-1} V \\Sigma U^* \\\\ & = \\lim_{\\alpha \\rightarrow 0}( V(\\alpha I + \\Sigma^2) V^*)^{-1} V \\Sigma U^* = V \\lim_{\\alpha \\rightarrow 0}(\\alpha I + \\Sigma^2)^{-1} \\Sigma U^* = V \\Sigma^{\\dagger} U^*.\n\\end{align*}\n\nМожно проверить, что \\Sigma^{\\dagger} содержит только обратные значения ненулевых сингулярных чисел.\nЕсли сингулярные числа малы, можно пропустить их обращение. Это приведет к решению, которое менее чувствительно к шуму в правой части.\nЧисло обусловленности для евклидовой нормы по-прежнему равно отношению наибольшего и наименьшего ненулевого сингулярного числа."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#канонический-способ-решения-задачи-наименьших-квадратов",
    "href": "lectures/lecture-4/lecture-4.html#канонический-способ-решения-задачи-наименьших-квадратов",
    "title": "Итоги",
    "section": "Канонический способ решения задачи наименьших квадратов",
    "text": "Канонический способ решения задачи наименьших квадратов\nЗаключается в использовании QR разложения.\n\nЛюбая матрица может быть представлена в виде произведения\n\n A = Q R, \nгде Q - унитарная матрица, а R - верхнетреугольная матрица (подробности в следующих лекциях).\n\nТогда, если A имеет полный ранг по столбцам, то\n\n x = A^{\\dagger}b = (A^*A)^{-1}A^*b = ((QR)^*(QR))^{-1}(QR)^*b = (R^*Q^*QR)^{-1}R^*Q^*b = R^{-1}Q^*b.  \n\nТаким образом, нахождение оптимального x эквивалентно решению\n\n Rx = Q^* b. \n\nПоскольку R верхнетреугольная, решение этой линейной системы требует \\mathcal{O}(n^2) операций.\nТакже это более устойчиво, чем использование псевдообратной матрицы напрямую."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#расширение-системы-до-большей-размерности",
    "href": "lectures/lecture-4/lecture-4.html#расширение-системы-до-большей-размерности",
    "title": "Итоги",
    "section": "Расширение системы до большей размерности",
    "text": "Расширение системы до большей размерности\n\nВместо решения A^* A x = A^* b, введем новую переменную r = Ax - b и получим\n\nA^* r = 0, \\quad r = Ax - b,\nили в блочной форме\n \\begin{pmatrix} 0 & A^* \\\\ A & -I \\end{pmatrix} \\begin{pmatrix} x \\\\ r \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ b \\end{pmatrix}, \nобщий размер системы равен (n + m), а число обусловленности совпадает с числом обусловленности для A - Как определить число обусловленности для прямоугольной матрицы?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#пример-мнк",
    "href": "lectures/lecture-4/lecture-4.html#пример-мнк",
    "title": "Итоги",
    "section": "Пример МНК",
    "text": "Пример МНК\nРассмотрим двумерный пример. Предположим, что у нас есть линейная модель\ny = ax + b\nи зашумленные данные (x_1, y_1), \\dots (x_n, y_n). Тогда линейная система для коэффициентов будет выглядеть следующим образом\n\n\\begin{split}\na x_1 &+ b &= y_1 \\\\\n&\\vdots \\\\\na x_n &+ b &= y_n \\\\\n\\end{split}\n\nили в матричной форме\n\n\\begin{pmatrix}\nx_1 & 1 \\\\\n\\vdots & \\vdots \\\\\nx_n & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na \\\\\nb\n\\end{pmatrix} =\n\\begin{pmatrix}\ny_1 \\\\\n\\vdots  \\\\\ny_n \\\\\n\\end{pmatrix},\n\nчто представляет собой переопределенную систему.\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na_exact = 1.\nb_exact = 2.\n\nn = 10\nxi = np.arange(n)\nyi = a_exact * xi + b_exact + 2 * np.random.normal(size=n)\n\nA = np.array([xi, np.ones(n)])\ncoef = np.linalg.pinv(A).T.dot(yi) # coef is [a, b]\n\nplt.plot(xi, yi, 'o', label='$(x_i, y_i)$')\nplt.plot(xi, coef[0]*xi + coef[1], label='Least squares')\nplt.legend(loc='best', fontsize=18)\nplt.grid(True)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#следующая-лекция",
    "href": "lectures/lecture-4/lecture-4.html#следующая-лекция",
    "title": "Итоги",
    "section": "Следующая лекция",
    "text": "Следующая лекция\n\nСобственные векторы и собственные значения\nТеорема Шура\n\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#previous-lecture",
    "href": "lectures/lecture-15/lecture-15.html#previous-lecture",
    "title": "",
    "section": "Previous lecture",
    "text": "Previous lecture\n\nKrylov methods: Arnoldi relation, CG, GMRES\nPreconditioners\n\nJacobi\nGauss-Seidel\nSSOR\nILU and its modifications"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#other-structured-matrices",
    "href": "lectures/lecture-15/lecture-15.html#other-structured-matrices",
    "title": "",
    "section": "Other structured matrices",
    "text": "Other structured matrices\n\nUp to now, we discussed preconditioning only for sparse matrices\nBut iterative methods work well for any matrices that have fast black-box matrix-by-vector product\nImportant class of such matrices are Toeplitz matrices (and Hankel matrices) and their multilevel variants\n\nThey are directly connected to the convolution operation and Fast Fourier Transform."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolution",
    "href": "lectures/lecture-15/lecture-15.html#convolution",
    "title": "",
    "section": "Convolution",
    "text": "Convolution\n\nOne of the key operation in signal processing/machine learning is the convolution of two functions.\nLet x(t) and y(t) be two given functions. Their convolution is defined as\n\n(x * y)(t) = \\int_{-\\infty}^{\\infty} x(\\tau) y(t -  \\tau) d \\tau."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolution-theorem-and-fourier-transform",
    "href": "lectures/lecture-15/lecture-15.html#convolution-theorem-and-fourier-transform",
    "title": "",
    "section": "Convolution theorem and Fourier transform",
    "text": "Convolution theorem and Fourier transform\nA well-known fact: a convolution in the time domain is a product in the frequency domain.\n\nTime-frequency transformation is given by the Fourier transform:\n\n\\widehat{x}(w) = (\\mathcal{F}(x))(w) = \\int_{-\\infty}^{\\infty} e^{i w t} x(t) dt.\n\nThen,\n\n\\mathcal{F}(x * y) = \\mathcal{F}(x) \\mathcal{F}(y).\n\nThus, the “algorithm” for the computation of the convolution can be:\n\n\nCompute Fourier transform of x(t) and y(t).\nCompute their product\nCompute inverse Fourier transform"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#discrete-convolution-operation",
    "href": "lectures/lecture-15/lecture-15.html#discrete-convolution-operation",
    "title": "",
    "section": "Discrete convolution operation",
    "text": "Discrete convolution operation\n(x * y)(t) = \\int_{-\\infty}^{\\infty} x(\\tau) y(t -  \\tau) d \\tau.\nLet us approximate the integral by a quadrature sum on a uniform grid, and store the signal at equidistant points.\nThen we are left with the summation\nz_i = \\sum_{j=0}^{n-1} x_j y_{i - j},\nwhich is called discrete convolution. This can be thought as an application of a filter with coefficients x to a signal y.\nThere are different possible filters for different purposes, but they all utilize the shift-invariant structure."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#discrete-convolution-and-toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#discrete-convolution-and-toeplitz-matrices",
    "title": "",
    "section": "Discrete convolution and Toeplitz matrices",
    "text": "Discrete convolution and Toeplitz matrices\nA discrete convolution can be thought as a matrix-by-vector product:\nz_i = \\sum_{j=0}^{n-1} x_j y_{i - j}, \\Leftrightarrow z = Ax\nwhere the matrix A elements are given as a_{ij} = y_{i-j}, i.e., they depend only on the difference between the row index and the column index."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-matrices-definition",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-matrices-definition",
    "title": "",
    "section": "Toeplitz matrices: definition",
    "text": "Toeplitz matrices: definition\nA matrix is called Toeplitz if its elements are defined as\na_{ij} = t_{i - j}.\n\nA Toeplitz matrix is completely defined by its first column and first row (i.e., 2n-1 parameters).\nIt is a dense matrix, however it is a structured matrix (i.e., defined by \\mathcal{O}(n) parameters).\nAnd the main operation in the discrete convolution is the product of Toeplitz matrix by vector.\nCan we compute it faster than \\mathcal{O}(n^2)?"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-and-circulant-matrix",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-and-circulant-matrix",
    "title": "",
    "section": "Toeplitz and circulant matrix",
    "text": "Toeplitz and circulant matrix\n\nFor a special class of Toeplitz matrices, named circulant matrices the fast matrix-by-vector product can be done.\nA matrix C is called circulant, if\n\nC_{ij} = c_{i - j \\mod n},\ni.e. it periodicaly wraps\nC = \\begin{bmatrix}\nc_0 & c_3 & c_2 & c_1 \\\\\nc_1 & c_0 & c_3 & c_2 \\\\\nc_2 & c_1 & c_0 & c_3 \\\\\nc_3 & c_2 & c_1 & c_0 \\\\\n\\end{bmatrix}.\n\n\nThese matrices have the same eigenvectors, given by the Discrete Fourier Transform (DFT)."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#spectral-theorem-for-circulant-matrices",
    "href": "lectures/lecture-15/lecture-15.html#spectral-theorem-for-circulant-matrices",
    "title": "",
    "section": "Spectral theorem for circulant matrices",
    "text": "Spectral theorem for circulant matrices\nTheorem:\nAny circulant matrix can be represented in the form\nC = \\frac{1}{n} F^* \\Lambda F,\nwhere F is the Fourier matrix with the elements\nF_{kl} = w_n^{kl}, \\quad k, l = 0, \\ldots, n-1, \\quad w_n = e^{-\\frac{2 \\pi i}{n}},\nand matrix \\Lambda = \\text{diag}(\\lambda) is the diagonal matrix and\n\\lambda = F c, \nwhere c is the first column of the circulant matrix C.\nThe proof will be later: now we need to study the FFT matrix."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fourier-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fourier-matrix",
    "title": "",
    "section": "Fourier matrix",
    "text": "Fourier matrix\nThe Fourier matrix is defined as:\n\nF_n =\n\\begin{pmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & w^{1\\cdot 1}_n & w^{1\\cdot 2}_n & \\dots & w^{1\\cdot (n-1)}_n\\\\\n1 & w^{2\\cdot 1}_n & w^{2\\cdot 2}_n & \\dots & w^{2\\cdot (n-1)}_n\\\\\n\\dots & \\dots & \\dots &\\dots &\\dots \\\\\n1 & w^{(n-1)\\cdot 1}_n & w^{(n-1)\\cdot 2}_n & \\dots & w^{(n-1)\\cdot (n-1)}_n\\\\\n\\end{pmatrix},\n\nor equivalently\n F_n = \\{ w_n^{kl} \\}_{k,l=0}^{n-1}, \nwhere\nw_n = e^{-\\frac{2\\pi i}{n}}.\nProperties: * Symmetric (not Hermitian!) * Unitary up to a scaling factor: F_n^* F_n = F_n F_n^* = nI (check this fact). Therefore F_n^{-1} = \\frac{1}{n}F^*_n * Can be multiplied by a vector (called discrete Fourier transform or DFT) with \\mathcal{O}(n \\log n) complexity (called fast Fourier transform or FFT)! FFT helps to analyze spectrum of a signal and, as we will see later, helps to do fast mutiplications with certain types of matrices.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nN = 1000\ndt = 1.0 / 800.0\nx = np.linspace(0.0, N*dt, N)\ny = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x) + 0.2*np.sin(300.0 * 2.0*np.pi*x)\nplt.plot(x, y)\nplt.xlabel('Time')\nplt.ylabel('Signal')\nplt.title('Initial signal')\n\nText(0.5, 1.0, 'Initial signal')\n\n\n\n\n\n\n\n\n\n\nyf = np.fft.fft(y)\nxf = np.linspace(0.0, 1.0/(2.0*dt), N//2)\nplt.plot(xf, 2.0/N * np.abs(yf[0:N//2])) #Note: N/2 to N will give negative frequencies\nplt.xlabel('Frequency')\nplt.ylabel('Amplitude')\nplt.title('Discrete Fourier transform')\n\nText(0.5, 1.0, 'Discrete Fourier transform')"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-fourier-transform-fft",
    "href": "lectures/lecture-15/lecture-15.html#fast-fourier-transform-fft",
    "title": "",
    "section": "Fast Fourier transform (FFT)",
    "text": "Fast Fourier transform (FFT)\nHere we consider a matrix interpretation of the standard Cooley-Tukey algorithm (1965), which has underlying divide and conquer idea. Note that in packages more advanced versions are used.\n\nLet n be a power of 2.\nFirst of all we  permute the rows  of the Fourier matrix such that the first n/2 rows of the new matrix had row numbers 1,3,5,\\dots,n-1 and the last n/2 rows had row numbers 2,4,6\\dots,n.\nThis permutation can be expressed in terms of multiplication by permutation matrix P_n:\n\n\nP_n =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 0 & 1 & 0 &\\dots & 0 & 0 \\\\\n\\vdots & & & & & & \\vdots \\\\\n0 & 0 & 0 & 0 &\\dots & 1 & 0 \\\\\n\\hline\n0 & 1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 0 & 0 & 1 &\\dots & 0 & 0 \\\\\n\\vdots & & & & & & \\vdots \\\\\n0 & 0 & 0 & 0 &\\dots & 0 & 1\n\\end{pmatrix},\n\nHence,\n\nP_n F_n =\n\\begin{pmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & w^{2\\cdot 1}_n & w^{2\\cdot 2}_n & \\dots & w^{2\\cdot (n-1)}_n\\\\\n1 & w^{4\\cdot 1}_n & w^{4\\cdot 2}_n & \\dots & w^{4\\cdot (n-1)}_n\\\\\n\\vdots & & & & \\vdots\\\\\n1 & w^{(n-2)\\cdot 1}_n & w^{(n-2)\\cdot 2}_n & \\dots & w^{(n-2)\\cdot (n-1)}_n\\\\\n\\hline\n1 & w^{1\\cdot 1}_n & w^{1\\cdot 2}_n & \\dots & w^{1\\cdot (n-1)}_n\\\\\n1 & w^{3\\cdot 1}_n & w^{3\\cdot 2}_n & \\dots & w^{3\\cdot (n-1)}_n\\\\           \n\\vdots & & & & \\vdots\\\\\n1 & w^{(n-1)\\cdot 1}_n & w^{(n-1)\\cdot 2}_n & \\dots & w^{(n-1)\\cdot (n-1)}_n\\\\\n\\end{pmatrix},\n\nNow let us imagine that we separated its columns and rows by two parts each of size n/2.\nAs a result we get 2\\times 2 block matrix that has the following form\n\nP_n F_n =\n\\begin{pmatrix}\n\\left\\{w^{2kl}_n\\right\\} & \\left\\{w_n^{2k\\left(\\frac{n}{2} + l\\right)}\\right\\} \\\\\n\\left\\{w_n^{(2k+1)l}\\right\\} & \\left\\{w_n^{(2k+1)\\left(\\frac{n}{2} + l\\right)}\\right\\}\n\\end{pmatrix},\n\\quad k,l = 0,\\dots, \\frac{n}{2}-1.\n\nSo far it does not look like something that works faster :) But we will see that in a minute. Lets have a more precise look at the first block \\left\\{w^{2kl}_n\\right\\}:\n\nw^{2kl}_n = e^{-2kl\\frac{2\\pi i}{n}} = e^{-kl\\frac{2\\pi i}{n/2}} = w^{kl}_{n/2}.\n\nSo this block is exactly twice smaller Fourier matrix F_{n/2}!\n\nThe block \\left\\{w_n^{(2k+1)l}\\right\\} can be written as\n\nw_n^{(2k+1)l} = w_n^{2kl + l} = w_n^{l} w_n^{2kl} = w_n^{l} w_{n/2}^{kl},\n\nwhich can be written as W_{n/2}F_{n/2}, where\nW_{n/2} = \\text{diag}(1,w_n,w_n^2,\\dots,w_n^{n/2-1}).\nDoing the same tricks for the other blocks we will finally get\n\nP_n F_n =\n\\begin{pmatrix}\nF_{n/2} & F_{n/2} \\\\\nF_{n/2}W_{n/2} & -F_{n/2}W_{n/2}\n\\end{pmatrix} =\n\\begin{pmatrix}\nF_{n/2} & 0 \\\\\n0 & F_{n/2}\n\\end{pmatrix}\n\\begin{pmatrix}\nI_{n/2} & I_{n/2} \\\\\nW_{n/2} & -W_{n/2}\n\\end{pmatrix}.\n\n\nThus, we reduced multiplication by F_n to 2 multiplications by F_{n/2} and cheap multiplications by diagonal matrices.\nIf we apply the obtained expressions recursively to F_{n/2}, we will get \\mathcal{O}(n\\log n)  complexity.\n\n\n#FFT vs full matvec\nimport time\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg\n\nn = 10000\nF = sp.linalg.dft(n)\nx = np.random.randn(n)\n\ny_full = F.dot(x)\n\nfull_mv_time = %timeit -q -o F.dot(x)\nprint('Full matvec time =', full_mv_time.average)\n\ny_fft = np.fft.fft(x)\nfft_mv_time = %timeit -q -o np.fft.fft(x)\nprint('FFT time =', fft_mv_time.average)\n\nprint('Relative error =', (np.linalg.norm(y_full - y_fft)) / np.linalg.norm(y_full))\n\nFull matvec time = 0.016554963692857142\nFFT time = 6.18095107428571e-05\nRelative error = 1.5329028805883414e-12"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#circulant-matrices",
    "href": "lectures/lecture-15/lecture-15.html#circulant-matrices",
    "title": "",
    "section": "Circulant matrices",
    "text": "Circulant matrices\nFFT helps to multiply fast by certain types of matrices. We start from a circulant matrix:\n\nC =\n\\begin{pmatrix}\nc_0 & c_{n-1} & c_{n-2} & \\dots & c_1 \\\\\nc_{1} & c_{0} & c_{n-1} & \\dots & c_2 \\\\\nc_{2} & c_{1} & c_0 & \\dots & c_3 \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\nc_{n-1} & c_{n-2} & c_{n-3} & \\dots & c_0\n\\end{pmatrix}\n\nTheorem. Let C be a circulant matrix of size n\\times n and let c be it’s first column , then\n\nC = \\frac{1}{n} F_n^* \\text{diag}(F_n c) F_n\n\nProof. - Consider a number\n\\lambda (\\omega) = c_0 + \\omega c_1 + \\dots + \\omega^{n-1} c_{n-1},\nwhere \\omega is any number such that \\omega^n=1. - Lets multiply \\lambda by 1,\\omega,\\dots, \\omega^{n-1}:\n\n\\begin{split}\n\\lambda & = c_0 &+& \\omega c_1 &+& \\dots &+& \\omega^{n-1} c_{n-1},\\\\\n\\lambda\\omega & = c_{n-1} &+& \\omega c_0 &+& \\dots &+& \\omega^{n-1} c_{n-2},\\\\\n\\lambda\\omega^2 & = c_{n-2} &+& \\omega c_{n-1} &+& \\dots &+& \\omega^{n-1} c_{n-3},\\\\\n&\\dots\\\\\n\\lambda\\omega^{n-1} & = c_{1} &+& \\omega c_{2} &+& \\dots &+& \\omega^{n-1} c_{0}.\n\\end{split}\n\n\nTherefore,\n\n\n\\lambda(\\omega) \\cdot \\begin{pmatrix} 1&\\omega & \\dots& \\omega^{n-1} \\end{pmatrix} =\n\\begin{pmatrix} 1&\\omega&\\dots& \\omega^{n-1} \\end{pmatrix} \\cdot C.\n\n\nWriting this for \\omega = 1,w_n, \\dots, w_n^{n-1} we get\n\n\n\\Lambda F_n = F_n C\n\nand finally\n\nC = \\frac{1}{n} F^*_n \\Lambda F_n, \\quad \\text{where}\\quad \\Lambda = \\text{diag}(F_nc) \\qquad\\blacksquare"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-circulant-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-circulant-matrix",
    "title": "",
    "section": "Fast matvec with circulant matrix",
    "text": "Fast matvec with circulant matrix\n\nRepresentation $C = F^* (F_n c) F_n $ gives us an explicit way to multiply a vector x by C in \\mathcal{O}(n\\log n) operations.\nIndeed,\n\n\nCx = \\frac{1}{n} F_n^* \\text{diag}(F_n c) F_n x = \\text{ifft}\\left( \\text{fft}(c) \\circ \\text{fft}(x)\\right)\n\nwhere \\circ denotes elementwise product (Hadamard product) of two vectors (since \\text{diag}(a)b = a\\circ b) and ifft denotes inverse Fourier transform F^{-1}_n.\n\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg\n\ndef circulant_matvec(c, x):\n    return np.fft.ifft(np.fft.fft(c) * np.fft.fft(x))\n\nn = 5000\nc = np.random.random(n)\nC = sp.linalg.circulant(c)\nx = np.random.randn(n)\n\n\ny_full = C.dot(x)\nfull_mv_time = %timeit -q -o C.dot(x)\nprint('Full matvec time =', full_mv_time.average)\n\n\ny_fft = circulant_matvec(c, x)\nfft_mv_time = %timeit -q -o circulant_matvec(c, x)\nprint('FFT time =', fft_mv_time.average)\n\nprint('Relative error =', (np.linalg.norm(y_full - y_fft)) / np.linalg.norm(y_full))\n\nFull matvec time = 0.003428939464285707\nFFT time = 0.0001001016214142856\nRelative error = 1.307050346126901e-15"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-matrices",
    "title": "",
    "section": "Toeplitz matrices",
    "text": "Toeplitz matrices\nNow we get back to Toeplitz matrices!\n\nT =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{-3}& \\dots & t_{1-n} \\\\\nt_{1} & t_{0} & t_{-1} & t_{-2}& \\dots & t_{2-n} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} &\\dots & t_{3-n} \\\\\nt_{3} & t_{2} & t_1 & t_0 & \\dots & t_{4-n} \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\\\\\nt_{n-1} & t_{n-2} & t_{n-3} & t_{n-4} &\\dots &t_0\n\\end{pmatrix},\n\nor equivalently T_{ij} = t_{i-j}.\nMatvec operation can be written as\n\ny_i = \\sum_{j=1}^n t_{i-j} x_j,\n\nwhich can be interpreted as a discrete convolution of filter t_i and signal x_i. For simplicity the size of the filter t is such that the sizes of the input and output signals are the same. Generally, filter size can be arbitrary.\nFast convolution computation has a variety of applications, for instance, in signal processing or partial differential and integral equations. For instance, here is the smoothing of a signal:\n\nfrom scipy import signal\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nalpha = 0.01\nsig = np.repeat([0., 1., 0.], 100)\nfilt = np.exp(-alpha * (np.arange(100)-50)**2)\nfiltered = signal.convolve(sig, filt, mode='same') / sum(filt)\n\nfig, (ax_orig, ax_filt, ax_filtered) = plt.subplots(3, 1, sharex=True)\nax_orig.plot(sig)\nax_orig.margins(0, 0.1)\nax_filt.plot(filt)\nax_filt.margins(0, 0.1)\nax_filtered.plot(filtered)\nax_filtered.margins(0, 0.1)\n\nax_orig.set_title('Original signal')\nax_filt.set_title('Filter')\nax_filtered.set_title('Convolution')\n\nfig.tight_layout()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-toeplitz-matrix",
    "title": "",
    "section": "Fast matvec with Toeplitz matrix",
    "text": "Fast matvec with Toeplitz matrix\nKey point: the multiplication by a Toeplitz matrix can be reduced to the multiplication by a circulant.\n\nIndeed, every Toeplitz matrix of size n\\times n can be embedded into a Circulant matrix C of size (2n - 1) \\times (2n - 1):\n\n\nC =\n\\begin{pmatrix}\nT & \\dots \\\\\n\\dots & \\dots\n\\end{pmatrix}.\n\n\nThe 3\\times 3 matrix T = \\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} \\\\\nt_{1} & t_{0} & t_{-1} \\\\\nt_{2} & t_{1} & t_0 \\\\\n\\end{pmatrix} can be embedded as follows\n\n\nC =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{2} & t_{1}\\\\\nt_{1} & t_{0} & t_{-1} & t_{-2} & t_{2} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} & t_{-2} \\\\\nt_{-2}& t_{2} & t_{1} & t_0 & t_{-1}  \\\\\nt_{-1} & t_{-2} & t_{2} & t_{1} & t_0  \n\\end{pmatrix}.\n\n\nFor matvec $\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\n=\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} \\\\\nt_{1} & t_{0} & t_{-1} \\\\\nt_{2} & t_{1} & t_0 \\\\\n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n$ we pad vector x with zeros:\n\n\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\star \\\\ \\star \\end{pmatrix} =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{2} & t_{1}\\\\\nt_{1} & t_{0} & t_{-1} & t_{-2} & t_{2} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} & t_{-2} \\\\\nt_{-2}& t_{2} & t_{1} & t_0 & t_{-1}  \\\\\nt_{-1} & t_{-2} & t_{2} & t_{1} & t_0  \n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix}=\n\\text{ifft}(\\text{fft}(\\begin{pmatrix} t_0 \\\\ t_{1} \\\\ t_{2} \\\\ t_{-2} \\\\ t_{-1} \\end{pmatrix})\\circ \\text{fft}(\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix})).\n\n\nNote that you do not need to form and store the whole matrix T\nFrom the Cooley-Tukey algorithm follows that the preferable size of circulant matrix is 2^k for some k. You can do it with zero padding of the appropriate size."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#multilevel-toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#multilevel-toeplitz-matrices",
    "title": "",
    "section": "Multilevel Toeplitz matrices",
    "text": "Multilevel Toeplitz matrices\nThe 2-dimensional convolution is defined as\n\ny_{i_1i_2} = \\sum_{j_1,j_2=1}^n t_{i_1-j_1, i_2-j_2} x_{j_1 j_2}.\n\nNote that x and y are 2-dimensional arrays and T is 4-dimensional. To reduce this expression to matrix-by-vector product we have to reshape x and y into long vectors:\n\n\\text{vec}(x) =\n\\begin{pmatrix}\nx_{11} \\\\ \\vdots \\\\ x_{1n} \\\\ \\hline \\\\ \\vdots \\\\ \\hline \\\\ x_{n1} \\\\ \\vdots \\\\ x_{nn}\n\\end{pmatrix},\n\\quad\n\\text{vec}(y) =\n\\begin{pmatrix}\ny_{11} \\\\ \\vdots \\\\ y_{1n} \\\\ \\hline \\\\ \\vdots \\\\ \\hline \\\\ y_{n1} \\\\ \\vdots \\\\ y_{nn}\n\\end{pmatrix}.\n\nIn this case matrix T is block Toeplitz with Toeplitz blocks: (BTTB)\n\nT =\n\\begin{pmatrix}\nT_0 & T_{-1} & T_{-2} &  \\dots & T_{1-n} \\\\\nT_{1} & T_{0} & T_{-1} & \\dots & T_{2-n} \\\\\nT_{2} & T_{1} & T_0 & \\dots & T_{3-n} \\\\\n\\dots & \\dots & \\dots &  \\dots & \\dots\\\\\nT_{n-1} & T_{n-2} & T_{n-3}  &\\dots &T_0\n\\end{pmatrix},\n\\quad \\text{where} \\quad\nT_k = t_{k, i_2 - j_2}\\quad  \\text{are Toeplitz matrices}"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-multilevel-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-multilevel-toeplitz-matrix",
    "title": "",
    "section": "Fast matvec with multilevel Toeplitz matrix",
    "text": "Fast matvec with multilevel Toeplitz matrix\nTo get fast matvec we need to embed block Toeplitz matrix with Toeplitz blocks into the block circulant matrix with circulant blocks. The analog of \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\star \\\\ \\star \\end{pmatrix} =\n\\text{ifft}(\\text{fft}(\\begin{pmatrix} t_0 \\\\ t_{1} \\\\ t_{2} \\\\ t_{-2} \\\\ t_{-1} \\end{pmatrix})\\circ\\text{fft}(\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix})). will look like  \\begin{pmatrix} y_{11} & y_{12} & y_{13} & \\star & \\star \\\\\ny_{21} & y_{22} & y_{23} & \\star & \\star \\\\  \ny_{31} & y_{32} & y_{33} & \\star & \\star \\\\\n\\star & \\star & \\star & \\star & \\star \\\\  \n\\star & \\star & \\star & \\star & \\star \\\\  \n\\end{pmatrix} = \\text{ifft2d}(\\text{fft2d}(\\begin{pmatrix} t_{0,0} & t_{1,0} & t_{2,0} & t_{-2,0} & t_{-1,0} \\\\\nt_{0,1} & t_{1,1} & t_{2,1} & t_{-2,1} & t_{-1,1} \\\\  \nt_{0,2} & t_{1,2} & t_{2,2} & t_{-2,2} & t_{-1,2} \\\\\nt_{0,-2} & t_{1,-2} & t_{2,-2} & t_{-2,-2} & t_{-1,-2} \\\\\nt_{0,-1} & t_{1,-1} & t_{2,-1} & t_{-2,-1} & t_{-1,-1}\n\\end{pmatrix}) \\circ \\text{fft2d}(\\begin{pmatrix}x_{11} & x_{12} & x_{13} & 0 & 0 \\\\\nx_{21} & x_{22} & x_{23} & 0 & 0 \\\\  \nx_{31} & x_{32} & x_{33} & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\  \n0 & 0 & 0 & 0 & 0 \\\\  \n\\end{pmatrix})), where fft2d is 2-dimensional fft that consists of one-dimensional transforms, applied first to rows and and then to columns (or vice versa).\n\n# Blurring and Sharpening Lena by convolution\n\nfrom scipy import signal\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib  inline\nfrom scipy import misc\nimport imageio\n\nfilter_size = 5\nfilter_blur = np.ones((filter_size, filter_size)) / filter_size**2\nlena = imageio.imread('./lena512.jpg')\n#lena = misc.face()\n#lena = lena[:, :, 0]\nblurred = signal.convolve2d(lena, filter_blur, boundary='symm', mode='same')\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nax[0, 0].imshow(lena[200:300, 200:300], cmap='gray')\nax[0, 0].set_title('Original Lena')\nax[0, 1].imshow(blurred[200:300, 200:300], cmap='gray')\nax[0, 1].set_title('Blurred Lena')\nax[1, 0].imshow((lena - blurred)[200:300, 200:300], cmap='gray')\nax[1, 0].set_title('Lena $-$ Blurred Lena')\nax[1, 1].imshow(((lena - blurred)*3 + blurred)[200:300, 200:300], cmap='gray')\nax[1, 1].set_title('$3\\cdot$(Lena $-$ Blurred Lena) + Blurred Lena')\nfig.tight_layout()\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In [7], line 8\n      6 get_ipython().run_line_magic('matplotlib', ' inline')\n      7 from scipy import misc\n----&gt; 8 import imageio\n     10 filter_size = 5\n     11 filter_blur = np.ones((filter_size, filter_size)) / filter_size**2\n\nModuleNotFoundError: No module named 'imageio'"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#solving-linear-systems-with-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#solving-linear-systems-with-toeplitz-matrix",
    "title": "",
    "section": "Solving linear systems with Toeplitz matrix",
    "text": "Solving linear systems with Toeplitz matrix\n\nConvolution is ok; but what about deconvolution, or solving linear systems with Toeplitz matrices?\n\nT x = f.\n\nFor the periodic case, where T = C is circulant,\n\nwe have the spectral theorem\nC = \\frac{1}{n}F^* \\Lambda F, \\quad C^{-1} = \\frac{1}{n}F^* \\Lambda^{-1} F,\nbut for a general Toeplitz matrices, it is not a trivial question."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#iterative-methods",
    "href": "lectures/lecture-15/lecture-15.html#iterative-methods",
    "title": "",
    "section": "Iterative methods",
    "text": "Iterative methods\n\nNot-a-bad recipe for Toeplitz linear system is to use iterative method (fast matvec is available).\nA good choice for a preconditioner is a circulant matrix."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#circulant-preconditioner",
    "href": "lectures/lecture-15/lecture-15.html#circulant-preconditioner",
    "title": "",
    "section": "Circulant preconditioner",
    "text": "Circulant preconditioner\n\nA natural idea is to use circulants as preconditioners, since they are easy to invert.\nThe first preconditioner was the preconditioner by Raymond Chan and Gilbert Strang, who proposed to take the first column of the matrix and use it to generate the circulant.\nThe second preconditioner is the Tony Chan preconditioner, which is also very natural:\n\nC = \\arg \\min_P \\Vert P - T \\Vert_F.\n\nA simple formula for the entries of C can be derived.\n\n\nimport numpy as np\nimport scipy.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport scipy as sp\n\nn = 100\nc = np.zeros(n)\nc[0] = -2\nc[1] = 1\nTm = sp.linalg.toeplitz(c, c)\n\n\nc1 = sp.linalg.circulant(c) #Strang preconditioner\nFmat = 1.0/np.sqrt(n) * np.fft.fft(np.eye(n)) #Poor man's Fourier matrix\n\nd2 = np.diag(Fmat.conj().dot(Tm).dot(Fmat))\nc2 = Fmat.dot(np.diag(d2)).dot(Fmat.conj().T)\n\n\nmat = np.linalg.inv(c1).dot(Tm)\nev = np.linalg.eigvals(mat).real\nplt.plot(np.sort(ev), np.ones(n), 'o')\nplt.xlabel('Eigenvalues for Strang preconditioner')\nplt.gca().get_yaxis().set_visible(False)\n\nmat = np.linalg.inv(c2).dot(Tm)\nev = np.linalg.eigvals(mat).real\nplt.figure()\nplt.plot(np.sort(ev), np.ones(n), 'o')\nplt.xlabel('Eigenvalues for T. Chan Preconditioner')\nplt.gca().get_yaxis().set_visible(False)"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolutions-in-neural-networks",
    "href": "lectures/lecture-15/lecture-15.html#convolutions-in-neural-networks",
    "title": "",
    "section": "Convolutions in neural networks",
    "text": "Convolutions in neural networks\n\nThe revolution in deep learning and computer vision is related to using Convolutional Neural Networks (CNN)\nThe most famous examples are\n\nAlexNet, 2012\nGoogLeNet, 2014\nVGG, 2015\n\nFurther improvements are based on more advanced tricks like normalizations, skip connections and so on\nMore details will be presented in Deep learning/computer vision courses"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#what-is-convolution-in-nn",
    "href": "lectures/lecture-15/lecture-15.html#what-is-convolution-in-nn",
    "title": "",
    "section": "What is convolution in NN?",
    "text": "What is convolution in NN?\n\nIn neural networks the convolution means not convolution but cross-correlation!\n\n (x \\star y)(t) = \\int_{-\\infty}^{+\\infty} x(\\tau)y(\\tau + t)d \\tau \n\nCompare with the definition of convolution from the first slide\n\n(x * y)(t) = \\int_{-\\infty}^{+\\infty} x(\\tau) y(t -  \\tau) d \\tau.\n\nSource is here\n\nConvolution and cross-correlation are related as\n\n x(t) \\star y(t) = x(-t) * y(t) \n\nHow this operation is performed in neural networks?\n\n\nSource of gif is here\n\nAlso nice presentation about the difference of this operations, detailed comparison and PyTorch examples is here"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#additional-remarks-about-this-operation",
    "href": "lectures/lecture-15/lecture-15.html#additional-remarks-about-this-operation",
    "title": "",
    "section": "Additional remarks about this operation",
    "text": "Additional remarks about this operation\n\nThis operation reflects the relations between the neighbour pixels in image\nMultiple filters/kernels can fit different features of the image\nThis is still linear transformation of the input, but it focus on local properties of data\nIt can be efficiently computed with GPU since the single set of simple instructions have to be applied to multiple data\nThe trained parameters here are filters that is used to produce the output, for batch of 3D images (RGB) they are 4-dimensional tensors"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#how-the-trained-filters-and-result-image-representation-looks-like",
    "href": "lectures/lecture-15/lecture-15.html#how-the-trained-filters-and-result-image-representation-looks-like",
    "title": "",
    "section": "How the trained filters and result image representation looks like",
    "text": "How the trained filters and result image representation looks like\n\nimport torchvision.models as models\n\nvgg16 = models.vgg16(pretrained=True)\nprint(vgg16)\n\n\nfor ch in vgg16.children():\n    features = ch\n    break\nprint(features)\nfor name, param in features.named_parameters(): \n    print(name, param.shape)\n\n\nplt.figure(figsize=(20, 17))\nfor i, filter in enumerate(features[2].weight):\n    plt.subplot(16, 16, i+1)\n    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\nfrom PIL import Image\nimg = Image.open(\"./tiger.jpeg\")\nplt.imshow(img)\nplt.show()\n\n\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimg_ = transform(img)\nimg_ = img_.unsqueeze(0)\nprint(img_.size())\nplt.imshow(img_[0].permute(1, 2, 0))\n\n\n# After the first convolutional layer\noutput1 = features[0](img_)\nprint(output1.size())\nplt.figure(figsize=(20, 17))\nfor i, f_map in enumerate(output1[0]):\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()\n\n\n# After the second convolutional layer\noutput2 = features[2](features[1](output1))\nprint(output2.size())\nplt.figure(figsize=(20, 17))\nfor i, f_map in enumerate(output2[0]):\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()\n\n\noutput3 = features[5](features[4](features[3](output2)))\nprint(output3.size())\nplt.figure(figsize=(20, 20))\nfor i, f_map in enumerate(output3[0]):\n    if i + 1 == 65:\n        break\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#image-transformations",
    "href": "lectures/lecture-15/lecture-15.html#image-transformations",
    "title": "",
    "section": "Image transformations",
    "text": "Image transformations\n\nDifferent filters highlight different features of the image\nPooling operation reduces the spatial dimensions and further convolutional layer moves it to channels dimension\nAfter all feature block one gets 512 channels (we start from 3 (RGB)) and significant reduction of the spatial size of image\nSuch operation extracts useful features to help classifier work better\nExactly this property of VGG-type networks is one of the ingredients of style transfer networks, see more details here"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#low-rank-and-fourier-transform",
    "href": "lectures/lecture-15/lecture-15.html#low-rank-and-fourier-transform",
    "title": "",
    "section": "Low-rank and Fourier transform",
    "text": "Low-rank and Fourier transform\nRecent achievements for learning structured matrices include [Monarch matrices] (https://arxiv.org/pdf/2204.00595.pdf) and here Monarch mixer\nMonarch matrix is given as\n\\mathbf{M}=\\left(\\prod_{i=1}^p \\mathbf{P}_i \\mathbf{B}_i\\right) \\mathbf{P}_0\nwhere each \\mathbf{P}_i is related to the ‘base \\sqrt[p]{N}’ variant of the bit-reversal permutation, and \\mathbf{B}_i is a block-diagonal matrix with block size b. Setting b=\\sqrt[p]{N} achieves sub-quadratic compute cost. For example, for p=2, b=\\sqrt{N}, Monarch matrices require O\\left(N^{3 / 2}\\right) compute in sequence length N."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#low-rank-characteristic-of-monarch-matrices",
    "href": "lectures/lecture-15/lecture-15.html#low-rank-characteristic-of-monarch-matrices",
    "title": "",
    "section": "Low-rank characteristic of Monarch matrices",
    "text": "Low-rank characteristic of Monarch matrices\nIf we treat Monarch matrix as a block matrix with m \\times m block, the elements have the form:\nM_{\\ell j k i}=L_{j \\ell k} R_{k j i}\nThis is rank-1 approximation in disguise."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#small-demo",
    "href": "lectures/lecture-15/lecture-15.html#small-demo",
    "title": "",
    "section": "Small demo",
    "text": "Small demo\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nn = 1024\n\nf = np.fft.fft(np.eye(n))\n\n\nprint('Rectangular block, approximate low rank')\nprint(np.linalg.svd(f[:16, :n//16])[1])\n\nprint('Special low-rank submatrices, exact low-rank')\nm1 = f[::16, ::n//16]\nprint(m1.shape)\nprint(np.linalg.svd(m1)[1])\n\nRectangular block, approximate low rank\n[2.83322231e+01 1.44781645e+01 3.38480168e+00 4.57202919e-01\n 4.42705408e-02 3.33299146e-03 2.03024059e-04 1.02439715e-05\n 4.34189364e-07 1.55704460e-08 4.73047845e-10 1.21164373e-11\n 2.58444552e-13 4.26313612e-15 1.71409604e-15 6.10988185e-16]\nSpecial low-rank submatrices, exact low-rank\n(64, 16)\n[3.20000000e+001 1.35213262e-014 3.69395699e-029 1.29572281e-044\n 1.33038472e-059 1.57392820e-074 1.98967388e-089 5.68668883e-105\n 2.40948199e-120 2.27046791e-135 4.41076069e-150 3.57770948e-166\n 3.18773973e-181 3.45410051e-196 3.80878284e-212 1.03087635e-228]"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#take-home-message",
    "href": "lectures/lecture-15/lecture-15.html#take-home-message",
    "title": "",
    "section": "Take home message",
    "text": "Take home message\n\nToeplitz and circulant matrices\nSpectral theorem\nFFT\nMultilevel Toeplitz matrices\nIntro to convolutional neural networks\n\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In [1], line 5\n      3     styles = open(\"./styles/custom.css\", \"r\").read()\n      4     return HTML(styles)\n----&gt; 5 css_styling()\n\nCell In [1], line 3, in css_styling()\n      2 def css_styling():\n----&gt; 3     styles = open(\"./styles/custom.css\", \"r\").read()\n      4     return HTML(styles)\n\nFile ~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282, in _modified_open(file, *args, **kwargs)\n    275 if file in {0, 1, 2}:\n    276     raise ValueError(\n    277         f\"IPython won't let you open fd={file} by default \"\n    278         \"as it is likely to crash IPython. If you know what you are doing, \"\n    279         \"you can use builtins' open.\"\n    280     )\n--&gt; 282 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: './styles/custom.css'"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#solution-of-linear-systems-and-minimization-of-functionals",
    "href": "lectures/lecture-13/lecture-13.html#solution-of-linear-systems-and-minimization-of-functionals",
    "title": "Questions?",
    "section": "Solution of linear systems and minimization of functionals",
    "text": "Solution of linear systems and minimization of functionals\n\nInstead of solving a linear system, we can minimize the residual:\n\nR(x) = \\Vert A x - f \\Vert^2_2.\n\nThe condition \\nabla R(x) = 0 gives\n\nA^* A x = A^* f,\nthus it has squared condition number, so direct minimization of the residual by standard optimization methods is rarely used.\n\nFor the symmetric positive definite case there is a much simpler functional."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#energy-functional",
    "href": "lectures/lecture-13/lecture-13.html#energy-functional",
    "title": "Questions?",
    "section": "Energy functional",
    "text": "Energy functional\nLet A = A^* &gt; 0, then the following functional\n\\Phi(x) = (Ax, x)  - 2(f, x)\nis called energy functional.\n\nProperties of energy functional\n\nIt is strictly convex (check!)\n\n \\Phi(\\alpha x + (1 - \\alpha)y) &lt; \\alpha \\Phi(x) + (1 - \\alpha) \\Phi(y)\n\nSince it is strictly convex, it has unique local minimum, which is also global\nIts global minimum x_* satisfies\n\nA x_* = f.\nIndeed,\n\\nabla \\Phi = 2(Ax - f).\nand the first order optimality condition \\nabla \\Phi (x_*) = 0 yields\nA x_* = f."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#approximation-of-the-solution-by-a-subspace",
    "href": "lectures/lecture-13/lecture-13.html#approximation-of-the-solution-by-a-subspace",
    "title": "Questions?",
    "section": "Approximation of the solution by a subspace",
    "text": "Approximation of the solution by a subspace\n\nGiven a linear M-dimensional subspace \\{y_1, \\dots, y_M\\}, we want to find an approximate solution in this basis, i.e. \n\nA x \\approx f, \\quad x = x_0 +  \\sum_{k=1}^M c_k y_k,\nwhere c is the vector of coefficients.\n\nIn the symmetric positive definite case we need to minimize\n\n(Ax, x) - 2(f, x)\nsubject to x = x_0 + Y c,\nwhere Y=[y_1,\\dots,y_M] is n \\times M and vector c has length M.\n\nUsing the representation of x, we have the following minimization for c:\n\n\\widehat{\\Phi}(c) = (A Y c, Y c) + 2 (Y^*Ax_0, c) - 2(f, Y c) = (Y^* A Y c, c) - 2(Y^* (f - Ax_0), c).\n\nNote that this is the same functional, but for the Galerkin projection of A\n\nY^* A Y c = Y^* (f - Ax_0) = Y^* r_0,\nwhich is an M \\times M linear system with symmetric positive definite matrix if Y has full column rank.\nBut how to choose Y?"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-random-projection",
    "href": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-random-projection",
    "title": "Questions?",
    "section": "Selection of the subspace: random projection",
    "text": "Selection of the subspace: random projection\n\nWe can generate Y with random numbers and then orthogonalize\nWhat quality of x will we get in this case?\nHow the derived quality relates to the conditioning of the matrix?\nSelection of the proper random matrix is important topic in dimensionality reduction theory and relates to random projection approach\n\n\nimport numpy as np\n\nn = 100\nA = np.random.randn(n, n)\nQ, _ = np.linalg.qr(A)\nk = 70\nA = Q.T @ np.diag([1e-6] * k + list(np.random.rand(n-k))) @ Q\nx_true = np.random.randn(n)\nrhs = A @ x_true\nM = n - k\nY = np.random.randn(n, M)\nA_proj = Y.T @ A @ Y\nrhs_proj = Y.T @ rhs\nprint(A_proj.shape)\nc = np.linalg.solve(A_proj, rhs_proj)\nx_proj = Y @ c\nprint(np.linalg.norm(A @ x_proj - rhs) / np.linalg.norm(rhs))\n\n(30, 30)\n0.00044969726264993147"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-krylov-subspace",
    "href": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-krylov-subspace",
    "title": "Questions?",
    "section": "Selection of the subspace: Krylov subspace",
    "text": "Selection of the subspace: Krylov subspace\nIn the Krylov subspace we generate the whole subspace from a single vector r_0 = f - Ax_0:\ny_0\\equiv k_0 = r_0, \\quad y_1\\equiv k_1 = A r_0, \\quad y_2\\equiv k_2 = A^2 r_0, \\ldots, \\quad y_{M-1}\\equiv k_{M-1} = A^{M-1} r_0.\nThis gives the Krylov subpace of the M-th order\n\\mathcal{K}_M(A, r_0) = \\mathrm{Span}(r_0, Ar_0, \\ldots, A^{M-1} r_0).\n\nIt is known to be quasi-optimal space given only matrix-vector product operation.\nKey reference here is “On the numerical solution of equation by which are determined in technical problems the frequencies of small vibrations of material systems”, A. N. Krylov, 1931, text in russian"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#solution-x_-lies-in-the-krylov-subspace-x_-in-mathcalk_na-f",
    "href": "lectures/lecture-13/lecture-13.html#solution-x_-lies-in-the-krylov-subspace-x_-in-mathcalk_na-f",
    "title": "Questions?",
    "section": "Solution x_* lies in the Krylov subspace: x_* \\in \\mathcal{K}_n(A, f)",
    "text": "Solution x_* lies in the Krylov subspace: x_* \\in \\mathcal{K}_n(A, f)\n\nAccording to Cayley–Hamilton theorem: p(A) = 0, where p(\\lambda) = \\det(A - \\lambda I)\np(A)f = A^nf + a_1A^{n-1}f + \\ldots + a_{n-1}Af + a_n f = 0\nA^{-1}p(A)f = A^{n-1}f + a_1A^{n-2}f + \\ldots + a_{n-1}f + a_nA^{-1}f = 0\nx_* = A^{-1}f = -\\frac{1}{a_n}(A^{n-1}f + a_1A^{n-2}f + \\ldots + a_{n-1}f)\nThus, x_* \\in \\mathcal{K}_n(A, f)"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#ill-conditioned-of-the-natural-basis",
    "href": "lectures/lecture-13/lecture-13.html#ill-conditioned-of-the-natural-basis",
    "title": "Questions?",
    "section": "Ill-conditioned of the natural basis",
    "text": "Ill-conditioned of the natural basis\nThe natural basis in the Krylov subspace is very ill-conditioned, since\nk_i = A^i r_0 \\rightarrow \\lambda_\\max^i v,\nwhere v is the eigenvector, corresponding to the maximal eigenvalue of A, i.e. k_i become more and more collinear for large i.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.sparse as spsp\n%matplotlib inline\n\nn = 100\nex = np.ones(n);\nA = spsp.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nf = np.ones(n)\nx0 = np.random.randn(n)\n\nsubspace_order = 10\nkrylov_vectors = np.zeros((n, subspace_order))\nkrylov_vectors[:, 0] = f - A.dot(x0)\nfor i in range(1, subspace_order):\n    krylov_vectors[:, i] = A.dot(krylov_vectors[:, i-1])\n    \ns = np.linalg.svd(krylov_vectors, compute_uv=False)\nprint(\"Condition number = {}\".format(s.max() / s.min()))\n\nCondition number = 497674309.99072236\n\n\nSolution: Compute orthogonal basis in the Krylov subspace."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#good-basis-in-a-krylov-subspace",
    "href": "lectures/lecture-13/lecture-13.html#good-basis-in-a-krylov-subspace",
    "title": "Questions?",
    "section": "Good basis in a Krylov subspace",
    "text": "Good basis in a Krylov subspace\nIn order to have stability, we first orthogonalize the vectors from the Krylov subspace using Gram-Schmidt orthogonalization process (or, QR-factorization).\nK_j = \\begin{bmatrix} r_0 & Ar_0 & A^2 r_0 & \\ldots & A^{j-1} r_0\\end{bmatrix} = Q_j R_j, \nand the solution will be approximated as\nx \\approx x_0 + Q_j c."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#short-way-to-arnoldi-relation",
    "href": "lectures/lecture-13/lecture-13.html#short-way-to-arnoldi-relation",
    "title": "Questions?",
    "section": "Short way to Arnoldi relation",
    "text": "Short way to Arnoldi relation\nStatement. The Krylov matrix K_j satisfies an important recurrent relation (called Arnoldi relation)\nA Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1},\nwhere H_j is upper Hessenberg, and Q_{j+1} = [q_0,\\dots,q_j] has orthogonal columns that spans columns of K_{j+1}.\nLet us prove it (consider j = 3 for simplicity):\nA \\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} = \\begin{bmatrix} k_1 & k_2 & k_3 \\end{bmatrix} = \\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} \\begin{bmatrix} 0 & 0 & \\alpha_0 \\\\ 1 & 0  & \\alpha_1 \\\\ 0 & 1  & \\alpha_2 \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 & 0 & k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2 \\end{bmatrix}, \nwhere \\alpha_s will be selected later. Denote \\widehat{k}_3 = k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2.\nIn the matrix form,\nA K_3 = K_3 Z + \\widehat k_3 e^{\\top}_2,\nwhere Z is the lower shift matrix with the last column (\\alpha_0,\\alpha_1,\\alpha_2)^T, and e_2 is the last column of the identity matrix.\nLet\nK_3 = Q_3 R_3\nbe the QR-factorization. Then,\nA Q_3 R_3 = Q_3 R_3 Z + \\widehat{k}_3 e^{\\top}_2,\n A Q_3 = Q_3 R_3 Z R_3^{-1} + \\widehat{k}_3 e^{\\top}_2 R_3^{-1}.\nNote that\ne^{\\top}_2 R_3^{-1} = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\end{bmatrix}  = \\gamma e^{\\top}_2, and\nR_3 Z R_3^{-1} = \\begin{bmatrix} * & * & * \\\\* & * & * \\\\  0 & * & * \\\\ \\end{bmatrix},\nin the general case it will be an upper Hessenberg matrix H, i.e. a matrix that\nH_{ij} = 0, \\quad \\mbox{if } i &gt; j + 1."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#almost-arnoldi-relation",
    "href": "lectures/lecture-13/lecture-13.html#almost-arnoldi-relation",
    "title": "Questions?",
    "section": "(Almost) Arnoldi relation",
    "text": "(Almost) Arnoldi relation\nLet Q_j be the orthogonal basis in the Krylov subspace, then we have almost the Arnoldi relation\nA Q_j = Q_j H_j +  \\gamma\\widehat{k}_j e^{\\top}_{j-1},\nwhere H_j is an upper Hessenberg matrix, and\n\\widehat{k}_j = k_j - \\sum_{s=0}^{j-1} \\alpha_s k_s.\nWe select \\alpha_s in such a way that\nQ^*_j \\widehat{k}_j = 0.\nThen, \\widehat{k}_j = h_{j, j-1} q_j, where q_j is the last column of Q_{j+1}."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#arnoldi-relation-final-formula",
    "href": "lectures/lecture-13/lecture-13.html#arnoldi-relation-final-formula",
    "title": "Questions?",
    "section": "Arnoldi relation: final formula",
    "text": "Arnoldi relation: final formula\nWe have\nA Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1}.\n\nThis is the crucial formula for the efficient generation of such subspaces.\nFor non-symmetric case, it is just modified Gram-Schmidt.\nFor the symmetric case, we have a much simpler form (Lanczos process)."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#lanczos-process",
    "href": "lectures/lecture-13/lecture-13.html#lanczos-process",
    "title": "Questions?",
    "section": "Lanczos process",
    "text": "Lanczos process\nIf A = A^*, then\nQ^*_j A Q_j = H_j, \nthus H_j is hermitian, and thus it is tridiagonal, H_j = T_j.\nThis gives a short-term recurrence relation to generate the Arnoldi vectors q_j without full orthogonalization."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#lanczos-process-2",
    "href": "lectures/lecture-13/lecture-13.html#lanczos-process-2",
    "title": "Questions?",
    "section": "Lanczos process (2)",
    "text": "Lanczos process (2)\n A Q_j = Q_j T_j + t_{j, j-1} q_j e^{\\top}_{j-1}.\nIn order to get q_j, we need to compute just the last column of\nt_{j, j-1} q_j = (A Q_j - Q_j T_j) e_{j-1} = A q_{j-1} - t_{j-1, j-1} q_{j-1} - t_{j-2, j-1} q_{j-2}. \nThe coefficients \\alpha_j = t_{j-1, j-1} and \\beta_j = t_{j-2, j-1} can be recovered from orthogonality constraints\n(q_j, q_{j-1}) = 0, \\quad (q_j, q_{j-2}) = 0\nAll the other constraints will be satisfied automatically!!\nAnd we only need to store two vectors to get the new one."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#from-direct-lanczos-method-to-the-conjugate-gradient",
    "href": "lectures/lecture-13/lecture-13.html#from-direct-lanczos-method-to-the-conjugate-gradient",
    "title": "Questions?",
    "section": "From direct Lanczos method to the conjugate gradient",
    "text": "From direct Lanczos method to the conjugate gradient\nWe can now get from the Lanczos recurrence to the famous conjugate gradient method.\nWe have for A = A^* &gt; 0\nA Q_j = Q_j T_j + T_{j, j-1} q_j.\nRecall that when we minimize energy functional in basis Y we get a system Y^* A Y c = Y^* f,. Here Y = Q_j, so the approximate solution of Ax \\approx f with x_j = x_0 + Q_j c_j can be found by solving a small system\nQ^*_j A Q_j c_j = T_j c_j = Q^*_j r_0 .\nSince f is the first Krylov subspace, then Note!!! (recall what the first column in Q_j is)\nQ^*_j r_0  = \\Vert r_0 \\Vert_2 e_0 = \\gamma e_0.\nWe have a tridiagonal system of equations for c:\nT_j c_j = \\gamma e_0\nand x_j = Q_j c_j.\nWe could stop at this point, but we want short recurrent formulas instead of solving linear system with matrix T_j at each step.\nDerivation of the following update formulas is not required on the oral exam!\n\nSince A is positive definite, T_j is also positive definite, and it allows an LU decomposition\nT_j = L_j U_j, where L_j is a bidiagonal matrix with ones on the diagonal, U_j is a upper bidiagonal matrix.\n\n T_j = \\begin{bmatrix} a_1 & b_1 &  & \\\\ b_1 & a_2 & b_2 & \\\\ & \\ddots & \\ddots & \\ddots & \\\\ & & b_{j-1} & a_{j-1} & b_j \\\\ & & & b_j & a_j \\end{bmatrix} = \\begin{bmatrix} 1 & &  & \\\\ c_1 & 1 &  & \\\\ & \\ddots & \\ddots &  & \\\\ & & c_{j-1} & 1 & \\\\ & & & c_j & 1 \\end{bmatrix} \\begin{bmatrix} d_1 & b_1 &  & \\\\ & d_1 & b_2 & \\\\ & & \\ddots & \\ddots & \\\\ & & & d_{j-1} & b_j \\\\ & & & & d_j \\end{bmatrix} \n\nWe need to define one subdiagonal in L (with elements c_1, \\ldots, c_{j-1}), main diagonal of U_j (with elements d_0, \\ldots, d_{j-1} and superdiagonal of U_j (with elements b_1, \\ldots, b_{j-1}).\nThey have convenient recurrences:\n\nc_i = b_i/d_{i-1}, \\quad d_i = \\begin{cases} a_1, & \\mbox{if } i = 1, \\\\\na_i - c_i b_i, & \\mbox{if } i &gt; 1. \\end{cases}\n\nFor the solution we have\n\nx_j = Q_j T^{-1}_j \\gamma e_0  = \\gamma Q_j (L_j U_j)^{-1} e_0  = \\gamma Q_j U^{-1}_j L^{-1}_j e_0.\n\nWe introduce two new quantities:\n\nP_j = Q_j U^{-1}_j, \\quad z_j = \\gamma L^{-1}_j e_0.\n\nNow we have the following equation for x_j:\n\n x_j = P_j z_j\n\nDue to the recurrence relations, we have\n\nP_j = \\begin{bmatrix} P_{j-1} & p_j \\end{bmatrix}, \nand\nz_j = \\begin{bmatrix} z_{j-1} \\\\ \\xi_{j} \\end{bmatrix}.\n\nFor p_j and \\xi_j we have short-term recurrence relations (due to bidiagonal structure)\n\np_j = \\frac{1}{d_j}\\left(q_j - b_j p_{j-1} \\right), \\quad \\xi_j = -c_j \\xi_{j-1}.\n\nThus, we arrive at short-term recurrence for x_j:\n\nx_j = P_j z_j = P_{j-1} z_{j-1} + \\xi_j p_j = x_{j-1} + \\xi_j p_j.\nand q_j are found from the Lanczos relation (see slides above).\n\nThis method for solving linear systems is called a direct Lanczos method. It is closely related to the conjugate gradient method."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#direct-lanczos-method",
    "href": "lectures/lecture-13/lecture-13.html#direct-lanczos-method",
    "title": "Questions?",
    "section": "Direct Lanczos method",
    "text": "Direct Lanczos method\nWe have the direct Lanczos method, where we store\np_{j-1}, q_j, x_{j-1} to get a new estimate of x_j.\nThe main problem is with q_j: we have the three-term recurrence, but in the floating point arithmetic the orthogonality is can be lost, leading to numerical errors.\nLet us do some demo.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy as sp\nimport scipy.sparse as spsp\nfrom scipy.sparse import csc_matrix\n\nn = 128\nex = np.ones(n);\nA = spsp.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\n\nnit = 64\nq1 = rhs/np.linalg.norm(rhs)\nq2 = A.dot(q1)\nq2 = q2 - np.dot(q2, q1)*q1\nq2 = q2/np.linalg.norm(q2)\nqall = [q1, q2]\nfor i in range(nit):\n    qnew = A.dot(qall[-1])\n    qnew = qnew - np.dot(qnew, qall[-1])*qall[-1]\n    qnew = qnew/np.linalg.norm(qnew)\n    qnew = qnew - np.dot(qnew, qall[-2])*qall[-2]\n    qnew = qnew/np.linalg.norm(qnew)\n    qall.append(qnew)\nqall_mat = np.vstack(qall).T\nprint(np.linalg.norm(qall_mat.T.dot(qall_mat) - np.eye(qall_mat.shape[1])))\n\n1.9605915654183865"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#conjugate-gradient-method",
    "href": "lectures/lecture-13/lecture-13.html#conjugate-gradient-method",
    "title": "Questions?",
    "section": "Conjugate gradient method",
    "text": "Conjugate gradient method\nInstead of q_j (last vector in the modified Gram-Schmidt process), it is more convenient to work with the residual\nr_j = f - A x_j.\nThe resulting recurrency has the form\nx_j = x_{j-1} + \\alpha_{j-1} p_{j-1}\nr_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}\np_j = r_j + \\beta_j p_{j-1}.\nHence the name conjugate gradient: to the gradient r_j we add a conjugate direction p_j.\nWe have orthogonality of residuals (check!):\n(r_i, r_j) = 0, \\quad i \\ne j\nand A-orthogonality of conjugate directions (check!):\n (A p_i, p_j) = 0,\nwhich can be checked from the definition.\nThe equations for \\alpha_j and \\beta_j can be now defined explicitly from these two properties."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#cg-final-formulas",
    "href": "lectures/lecture-13/lecture-13.html#cg-final-formulas",
    "title": "Questions?",
    "section": "CG final formulas",
    "text": "CG final formulas\nWe have (r_{j}, r_{j-1}) = 0 = (r_{j-1} - \\alpha_{j-1} A p_{j-1}, r_{j-1}),\nthus\n\\alpha_{j-1} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, r_{j-1})} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, p_{j-1} - \\beta_{j-1}p_{j-2})} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, p_{j-1})}.\nIn the similar way, we have\n\\beta_{j-1} = \\frac{(r_j, r_j)}{(r_{j-1}, r_{j-1})}.\nRecall that\nx_j = x_{j-1} + \\alpha_{j-1} p_{j-1}\nr_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}\np_j = r_j + \\beta_j p_{j-1}.\nOnly one matrix-by-vector product per iteration."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#cg-derivation-overview",
    "href": "lectures/lecture-13/lecture-13.html#cg-derivation-overview",
    "title": "Questions?",
    "section": "CG derivation overview",
    "text": "CG derivation overview\n\nWant to find x_* in Krylov subspace\nBut natural basis is ill-conditioned, therefore we need orthogonalization\nDerive recurrent equation for sequential orthogonalization of the Krylov subspace basis\n\nArnoldi process for non-symmetric matrix\nLanczos process for symmetrix matrix\n\nClever re-writing of these formulas gives short recurrence"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#some-history",
    "href": "lectures/lecture-13/lecture-13.html#some-history",
    "title": "Questions?",
    "section": "Some history",
    "text": "Some history\nMore details here: https://www.siam.org/meetings/la09/talks/oleary.pdf\nWhen Hestenes worked on conjugate bases in 1936, he was advised by a Harvard professor that it was too obvious for publication - CG doesn’t work on slide rules.  - CG has little advantage over Gauss elimination for computation with calculators. - CG is not well suited for a room of human “computers” – too much data exchange."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#properties-of-the-cg-method",
    "href": "lectures/lecture-13/lecture-13.html#properties-of-the-cg-method",
    "title": "Questions?",
    "section": "Properties of the CG method",
    "text": "Properties of the CG method\n\nWe need to store 3 vectors.\nSince it generates A-orthogonal sequence p_1, \\ldots, p_N, after n steps it should stop (i.e., p_{N+1} = 0.)\nIn practice it does not have this property in finite precision, thus after its invention in 1952 by Hestens and Stiefel it was labeled unstable.\nIn fact, it is a brilliant iterative method."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#a-optimality",
    "href": "lectures/lecture-13/lecture-13.html#a-optimality",
    "title": "Questions?",
    "section": "A-optimality",
    "text": "A-optimality\nEnergy functional can be written as\n(Ax, x) - 2(f, x) = (A (x - x_*), (x - x_*)) - (Ax _*, x_*),\nwhere A x_* = f. Up to a constant factor,\n (A(x - x_*), (x -x_*)) = \\Vert x - x_* \\Vert^2_A\nis the A-norm of the error."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#convergence",
    "href": "lectures/lecture-13/lecture-13.html#convergence",
    "title": "Questions?",
    "section": "Convergence",
    "text": "Convergence\nThe CG method computes x_k that minimizes the energy functional over the Krylov subspace, i.e. x_k = p(A)f, where p is a polynomial of degree k+1, so\n\\Vert x_k - x_* \\Vert_A  =  \\inf\\limits_{p} \\Vert \\left(p(A) - A^{-1}\\right) f \\Vert_A. \nUsing eigendecomposition of A we have\nA = U \\Lambda U^*, \\quad  g = U^* f, and\n\\Vert x - x_* \\Vert^2_A = \\displaystyle{\\inf_p} \\Vert \\left(p(\\Lambda) - \\Lambda^{-1}\\right) g \\Vert_\\Lambda^2 = \\displaystyle{\\inf_p}\n\\displaystyle{\\sum_{i=1}^n} \\frac{(\\lambda_i p(\\lambda_i) - 1)^2 g^2_i}{\\lambda_i} = \\displaystyle{\\inf_{q, q(0) = 1}} \\displaystyle{\\sum_{i=1}^n} \\frac{q(\\lambda_i)^2 g^2_i}{\\lambda_i}\nSelection of the optimal q depends on the eigenvalue distribution."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#absolute-and-relative-error",
    "href": "lectures/lecture-13/lecture-13.html#absolute-and-relative-error",
    "title": "Questions?",
    "section": "Absolute and relative error",
    "text": "Absolute and relative error\nWe have\n\\Vert x - x_* \\Vert^2_A \\leq \\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} \\inf_{q, q(0)=1} \\max_{j} q({\\lambda_j})^2\nThe first term is just\n\\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} = (A^{-1} f, f) = \\Vert x_* \\Vert^2_A.\nAnd we have relative error bound\n\\frac{\\Vert x - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})|,\nso if matrix has only 2 different eigenvalues, then there exists a polynomial of degree 2 such that q({\\lambda_1}) =q({\\lambda_2})=0, so in this case CG converges in 2 iterations.\n\n\nIf eigenvalues are clustered and there are l outliers, then after first \\mathcal{O}(l) iterations CG will converge as if there are no outliers (and hence the effective condition number is smaller).\nThe intuition behind this fact is that after \\mathcal{O}(l) iterations the polynomial has degree more than l and thus is able to zero l outliers. \n\nLet us find another useful upper-bound estimate of convergence. Since\n\n\\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})| \\leq \\inf_{q, q(0)=1} \\max_{\\lambda\\in[\\lambda_\\min,\\lambda_\\max]} |q({\\lambda})|\n\nThe last term is just the same as for the Chebyshev acceleration, thus the same upper convergence bound holds:\n\\frac{\\Vert x_k - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\gamma \\left( \\frac{\\sqrt{\\mathrm{cond}(A)}-1}{\\sqrt{\\mathrm{cond}(A)}+1}\\right)^k."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#finite-termination-clusters",
    "href": "lectures/lecture-13/lecture-13.html#finite-termination-clusters",
    "title": "Questions?",
    "section": "Finite termination & clusters",
    "text": "Finite termination & clusters\n\nIf A has only m different eigenvalues, CG converges in m iterations (proof in the blackboard).\nIf A has m “clusters” of eigenvalues, CG converges cluster-by-cluster.\n\nAs a result, better convergence than Chebyshev acceleration, but slightly higher cost per iteration."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#summary",
    "href": "lectures/lecture-13/lecture-13.html#summary",
    "title": "Questions?",
    "section": "Summary",
    "text": "Summary\nCG is the method of choice for symmetric positive definite systems:\n\n\\mathcal{O}(n) memory\nSquare root of condition number in the estimates\nAutomatic ignoring of the outliers/clusters\nA-optimality property"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#non-linear-conjugate-gradient-method",
    "href": "lectures/lecture-13/lecture-13.html#non-linear-conjugate-gradient-method",
    "title": "Questions?",
    "section": "Non-linear conjugate gradient method",
    "text": "Non-linear conjugate gradient method\n\nCG minimizes the energy functional, which is quadratic in x\nCG formulas were used as starting point in developing methods to minimize arbitrary convex function\nMost popular CG extensions (so-called non-linear CG method) are\n\nHestenes-Stiefel method\nPolak-Ribiere method - original paper in French\nFletcher–Reeves method"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#non-symmetric-systems-and-the-generalized-minimal-residual-method-gmres-y.-saad-m.-schultz-1986",
    "href": "lectures/lecture-13/lecture-13.html#non-symmetric-systems-and-the-generalized-minimal-residual-method-gmres-y.-saad-m.-schultz-1986",
    "title": "Questions?",
    "section": "Non-symmetric systems and the generalized minimal residual method (GMRES) (Y. Saad, M. Schultz, 1986)",
    "text": "Non-symmetric systems and the generalized minimal residual method (GMRES) (Y. Saad, M. Schultz, 1986)\nBefore we discussed symmetric positive definite systems. What happens if A is non-symmetric?\nWe can still orthogonalize the Krylov subspace using Arnoldi process, and get\nA Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1}.\nLet us rewrite the latter expression as\n A Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1} = Q_{j+1} \\widetilde H_j, \\quad \\widetilde H_j =\n\\begin{bmatrix} h_{0,0} & h_{0,1} & \\dots & h_{0,j-2} & h_{0,j-1} \\\\ h_{1,0} & h_{1,1} & \\dots & h_{1,j-2} & h_{1,j-1} \\\\ 0& h_{2,2} &  \\dots & h_{2,j-2} & h_{2,j-1} \\\\\n0& 0 & \\ddots & \\vdots & \\vdots  \\\\\n0& 0 &  & h_{j,j-1} & h_{j-1,j-1} \\\\ 0& 0 & \\dots & 0 & h_{j,j-1}\\end{bmatrix}\nThen, if we need to minimize the residual over the Krylov subspace, we have\nx_j = x_0 + Q_j c_j \nand x_j has to be selected as\n \\Vert A x_j - f \\Vert_2 =  \\Vert A Q_j c_j - r_0 \\Vert_2 \\rightarrow \\min_{c_j}.\nUsing the Arnoldi recursion, we have\n \\Vert Q_{j+1} \\widetilde H_j c_j -  r_0 \\Vert_2 \\rightarrow \\min_{c_j}.\nUsing the orthogonal invariance under multiplication by unitary matrix, we get\n \\Vert \\widetilde H_j c_j - \\gamma e_0 \\Vert_2 \\rightarrow \\min_{c_j},\nwhere we have used that Q^*_{j+1} r_0 = \\gamma e_0, \\gamma = \\Vert r_0 \\Vert\n\nThis is just a linear least squares with (j+1) equations and j unknowns.\nThe matrix is also upper Hessenberg, thus its QR factorization can be computed in a very cheap way.\nThis allows the computation of c_j. This method is called GMRES (generalized minimal residual)"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#summary-of-the-gmres",
    "href": "lectures/lecture-13/lecture-13.html#summary-of-the-gmres",
    "title": "Questions?",
    "section": "Summary of the GMRES",
    "text": "Summary of the GMRES\n\nMinimizes the residual directly\nNo normal equations\nMemory grows with the number of iterations as \\mathcal{O}(nj), so restarts typically implemented (just start GMRES from the new initial guess).\n\n\nimport scipy.sparse.linalg as la\nfrom scipy.sparse import csc_matrix, csr_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n%matplotlib inline\nn = 150\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\n\nplt.figure(figsize=(10, 5))\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nfor restart in [5, 40, 200]:\n    hist = []\n    def callback(rk):\n        hist.append(np.linalg.norm(rk) / np.linalg.norm(rhs))\n    st = time.time()\n    sol = la.gmres(A, rhs, x0=np.zeros(n*n), maxiter=200, restart=restart, callback=callback, tol=1e-16)\n    current_time = time.time() - st\n    ax1.semilogy(np.array(hist), label='rst={}'.format(restart))\n    ax2.semilogy([current_time * i / len(hist) for i in range(len(hist))], np.array(hist), label='rst={}'.format(restart))\n    \n\nax1.legend(loc='best')\nax2.legend(loc='best')\nax1.set_xlabel(\"Number of outer iterations\", fontsize=20)\nax2.set_xlabel(\"Time, sec\", fontsize=20)\nax1.set_ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nax2.set_ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nplt.sca(ax1)\nplt.yticks(fontsize=20)\nplt.sca(ax2)\nplt.yticks(fontsize=20)\nf.tight_layout()\n\n&lt;Figure size 1000x500 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as la\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Example from http://www.caam.rice.edu/~embree/39961.pdf\n\nA = np.array([[1, 1, 1],\n              [0, 1, 3],\n              [0, 0, 1]]\n            )\nrhs = np.array([2, -4, 1])\nx0 = np.zeros(3)\n\nfor restart in [1, 2, 3]:\n    hist = []\n    def callback(rk):\n        hist.append(np.linalg.norm(rk)/np.linalg.norm(rhs))\n    _ = la.gmres(A, rhs, x0=x0, maxiter=20, restart=restart, callback=callback)\n    plt.semilogy(np.array(hist), label='rst={}'.format(restart))\nplt.legend(fontsize=22)\nplt.xlabel(\"Number of outer iterations\", fontsize=20)\nplt.ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#next-lecture",
    "href": "lectures/lecture-13/lecture-13.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nIterative methods continued (BiCG, MINRES)\nPreconditioners"
  }
]