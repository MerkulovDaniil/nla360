[
  {
    "objectID": "lectures/lecture-14/lecture-14.html#previous-part",
    "href": "lectures/lecture-14/lecture-14.html#previous-part",
    "title": "Questions?",
    "section": "Previous part",
    "text": "Previous part\n\nArnoldi orthogonalization of Krylov subspaces\nLanczos for the symmetric case\nEnergy functional and conjugate gradient method\nConvergence analysis\nNon-symmetric case: idea of GMRES"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#now-we-will-cover-the-following-topics",
    "href": "lectures/lecture-14/lecture-14.html#now-we-will-cover-the-following-topics",
    "title": "Questions?",
    "section": "Now we will cover the following topics",
    "text": "Now we will cover the following topics\n\nMore iterative methods: MINRES, BiCG and BiCGStab\nThe concept of preconditioners"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#what-methods-to-use",
    "href": "lectures/lecture-14/lecture-14.html#what-methods-to-use",
    "title": "Questions?",
    "section": "What methods to use",
    "text": "What methods to use\n\nIf a matrix is symmetric (Hermitian) positive definite, use CG method.\nIf a matrix is symmetric but indefinite, we can use MINRES method (GMRES applied to a symmetric system)\nIf a matrix is non-symmetric and not very big, use GMRES\nIf a matrix is non-symmetric and we can store limited amount of vectors, use either: GMRES with restarts, or BiCGStab (the latter of the product with A^{\\top} is also available).\n\n\nMore detailed flowchart from this book"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#minres",
    "href": "lectures/lecture-14/lecture-14.html#minres",
    "title": "Questions?",
    "section": "MINRES",
    "text": "MINRES\nThe MINRES method is GMRES applied to a symmetric system.\n\nWe search the solution in the subspace spanned by columns of Q_j\n\n x_j = x_0 + Q_j c_j \n\nWe minimize the residual norm\n\n\\Vert Ax_j - f\\Vert_2 = \\Vert Ax_0 + AQ_j c_j - f\\Vert_2 =  \\Vert A Q_j c_j - r_0 \\Vert_2 = \\Vert Q_j H_j c_j + h_{j, j-1} q_j c_j - r_0 \\Vert_2 = \\Vert Q_{j+1} \\widetilde{H}_{j+1}  c_j - r_0 \\Vert_2 \\rightarrow \\min_{c_j}\nwhich is equivalent to a linear least squares with an almost tridiagonal matrix\n\\Vert \\widetilde{H}_{j+1} c_{j} - \\gamma e_0 \\Vert_2 \\rightarrow \\min_{c_{j}}.\n\nIn a similar fashion, we can derive short-term recurrences.\nA careful implementation of MINRES requires at most 5 vectors to be stored."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#difference-between-minres-and-cg",
    "href": "lectures/lecture-14/lecture-14.html#difference-between-minres-and-cg",
    "title": "Questions?",
    "section": "Difference between MINRES and CG",
    "text": "Difference between MINRES and CG\n\nMINRES minimizes \\Vert Ax_k - f \\Vert_2 over the Krylov subspace\nCG minimize (Ax, x) - 2(f, x) over the Krylov subspace\nMINRES works for indefinite (i.e., non-positive definite) problems.\n\nCG stores less vectors (3 instead of 5).\nNow, let us talk about non-symmetric systems.\n\nimport scipy.sparse.linalg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\n# Create a symmetric indefinite matrix\nn = 200\ndiag = np.ones(n)\n# Create a discretized 1D Helmholtz operator: -d^2/dx^2 - k^2\nh = 1.0/n  # Grid spacing\nk = 0     # Wave number\n\n# Create diagonals for tridiagonal matrix\nmain_diag = -2.00/h**2 + k**2  # Main diagonal: -2/h^2 + k^2 \noff_diag = 1.0/h**2           # Off diagonals: 1/h^2\n\n# Create arrays for the three diagonals\nmain = np.ones(n) * main_diag\nupper = np.ones(n) * off_diag  \nlower = np.ones(n) * off_diag\n\n# Construct tridiagonal matrix using sparse diagonals\n# The error occurs because spdiags expects diagonals to be provided in a 2D array\n# where each row represents a diagonal. Let's stack them correctly:\nA = sp.sparse.spdiags([lower, main, upper], [-1, 0, 1], n, n, format='csr')\n# Random right-hand side\nnp.random.seed(42)\nb = np.random.randn(n)\n\n# Lists to store residual norms\nres_minres = []\nres_cg = []\n\ndef callback_minres(xk):\n    res_minres.append(np.linalg.norm(b - A @ xk))\n    \ndef callback_cg(xk):\n    res_cg.append(np.linalg.norm(b - A @ xk))\n\n# Solve with MINRES\nx_minres = scipy.sparse.linalg.minres(A, b, callback=callback_minres, maxiter=500, rtol=1e-10)\n\n# Try to solve with CG (may not converge due to indefiniteness)\ntry:\n    x_cg = scipy.sparse.linalg.cg(A, b, callback=callback_cg, maxiter=500, rtol=1e-10)\nexcept:\n    print(\"CG failed to converge as expected for indefinite system\")\n\n# Plot convergence\nplt.figure(figsize=(8, 6))\nplt.semilogy(res_minres, 'b-', label='MINRES')\nif len(res_cg) &gt; 0:\n    plt.semilogy(res_cg, 'r--', label='CG')\nplt.xlabel('Iteration')\nplt.ylabel('Residual norm')\nplt.title('Convergence comparison for symmetric indefinite system')\nplt.legend()\nplt.grid(True)\nnp.max(np.linalg.eigvals(A.todense()))\n\nnp.float64(-9.771444747833783)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#non-symmetric-systems",
    "href": "lectures/lecture-14/lecture-14.html#non-symmetric-systems",
    "title": "Questions?",
    "section": "Non-symmetric systems",
    "text": "Non-symmetric systems\n\nThe main disadvantage of GMRES: we have to store all the vectors, so the memory cost grows with each step.\nWe can do restarts (i.e. get a new residual and a new Krylov subspace): we find some approximate solution x and now solve the linear system for the correction:\n\nA(x + e) = f, \\quad Ae = f - Ax,\nand generate the new Krylov subspace from the residual vector. This spoils the convergence, as we will see from the demo.\n\nimport scipy.sparse.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rc(\"text\", usetex=False)\nimport numpy as np\nimport scipy as sp\n\nn = 300\nex = np.ones(n);\nA = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.random.randn(n)\n\nres_gmres_rst = []\nres_gmres = []\ndef gmres_rst_cl(r):\n    res_gmres_rst.append(np.linalg.norm(r))\n    \ndef gmres_rst(r):\n    res_gmres.append(np.linalg.norm(r))\n\nsol = scipy.sparse.linalg.gmres(A, rhs, restart=20, callback=gmres_rst_cl)\nsol = scipy.sparse.linalg.gmres(A, rhs, restart=n, callback=gmres_rst)\n\nlim = 300\nplt.semilogy(res_gmres_rst[:lim], marker='.',color='k', label='GMRES, restart=20')\nplt.semilogy(res_gmres[:lim], marker='x',color='r', label='GMRES, no restart')\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Residual norm', fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.legend(fontsize=20)\n\n\n\n\n\n\n\n\n\nHow to avoid such spoiling of convergence?\n\nBiConjugate Gradient method (named BiCG, proposed by Fletcher, original paper) avoids that using “short recurrences” like in the CG method."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#idea-of-biconjugate-gradient",
    "href": "lectures/lecture-14/lecture-14.html#idea-of-biconjugate-gradient",
    "title": "Questions?",
    "section": "Idea of biconjugate gradient",
    "text": "Idea of biconjugate gradient\nIdea of BiCG method is to use the normal equations:\nA^* A x = A^* f,\nand apply the CG method to it.\n\nThe condition number has squared, thus we need stabilization.\nThe stabilization idea proposed by Van der Vorst et al. improves the stability (later in the lecture)\n\nLet us do some demo for a simple non-symmetric matrix to demonstrate instability of BiCG method.\n\nres_all_bicg = []\ndef bicg_cl(x):\n    res_all_bicg.append(np.linalg.norm(A.dot(x) - rhs))\n    \nsol = scipy.sparse.linalg.bicg(A, rhs, x0=np.zeros(n), callback=bicg_cl)\nplt.semilogy(res_all_bicg, label='BiCG')\nplt.semilogy(res_gmres_rst[:n], label='GMRES, restart=20')\nplt.semilogy(res_gmres, label='GMRES, no restart')\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Residual norm', fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.legend(fontsize=20)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#biconjugate-gradients",
    "href": "lectures/lecture-14/lecture-14.html#biconjugate-gradients",
    "title": "Questions?",
    "section": "BiConjugate Gradients",
    "text": "BiConjugate Gradients\nThere are two options:\n\nUse \\mathcal{K}(A^* A, A^* f) to generate the subspace. That leads to square of condition number\nInstead, use two Krylov subspaces \\mathcal{K}(A) and \\mathcal{K}(A^*) to generate two basises that are biorthogonal (so-called biorthogonal Lanczos).\n\nThe goal is to compute the Petrov-Galerkin projection\nW^* A V \\widehat{x} = W^* f\nwith columns W from the Krylov subspace of A^*, V from A (cf. with CG case).\nThat may lead to instabilities if we try to recompute the solutions in the efficient way. It is related to the pivoting (which we did not use in CG), and it is not naturally implemented here."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#notes-about-bicg",
    "href": "lectures/lecture-14/lecture-14.html#notes-about-bicg",
    "title": "Questions?",
    "section": "Notes about BiCG",
    "text": "Notes about BiCG\nA practical implementation of BiCG uses two-sided Lanczos process: generating Krylov subspace for A and A^{\\top}\nIn partucular\n\n\\alpha_j = \\frac{(r_j, \\hat{r}_j)}{(Ap_j, \\hat{p}_j)}\n$x_{j+1} = x_j + _j p_j $\nr_{j+1} = r_j - \\alpha_j Ap_j\n\\hat{r}_{j+1} = \\hat{r}_j - \\alpha_j A^{\\top}\\hat{p}_j\n\\beta_j = \\frac{(r_{j+1}, \\hat{r}_{j+1})}{(r_j, \\hat{r}_j)}\np_{j+1} = r_{j+1} + \\beta_j p_j\n\\hat{p}_{j+1} = \\hat{r}_{j+1} - \\beta_j \\hat{p}_j\n\nNow we move to the stable version of the BiCG method"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#bicgstab",
    "href": "lectures/lecture-14/lecture-14.html#bicgstab",
    "title": "Questions?",
    "section": "BiCGStab",
    "text": "BiCGStab\n\nBiCGStab is frequently used, and represent a stabilized version of BiCG. It has faster and smoother convergence than original BiCG method.\nThe formulas can be found, for example, here\nIt is a combination of BiCG step followed by GMRES(1) step in order to smooth the convergence.\nFor more details, please consult the book “Iterative Krylov Methods for Large Linear Systems” by H. Van-der Vorst.\n\nA short demo to compare “Stabilized” vs “Non-stabilized” versions.\n\nimport scipy.sparse.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\nn = 600\n\nex = np.ones(n);\nA = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr') \nrhs = np.random.randn(n)\n\n# ee = sp.sparse.eye(n)\n# A = -sp.sparse.spdiags(np.vstack((ex,  -(2 + 1./n)*ex, (1 + 1./n) * ex)), [-1, 0, 1], n, n, 'csr')\n# A = sp.sparse.kron(A, ee) + sp.sparse.kron(ee, A)\n# rhs = np.ones(n * n)\n\nprint(\"Dimension of the linear system = {}\".format(A.shape[0]))\n\nres_all_bicg = []\nres_all_bicgstab = []\ndef bicg_cl(x):\n    res_all_bicg.append(np.linalg.norm(A.dot(x) - rhs))\n\ndef bicgstab_cl(x):\n    res_all_bicgstab.append(np.linalg.norm(A.dot(x) - rhs))\n\nres_gmres_rst = []\nres_gmres = []\ndef gmres_rst_cl(r):\n    res_gmres_rst.append(np.linalg.norm(r))\n    \ndef gmres_rst(r):\n    res_gmres.append(np.linalg.norm(r))\n\nsol2 = scipy.sparse.linalg.gmres(A, rhs, restart=20, callback=gmres_rst_cl)\nsol2 = scipy.sparse.linalg.gmres(A, rhs, restart=n, callback=gmres_rst)\n\n    \nsol2 = scipy.sparse.linalg.bicg(A, rhs, x0=np.zeros(A.shape[0]), callback=bicg_cl)\nsol2 = scipy.sparse.linalg.bicgstab(A, rhs, x0=np.zeros(A.shape[0]), callback=bicgstab_cl)\nres_all_bicg = np.array(res_all_bicg)/res_all_bicg[0]\nres_all_bicgstab = np.array(res_all_bicgstab)/res_all_bicgstab[0]\nres_gmres_rst = np.array(res_gmres_rst)/res_gmres_rst[0]\nres_gmres = np.array(res_gmres)/res_gmres[0]\n\nlim = 500\nplt.semilogy(res_all_bicgstab[:lim], marker='.',color='k', label='BiCGStab')\nplt.semilogy(res_all_bicg[:lim], marker='x',color='r', label='BiCG')\nplt.semilogy(res_gmres_rst[:lim], marker='.',color='y', label='GMRES res(n)')\nplt.semilogy(res_gmres[:lim], marker='x',color='g', label='GMRES')\n\nplt.xlabel('Iteration number', fontsize=20)\nplt.ylabel('Retative residual norm', fontsize=20)\nplt.legend(loc='best', fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\nDimension of the linear system = 600"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#nonlinear-gmres-or-anderson-acceleration",
    "href": "lectures/lecture-14/lecture-14.html#nonlinear-gmres-or-anderson-acceleration",
    "title": "Questions?",
    "section": "“Nonlinear GMRES” or Anderson acceleration",
    "text": "“Nonlinear GMRES” or Anderson acceleration\nWe can apply the GMRES-like idea to speed up the convergence of a given fixed-point iteration\nx_{k+1} = \\Phi(x_k).\nThis was actually older than the GMRES, and known as an Direct Inversion in Iterated Subspaces in Quantum Chemistry, or Anderson Acceleration.\nIdea: use history for the update\nx_{k+1} = \\Phi(x_k) + \\sum_{s=1}^m \\alpha_s (x_{k - s} - \\Phi(x_{k - s})), \nand the parameters \\alpha_s are selected to minimize the norm of the residual\n \\min_{\\alpha} \\left \\| \\sum_{s=1}^m \\alpha_s (x_{k - s} - \\Phi(x_{k - s})) \\right\\|_2, \\quad \\sum_{s=1}^m \\alpha_s = 1\nMore details see in the original paper"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#battling-the-condition-number",
    "href": "lectures/lecture-14/lecture-14.html#battling-the-condition-number",
    "title": "Questions?",
    "section": "Battling the condition number",
    "text": "Battling the condition number\n\nThe condition number problem is un-avoidable if only the matrix-by-vector product is used.\nThus we need an army of preconditioners to solve it.\nThere are several general purpose preconditioners that we can use, but often for a particular problem a special design is needed."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#preconditioner-general-concept",
    "href": "lectures/lecture-14/lecture-14.html#preconditioner-general-concept",
    "title": "Questions?",
    "section": "Preconditioner: general concept",
    "text": "Preconditioner: general concept\nThe general concept of the preconditioner is simple:\nGiven a linear system\nA x = f,\nwe want to find the matrix P_R and/or P_L such that\n\nCondition number of AP_R^{-1} (right preconditioner) or P^{-1}_LA (right preconditioner) or P^{-1}_L A P_R^{-1} is better than for A\nWe can easily solve P_Ly = g or P_Ry = g for any g (otherwise we could choose e.g. P_L = A)\n\nThen we solve for (right preconditioner)\n AP_R^{-1} y = f \\quad \\Rightarrow \\quad P_R x = y\nor (left preconditioner)\n P_L^{-1} A x = P_L^{-1}f, or even both  P_L^{-1} A P_R^{-1} y = P_L^{-1}f \\quad \\Rightarrow \\quad P_R x = y.\nThe best choice is of course P = A, but this does not make life easier.\nOne of the ideas is to use other iterative methods (beside Krylov) as preconditioners."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#other-iterative-methods-as-preconditioners",
    "href": "lectures/lecture-14/lecture-14.html#other-iterative-methods-as-preconditioners",
    "title": "Questions?",
    "section": "Other iterative methods as preconditioners",
    "text": "Other iterative methods as preconditioners\nThere are other iterative methods that we have not mentioned.\n\nJacobi method\nGauss-Seidel\nSOR(\\omega) (Successive over-relaxation) and its symmetric modification SSOR(\\omega)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#jacobi-method-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#jacobi-method-as-preconditioner",
    "title": "Questions?",
    "section": "Jacobi method (as preconditioner)",
    "text": "Jacobi method (as preconditioner)\nConsider again the matrix with non-zero diagonal. To get the Jacobi method you express the diagonal element:\na_{ii} x_i = -\\sum_{i \\ne j} a_{ij} x_j + f_i\nand use this to iteratively update x_i:\n x_i^{(k+1)} = -\\frac{1}{a_{ii}}\\left( \\sum_{i \\ne j} a_{ij} x_j^{(k)} + f_i \\right),\nor in the matrix form\n\nx^{(k+1)} = D^{-1}\\left((D-A)x^{(k)} + f\\right)\n\nwhere D = \\mathrm{diag}(A) and finally\n\nx^{(k+1)} = x^{(k)} - D^{-1}(Ax^{(k)} - f).\n\nSo, Jacobi method is nothing, but simple Richardson iteration with \\tau=1 and left preconditioner P = D - diagonal of a matrix. Therefore we will refer to P = \\mathrm{diag}(A) as Jacobi preconditioner. Note that it can be used for any other method like Chebyshev or Krylov-type methods."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#properties-of-the-jacobi-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#properties-of-the-jacobi-preconditioner",
    "title": "Questions?",
    "section": "Properties of the Jacobi preconditioner",
    "text": "Properties of the Jacobi preconditioner\nJacobi preconditioner:\n\nVery easy to compute and apply\nWorks well for diagonally dominant matrices (remember the Gershgorin circle theorem!)\nUseless if all diagonal entries are the same (proportional to the identity matrix)"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-as-preconditioner",
    "title": "Questions?",
    "section": "Gauss-Seidel (as preconditioner)",
    "text": "Gauss-Seidel (as preconditioner)\nAnother well-known method is Gauss-Seidel method.\nIts canonical form is very similar to the Jacobi method, with a small difference. When we update x_i as\nx_i^{(k+1)} := -\\frac{1}{a_{ii}}\\left( \\sum_{j =1}^{i-1} a_{ij} x_j^{(k+1)} +\\sum_{j = i+1}^n a_{ij} x_j^{(k)} - f_i \\right)\nwe use it in the later updates. In the Jacobi method we use the full vector from the previous iteration.\nIts matrix form is more complicated."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-matrix-version",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-matrix-version",
    "title": "Questions?",
    "section": "Gauss-Seidel: matrix version",
    "text": "Gauss-Seidel: matrix version\nGiven A = A^{*} &gt; 0 we have\nA = L + D + L^{*},\nwhere D is the diagonal of A, L is lower-triangular part with zero on the diagonal.\nOne iteration of the GS method reads\n\nx^{(k+1)} = x^{(k)} - (L + D)^{-1}(Ax^{(k)} - f).\n\nand we refer to the preconditioner P = L+D as Gauss-Seidel preconditioner.\nGood news: $(I - (L+D)^{-1} A) &lt; 1, $ where \\rho is the spectral radius,\ni.e. for a positive definite matrix GS-method always converges."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#gauss-seidel-and-coordinate-descent",
    "href": "lectures/lecture-14/lecture-14.html#gauss-seidel-and-coordinate-descent",
    "title": "Questions?",
    "section": "Gauss-Seidel and coordinate descent",
    "text": "Gauss-Seidel and coordinate descent\nGS-method can be viewed as a coordinate descent method, applied to the energy functional\nF(x) = (Ax, x) - 2(f, x)\nwith the iteration\nx_i := \\arg \\min_z F(x_1, \\ldots, x_{i-1}, z, x_{i+1}, \\ldots, x_d).\nMoreover, the order in which we eliminate variables, is really important!"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#side-note-nonlinear-gauss-seidel-a.k.a-coordinate-descent",
    "href": "lectures/lecture-14/lecture-14.html#side-note-nonlinear-gauss-seidel-a.k.a-coordinate-descent",
    "title": "Questions?",
    "section": "Side note: Nonlinear Gauss-Seidel (a.k.a coordinate descent)",
    "text": "Side note: Nonlinear Gauss-Seidel (a.k.a coordinate descent)\nIf F is given, and we optimize one coordinate at a time, we have\nx_i := \\arg \\min_z F(x_1, \\ldots, x_{i-1}, z, x_{i+1}, \\ldots, x_d).\nNote the convergence result for block coordinate descent for the case of a general functional F:\nit converges locally with the speed of the GS-method applied to the Hessian\nH = \\nabla^2 F\nof the functional.\nThus, if F is twice differentiable and x_* is the local minimum, then H &gt; 0 can Gauss-Seidel converges."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#successive-overrelaxation-as-preconditioner",
    "href": "lectures/lecture-14/lecture-14.html#successive-overrelaxation-as-preconditioner",
    "title": "Questions?",
    "section": "Successive overrelaxation (as preconditioner)",
    "text": "Successive overrelaxation (as preconditioner)\nWe can even introduce a parameter \\omega into the GS-method preconditioner, giving a successive over-relaxation (SOR(\\omega)) method:\n\nx^{(k+1)} = x^{(k)} - \\omega (D + \\omega L)^{-1}(Ax^{(k)} - f).\n\nP = \\frac{1}{\\omega}(D+\\omega L).\n\nConverges for 0&lt;\\omega &lt; 2.\nOptimal selection of \\omega is not trivial. If the Jacobi method converges, then \\omega^* = \\frac{2}{1 + \\sqrt{1 - \\rho_J^2}}, where \\rho_J is spectral radius of Jacobi iterations\nNote that \\omega = 1 gives us a Gauss-Seidel preconditioner."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#preconditioners-for-sparse-matrices",
    "href": "lectures/lecture-14/lecture-14.html#preconditioners-for-sparse-matrices",
    "title": "Questions?",
    "section": "Preconditioners for sparse matrices",
    "text": "Preconditioners for sparse matrices\n\nIf A is sparse, one iteration of Jacobi, GS and SOR method is cheap (what complexity?).\nFor GS, we need to solve linear system with a sparse triangular matrix L, which costs \\mathcal{O}(nnz).\nFor sparse matrices, however, there are more complicated algorithms, based on the idea of approximate LU-decomposition.\nRemember the motivation for CG: possibility of the early stopping, how to do approximate LU-decomposition for a sparse matrix?"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#remember-the-gaussian-elimination",
    "href": "lectures/lecture-14/lecture-14.html#remember-the-gaussian-elimination",
    "title": "Questions?",
    "section": "Remember the Gaussian elimination",
    "text": "Remember the Gaussian elimination\n\nDecompose the matrix A in the form\n\nA = P_1 L U P^{\\top}_2, \nwhere P_1 and P_2 are certain permutation matrices (which do the pivoting).\n\nThe most natural idea is to use sparse L and U.\nIt is not possible without fill-in growth for example for matrices, coming from 2D/3D Partial Differential equations (PDEs).\nWhat to do?"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#incomplete-lu",
    "href": "lectures/lecture-14/lecture-14.html#incomplete-lu",
    "title": "Questions?",
    "section": "Incomplete LU",
    "text": "Incomplete LU\n\nSuppose you want to eliminate a variable x_1, and the equations have the form\n\n5 x_1 + x_4 + x_{10} = 1, \\quad 3 x_1 + x_4 + x_8 = 0, \\ldots,\nand in all other equations x_1 are not present.\n\nAfter the elimination, only x_{10} will enter additionally to the second equation (new fill-in).\n\nx_4 + x_8 + 3(1 - x_4 - x_{10})/5 = 0\n\nIn the Incomplete LU case (actually, ILU(0)) we just throw away the new fill-in."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#incomplete-lu-formal-definition",
    "href": "lectures/lecture-14/lecture-14.html#incomplete-lu-formal-definition",
    "title": "Questions?",
    "section": "Incomplete-LU: formal definition",
    "text": "Incomplete-LU: formal definition\nWe run the usual LU-decomposition cycle, but avoid inserting non-zeros other than the initial non-zero pattern.\n    L = np.zeros((n, n))\n    U = np.zeros((n, n))\n    for k in range(n): #Eliminate one row   \n        L[k, k] = 1\n        for i in range(k+1, n):\n            L[i, k] = a[i, k] / a[k, k]\n            for j in range(k+1, n):\n                a[i, j] = a[i, j] - L[i, k] * a[k, j]  #New fill-ins appear here\n        for j in range(k, n):\n            U[k, j] = a[k, j]"
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#iluk",
    "href": "lectures/lecture-14/lecture-14.html#iluk",
    "title": "Questions?",
    "section": "ILU(k)",
    "text": "ILU(k)\n\nYousef Saad (who is the author of GMRES) also had a seminal paper on the Incomplete LU decomposition\nA good book on the topic is Iterative methods for sparse linear systems by Y. Saad, 2003\nAnd he proposed ILU(k) method, which has a nice interpretation in terms of graphs."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#iluk-idea",
    "href": "lectures/lecture-14/lecture-14.html#iluk-idea",
    "title": "Questions?",
    "section": "ILU(k): idea",
    "text": "ILU(k): idea\n\nThe idea of ILU(k) is very instructive and is based on the connection between sparse matrices and graphs.\nSuppose you have an n \\times n matrix A and a corresponding adjacency graph.\nThen we eliminate one variable (vertex) and get a smaller system of size (n-1) \\times (n-1).\nNew edges (=fill-in) appears between high-order neighbors."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#lu-graphs",
    "href": "lectures/lecture-14/lecture-14.html#lu-graphs",
    "title": "Questions?",
    "section": "LU & graphs",
    "text": "LU & graphs\n\nThe new edge can appear only between vertices that had common neighbour: it means, that they are second-order neigbours.\n\nThis is also a sparsity pattern of the matrix A^2.\nThe ILU(k) idea is to leave only the elements in L and U that are k-order neighbours in the original graph.\nThe ILU(2) is very efficient, but for some reason completely abandoned (i.e. there is no implementation in MATLAB and SciPy).\nThere is an original Sparsekit software by Saad, which works quite well."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#ilu-thresholded-ilut",
    "href": "lectures/lecture-14/lecture-14.html#ilu-thresholded-ilut",
    "title": "Questions?",
    "section": "ILU Thresholded (ILUT)",
    "text": "ILU Thresholded (ILUT)\nA much more popular approach is based on the so-called thresholded LU.\nYou do the standard Gaussian elimination with fill-ins, but either:\n\nThrow away elements that are smaller than threshold, and/or control the amount of non-zeros you are allowed to store.\nThe smaller is the threshold, the better is the preconditioner, but more memory it takes.\n\nIt is denoted ILUT(\\tau)."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#symmetric-positive-definite-case",
    "href": "lectures/lecture-14/lecture-14.html#symmetric-positive-definite-case",
    "title": "Questions?",
    "section": "Symmetric positive definite case",
    "text": "Symmetric positive definite case\n\nIn the SPD case, instead of incomplete LU you can use Incomplete Cholesky, which is twice faster and consumes twice less memory.\nHowever, Incomplete Cholesky may not exist.\nBoth ILUT and Ichol are implemented in SciPy and you can try."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#second-order-lu-preconditioners",
    "href": "lectures/lecture-14/lecture-14.html#second-order-lu-preconditioners",
    "title": "Questions?",
    "section": "Second-order LU preconditioners",
    "text": "Second-order LU preconditioners\n\nThere is a more efficient (but much less popular due to the limit of open-source implementations) second-order LU factorization proposed by I. Kaporin\nThe idea (for symmetric matrices) is to approximate the matrix in the form\n\nA \\approx U_2 U^{\\top}_2 + U^{\\top}_2 R_2 + R^{\\top}_2 U_2,\nwhich is just the expansion of the UU^{\\top} with respect to the perturbation of U.\n\nU_1 and U_2 are upper-triangular and sparse, whereare R_2 is small with respect to the drop tolerance parameter."
  },
  {
    "objectID": "lectures/lecture-14/lecture-14.html#summary-of-this-part",
    "href": "lectures/lecture-14/lecture-14.html#summary-of-this-part",
    "title": "Questions?",
    "section": "Summary of this part",
    "text": "Summary of this part\n\nJacobi, Gauss-Seidel, SSOR (as preconditioners)\nIncomplete LU, three flavours: ILU(k), ILUT, ILU2"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#previous-lecture",
    "href": "lectures/lecture-12/lecture-12.html#previous-lecture",
    "title": "The main topics for today",
    "section": "Previous lecture",
    "text": "Previous lecture\n\nDirect methods for LU\nGraph separators"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#matrix-as-a-black-box",
    "href": "lectures/lecture-12/lecture-12.html#matrix-as-a-black-box",
    "title": "The main topics for today",
    "section": "Matrix as a black box",
    "text": "Matrix as a black box\n\nWe have now an absolutely different view on a matrix: matrix is now a linear operator, that acts on a vector,\nand this action can be computed in \\mathcal{O}(N) operations.\nThis is the only information we know about the matrix: the  matrix-by-vector product (matvec) \nCan we solve linear systems using only matvecs?\nOf course, we can multiply by the colums of the identity matrix, and recover the full matrix, but it is not what we need."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#richardson-iteration",
    "href": "lectures/lecture-12/lecture-12.html#richardson-iteration",
    "title": "The main topics for today",
    "section": "Richardson iteration",
    "text": "Richardson iteration\nThe simplest idea is the “simple iteration method” or Richardson iteration.\nAx = f, \\tau  (Ax - f) = 0, x - \\tau (Ax - f) = x, x_{k+1} = x_k - \\tau (Ax_k - f),\nwhere \\tau is the iteration parameter, which can be always chosen such that the method converges."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#connection-to-odes",
    "href": "lectures/lecture-12/lecture-12.html#connection-to-odes",
    "title": "The main topics for today",
    "section": "Connection to ODEs",
    "text": "Connection to ODEs\n\nThe Richardson iteration has a deep connection to the Ordinary Differential Equations (ODE).\nConsider a time-dependent problem\n\n\\frac{dy}{dt} + A y = f, \\quad y(0) = y_0.\n\nThen y(t) \\rightarrow A^{-1} f as t \\rightarrow \\infty, and the Euler scheme reads\n\n\\frac{y_{k+1} - y_k}{\\tau} = -A y_k + f.\nwhich leads to the Richardson iteration\n\n    y_{k+1} = y_k - \\tau(Ay_k -f)"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#convergence-of-the-richardson-method",
    "href": "lectures/lecture-12/lecture-12.html#convergence-of-the-richardson-method",
    "title": "The main topics for today",
    "section": "Convergence of the Richardson method",
    "text": "Convergence of the Richardson method\n\nLet x_* be the solution; introduce an error e_k = x_{k} - x_*, then\n\n\n     e_{k+1} = (I - \\tau A) e_k,\n\ntherefore if \\Vert I - \\tau A \\Vert &lt; 1 in any norm, the iteration converges.\n\nFor symmetric positive definite case it is always possible to select \\tau such that the method converges.\nWhat about the non-symmetric case? Below demo will be presented…"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#optimal-parameter-choice",
    "href": "lectures/lecture-12/lecture-12.html#optimal-parameter-choice",
    "title": "The main topics for today",
    "section": "Optimal parameter choice",
    "text": "Optimal parameter choice\n\nThe choise of \\tau that minimizes \\|I - \\tau A\\|_2 for A = A^* &gt; 0 is (prove it!)\n\n\n  \\tau_\\mathrm{opt} = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}.\n\nwhere \\lambda_{\\min} is the minimal eigenvalue, and \\lambda_{\\max} is the maximal eigenvalue of the matrix A.\n\nSo, to find optimal parameter, we need to know the bounds of the spectrum of the matrix A, and we can compute it by using power method."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#condition-number-and-convergence-speed",
    "href": "lectures/lecture-12/lecture-12.html#condition-number-and-convergence-speed",
    "title": "The main topics for today",
    "section": "Condition number and convergence speed",
    "text": "Condition number and convergence speed\nEven with the optimal parameter choice, the error at the next step satisfies\n\\|e_{k+1}\\|_2 \\leq q \\|e_k\\|_2 , \\quad\\rightarrow \\quad \\|e_k\\|_2 \\leq q^{k} \\|e_0\\|_2,\nwhere\n\n   q = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{\\mathrm{cond}(A) - 1}{\\mathrm{cond}(A)+1},\n\n\\mathrm{cond}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \\quad \\text{for} \\quad A=A^*&gt;0\nis the condition number of A.\nLet us do some demo…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rc(\"text\", usetex=True)\nimport scipy as sp\nimport scipy.sparse\nimport scipy.sparse.linalg as spla\nimport scipy\nfrom scipy.sparse import csc_matrix\nn = 500\nex = np.ones(n);\nA = sp.sparse.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\nev1, vec = spla.eigsh(A, k=2, which='LA')\nev2, vec = spla.eigsh(A, k=2, which='SA')\nlam_max = ev1[0]\nlam_min = ev2[0]\n\ntau_opt = 2.0/(lam_max + lam_min)\n\nfig, ax = plt.subplots()\nplt.close(fig)\n\nniters = 1000\nx = np.zeros(n)\nres_richardson = []\nfor i in range(niters):\n    rr = A.dot(x) - rhs\n    x = x - tau_opt * rr\n    res_richardson.append(np.linalg.norm(rr))\n#Convergence of an ordinary Richardson (with optimal parameter)\nplt.semilogy(res_richardson)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nprint(\"Maximum eigenvalue = {}, minimum eigenvalue = {}\".format(lam_max, lam_min))\ncond_number = lam_max.real / lam_min.real\nprint(\"Condition number = {}\".format(cond_number))\n#print(np.array(res_richardson)[1:] / np.array(res_richardson)[:-1])\nprint(\"Theoretical factor: {}\".format((cond_number - 1) / (cond_number + 1)))\n\nMaximum eigenvalue = 3.99984271815512, minimum eigenvalue = 3.932084756989659e-05\nCondition number = 101723.207035276\nTheoretical factor: 0.9999803389964071\n\n\n\n\n\n\n\n\n\n\nThus, for ill-conditioned matrices the error of the simple iteration method decays very slowly.\nThis is another reason why condition number is so important:\n\nBesides the bound on the error in the solution, it also gives an estimate of the number of iterations for the iterative methods.\n\nMain questions for the iterative method is how to make the matrix better conditioned.\nThe answer is  use preconditioners. Preconditioners will be discussed in further lectures.\n\n\nConsider non-hermitian matrix A\nPossible cases of Richardson iteration behaviour: - convergence - divergence - almost stable trajectory\nQ: how can we identify our case before running iterative method?\n\n# B = np.random.randn(2, 2)\nB = np.array([[1, 2], [-1, 0]])\n# B = np.array([[0, 1], [-1, 0]])\nx_true = np.zeros(2)\nf = B.dot(x_true)\neigvals = np.linalg.eigvals(B)\nprint(\"Spectrum of the matrix = {}\".format(eigvals))\n\n# Run Richardson iteration\nx = np.array([0, -1])\ntau = 1e-2\nconv_x = [x]\nr = B.dot(x) - f\nconv_r = [np.linalg.norm(r)]\nnum_iter = 1000\nfor i in range(num_iter):\n    x = x - tau * r\n    conv_x.append(x)\n    r = B.dot(x) - f\n    conv_r.append(np.linalg.norm(r))\n\nSpectrum of the matrix = [0.5+1.32287566j 0.5-1.32287566j]\n\n\n\nplt.semilogy(conv_r)\nplt.xlabel(\"Number of iteration, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm\", fontsize=20)\n\nText(0, 0.5, 'Residual norm')\n\n\n\n\n\n\n\n\n\n\nplt.scatter([x[0] for x in conv_x], [x[1] for x in conv_x])\nplt.xlabel(\"$x$\", fontsize=20)\nplt.ylabel(\"$y$\", fontsize=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.title(\"$x_0 = (0, -1)$\", fontsize=20)\n\nText(0.5, 1.0, '$x_0 = (0, -1)$')"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#better-iterative-methods",
    "href": "lectures/lecture-12/lecture-12.html#better-iterative-methods",
    "title": "The main topics for today",
    "section": "Better iterative methods",
    "text": "Better iterative methods\nBut before preconditioners, we can use better iterative methods.\nThere is a whole zoo of iterative methods, but we need to know just few of them."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#attempt-1-the-steepest-descent-method",
    "href": "lectures/lecture-12/lecture-12.html#attempt-1-the-steepest-descent-method",
    "title": "The main topics for today",
    "section": "Attempt 1: The steepest descent method",
    "text": "Attempt 1: The steepest descent method\n\nSuppose we change \\tau every step, i.e. \n\n\n   x_{k+1} = x_k - \\tau_k (A x_k - f).\n\n\nA possible choice of \\tau_k is such that it minimizes norm of the current residual\n\n \\tau_k = \\arg\\min_{\\tau} \\|A(x_k - \\tau_k (A x_k - f)) - f\\|_2^2.\n\nThis problem can be solved analytically (derive this solution!)\n\n \\tau_k = \\frac{r_k^{\\top}r_k}{r_k^{\\top}Ar_k}, \\quad r_k = Ax_k - f \n\nThis method is called the steepest descent.\nHowever, it still converges similarly to the Richardson iteration."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#attempt-2-chebyshev-iteration",
    "href": "lectures/lecture-12/lecture-12.html#attempt-2-chebyshev-iteration",
    "title": "The main topics for today",
    "section": "Attempt 2: Chebyshev iteration",
    "text": "Attempt 2: Chebyshev iteration\nAnother way to find \\tau_k is to consider\ne_{k+1} = (I - \\tau_k A) e_k = (I - \\tau_k A) (I - \\tau_{k-1} A)  e_{k-1} = \\ldots = p(A) e_0, \nwhere p(A) is a matrix polynomial (simplest matrix function)\n\n   p(A) = (I - \\tau_k A) \\ldots (I - \\tau_0 A),\n\nand p(0) = 1."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#optimal-choice-of-time-steps",
    "href": "lectures/lecture-12/lecture-12.html#optimal-choice-of-time-steps",
    "title": "The main topics for today",
    "section": "Optimal choice of time steps",
    "text": "Optimal choice of time steps\nThe error is written as\ne_{k+1} = p(A) e_0, \nand hence\n\\|e_{k+1}\\| \\leq \\|p(A)\\| \\|e_0\\|, \nwhere p(0) = 1 and p(A) is a matrix polynomial.\nTo get better error reduction, we need to minimize\n\\Vert p(A) \\Vert\nover all possible polynomials p(x) of degree k+1 such that p(0)=1. We will use \\|\\cdot\\|_2."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros",
    "href": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros",
    "title": "The main topics for today",
    "section": "Polynomials least deviating from zeros",
    "text": "Polynomials least deviating from zeros\nImportant special case: A = A^* &gt; 0.\nThen A = U \\Lambda U^*,\nand\n\\Vert p(A) \\Vert_2 = \\Vert U p(\\Lambda) U^* \\Vert_2 = \\Vert p(\\Lambda) \\Vert_2 = \\max_i |p(\\lambda_i)| \\overset{!}{\\leq}\n\\max_{\\lambda_\\min \\leq \\lambda {\\leq} \\lambda_\\max} |p(\\lambda)|.\nThe latter inequality is the only approximation. Here we make a  crucial assumption  that we do not want to benefit from distribution of spectra between \\lambda_\\min and \\lambda_\\max.\nThus, we need to find a polynomial such that p(0) = 1, that has the least possible deviation from 0 on [\\lambda_\\min, \\lambda_\\max]."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros-2",
    "href": "lectures/lecture-12/lecture-12.html#polynomials-least-deviating-from-zeros-2",
    "title": "The main topics for today",
    "section": "Polynomials least deviating from zeros (2)",
    "text": "Polynomials least deviating from zeros (2)\nWe can do the affine transformation of the interval [\\lambda_\\min, \\lambda_\\max] to the interval [-1, 1]:\n\n\\xi = \\frac{{\\lambda_\\max + \\lambda_\\min - (\\lambda_\\min-\\lambda_\\max)x}}{2}, \\quad x\\in [-1, 1].\n\nThe problem is then reduced to the problem of finding the polynomial least deviating from zero on an interval [-1, 1]."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#exact-solution-chebyshev-polynomials",
    "href": "lectures/lecture-12/lecture-12.html#exact-solution-chebyshev-polynomials",
    "title": "The main topics for today",
    "section": "Exact solution: Chebyshev polynomials",
    "text": "Exact solution: Chebyshev polynomials\nThe exact solution to this problem is given by the famous Chebyshev polynomials of the form\nT_n(x) =  \\cos (n \\arccos x)"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#what-do-you-need-to-know-about-chebyshev-polynomials",
    "href": "lectures/lecture-12/lecture-12.html#what-do-you-need-to-know-about-chebyshev-polynomials",
    "title": "The main topics for today",
    "section": "What do you need to know about Chebyshev polynomials",
    "text": "What do you need to know about Chebyshev polynomials\n\nThis is a polynomial!\nWe can express T_n from T_{n-1} and T_{n-2}:\n\nT_n(x) = 2x T_{n-1}(x) - T_{n-2}(x), \\quad T_0(x)=1, \\quad T_1(x)=x\n\n|T_n(x)| \\leq 1 on x \\in [-1, 1].\nIt has (n+1) alternation points, where the maximal absolute value is achieved (this is the sufficient and necessary condition for the optimality) (Chebyshev alternance theorem, no proof here).\nThe roots are just\n\nn \\arccos x_k = \\frac{\\pi}{2} + \\pi k, \\quad \\rightarrow\\quad x_k = \\cos \\frac{\\pi(2k + 1)}{2n}, \\; k = 0, \\ldots,n-1\nWe can plot them…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nx1 = np.linspace(-1, 1, 128)\nx2 = np.linspace(-1.1, 1.1, 128)\np = np.polynomial.Chebyshev((0, 0, 0, 0, 0, 0, 0, 0, 0, 1), (-1, 1)) #These are Chebyshev series, a proto of \"chebfun system\" in MATLAB\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(x1, p(x1))\nax1.set_title('Interval $x\\in[-1, 1]$')\nax2.plot(x2, p(x2))\nax2.set_title('Interval $x\\in[-1.1, 1.1]$')\n\nText(0.5, 1.0, 'Interval $x\\\\in[-1.1, 1.1]$')"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#convergence-of-the-chebyshev-accelerated-richardson-iteration",
    "href": "lectures/lecture-12/lecture-12.html#convergence-of-the-chebyshev-accelerated-richardson-iteration",
    "title": "The main topics for today",
    "section": "Convergence of the Chebyshev-accelerated Richardson iteration",
    "text": "Convergence of the Chebyshev-accelerated Richardson iteration\nNote that p(x) = (1-\\tau_n x)\\dots (1-\\tau_0 x), hence roots of p(x) are 1/\\tau_i and that we additionally need to map back from [-1,1] to [\\lambda_\\min, \\lambda_\\max]. This results into\n\\tau_i = \\frac{2}{\\lambda_\\max + \\lambda_\\min - (\\lambda_\\max - \\lambda_\\min)x_i}, \\quad x_i = \\cos \\frac{\\pi(2i + 1)}{2n}\\quad i=0,\\dots,n-1\nThe convergence (we only give the result without the proof) is now given by\n\n   e_{k+1} \\leq C q^k e_0, \\quad q = \\frac{\\sqrt{\\mathrm{cond}(A)}-1}{\\sqrt{\\mathrm{cond}(A)}+1},\n\nwhich is better than in the Richardson iteration.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 64\nex = np.ones(n);\n\nA = sp.sparse.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\nev1, vec = spla.eigsh(A, k=2, which='LA')\nev2, vec = spla.eigsh(A, k=2, which='SA')\nlam_max = ev1[0]\nlam_min = ev2[0]\n\nniters = 64\nroots = [np.cos((np.pi * (2 * i + 1)) / (2 * niters)) for i in range(niters)]\ntaus = [(lam_max + lam_min - (lam_min - lam_max) * r) / 2 for r in roots]\nx = np.zeros(n)\nr = A.dot(x) - rhs\nres_cheb = [np.linalg.norm(r)]\n\nprint(1/np.array(taus))\n\nfor i in range(niters):\n    x = x - 1.0 / taus[i] * r\n    r = A.dot(x) - rhs\n    res_cheb.append(np.linalg.norm(r))\n    \nplt.semilogy(res_richardson, label=\"Richardson\")\nplt.semilogy(res_cheb, label=\"Chebyshev\")\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n[2.50622630e-01 2.50924658e-01 2.51530169e-01 2.52442095e-01\n 2.53664865e-01 2.55204461e-01 2.57068471e-01 2.59266178e-01\n 2.61808653e-01 2.64708878e-01 2.67981890e-01 2.71644951e-01\n 2.75717745e-01 2.80222614e-01 2.85184826e-01 2.90632893e-01\n 2.96598938e-01 3.03119118e-01 3.10234126e-01 3.17989766e-01\n 3.26437629e-01 3.35635888e-01 3.45650225e-01 3.56554925e-01\n 3.68434169e-01 3.81383570e-01 3.95511991e-01 4.10943730e-01\n 4.27821135e-01 4.46307759e-01 4.66592186e-01 4.88892691e-01\n 5.13462953e-01 5.40599097e-01 5.70648426e-01 6.04020340e-01\n 6.41200067e-01 6.82766079e-01 7.29412363e-01 7.81977158e-01\n 8.41480389e-01 9.09172947e-01 9.86602291e-01 1.07570085e+00\n 1.17890673e+00 1.29933105e+00 1.44099335e+00 1.60915915e+00\n 1.81083297e+00 2.05549470e+00 2.35622630e+00 2.73148428e+00\n 3.20797673e+00 3.82550533e+00 4.64546321e+00 5.76650218e+00\n 7.35516474e+00 9.71018736e+00 1.34098577e+01 1.96891165e+01\n 3.15513749e+01 5.76947930e+01 1.29218671e+02 3.40581915e+02]\n\n\n\n\n\n\n\n\n\n\nWhat happened with great Chebyshev iterations?\n\nniters = 64\nroots = [np.cos((np.pi * (2 * i + 1)) / (2 * niters)) for i in range(niters)]\ntaus = [(lam_max + lam_min - (lam_min - lam_max) * r) / 2 for r in roots]\nx = np.zeros(n)\nr = A.dot(x) - rhs\nres_cheb_even = [np.linalg.norm(r)]\n#print(taus)\n\n# Implementation may be non-optimal if number of iterations is not power of two\ndef leb_shuffle_2n(n):\n    if n == 1:\n        return np.array([0,], dtype=int)\n    else:\n        prev = leb_shuffle_2n(n // 2)\n        ans = np.zeros(n, dtype=int)\n        ans[::2] = prev\n        ans[1::2] = n - 1 - prev\n        return ans\n\ngood_perm_even = leb_shuffle_2n(niters)\nprint(good_perm_even, len(good_perm_even))\n# good_perm_even = np.random.permutation([i for i in range(niters)])\nts = np.array(taus)[good_perm_even]\nplt.figure()\nplt.plot(1/ts)\nfor i in range(niters):\n    x = x - 1.0/taus[good_perm_even[i]] * r\n    r = A.dot(x) - rhs\n    res_cheb_even.append(np.linalg.norm(r))\nplt.figure()\n    \nplt.semilogy(res_richardson, label=\"Richardson\")\nplt.semilogy(res_cheb_even, label=\"Chebyshev\")\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of iterations, $k$\", fontsize=20)\nplt.ylabel(\"Residual norm, $\\|Ax_k - b\\|_2$\", fontsize=20)\nplt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n[ 0 63 31 32 15 48 16 47  7 56 24 39  8 55 23 40  3 60 28 35 12 51 19 44\n  4 59 27 36 11 52 20 43  1 62 30 33 14 49 17 46  6 57 25 38  9 54 22 41\n  2 61 29 34 13 50 18 45  5 58 26 37 10 53 21 42] 64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPermutation of roots of Chebyshev polynomial has crucial effect on convergence\nOn the optimal permutation you can read in paper (V. Lebedev, S. Finogenov 1971) (ru, en)\n\n\n\nChebfun project\n\nOpensource project for numerical computing (Python and Matlab interfaces)\nIt is based on numerical algorithms working with piecewise polynomial interpolants and Chebyshev polynomials\nThis project was initiated by Nick Trefethen and his student Zachary Battles in 2002, see paper on chebfun project in SISC\nChebfun toolbox focuses mostly on the following problems\n\nApproximation\nQuadrature\nODE\nPDE\nRootfinding\n1D global optimization"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#beyond-chebyshev",
    "href": "lectures/lecture-12/lecture-12.html#beyond-chebyshev",
    "title": "The main topics for today",
    "section": "Beyond Chebyshev",
    "text": "Beyond Chebyshev\n\nWe have made an important assumption about the spectrum: it is contained within an interval over the real line (and we need to know the bounds)\nIf the spectrum is contained within two intervals, and we know the bounds, we can also put the optimization problem for the optimal polynomial."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#spectrum-of-the-matrix-contained-in-multiple-segments",
    "href": "lectures/lecture-12/lecture-12.html#spectrum-of-the-matrix-contained-in-multiple-segments",
    "title": "The main topics for today",
    "section": "Spectrum of the matrix contained in multiple segments",
    "text": "Spectrum of the matrix contained in multiple segments\n\nFor the case of two segments the best polynomial is given by Zolotarev polynomials (expressed in terms of elliptic functions). Original paper was published in 1877, see details here\nFor the case of more than two segments the best polynomial can be expressed in terms of hyperelliptic functions"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#how-can-we-make-it-better",
    "href": "lectures/lecture-12/lecture-12.html#how-can-we-make-it-better",
    "title": "The main topics for today",
    "section": "How can we make it better",
    "text": "How can we make it better\n\nThe implementation of the Chebyshev acceleration requires the knowledge of the spectrum.\nIt only stores the previous vector x_k and computes the new correction vector\n\nr_k = A x_k - f.\n\nIt belongs to the class of two-term iterative methods, i.e. it approximates x_{k+1} using 2 vectors: x_k and r_k.\nIt appears that if we store more vectors, then we can go without the spectrum estimation (and better convergence in practice)!"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#crucial-point-krylov-subspace",
    "href": "lectures/lecture-12/lecture-12.html#crucial-point-krylov-subspace",
    "title": "The main topics for today",
    "section": "Crucial point: Krylov subspace",
    "text": "Crucial point: Krylov subspace\nThe Chebyshev method produces the approximation of the form\nx_{k+1} = x_0 + p(A) r_0,\ni.e. it lies in the Krylov subspace of the matrix which is defined as\n\n   \\mathcal{K}_k(A, r_0) = \\mathrm{Span}(r_0, Ar_0, A^2 r_0, \\ldots, A^{k-1}r_0 )\n\nThe most natural approach then is to find the vector in this linear subspace that minimizes certain norm of the error"
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#idea-of-krylov-methods",
    "href": "lectures/lecture-12/lecture-12.html#idea-of-krylov-methods",
    "title": "The main topics for today",
    "section": "Idea of Krylov methods",
    "text": "Idea of Krylov methods\nThe idea is to minimize given functional: - Energy norm of error for systems with hermitian positive-definite matrices (CG method). - Residual norm for systems with general matrices (MINRES and GMRES methods). - Rayleigh quotient for eigenvalue problems (Lanczos method).\nTo make methods practical one has to 1. Orthogonalize vectors A^i r_0 of the Krylov subspace for stability (Lanczos process). 2. Derive recurrent formulas to decrease complexity.\nWe will consider these methods in details on the next lecture."
  },
  {
    "objectID": "lectures/lecture-12/lecture-12.html#take-home-message",
    "href": "lectures/lecture-12/lecture-12.html#take-home-message",
    "title": "The main topics for today",
    "section": "Take home message",
    "text": "Take home message\n\nMain idea of iterative methods\nRichardson iteration: hermitian and non-hermitian case\nChebyshev acceleration\nDefinition of Krylov subspace\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/general_info.html#about-the-course",
    "href": "lectures/general_info.html#about-the-course",
    "title": "",
    "section": "About the course",
    "text": "About the course\n\nThe course is about faster and more accurate computations (in scientific computing, in ML…)\nThree main problems:\n\nsolving linear systems\nfinding eigenvalues and eigenvectors\ncomputing matrix functions (matrix exponential especially)\n\nThe methods are different for small-scale and large-scale problems:\n\nusing matrix decompositions\niterative methods"
  },
  {
    "objectID": "lectures/general_info.html#learning-outcomes",
    "href": "lectures/general_info.html#learning-outcomes",
    "title": "",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nSolve medium-scale numerical linear algebra problems (solve linear systems, compute eigenvalues and eigenvectors, solve linear least squares) using matrix factorizations\nIterative methods for sparse/structured systems\nFind which methods are the most appropriate for the particular problem\nFind appropriate software"
  },
  {
    "objectID": "lectures/general_info.html#approximate-syllabus",
    "href": "lectures/general_info.html#approximate-syllabus",
    "title": "",
    "section": "(Approximate) Syllabus",
    "text": "(Approximate) Syllabus\n\nWeek 1: Intro and floating points arithmetics, matrices, vectors, norms, ranks.\nWeek 2: Matrix decompositions 1: SVD and its applications\nWeek 3: Matrix decompositions 2: Linear systems and LU, eigendecomposition\nWeek 4: Matrix decompositions 3: QR and Schur + project proposal deadline\nWeek 5: Sparse and structured matrices, iterative methods (part 1)\nWeek 6: Iterative methods (part 2), matrix functions and advanced topics\nWeek 7: Oral exam (two days)\nWeek 8: Application period and project presentations"
  },
  {
    "objectID": "lectures/general_info.html#lecture-access",
    "href": "lectures/general_info.html#lecture-access",
    "title": "",
    "section": "Lecture access",
    "text": "Lecture access\n\nLectures can be downloaded and viewed on GitHub: (link distributed soon)\nTelegram chat for the course (link distributed soon)"
  },
  {
    "objectID": "lectures/general_info.html#team",
    "href": "lectures/general_info.html#team",
    "title": "",
    "section": "Team",
    "text": "Team\nCourse instructor: Ivan Oseledets\nTAs: Will be distributed soon"
  },
  {
    "objectID": "lectures/general_info.html#how-do-we-grade",
    "href": "lectures/general_info.html#how-do-we-grade",
    "title": "",
    "section": "How do we grade",
    "text": "How do we grade\n\n50% homework. Includes 3 problem sets with coding in Python and theoretical problems.\n20% final exam - the format and rules will be announced later\n15% Midterm test – written test with simple problems.\n15% project. More details and policies about this activity will be presented later.\n10% Bonus. Solve bonus tasks from HW.\n\nTotal maximum is 110%."
  },
  {
    "objectID": "lectures/general_info.html#problem-sets",
    "href": "lectures/general_info.html#problem-sets",
    "title": "",
    "section": "Problem sets",
    "text": "Problem sets\n\nHomework is distributed in Jupyter notebooks\nProblem sets contain both theoretical and programming tasks\nNo hand-written solutions are accepted, only Markdown and \\LaTeX text in the single Jupyter Notebook file are Ok."
  },
  {
    "objectID": "lectures/general_info.html#problem-set-rules",
    "href": "lectures/general_info.html#problem-set-rules",
    "title": "",
    "section": "Problem set rules",
    "text": "Problem set rules\n\nSolutions must be submitted on Canvas before the deadline\nDeadlines are strict. After the deadline Canvas submission is closed. Only the last submission will be graded.\nDeadline for every problem set will be announced at the moment of publishing\nProblem sets will be checked for plagiarism. If noticed, the score will be divided by a number of similar works. You can use ChatGPT on them, but you are responsible for the quality and if we have suspicions, we will arrange inperson meetings about ‘understanding’ of them."
  },
  {
    "objectID": "lectures/general_info.html#attendance",
    "href": "lectures/general_info.html#attendance",
    "title": "",
    "section": "Attendance",
    "text": "Attendance\nAttendance is not strict, but do not disappoint us."
  },
  {
    "objectID": "lectures/general_info.html#exam",
    "href": "lectures/general_info.html#exam",
    "title": "",
    "section": "Exam",
    "text": "Exam\n\nClassical oral exam with a list of questions (questions from 2020)\nIf you fail exam, you get zero for the course\nThe list of questions such that if you can not answer any question from this list during the final exam, you will automatically get F for this course. It will be probably updated by the end of November."
  },
  {
    "objectID": "lectures/general_info.html#projects",
    "href": "lectures/general_info.html#projects",
    "title": "",
    "section": "Projects",
    "text": "Projects\n\nWe give you time (\\approx one week) and points (30% of your final grade) to test studied methods in real-world problems\nWe will provide some examples in the second part of this class\nFor inspiration you can checkout recent conference papers (e.g. here)\n2–5 students per team\nStart thinking about your project topics as early as possible!"
  },
  {
    "objectID": "lectures/general_info.html#grades",
    "href": "lectures/general_info.html#grades",
    "title": "",
    "section": "Grades",
    "text": "Grades\n\nA: 86 - 100 %\nB: 70 - 85 %\nC: 50 - 70 %\nD: 30 - 50 %\nE: 15 - 30 %\nF: 0 - 15 %\n\nBut they can be slightly adjusted."
  },
  {
    "objectID": "lectures/general_info.html#python",
    "href": "lectures/general_info.html#python",
    "title": "",
    "section": "Python",
    "text": "Python\n\nWe will use Python ecosystem for programming.\nPlease, follow the rules that will be given in the problem sets"
  },
  {
    "objectID": "lectures/general_info.html#materials",
    "href": "lectures/general_info.html#materials",
    "title": "",
    "section": "Materials",
    "text": "Materials\nOur materials: - Lecture notes are avaliable online - Matrix decomposition cheat sheet\nIf you have difficulties with basic linear algebra: - Cheat sheet with basics - Gilbert Strang book “Introduction to Linear Algebra” - Gilbert Strang has recorded lectures on YouTube\nComprehensive NLA books: - Gene H. Golub, Charles. F. Van Loan, “Matrix computations” (4th edition) - Lloyd N. Trefethen and David Bau III, “Numerical Linear Algebra” - Eugene. E. Tyrtyshnikov, “Brief introduction to numerical analysis” - James W. Demmel, “Numerical Linear Algebra”\n\nMany applications of linear algebra you can find in Introduction to Applied Linear Algebra by S. Boyd and L. Vandenberghe"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#предыдущая-лекция",
    "href": "lectures/lecture-3/lecture-3.html#предыдущая-лекция",
    "title": "",
    "section": "Предыдущая лекция",
    "text": "Предыдущая лекция\n\nПиковая производительность алгоритма\nСложность алгоритмов умножения матриц\nИдея блочного разбиения (почему это хорошо?)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сегодняшняя-лекция",
    "href": "lectures/lecture-3/lecture-3.html#сегодняшняя-лекция",
    "title": "",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\n\nРанг матрицы\nСкелетное разложение\nАппроксимация низкого ранга\nСингулярное разложение (SVD)\nПрименения SVD"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#матрицы-и-линейные-пространства",
    "href": "lectures/lecture-3/lecture-3.html#матрицы-и-линейные-пространства",
    "title": "",
    "section": "Матрицы и линейные пространства",
    "text": "Матрицы и линейные пространства\n\nМатрицу можно рассматривать как последовательность векторов, которые являются столбцами матрицы:\n\n A = [a_1, \\ldots, a_m], \nгде a_m \\in \\mathbb{C}^{n\\times 1}.\n\nПроизведение матрицы на вектор эквивалентно взятию линейной комбинации этих столбцов\n\n y =  Ax \\quad \\Longleftrightarrow \\quad y = a_1 x_1 + a_2 x_2 + \\ldots +a_m x_m. \n\nЭто частный случай блочной матричной нотации (столбцы также являются блоками), которую мы уже видели (разбиение на блоки для соответствия кэш-памяти, алгоритм Штрассена)."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#линейная-зависимость",
    "href": "lectures/lecture-3/lecture-3.html#линейная-зависимость",
    "title": "",
    "section": "Линейная зависимость",
    "text": "Линейная зависимость\nОпределение. Векторы a_i называются линейно зависимыми, если существуют одновременно ненулевые коэффициенты x_i такие, что\n\\sum_i a_i x_i = 0,\nили в матричной форме\n Ax = 0, \\quad \\Vert x \\Vert \\ne 0. \nВ этом случае мы говорим, что матрица A имеет нетривиальное нуль-пространство (или ядро), обозначаемое N(A) (или \\text{ker}(A)).\nВекторы, которые не являются линейно зависимыми, называются линейно независимыми."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#линейное-векторное-пространство",
    "href": "lectures/lecture-3/lecture-3.html#линейное-векторное-пространство",
    "title": "",
    "section": "Линейное (векторное) пространство",
    "text": "Линейное (векторное) пространство\nЛинейное пространство, натянутое на векторы \\{a_1, \\ldots, a_m\\}, определяется как множество всех возможных векторов вида\n \\mathcal{L}(a_1, \\ldots, a_m) = \\left\\{y: y = \\sum_{i=1}^m a_i x_i, \\, \\forall x_i, \\, i=1,\\dots, n \\right\\}, \nВ матричной форме линейное пространство - это множество всех y таких, что\ny = A x.\nЭто множество также называется областью значений (или образом) матрицы, обозначаемой \\text{range}(A) (или \\text{im}(A)) соответственно."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#размерность-линейного-пространства",
    "href": "lectures/lecture-3/lecture-3.html#размерность-линейного-пространства",
    "title": "",
    "section": "Размерность линейного пространства",
    "text": "Размерность линейного пространства\n\nРазмерность линейного пространства \\text{im}(A), обозначаемая как \\text{dim}\\, \\text{im} (A), это минимальное количество векторов, необходимых для представления каждого вектора из \\text{im} (A).\nРазмерность \\text{im}(A) имеет прямую связь с рангом матрицы."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#ранг-матрицы",
    "href": "lectures/lecture-3/lecture-3.html#ранг-матрицы",
    "title": "",
    "section": "Ранг матрицы",
    "text": "Ранг матрицы\n\nРанг матрицы A - это максимальное число линейно независимых столбцов в матрице A, или размерность пространства столбцов = \\text{dim} \\, \\text{im}(A).\nВы также можете использовать линейные комбинации строк для определения ранга, т.е. формально существуют два ранга: столбцовый ранг и строчный ранг матрицы.\n\nТеорема\nРазмерность пространства столбцов матрицы равна размерности пространства её строк.\nДоказательство\n\nВ матричной форме этот факт можно записать как \\mathrm{dim}\\ \\mathrm{im} (A) = \\mathrm{dim}\\ \\mathrm{im} (A^\\top).\nТаким образом, существует единственный ранг!"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#матрица-полного-ранга",
    "href": "lectures/lecture-3/lecture-3.html#матрица-полного-ранга",
    "title": "",
    "section": "Матрица полного ранга",
    "text": "Матрица полного ранга\n\nМатрица A \\in \\mathbb{R}^{m \\times n} называется матрицей полного ранга, если \\mathrm{rank}(A) = \\min(m, n).\n\nПредположим, у нас есть линейное пространство, порожденное n векторами. Пусть эти векторы случайны, с элементами из стандартного нормального распределения \\mathcal{N}(0, 1).\nВ: Какова вероятность того, что это подпространство имеет размерность m &lt; n?\nО: Случайная матрица имеет полный ранг с вероятностью 1."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#скелетное-разложение",
    "href": "lectures/lecture-3/lecture-3.html#скелетное-разложение",
    "title": "",
    "section": "Скелетное разложение",
    "text": "Скелетное разложение\nОчень полезным представлением для вычисления ранга матрицы является скелетное разложение, которое тесно связано с рангом. Это разложение объясняет, почему и как матрицы низкого ранга могут быть сжаты.\nЕго можно графически представить следующим образом:\n или в матричной форме\n A = C \\widehat{A}^{-1} R, \nгде C - это некоторые k=\\mathrm{rank}(A) столбцов матрицы A, R - некоторые k строк матрицы A, а \\widehat{A} - невырожденная подматрица на их пересечении.\n\nЗамечание\nМы еще формально не определили обратную матрицу, поэтому напомним:\n\nОбратной матрицей для матрицы P является матрица Q = P^{-1} такая, что P Q = QP = I.\nЕсли матрица квадратная и имеет полный ранг, то обратная матрица существует.\n\n\n\nДоказательство скелетного разложения\n\nПусть C\\in \\mathbb{C}^{n\\times k} - это k столбцов, основанных на невырожденной подматрице \\widehat{A}. Следовательно, они линейно независимы.\nВозьмем любой другой столбец a_i матрицы A. Тогда a_i можно представить как линейную комбинацию столбцов C, т.е. a_i = C x_i, где x_i - вектор коэффициентов.\na_i = C x_i - это n уравнений. Мы берем k уравнений из них, соответствующих строкам, содержащим \\widehat{A}, и получаем уравнение\n\n\\widehat{r}_i = \\widehat{A} x_i \\quad \\Longrightarrow \\quad x_i = \\widehat{A}^{-1} \\widehat r_i\nТаким образом, a_i = C\\widehat{A}^{-1} \\widehat r_i для каждого i и\nA = [a_1,\\dots, a_m] = C\\widehat{A}^{-1} R.\n\n\nБолее подробный взгляд на скелетное разложение\n\nЛюбая матрица ранга r может быть записана в форме\n\nA = C \\widehat{A}^{-1} R,\nгде C имеет размер n \\times r, R имеет размер r \\times m и \\widehat{A} имеет размер r \\times r, или\n A = UV, \nгде U и V не являются уникальными, например, U = C \\widehat{A}^{-1}, V=R.\n\nФорма A = U V является стандартной для скелетного разложения.\nТаким образом, каждая матрица ранга r может быть записана как произведение “тонкой” (“высокой”) матрицы U на “широкую” (“короткую”) матрицу V.\n\nВ индексной форме это\n a_{ij} = \\sum_{\\alpha=1}^r u_{i \\alpha} v_{\\alpha j}. \nДля ранга 1 имеем\n a_{ij} = u_i v_j, \nт.е. это разделение индексов, и матрица ранга r является суммой матриц ранга 1!\n\n\nХранение\nИнтересно отметить, что для матрицы ранга r\nA = U V\nможно хранить только U и V, что дает нам (n+m) r параметров, поэтому это может использоваться для сжатия. Мы также можем вычислять произведение матрицы на вектор Ax гораздо быстрее:\n\nУмножение y = Vx требует \\mathcal{O}(mr) операций.\nУмножение z = Uy требует \\mathcal{O}(nr) операций.\n\nТо же самое работает для сложения, поэлементного умножения и т.д. Для сложения:\n    A_1 + A_2 = U_1 V_1 + U_2 V_2 = [U_1|U_2] [V_1^\\top|V_2^\\top]^\\top \n\n#A fast matrix-by-vector product demo\nimport numpy as np\nn = 4096\nr = 10\nu = np.random.normal(size=(n, r))\nv = np.random.normal(size=(n, r))\na = u @ v.T\nx = np.random.normal(size=(n,))\nprint(n*n/(2*n*r))\n%timeit a @ x\n%timeit u @ (v.T @ x)\n\n204.8\n2.39 ms ± 114 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n29.3 μs ± 293 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#computing-matrix-rank",
    "href": "lectures/lecture-3/lecture-3.html#computing-matrix-rank",
    "title": "",
    "section": "Computing matrix rank",
    "text": "Computing matrix rank\nWe can also try to compute the matrix rank using the built-in np.linalg.matrix_rank function\n\n#Computing matrix rank\nimport numpy as np\nn = 50 \na = np.ones((n, n))\nprint('Rank of the matrix:', np.linalg.matrix_rank(a))\nb = a + 1e-7 * np.random.randn(*a.shape)\nprint('Rank of the matrix:', np.linalg.matrix_rank(b, tol=1e-7))\n\na = np.eye(2000)*0.1\nnp.linalg.det(a)\n\nRank of the matrix: 1\nRank of the matrix: 46\n\n\nnp.float64(0.0)\n\n\n Таким образом, малые возмущения могут существенно влиять на ранг! \n\nНеустойчивость ранга матрицы\nДля любой матрицы A ранга r с r &lt; \\min(m, n) существует матрица B такая, что ее ранг равен \\min(m, n) и\n \\Vert A - B \\Vert = \\epsilon. \nВопрос: Означает ли это, что численно ранг матрицы не имеет смысла? (То есть, малые возмущения приводят к полному рангу!)\nОтвет: Нет. Мы должны найти матрицу B такую, что \\|A-B\\| \\leq \\epsilon и B имеет минимальный ранг. Поэтому мы можем вычислить ранг только с заданной точностью \\epsilon. Одним из подходов к вычислению ранга матрицы r является SVD."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#аппроксимация-низкого-ранга",
    "href": "lectures/lecture-3/lecture-3.html#аппроксимация-низкого-ранга",
    "title": "",
    "section": "Аппроксимация низкого ранга",
    "text": "Аппроксимация низкого ранга\nВажной задачей во многих приложениях является нахождение аппроксимации низкого ранга для заданной матрицы с заданной точностью \\epsilon или рангом r.  Примеры: * анализ главных компонент * рекомендательные системы * метод наименьших квадратов * сжатие нейронных сетей\nЭти задачи могут быть решены с помощью SVD."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сингулярное-разложение",
    "href": "lectures/lecture-3/lecture-3.html#сингулярное-разложение",
    "title": "",
    "section": "Сингулярное разложение",
    "text": "Сингулярное разложение\nДля вычисления аппроксимации низкого ранга нам необходимо вычислить сингулярное разложение (SVD).\nТеорема Любая матрица A\\in \\mathbb{C}^{n\\times m} может быть представлена как произведение трех матриц:\n A = U \\Sigma V^*, \nгде - U - унитарная матрица размера n \\times K, - V - унитарная матрица размера m \\times K, K = \\min(m, n), - \\Sigma - диагональная матрица с неотрицательными элементами \\sigma_1 \\geq  \\ldots, \\geq \\sigma_K на диагонали. - Более того, если \\text{rank}(A) = r, то \\sigma_{r+1} = \\dots = \\sigma_K = 0.\n\nДоказательство\n\nМатрица A^*A является эрмитовой, следовательно, диагонализируема в унитарном базисе (будет обсуждаться далее в курсе).\nA^*A\\geq0 (неотрицательно определена), поэтому собственные значения неотрицательны. Следовательно, существует унитарная матрица V = [v_1, \\dots, v_n] такая, что\n\n V^* A^* A V = \\text{diag}(\\sigma_1^2,\\dots, \\sigma_n^2), \\quad \\sigma_1\\geq \\sigma_2\\geq \\dots \\geq \\sigma_n. \nПусть \\sigma_i = 0 для i&gt;r, где r - некоторое целое число.  Пусть V_r= [v_1, \\dots, v_r], \\Sigma_r = \\text{diag}(\\sigma_1, \\dots,\\sigma_r). Следовательно\n V^*_r A^* A V_r = \\Sigma_r^2 \\quad \\Longrightarrow \\quad (\\Sigma_r^{-1} V_r^* A^*) (A V_r\\Sigma_r^{-1} ) = I. \nВ результате, матрица U_r = A V_r\\Sigma_r^{-1} удовлетворяет U_r^* U_r = I и, следовательно, имеет ортогональные столбцы.  Добавим к U_r любые ортогональные столбцы, которые ортогональны столбцам в U_r, и обозначим эту матрицу как U. Тогда\n AV = U \\begin{bmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}\\quad \\Longrightarrow \\quad U^* A V = \\begin{bmatrix}\\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}.\n\nПоскольку умножение на невырожденные матрицы не меняет ранг A, мы имеем r = \\text{rank}(A).\nСледствие 1: A = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_\\alpha v_\\alpha^* или поэлементно a_{ij} = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_{i\\alpha} \\overline{v}_{j\\alpha}\nСледствие 2: \\text{ker}(A) = \\mathcal{L}\\{v_{r+1},\\dots,v_n\\}\n\\text{im}(A) = \\mathcal{L}\\{u_{1},\\dots,u_r\\}\n\\text{ker}(A^*) = \\mathcal{L}\\{u_{r+1},\\dots,u_n\\}\n\\text{im}(A^*) = \\mathcal{L}\\{v_{1},\\dots,v_r\\}"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#теорема-экарта-янга",
    "href": "lectures/lecture-3/lecture-3.html#теорема-экарта-янга",
    "title": "",
    "section": "Теорема Экарта-Янга",
    "text": "Теорема Экарта-Янга\nНаилучшее приближение низкого ранга может быть вычислено с помощью SVD.\nТеорема: Пусть r &lt; \\text{rank}(A), A_r = U_r \\Sigma_r V_r^*. Тогда\n \\min_{\\text{rank}(B)=r} \\|A - B\\|_2 = \\|A - A_r\\|_2 = \\sigma_{r+1}. \nТо же самое верно для \\|\\cdot\\|_F, но \\|A - A_r\\|_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{\\min (n,m)}^2}."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#доказательство-1",
    "href": "lectures/lecture-3/lecture-3.html#доказательство-1",
    "title": "",
    "section": "Доказательство",
    "text": "Доказательство\n\nПоскольку \\text{rank} (B) = r, то \\text{dim}~\\text{ker}~B = n-r.\nСледовательно, существует z\\not=0 такой, что z\\in \\text{ker}(B) \\cap \\mathcal{L}(v_1,\\dots,v_{r+1}) (так как \\text{dim}\\{v_1,\\dots,v_{r+1}\\} = r+1).\nЗафиксируем \\|z\\| = 1. Тогда,\n\n \\|A-B\\|_2^2 \\geq \\|(A-B)z\\|_2^2 = \\|Az\\|_2^2 = \\| U\\Sigma V^* z\\|^2_2= \\|\\Sigma V^* z\\|^2_2 = \\sum_{i=1}^{n} \\sigma_i^2 (v_i^*z)^2 =\\sum_{i=1}^{r+1} \\sigma_i^2 (v_i^*z)^2 \\geq \\sigma_{r+1}^2\\sum_{i=1}^{r+1} (v_i^*z)^2 = \\sigma_{r+1}^2 \nтак как \\sigma_1\\geq \\dots \\geq \\sigma_{r+1} и \\sum_{i=1}^{r+1} (v_i^*z)^2 = \\|V^*z\\|_2^2 = \\|z\\|_2^2 = 1."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#основной-результат-о-приближении-низкого-ранга",
    "href": "lectures/lecture-3/lecture-3.html#основной-результат-о-приближении-низкого-ранга",
    "title": "",
    "section": "Основной результат о приближении низкого ранга",
    "text": "Основной результат о приближении низкого ранга\nСледствие: вычисление наилучшего приближения ранга r эквивалентно установке \\sigma_{r+1}= 0, \\ldots, \\sigma_K = 0. Ошибка\n \\min_{A_r} \\Vert A - A_r \\Vert_2 = \\sigma_{r+1}, \\quad \\min_{A_r} \\Vert A - A_r \\Vert_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{K}^2} \nвот почему важно смотреть на скорость убывания сингулярных значений."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#вычисление-svd",
    "href": "lectures/lecture-3/lecture-3.html#вычисление-svd",
    "title": "",
    "section": "Вычисление SVD",
    "text": "Вычисление SVD\n\nАлгоритмы для вычисления SVD сложны и будут обсуждаться позже.\nНо для численных расчетов мы уже можем использовать NumPy, JAX или PyTorch!\n\nВернемся к предыдущему примеру\n\n#Computing matrix rank\nimport numpy as np\nn = 50 \na = np.ones((n, n))\nprint('Rank of the matrix:', np.linalg.matrix_rank(a))\nb = a + 1e-5 * np.random.randn(*a.shape)\nprint('Rank of the matrix:', np.linalg.matrix_rank(b, tol=1e-7))\n\nRank of the matrix: 1\nRank of the matrix: 50\n\n\n\nu, s, v = np.linalg.svd(b) #b = u@jnp.diag(s)@v \nprint(s/s[0])\nprint(s[1]/s[0])\nr = 1\nu1 = u[:, :r]\ns1 = s[:r]\nv1 = v[:r, :]\na1 = u1.dot(np.diag(s1).dot(v1))\nprint(np.linalg.norm(b - a1, 2)/s[0])\n\n[1.00000000e+00 2.67591794e-06 2.56509433e-06 2.44789428e-06\n 2.37455067e-06 2.29593994e-06 2.28815285e-06 2.23799065e-06\n 2.05935979e-06 2.03632846e-06 1.99639422e-06 1.91341245e-06\n 1.83976105e-06 1.81660880e-06 1.74749549e-06 1.73011845e-06\n 1.64795628e-06 1.58936685e-06 1.52344312e-06 1.46849377e-06\n 1.41976378e-06 1.36738146e-06 1.32423364e-06 1.25094325e-06\n 1.20823796e-06 1.16024742e-06 1.08756172e-06 9.81247385e-07\n 9.32616886e-07 9.20283514e-07 8.93963429e-07 8.53777806e-07\n 8.01564035e-07 7.62399924e-07 7.12964817e-07 6.77836503e-07\n 6.64178420e-07 5.63347485e-07 5.17310367e-07 4.72905832e-07\n 4.24444379e-07 3.68835674e-07 3.07482216e-07 2.90162960e-07\n 2.44707429e-07 2.04581052e-07 1.82253444e-07 8.90398610e-08\n 7.58352672e-08 1.85055347e-08]\n2.675917943198842e-06\n2.6759179431971288e-06\n\n\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rc(\"text\", usetex=False)\n\nn = 100\na = [[1.0/(i-j+0.5) for i in range(n)] for j in range(n)] #Hilbert matrix \nda = np.array(a)\nu, s, v = np.linalg.svd(a)\ns - np.pi\n#plt.semilogy(s/s[0])\n#print(s[50] - np.pi)\n#plt.plot(s[:30], 'x')\n#s\n#plt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\n#plt.xlabel(r\"Singular value index, $i$\", fontsize=24)\n# plt.grid(True)\n# plt.xticks(fontsize=26)\n# plt.yticks(fontsize=26)\n# #We have very good low-rank approximation of it!\n\narray([ 3.99680289e-15,  1.77635684e-15,  1.77635684e-15,  1.33226763e-15,\n        1.33226763e-15,  1.33226763e-15,  1.33226763e-15,  1.33226763e-15,\n        1.33226763e-15,  1.33226763e-15,  1.33226763e-15,  1.33226763e-15,\n        1.33226763e-15,  1.33226763e-15,  1.33226763e-15,  1.33226763e-15,\n        1.33226763e-15,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  8.88178420e-16,  8.88178420e-16,\n        8.88178420e-16,  8.88178420e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  4.44089210e-16,\n        4.44089210e-16,  4.44089210e-16,  4.44089210e-16,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -4.44089210e-16, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -4.44089210e-16, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -4.44089210e-16, -4.44089210e-16,\n       -4.44089210e-16, -4.44089210e-16, -1.33226763e-15, -1.33226763e-15,\n       -1.77635684e-15, -4.44089210e-15, -6.08402217e-14, -5.76427794e-13,\n       -5.26201305e-12, -4.61763960e-11, -3.89348109e-10, -3.15097903e-09,\n       -2.44420360e-08, -1.81421391e-07, -1.28599019e-06, -8.68452531e-06,\n       -5.57096630e-05, -3.38178667e-04, -1.93260882e-03, -1.03148256e-02,\n       -5.06661203e-02, -2.21847627e-01, -8.08302651e-01, -2.19424535e+00])"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#линейный-факторный-анализ-и-низкий-ранг",
    "href": "lectures/lecture-3/lecture-3.html#линейный-факторный-анализ-и-низкий-ранг",
    "title": "",
    "section": "Линейный факторный анализ и низкий ранг",
    "text": "Линейный факторный анализ и низкий ранг\nРассмотрим линейную факторную модель,\ny = Ax, \nгде y - вектор длины n, а x - вектор длины r.\nДанные организованы в виде выборок: мы наблюдаем векторы\ny_1, \\ldots, y_T,\nно не знаем матрицу A, тогда факторная модель может быть записана как\n\n  Y = AX,\n\nгде Y имеет размер n \\times T, A имеет размер n \\times r и X имеет размер r \\times T.\n\nЭто в точности модель ранга r: она говорит нам, что векторы лежат в малом подпространстве.\nМы также можем использовать SVD для восстановления этого подпространства (но не независимых компонент).\nАнализ главных компонент может быть выполнен с помощью SVD, посмотрите реализацию в пакете sklearn."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#применения-svd",
    "href": "lectures/lecture-3/lecture-3.html#применения-svd",
    "title": "",
    "section": "Применения SVD",
    "text": "Применения SVD\n\nSVD чрезвычайно важен в вычислительной науке и инженерии.\nОн имеет много названий: Анализ главных компонент, Правильное ортогональное разложение, Эмпирические ортогональные функции\nТеперь мы рассмотрим сжатие плотных матриц и метод активных подпространств"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сжатие-плотных-матриц",
    "href": "lectures/lecture-3/lecture-3.html#сжатие-плотных-матриц",
    "title": "",
    "section": "Сжатие плотных матриц",
    "text": "Сжатие плотных матриц\nПлотные матрицы обычно требуют хранения N^2 элементов. Аппроксимация ранга-r может уменьшить это число до \\mathcal{O}(Nr)\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the style for a more beautiful plot\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 6))\n\n# Generate the matrix\nn = 256\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\n\n# Compute SVD\nu, s, v = np.linalg.svd(a[n//2:, :n//2])\n\n# Plot singular values\nplt.semilogy(s/s[0], linewidth=2.5, color='#1f77b4', marker='o', markersize=6, \n             markerfacecolor='white', markeredgewidth=1.5, markeredgecolor='#1f77b4')\n\n# Add labels and customize\nplt.ylabel(r\"$\\sigma_i / \\sigma_0$\", fontsize=24)\nplt.xlabel(r\"Singular value index, $i$\", fontsize=24)\nplt.grid(True, alpha=0.7)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\n\n# Add title and improve appearance\nplt.title(\"Normalized Singular Values\", fontsize=26, pad=20)\nplt.tight_layout()\n\n# Add a box around the plot\nplt.box(True)"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#сжатие-параметров-в-полносвязных-нейронных-сетях",
    "href": "lectures/lecture-3/lecture-3.html#сжатие-параметров-в-полносвязных-нейронных-сетях",
    "title": "",
    "section": "Сжатие параметров в полносвязных нейронных сетях",
    "text": "Сжатие параметров в полносвязных нейронных сетях\n\nОдним из основных строительных блоков современных глубоких нейронных сетей является полносвязный слой, также известный как линейный слой\nЭтот слой реализует действие линейной функции на входной вектор: f(x) = Wx + b, где W - обучаемая матрица, а b - обучаемый вектор смещения\nИ W, и b обновляются во время обучения сети в соответствии с некоторым методом оптимизации, например, SGD, Adam и т.д.\nОднако хранение обученных оптимальных параметров (W и b) может быть непозволительно затратным, если вы хотите перенести вашу обученную сеть на устройство с ограниченной памятью\nВ качестве возможного решения вы можете сжать матрицы W_i из i-го линейного слоя с помощью усеченного SVD на основе сингулярных значений!\nЧто вы получаете после такой аппроксимации W?\n\nэффективное использование памяти при хранении\nболее быстрый вывод\nумеренное снижение точности при решении целевой задачи, например, классификации изображений"
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#take-home-message",
    "href": "lectures/lecture-3/lecture-3.html#take-home-message",
    "title": "",
    "section": "Take home message",
    "text": "Take home message\n\nMatrix rank definition\nSkeleton approximation and dyadic representation of a rank-r matrix\nSingular value decomposition and Eckart-Young theorem\nThree applications of SVD (linear factor analysis, dense matrix compression, active subspaces)."
  },
  {
    "objectID": "lectures/lecture-3/lecture-3.html#next-lecture",
    "href": "lectures/lecture-3/lecture-3.html#next-lecture",
    "title": "",
    "section": "Next lecture",
    "text": "Next lecture\n\nLinear systems\nInverse matrix\nCondition number\nLinear least squares\nPseudoinverse\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#краткий-обзор-предыдущей-лекции",
    "href": "lectures/lecture-5/lecture-5.html#краткий-обзор-предыдущей-лекции",
    "title": "Матричное разложение: форма Шура",
    "section": "Краткий обзор предыдущей лекции",
    "text": "Краткий обзор предыдущей лекции\n\nЛинейные системы\nМетод Гаусса\nLU-разложение\nЧисло обусловленности как мера прямой устойчивости задачи"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#сегодняшняя-лекция",
    "href": "lectures/lecture-5/lecture-5.html#сегодняшняя-лекция",
    "title": "Матричное разложение: форма Шура",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\nСегодня мы поговорим о: - Собственных векторах и их приложениях (PageRank) - Вычислении собственных векторов методом степенной итерации - Теореме Шура - Нормальных матрицах"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#что-такое-собственный-вектор",
    "href": "lectures/lecture-5/lecture-5.html#что-такое-собственный-вектор",
    "title": "Матричное разложение: форма Шура",
    "section": "Что такое собственный вектор?",
    "text": "Что такое собственный вектор?\n\nОпределение. Вектор x \\ne 0 называется собственным вектором квадратной матрицы A, если существует число \\lambda такое, что\n\n Ax = \\lambda x. \n\nЧисло \\lambda называется собственным значением. Также используется термин собственная пара.\nПоскольку A - \\lambda I должна иметь нетривиальное ядро, собственные значения являются корнями характеристического многочлена\n\n \\det (A - \\lambda I) = 0."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#разложение-по-собственным-значениям",
    "href": "lectures/lecture-5/lecture-5.html#разложение-по-собственным-значениям",
    "title": "Матричное разложение: форма Шура",
    "section": "Разложение по собственным значениям",
    "text": "Разложение по собственным значениям\nЕсли матрица A размера n\\times n имеет n собственных векторов s_i, i=1,\\dots,n:\n As_i = \\lambda_i s_i, \nто это можно записать как\n A S = S \\Lambda, \\quad\\text{где}\\quad S=(s_1,\\dots,s_n), \\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n), \nили эквивалентно\n A = S\\Lambda S^{-1}.  Это называется собственное разложение (eigendecomposition) матрицы. Матрицы, которые могут быть представлены через разложение по собственным значениям, называются диагонализируемыми.\n\nСуществование\n\nКакие классы матриц диагонализируемы?\nПростой пример - это матрицы с различными собственными значениями.\nВ более общем случае, матрица диагонализируема тогда и только тогда, когда алгебраическая кратность каждого собственного значения (кратность собственного значения в характеристическом многочлене) равна его геометрической кратности (размерности собственного подпространства).\nДля наших целей наиболее важным классом диагонализируемых матриц является класс нормальных матриц:\n\nAA^* = A^* A.\n\nВы узнаете, как доказать, что нормальные матрицы диагонализируемы, через несколько слайдов (тема разложения Шура).\n\n\nПример\n\nВы можете просто проверить, что, например, матрица\n\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\nимеет одно собственное значение 1 кратности 2 (так как её характеристический многочлен равен p(\\lambda)=(1-\\lambda)^2), но только один собственный вектор \\begin{pmatrix} c \\\\ 0  \\end{pmatrix}, и, следовательно, матрица не диагонализируема."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#почему-собственные-векторы-и-собственные-значения-важны",
    "href": "lectures/lecture-5/lecture-5.html#почему-собственные-векторы-и-собственные-значения-важны",
    "title": "Матричное разложение: форма Шура",
    "section": "Почему собственные векторы и собственные значения важны?",
    "text": "Почему собственные векторы и собственные значения важны?\n\nСобственные векторы являются как важными вспомогательными инструментами, так и играют важную роль в приложениях.\n\nМожете привести несколько примеров?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#применения-собственных-значенийсобственных-векторов",
    "href": "lectures/lecture-5/lecture-5.html#применения-собственных-значенийсобственных-векторов",
    "title": "Матричное разложение: форма Шура",
    "section": "Применения собственных значений/собственных векторов",
    "text": "Применения собственных значений/собственных векторов\n\nТеория коммуникаций: теоретический предел количества передаваемой информации\nПроектирование мостов (машиностроение)\nПроектирование hi-fi аудиосистем\nКвантовая химия: весь наш микромир управляется уравнением Шрёдингера, которое является задачей на собственные значения:\n\n H \\psi = E \\psi, \n\nРедукция порядка моделей сложных систем\nАнализ графов (PageRank, кластеризация графов)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#eigenvalues-are-vibrational-frequencies",
    "href": "lectures/lecture-5/lecture-5.html#eigenvalues-are-vibrational-frequencies",
    "title": "Матричное разложение: форма Шура",
    "section": "Eigenvalues are vibrational frequencies",
    "text": "Eigenvalues are vibrational frequencies\nA typical computation of eigenvectors / eigenvectors is for studying\n\nVibrational computations of mechanical structures\nModel order reduction of complex systems\n\n\nfrom IPython.display import YouTubeVideo \nYouTubeVideo(\"VcCcMZo6J6w\")"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#google-pagerank",
    "href": "lectures/lecture-5/lecture-5.html#google-pagerank",
    "title": "Матричное разложение: форма Шура",
    "section": "Google PageRank",
    "text": "Google PageRank\n\nОдним из самых известных вычислений собственных векторов является Google PageRank.\nВ настоящее время он активно не используется Google, но был одной из основных особенностей на ранних этапах. Вопрос в том, как мы ранжируем веб-страницы, какая из них важна, а какая нет.\nВсё, что мы знаем о сети, это какая страница ссылается на какую. PageRank определяется рекурсивным определением.\nОбозначим через p_i важность i-й страницы.\nЗатем мы определяем эту важность как среднее значение всех важностей всех страниц, которые ссылаются на текущую страницу. Это даёт нам линейную систему\n\n p_i = \\sum_{j \\in N(i)} \\frac{p_j}{L(j)}, \nгде L(j) - количество исходящих ссылок на j-й странице, N(i) - все соседи i-й страницы. Это можно переписать как\n p = G p, \\quad G_{ij} = \\frac{1}{L(j)} \nили как задачу на собственные значения\n\n   Gp = 1 p,\n\nт.е. собственное значение 1 уже известно. Заметим, что G является левой стохастической матрицей, т.е. сумма элементов в каждом её столбце равна 1. Проверьте, что любая левая стохастическая матрица имеет максимальное собственное значение, равное 1."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#демонстрация",
    "href": "lectures/lecture-5/lecture-5.html#демонстрация",
    "title": "Матричное разложение: форма Шура",
    "section": "Демонстрация",
    "text": "Демонстрация\n\nМы можем вычислить PageRank, используя некоторые пакеты Python.\nМы будем использовать пакет networkx для работы с графами, который можно установить с помощью\nМы будем использовать простой пример сети карате-клуба Захари.\nЭти данные были собраны вручную в 1977 году и представляют собой классический набор данных социальной сети.\n\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport networkx as nx\nkn = nx.read_gml('karate.gml')\n#nx.write_gml(kn, 'karate2.gml')\nnx.draw_networkx(kn, node_color=\"red\") #Draw the graph\n\n\n\n\n\n\n\n\n\nТеперь мы можем вычислить PageRank, используя встроенную функцию NetworkX.\nМы также отображаем размер узлов больше, если их PageRank больше.\n\n\npr = nx.algorithms.link_analysis.pagerank(kn)\npr_vector = list(pr.values())\npr_vector = np.array(pr_vector) * 3000\nnx.draw_networkx(kn, node_size=pr_vector, node_color=\"red\", labels=None)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений",
    "href": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений",
    "title": "Матричное разложение: форма Шура",
    "section": "Вычисление собственных значений",
    "text": "Вычисление собственных значений\n\nКак вычислить собственные значения и собственные векторы?\n\nСуществует два типа задач на собственные значения:\n\nполная задача на собственные значения (требуются все собственные значения и собственные векторы)\nчастичные собственные значения (требуются минимальные/максимальные собственные значения, собственные значения в заданной области)"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений-через-характеристические-уравнения",
    "href": "lectures/lecture-5/lecture-5.html#вычисление-собственных-значений-через-характеристические-уравнения",
    "title": "Матричное разложение: форма Шура",
    "section": "Вычисление собственных значений через характеристические уравнения",
    "text": "Вычисление собственных значений через характеристические уравнения\nЗадача на собственные значения имеет вид\n Ax = \\lambda x, \nили\n (A - \\lambda I) x = 0, \nследовательно, матрица A - \\lambda I имеет нетривиальное ядро и должна быть сингулярной.\nЭто означает, что определитель\n p(\\lambda) = \\det(A - \\lambda I) = 0. \n\nЭто уравнение называется характеристическим уравнением и является многочленом порядка n.\nМногочлен степени n имеет n комплексных корней!"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#вспомним-определение-определителя",
    "href": "lectures/lecture-5/lecture-5.html#вспомним-определение-определителя",
    "title": "Матричное разложение: форма Шура",
    "section": "Вспомним определение определителя",
    "text": "Вспомним определение определителя\nОпределитель квадратной матрицы A определяется как\n\\det A = \\sum_{\\sigma \\in S_n} \\mathrm{sgn}({\\sigma})\\prod^n_{i=1} a_{i, \\sigma_i},\nгде - S_n - множество всех перестановок чисел 1, \\ldots, n - \\mathrm{sgn} - знак перестановки ( (-1)^p, где p - число транспозиций, которые нужно сделать)."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#свойства-определителя",
    "href": "lectures/lecture-5/lecture-5.html#свойства-определителя",
    "title": "Матричное разложение: форма Шура",
    "section": "Свойства определителя",
    "text": "Свойства определителя\nОпределитель имеет много полезных свойств:\n1. \\det(AB) = \\det(A) \\det(B)\n2. Если одна строка представлена в виде суммы двух векторов, определитель является суммой двух определителей\n3. “Разложение по минорам”: мы можем разложить определитель по выбранной строке или столбцу.\n\nЕсли делать это через разложение по минорам, получаем экспоненциальную сложность по n.\nМожем ли мы достичь сложности \\mathcal{O}(n^3)?"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#собственные-значения-и-характеристическое-уравнение",
    "href": "lectures/lecture-5/lecture-5.html#собственные-значения-и-характеристическое-уравнение",
    "title": "Матричное разложение: форма Шура",
    "section": "Собственные значения и характеристическое уравнение",
    "text": "Собственные значения и характеристическое уравнение\n\nТеперь вернемся к собственным значениям.\nХарактеристическое уравнение может быть использовано для вычисления собственных значений, что приводит к наивному алгоритму:\n\np(\\lambda) = \\det(A - \\lambda I)\n\nВычислить коэффициенты многочлена\nВычислить корни\n\nЭто хорошая идея?\nПоделитесь своим мнением\nМы можем сделать короткую демонстрацию этого\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn = 40\na = [[1.0 / (i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\nev = np.linalg.eigvals(a)\n# There is a special numpy function for characteristic polynomial\ncf = np.poly(a)\nev_roots = np.roots(cf)\n# print('Coefficients of the polynomial:', cf)\n# print('Polynomial roots:', ev_roots)\nplt.scatter(ev_roots.real, ev_roots.imag, marker='x', label='roots')\nb = a + 1e-3 * np.random.randn(n, n)\nev_b = np.linalg.eigvals(b)\nplt.scatter(ev_b.real, ev_b.imag, marker='o', label='Lapack')\n# plt.scatter(ev_roots.real, ev_roots.imag, marker='o', label='Brute force')\nplt.legend(loc='best')\nplt.xlabel('Real part')\nplt.ylabel('Imaginary part')\n\nText(0, 0.5, 'Imaginary part')\n\n\n\n\n\n\n\n\n\n\nМораль\n\nНе делайте этого, если у вас нет веской причины.\nПоиск корней многочлена очень плохо обусловлен (может быть намного лучше, но не с мономами \\{1,x,x^2,\\dots\\}!). Обратите внимание, что матрица Грама мономов\n\nh_{ij} = \\int_0^1 x^i x^j\\, dx = \\frac{1}{i+j+1},\nявляется матрицей Гильберта, которая имеет экспоненциальное убывание сингулярных значений. - Таким образом, мономы “почти” линейно зависимы."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#степенной-метод",
    "href": "lectures/lecture-5/lecture-5.html#степенной-метод",
    "title": "Матричное разложение: форма Шура",
    "section": "Степенной метод",
    "text": "Степенной метод\n\nНас часто интересует вычисление части спектра, например, наибольших или наименьших собственных значений.\nТакже интересно отметить, что для эрмитовых матриц (A = A^*) собственные значения всегда действительны (докажите это!).\nСтепенной метод является простейшим методом для вычисления наибольшего по модулю собственного значения.\nЭто также наш первый пример итерационного метода и метода Крылова."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#степенной-метод-1",
    "href": "lectures/lecture-5/lecture-5.html#степенной-метод-1",
    "title": "Матричное разложение: форма Шура",
    "section": "Степенной метод",
    "text": "Степенной метод\n\nЗадача на собственные значения\n\nAx = \\lambda x, \\quad \\Vert x \\Vert_2 = 1 \\ \\text{для устойчивости}.\nможет быть переписана как итерация неподвижной точки. - Эта итерация называется степенным методом и находит наибольшее по модулю собственное значение матрицы A.\nСтепенной метод имеет вид\n x_{k+1} = A x_k, \\quad x_{k+1} := \\frac{x_{k+1}}{\\Vert x_{k+1} \\Vert_2}\nи\n x_{k+1}\\to v_1,\nгде Av_1 = \\lambda_1 v_1 и \\lambda_1 - наибольшее собственное значение, а v_1 - соответствующий собственный вектор.\n\nНа (k+1)-й итерации приближение к \\lambda_1 можно найти как\n\n \\lambda^{(k+1)} = (Ax_{k+1}, x_{k+1}), \n\nЗаметим, что \\lambda^{(k+1)} не требуется для (k+2)-й итерации, но может быть полезно для измерения ошибки на каждой итерации: \\|Ax_{k+1} - \\lambda^{(k+1)}x_{k+1}\\|.\nСходимость является геометрической, но коэффициент сходимости равен q^k, где q = \\left|\\frac{\\lambda_{2}}{\\lambda_{1}}\\right| &lt; 1, для \\lambda_1&gt;\\lambda_2\\geq\\dots\\geq \\lambda_n и k - номер итерации.\nЭто означает, что сходимость может быть произвольно малой. Чтобы доказать это, достаточно рассмотреть диагональную матрицу 2 \\times 2."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#анализ-сходимости-для-aa",
    "href": "lectures/lecture-5/lecture-5.html#анализ-сходимости-для-aa",
    "title": "Матричное разложение: форма Шура",
    "section": "Анализ сходимости для A=A^*",
    "text": "Анализ сходимости для A=A^*\nДавайте более точно рассмотрим степенной метод, когда A является эрмитовой матрицей. Через два слайда вы узнаете, что каждая эрмитова матрица диагонализируема. Следовательно, существует ортонормированный базис из собственных векторов v_1,\\dots,v_n такой, что Av_i = \\lambda_i v_i. Разложим x_0 в сумму v_i с коэффициентами c_i:\n x_0 = c_1 v_1 + \\dots + c_n v_n. \nПоскольку v_i являются собственными векторами, мы имеем\n\n\\begin{split}\nx_1 &= \\frac{Ax_0}{\\|Ax_0\\|} = \\frac{c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n}{\\|c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n \\|}  \\\\\n&\\vdots\\\\\nx_k &= \\frac{Ax_{k-1}}{\\|Ax_{k-1}\\|} = \\frac{c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n}{\\|c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n \\|}\n\\end{split}\n\nТеперь вы видите, что\n\nx_k = \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\frac{ v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k}v_2 + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}v_n}{\\left\\|v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k}v_2 + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}v_n\\right\\|},\n\nчто сходится к v_1, поскольку \\left| \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\right| = 1 и \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k \\to 0, если |\\lambda_2|&lt;|\\lambda_1|."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#что-нужно-помнить-о-степенном-методе",
    "href": "lectures/lecture-5/lecture-5.html#что-нужно-помнить-о-степенном-методе",
    "title": "Матричное разложение: форма Шура",
    "section": "Что нужно помнить о степенном методе",
    "text": "Что нужно помнить о степенном методе\n\nСтепенной метод дает оценку наибольшего по модулю собственного значения или спектрального радиуса заданной матрицы\nОдин шаг требует одного умножения матрицы на вектор. Если матрица позволяет выполнять умножение на вектор за \\mathcal{O}(n) операций (например, если она разреженная), то степенной метод применим для больших n.\nСходимость может быть медленной\nЕсли требуется только грубая оценка, достаточно всего нескольких итераций\nВектор решения находится в подпространстве Крылова \\{x_0, Ax_0,\\dots,A^{k}x_0\\} и имеет вид \\mu A^k x_0, где \\mu - нормировочная константа."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#теорема-шура",
    "href": "lectures/lecture-5/lecture-5.html#теорема-шура",
    "title": "Матричное разложение: форма Шура",
    "section": "Теорема Шура",
    "text": "Теорема Шура\nТеорема: Каждая матрица A \\in \\mathbb{C}^{n \\times n} может быть представлена в форме Шура A = UTU^*, где U - унитарная, а T - верхнетреугольная матрица.\nСхема доказательства. 1. Каждая матрица имеет по крайней мере 1 ненулевой собственный вектор (возьмем корень характеристического многочлена, (A-\\lambda I) сингулярна, имеет нетривиальное нуль-пространство). Пусть\nAv_1 = \\lambda_1 v_1, \\quad \\Vert v_1 \\Vert_2 = 1\n\nПусть U_1 = [v_1,v_2,\\dots,v_n], где v_2,\\dots, v_n - любые векторы, ортогональные v_1. Тогда\n\n U^*_1 A U_1 = \\begin{pmatrix} \\lambda_1 & *  \\\\ 0 & A_2  \\end{pmatrix}, \nгде A_2 - матрица размера (n-1) \\times (n-1). Это называется блочно-треугольной формой. Теперь мы можем работать только с A_2 и так далее.\nПримечание: Поскольку в этом доказательстве нам нужны собственные векторы, это доказательство не является практическим алгоритмом."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#применение-теоремы-шура",
    "href": "lectures/lecture-5/lecture-5.html#применение-теоремы-шура",
    "title": "Матричное разложение: форма Шура",
    "section": "Применение теоремы Шура",
    "text": "Применение теоремы Шура\n\nВажное применение теоремы Шура: нормальные матрицы.\nОпределение. Матрица A называется нормальной матрицей, если\n\n AA^* = A^* A. \nВопрос: Примеры нормальных матриц?\nПримеры: эрмитовы матрицы, унитарные матрицы."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#нормальные-матрицы",
    "href": "lectures/lecture-5/lecture-5.html#нормальные-матрицы",
    "title": "Матричное разложение: форма Шура",
    "section": "Нормальные матрицы",
    "text": "Нормальные матрицы\nТеорема: A является нормальной матрицей тогда и только тогда, когда A = U \\Lambda U^*, где U - унитарная, а \\Lambda - диагональная матрица.\nСхема доказательства: - Одно направление очевидно (если разложение выполняется, то матрица нормальная). - Другое направление сложнее. Рассмотрим форму Шура матрицы A. Тогда AA^* = A^*A означает TT^* = T^* T. - Рассматривая элементы, мы сразу видим, что единственная верхнетреугольная матрица T, удовлетворяющая условию TT^* = T^* T, - это диагональная матрица!\n\nВажное следствие\nТаким образом, каждая нормальная матрица унитарно диагонализуема, что означает, что она может быть диагонализована унитарной матрицей U.\nДругими словами, каждая нормальная матрица имеет ортогональный базис из собственных векторов."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#как-мы-вычисляем-разложение-шура",
    "href": "lectures/lecture-5/lecture-5.html#как-мы-вычисляем-разложение-шура",
    "title": "Матричное разложение: форма Шура",
    "section": "Как мы вычисляем разложение Шура?",
    "text": "Как мы вычисляем разложение Шура?\n\nВсё хорошо, но как мы вычисляем форму Шура?\nЭто будет рассмотрено в следующей лекции."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#спектр-и-псевдоспектр",
    "href": "lectures/lecture-5/lecture-5.html#спектр-и-псевдоспектр",
    "title": "Матричное разложение: форма Шура",
    "section": "Спектр и псевдоспектр",
    "text": "Спектр и псевдоспектр\n\nДля линейных динамических систем, заданных матрицей A, спектр может многое рассказать о системе (например, о стабильности, …)\nОднако для ненормальных матриц спектр может быть неустойчивым относительно малых возмущений.\nДля измерения таких возмущений было разработано понятие псевдоспектра."
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#псевдоспектр",
    "href": "lectures/lecture-5/lecture-5.html#псевдоспектр",
    "title": "Матричное разложение: форма Шура",
    "section": "Псевдоспектр",
    "text": "Псевдоспектр\nМы рассматриваем объединение всех возможных собственных значений всех возмущений матрицы A.\n\\Lambda_{\\epsilon}(A) = \\{ \\lambda \\in \\mathbb{C}: \\exists E, x \\ne 0: (A + E) x = \\lambda x, \\quad \\Vert E \\Vert_2 \\leq \\epsilon. \\}\n\nДля малых E и нормальных матриц A это будут окружности вокруг собственных значений, для ненормальных матриц структура может быть совершенно иной. Подробнее: http://www.cs.ox.ac.uk/pseudospectra/"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#summary-of-todays-lecture",
    "href": "lectures/lecture-5/lecture-5.html#summary-of-todays-lecture",
    "title": "Матричное разложение: форма Шура",
    "section": "Summary of todays lecture",
    "text": "Summary of todays lecture\n\nEigenvalues, eigenvectors\nPower method\nSchur theorem\nNormal matrices\nSome advanced topics"
  },
  {
    "objectID": "lectures/lecture-5/lecture-5.html#next-lecture",
    "href": "lectures/lecture-5/lecture-5.html#next-lecture",
    "title": "Матричное разложение: форма Шура",
    "section": "Next lecture",
    "text": "Next lecture\n\nReview of the considered matrix decompositions\nPractical way to compute QR decomposition\nAlmost practical method for computing eigenvalues and eigenvectors\n\n\nQuestions?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#краткий-обзор-предыдущей-лекции",
    "href": "lectures/lecture-9/lecture-9.html#краткий-обзор-предыдущей-лекции",
    "title": "Questions?",
    "section": "Краткий обзор предыдущей лекции",
    "text": "Краткий обзор предыдущей лекции\n\nРандомизированное умножение матриц\nОценка следа по методу Хатчинсона\nРандомизированное SVD\nМетод Качмажа"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#план-сегодняшней-лекции",
    "href": "lectures/lecture-9/lecture-9.html#план-сегодняшней-лекции",
    "title": "Questions?",
    "section": "План сегодняшней лекции",
    "text": "План сегодняшней лекции\n\nПростая тема в параллельных вычислениях в линейной алгебре (на примере умножения матрицы на вектор)\nЧасть о разреженных матрицах (с отдельным планом)"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#матрицы-большого-размера",
    "href": "lectures/lecture-9/lecture-9.html#матрицы-большого-размера",
    "title": "Questions?",
    "section": "Матрицы большого размера",
    "text": "Матрицы большого размера\n\nЕсли размер плотной матрицы огромен, то она не может быть сохранена в памяти\nВозможные варианты\n\nЭта матрица структурированная, например, блочная теплицева с теплицевыми блоками (следующие лекции). Тогда возможно сжатое хранение\nДля неструктурированных плотных матриц помогает распределенная память\nMPI для обработки матриц с распределенным хранением\n\n\n\nРаспределенная память и MPI\n\nРазделение матрицы на блоки и хранение их на разных машинах\nКаждая машина имеет свое собственное адресное пространство и не может повредить данные на других машинах\nВ этом случае машины взаимодействуют друг с другом для объединения результатов вычислений\nMPI (Message Passing Interface) - это стандарт для параллельных вычислений в распределенной памяти\n\n\n\nПример: умножение матрицы на вектор\n\nПредположим, что вы хотите вычислить Ax, и матрица A не может быть сохранена в доступной памяти\nТогда вы можете разделить её на блоки и распределить блоки на отдельные машины\nВозможные стратегии\n\n1D блочное разделение разбивает только строки на блоки\n2D блочное разделение разбивает как строки, так и столбцы\n\n\n\n1D blocking scheme\n\n\n\nTotal time of computing matvec with 1D blocking\n\nEach machine has $n / p $ complete rows and n / p elements of vector\nTotal operations are n^2 / p\nTotal time for sending and writing data are t_s \\log p + t_w n, where t_s time unit for sending and t_w time unit for writing\n\n\n\n2D blocking scheme\n\n\n\nОбщее время вычисления умножения матрицы на вектор с 2D блочным разделением\n\nКаждая машина имеет блок размера n / \\sqrt{p} и n / \\sqrt{p} элементов вектора\nОбщее количество операций составляет n^2 / p\nОбщее время для отправки и записи данных приблизительно равно t_s \\log p + t_w (n/\\sqrt{p}) \\log p, где t_s - единица времени для отправки, а t_w - единица времени для записи\n\n\n\n\nПакеты, поддерживающие распределенное хранение\n\nScaLAPACK\nTrilinos\n\nВ Python вы можете использовать mpi4py для параллельного программирования вашего алгоритма.\n\nPyTorch поддерживает распределенное обучение и хранение данных, подробности здесь\n\n\n\nРезюме по обработке больших неструктурированных матриц\n\nРаспределенный способ хранения\nMPI\nПакеты, использующие параллельные вычисления\nРазличные стратегии блочного разделения"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#введение-в-разреженные-матрицы",
    "href": "lectures/lecture-9/lecture-9.html#введение-в-разреженные-матрицы",
    "title": "Questions?",
    "section": "Введение в разреженные матрицы",
    "text": "Введение в разреженные матрицы\n\nДля задач плотной линейной алгебры мы ограничены памятью для хранения полной матрицы, это N^2 параметров.\nКласс разреженных матриц, где большинство элементов равны нулю, позволяет нам как минимум хранить такие матрицы.\n\nВопрос в том, можем ли мы:\n\nрешать линейные системы\nрешать задачи на собственные значения\n\nс разреженными матрицами"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#план-следующей-части-лекции",
    "href": "lectures/lecture-9/lecture-9.html#план-следующей-части-лекции",
    "title": "Questions?",
    "section": "План следующей части лекции",
    "text": "План следующей части лекции\nТеперь мы поговорим о разреженных матрицах, где они возникают, как мы их храним, как мы с ними работаем.\n\nФорматы: список списков и формат сжатых разреженных строк, связь с графами\nУмножение матрицы на вектор\nПараллельная обработка разреженных матриц\nБыстрые прямые решатели для метода Гаусса (начало)"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#применения-разреженных-матриц",
    "href": "lectures/lecture-9/lecture-9.html#применения-разреженных-матриц",
    "title": "Questions?",
    "section": "Применения разреженных матриц",
    "text": "Применения разреженных матриц\nРазреженные матрицы возникают в:\n\nдифференциальных уравнениях в частных производных (ДУЧП), математическом моделировании\nанализе графов, например, при анализе социальных сетей\nрекомендательных системах\nвезде, где отношения между объектами являются “разреженными”.\n\n\nРазреженные матрицы повсеместны в ДУЧП\nПростейшее дифференциальное уравнение в частных производных (ДУЧП), называемое\nУравнение Лапласа:\n\n   \\Delta T = \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} = f(x,y), \\quad x,y\\in \\Omega\\equiv[0,1]^2,\n\n\n    T_{\\partial\\Omega} = 0.\n\n\nDiscretization\n\\frac{\\partial^2 T}{\\partial x^2} \\approx \\frac{T(x+h) + T(x-h) - 2T(x)}{h^2} + \\mathcal{O}(h^2),\nsame for \\frac{\\partial^2 T}{\\partial y^2}, and we get a linear system.\nFirst, let us consider one-dimensional case:\nПосле дискретизации одномерного уравнения Лапласа с граничными условиями Дирихле мы получаем\n\\frac{u_{i+1} + u_{i-1} - 2u_i}{h^2} = f_i,\\quad i=1,\\dots,N-1\n u_{0} = u_N = 0 или в матричной форме\n A u = f, и (для n = 5) A=-\\frac{1}{h^2}\\begin{bmatrix} 2 & -1 & 0 & 0 & 0\\\\ -1 & 2 & -1 & 0 &0 \\\\ 0 & -1 & 2& -1 & 0 \\\\ 0 & 0 & -1 & 2  &-1\\\\ 0 & 0 & 0 & -1 & 2 \\end{bmatrix}\nМатрица является трехдиагональной и разреженной\n(а также теплицевой: все элементы на диагонали одинаковы)\n\n\nБлочная структура в 2D\nВ двух измерениях мы получаем уравнение вида\n-\\frac{4u_{ij} -u_{(i-1)j} - u_{(i+1)j} - u_{i(j-1)}-u_{i(j+1)}}{h^2} = f_{ij},\nили в форме кронекерова произведения\n\\Delta_{2D} = \\Delta_{1D} \\otimes I + I \\otimes \\Delta_{1D},\nгде \\Delta_{1D} - одномерный оператор Лапласа, а \\otimes - кронекерово произведение матриц.\nДля матриц A\\in\\mathbb{R}^{n\\times m} и B\\in\\mathbb{R}^{l\\times k} их кронекерово произведение определяется как блочная матрица вида\n\n   A\\otimes B = \\begin{bmatrix}a_{11}B & \\dots & a_{1m}B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n1}B & \\dots & a_{nm}B\\end{bmatrix}\\in\\mathbb{R}^{nl\\times mk}.\n\nВ блочной матричной форме 2D-матрица Лапласа может быть записана в следующем виде:\nA = -\\frac{1}{h^2}\\begin{bmatrix} \\Delta_1 + 2I & -I & 0 & 0 & 0\\\\ -I & \\Delta_1 + 2I  & -I & 0 &0 \\\\ 0 & -I & \\Delta_1 + 2I & -I & 0 \\\\ 0 & 0 & -I & \\Delta_1 + 2I   &-I\\\\ 0 & 0 & 0 & -I & \\Delta_1 + 2I \\end{bmatrix}\n\nКраткий список свойств кронекерова произведения\n\nОно билинейно\n(A\\otimes B) (C\\otimes D) = AC \\otimes BD\nПусть \\mathrm{vec}(X) - векторизация матрицы X по столбцам. Тогда \\mathrm{vec}(AXB) = (B^T \\otimes A) \\mathrm{vec}(X).\n\n\n\n\n\nРазреженные матрицы помогают в вычислительной теории графов\n\nГрафы представляются матрицей смежности, которая обычно разрежена\nЧисленное решение задач теории графов основано на обработке этой разреженной матрицы\n\nОбнаружение сообществ и кластеризация графов\nОбучение ранжированию\nСлучайные блуждания\nДругие\n\nПример: вероятно, самый большой общедоступный граф гиперссылок состоит из 3,5 миллиардов веб-страниц и 128 миллиардов гиперссылок, подробнее см. здесь\nГрафы среднего масштаба для тестирования ваших алгоритмов доступны в Стэнфордской коллекции больших сетевых данных\n\n\n\nКоллекция разреженных матриц SuiteSparse (ранее известная как коллекция разреженных матриц Флориды)\nБольше разреженных матриц вы можете найти в коллекции матриц SuiteSparse, которая содержит различные типы матриц для разных приложений.\n\nfrom IPython.display import IFrame\nIFrame(\"http://yifanhu.net/GALLERY/GRAPHS/search.html\", width=700, height=450)\n\n\n        \n        \n\n\n\n\nSparse matrices and deep learning\n\nDNN has a lot of parameters\nSome of them may be redundant\nHow to prune the parameters without significantly accuracy reduction?\nSparse variational dropout method leads to significantly sparse filters in DNN almost without accuracy decreasing: the idea of pruning"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#разреженная-матрица-построение",
    "href": "lectures/lecture-9/lecture-9.html#разреженная-матрица-построение",
    "title": "Questions?",
    "section": "Разреженная матрица: построение",
    "text": "Разреженная матрица: построение\n\nМы можем создать разреженную матрицу, используя пакет scipy.sparse (на самом деле это не лучший пакет для разреженных матриц)\nМы можем работать с действительно большими размерами (по крайней мере, для хранения этой матрицы в памяти)\n\nОбратите внимание на следующие функции - Создание разреженных матриц с заданными диагоналями spdiags - Кронекерово произведение разреженных матриц kron - Также для разреженных матриц перегружена арифметика\n\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nfrom scipy.sparse import csc_matrix, csr_matrix\nimport matplotlib.pyplot as plt\nimport scipy.linalg\nimport scipy.sparse.linalg\n%matplotlib inline\nn = 128\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csc_matrix(A)\nplt.spy(A, aspect='equal', marker='.', markersize=5)\n\n\n\n\n\n\n\n\n\nСтруктура разреженности\n\nКоманда spy отображает структуру разреженности матрицы: пиксель (i, j) отображается, если соответствующий элемент матрицы ненулевой.\nСтруктура разреженности действительно важна для понимания сложности алгоритмов разреженной линейной алгебры.\nЧасто для анализа “насколько сложна” матрица достаточно только структуры разреженности.\n\n\n\nРазреженная матрица: определение\n\nОпределение “разреженной матрицы” заключается в том, что количество ненулевых элементов намного меньше общего числа элементов.\nВы можете выполнять основные операции линейной алгебры (в первую очередь решение линейных систем) быстрее, чем при работе с полной матрицей."
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#что-нам-нужно-выяснить-чтобы-понять-как-это-работает-на-самом-деле",
    "href": "lectures/lecture-9/lecture-9.html#что-нам-нужно-выяснить-чтобы-понять-как-это-работает-на-самом-деле",
    "title": "Questions?",
    "section": "Что нам нужно выяснить, чтобы понять, как это работает на самом деле",
    "text": "Что нам нужно выяснить, чтобы понять, как это работает на самом деле\n\nВопрос 1: Как хранить разреженную матрицу в памяти?\nВопрос 2: Как быстро умножать разреженную матрицу на вектор?\nВопрос 3: Как быстро решать линейные системы с разреженными матрицами?\n\n\nХранение разреженных матриц\nСуществует множество форматов хранения, важные из них:\n\nCOO (Координатный формат)\nLIL (Списки списков)\nCSR (сжатая разреженная строка)\nCSC (сжатый разреженный столбец)\nБлочные варианты\n\nВ scipy есть конструкторы для каждого из этих форматов, например\n\nКоординатный формат (COO)\nПростейший формат - это использование координатного формата для представления разреженной матрицы в виде позиций и значений ненулевых элементов.\n\n\nОсновные недостатки\n\nНе оптимален по хранению (почему?)\nНе оптимален для умножения матрицы на вектор (почему?)\nНе оптимален для удаления элементов, так как нужно выполнить nnz операций, чтобы найти один элемент (это хорошо в формате LIL)\n\nПервые два недостатка решаются форматом сжатой разреженной строки (CSR).\n\n\nСжатая разреженная строка (CSR)\nВ формате CSR матрица хранится в виде 3 различных массивов:\n\n\n\nРазреженные матрицы в PyTorch и Tensorflow\n\nPyTorch поддерживает разреженные матрицы, хранящиеся в формате COO\nНеполная обратная операция для этих матриц, см. сводку здесь\nTensorflow также поддерживает разреженные матрицы в формате COO\nСписок поддерживаемых операций находится здесь, и поддержка градиентов также ограничена\n\n\n\nCSR помогает в умножении разреженной матрицы на вектор (SpMV)\nДавайте проведем короткий тест на время выполнения\n\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nimport scipy.sparse.linalg\nfrom scipy.sparse import csc_matrix, csr_matrix, coo_matrix\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 1024\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\nB = coo_matrix(A)\n%timeit A.dot(rhs)\n%timeit B.dot(rhs)\n\n3.24 ms ± 74.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n19.7 ms ± 379 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nКак видите, CSR работает быстрее, и для более неструктурированных шаблонов выигрыш будет еще больше.\n\n\nРазреженные матрицы и эффективность\n\nРазреженные матрицы дают снижение вычислительной сложности.\nНо они не очень хороши для параллельной/GPU реализации.\nОни не дают максимальной эффективности из-за случайного доступа к данным.\nОбычно пиковая эффективность в 10\\%-15\\% считается хорошей.\n\n\n\nВспомним, как мы измеряем эффективность операций линейной алгебры\nСтандартный способ измерения эффективности операций линейной алгебры на конкретной вычислительной архитектуре - использовать flops (количество операций с плавающей точкой в секунду)\nМы можем измерить пиковую эффективность обычного умножения матрицы на вектор.\n\nimport numpy as np\nimport time\nn = 4000\nk = 1400\na = np.random.randn(n, n)\nv = np.random.randn(n, k)\nt = time.time()\nnp.dot(a, v)\nt = time.time() - t\nprint('Time: {0: 3.1e}, Efficiency: {1: 3.1e} Gflops'.\\\n      format(t,  ((k*2 * n ** 2)/t) / 10 ** 9))\n\nTime:  9.0e-02, Efficiency:  5.0e+02 Gflops\n\n\n\nimport scipy as sp\nn = 4000\nk = 2000\nex = np.ones(n)\na = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.random.randn(n, k)\nt = time.time()\na.dot(rhs)\nt = time.time() - t\nprint('Time: {0: 3.1e}, Efficiency: {1: 3.1e} Gflops'.\\\n      format(t,  (3 * n * k) / t / 10 ** 9))\n\nTime:  8.1e-03, Efficiency:  3.0e+00 Gflops\n\n\n\n\nСлучайный доступ к данным и кэш-промахи\n\nИзначально все элементы матрицы и вектора хранятся в оперативной памяти (RAM)\nЕсли вы хотите вычислить произведение матрицы на вектор, часть элементов матрицы и вектора перемещается в кэш (быстрая память малого объема, см. (лекцию об алгоритме Штрассена))\nПосле этого процессор берет данные из кэша для обработки и возвращает результат также в кэш\nЕсли процессору нужны данные, которых еще нет в кэше, такая ситуация называется кэш-промахом\nПри кэш-промахе требуемые данные перемещаются из оперативной памяти в кэш\n\nВопрос: что если в кэше нет свободного места?\n\nЧем больше кэш-промахов, тем медленнее вычисления\n\n\nCSR sparse matrix by vector product\n\n\n\nПереупорядочение уменьшает кэш-промахи\n\nЕсли ja хранит последовательные элементы, то они будут перемещены в кэш вместе, и количество кэш-промахов уменьшается\nЭто происходит, когда разреженная матрица является ленточной или по крайней мере блочно-диагональной\nМы можем преобразовать заданную разреженную матрицу в ленточную или блочно-диагональную с помощью перестановок\nПусть P - матрица перестановки строк, а Q - матрица перестановки столбцов\nA_1 = PAQ - матрица, которая имеет меньшую ширину ленты, чем A\ny = Ax \\to \\tilde{y} = A_1 \\tilde{x}, где \\tilde{x} = Q^{\\top}x и \\tilde{y} = Py\nРазделенная блочно-диагональная форма - это кэш-независимый формат для произведения разреженной матрицы на вектор\nЭто можно расширить для 2D, где разделены не только строки, но и столбцы\n\n\nПример, как выглядит переупорядочение\n\nSBD in 1D \n\n\n\n\nПроизведение разреженной транспонированной матрицы на вектор\n\nВ некоторых случаях важно вычислить не только Ax для разреженной A, но и A^{\\top}x\nБолее подробно это будет обсуждаться в лекции о методах Крылова для несимметричных линейных систем\nТранспонирование вычислительно затратно\nЗдесь предложен формат хранения compressed sparse block, подходящий для этого случая\n\n\nCompressed sparse block (CSB)\n\nРазделение матрицы на блоки\nХранение индексов блоков и индексов данных внутри каждого блока\nТаким образом, достигается разумное количество битов для хранения индексов\nПорядок блоков и элементов внутри блоков важен для параллельной реализации\nПереключение между строками блоков и столбцами блоков делает этот формат подходящим для умножения транспонированной матрицы на вектор"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#решение-линейных-систем-с-разреженными-матрицами",
    "href": "lectures/lecture-9/lecture-9.html#решение-линейных-систем-с-разреженными-матрицами",
    "title": "Questions?",
    "section": "Решение линейных систем с разреженными матрицами",
    "text": "Решение линейных систем с разреженными матрицами\n\nПрямые методы\n\nLU-разложение\nРяд методов переупорядочения для минимизации заполнения\n\nМетоды Крылова\n\nДавайте начнем с небольшой демонстрации решения разреженной линейной системы…\n\nimport matplotlib.pyplot as plt\nn = 512\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\n#A = csr_matrix(A)\nrhs = np.ones(n * n)\nsol = sp.sparse.linalg.spsolve(A, rhs)\n_, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(sol)\nax1.set_title('Not reshaped solution')\nax2.contourf(sol.reshape((n, n), order='f'))\nax2.set_title('Reshaped solution')\n\nText(0.5, 1.0, 'Reshaped solution')"
  },
  {
    "objectID": "lectures/lecture-9/lecture-9.html#основные-выводы",
    "href": "lectures/lecture-9/lecture-9.html#основные-выводы",
    "title": "Questions?",
    "section": "Основные выводы",
    "text": "Основные выводы\n\nО параллельном умножении матрицы на вектор и различных способах блочного разделения.\nФормат CSR для хранения\nПроблемы кэширования и параллельной обработки в работе с разреженными матрицами\nПереупорядочение и блочное разделение как способы решения этих проблем"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#краткий-обзор-предыдущей-лекции",
    "href": "lectures/lecture-6/lecture-6.html#краткий-обзор-предыдущей-лекции",
    "title": "",
    "section": "Краткий обзор предыдущей лекции",
    "text": "Краткий обзор предыдущей лекции\n\nСобственные векторы и собственные значения\nХарактеристический многочлен и почему это плохая идея\nСтепенной метод для нахождения ведущего (максимального по модулю) собственного значения и собственного вектора\nТеорема Шура: A = U T U^*\nНормальные матрицы: A^* A = A A^*\nПродвинутая тема: псевдоспектр"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#сегодняшняя-лекция",
    "href": "lectures/lecture-6/lecture-6.html#сегодняшняя-лекция",
    "title": "",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\n\nСегодня мы поговорим о матричных разложениях как общем инструменте\nОсновные матричные разложения в вычислительной линейной алгебре:\n\nLU-разложение и метод Гаусса — уже рассмотрены\nQR-разложение и алгоритм Грама-Шмидта\nРазложение Шура и QR-алгоритм\nМетоды вычисления SVD-разложения\n\nМы уже вводили QR-разложение некоторое время назад, но теперь мы обсудим его более подробно."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#общая-концепция-матричных-разложений",
    "href": "lectures/lecture-6/lecture-6.html#общая-концепция-матричных-разложений",
    "title": "",
    "section": "Общая концепция матричных разложений",
    "text": "Общая концепция матричных разложений\n\nВ вычислительной линейной алгебре нам нужно решать различные задачи, например:\n\nРешать линейные системы Ax = f\nВычислять собственные значения / собственные векторы\nВычислять сингулярные значения / сингулярные векторы\nВычислять обратные матрицы, иногда даже определители\nВычислять матричные функции такие как \\exp(A), \\cos(A) (это не поэлементные функции)\n\nДля этого мы представляем матрицу как сумму и/или произведение матриц с более простой структурой, чтобы решать упомянутые задачи быстрее / в более стабильной форме.\nЧто такое более простая структура?"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#что-такое-более-простая-структура",
    "href": "lectures/lecture-6/lecture-6.html#что-такое-более-простая-структура",
    "title": "",
    "section": "Что такое более простая структура",
    "text": "Что такое более простая структура\n\nМы уже встречали несколько классов матриц со структурой.\nДля плотных матриц наиболее важными классами являются\n\nунитарные матрицы\nверхние/нижние треугольные матрицы\nдиагональные матрицы"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#план",
    "href": "lectures/lecture-6/lecture-6.html#план",
    "title": "",
    "section": "План",
    "text": "План\nПлан сегодняшней лекции - обсудить разложения одно за другим и указать: - Как вычислить конкретное разложение - Когда разложение существует - Что делается в реальной жизни (LAPACK)."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#разложения-которые-мы-хотим-обсудить-сегодня",
    "href": "lectures/lecture-6/lecture-6.html#разложения-которые-мы-хотим-обсудить-сегодня",
    "title": "",
    "section": "Разложения, которые мы хотим обсудить сегодня",
    "text": "Разложения, которые мы хотим обсудить сегодня\n\nLU-разложение и разложение Холецкого — краткое напоминание, уже рассмотрены.\nQR-разложение и алгоритм Грама-Шмидта"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#plu-разложение",
    "href": "lectures/lecture-6/lecture-6.html#plu-разложение",
    "title": "",
    "section": "PLU разложение",
    "text": "PLU разложение\n\nЛюбая невырожденная матрица может быть представлена в виде\n\n A = P L U, \nгде P - матрица перестановки, L - нижняя треугольная матрица, U - верхняя треугольная.\n\nОсновная цель LU разложения - решение линейных систем, потому что\n\n A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, \nи это сводится к решению двух линейных систем\n L y = f,  \\quad U x = y \nс нижней и верхней треугольными матрицами соответственно.\nВопрос: какова сложность решения этих линейных систем?"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#положительно-определенные-матрицы-и-разложение-холецкого-напоминание",
    "href": "lectures/lecture-6/lecture-6.html#положительно-определенные-матрицы-и-разложение-холецкого-напоминание",
    "title": "",
    "section": "Положительно определенные матрицы и разложение Холецкого, напоминание",
    "text": "Положительно определенные матрицы и разложение Холецкого, напоминание\nЕсли матрица эрмитова положительно определенная, т.е.\n A = A^*, \\quad (Ax, x) &gt; 0, \\quad x \\ne 0, \nто она может быть представлена в виде\n A = RR^*, \nгде R - нижняя треугольная матрица.\nЭто нам понадобится для QR разложения."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#qr-разложение",
    "href": "lectures/lecture-6/lecture-6.html#qr-разложение",
    "title": "",
    "section": "QR разложение",
    "text": "QR разложение\n\nСледующее разложение: QR разложение.\nОпять же из названия понятно, что матрица представляется в виде произведения\n\n\n    A = Q R,\n\nгде Q - ортогональная (унитарная) по столбцам матрица, а R - верхняя треугольная матрица.\n\nРазмеры матриц: Q имеет размер n \\times m, R имеет размер m \\times m если n\\geq m. Смотрите наш постер для визуализации QR разложения\nQR разложение определено для любой прямоугольной матрицы."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#qr-разложение-применения",
    "href": "lectures/lecture-6/lecture-6.html#qr-разложение-применения",
    "title": "",
    "section": "QR разложение: применения",
    "text": "QR разложение: применения\nЭто разложение играет ключевую роль во многих задачах: - Вычисление ортогональных базисов в линейном пространстве - Используется в предварительной обработке для SVD - QR-алгоритм для вычисления собственных векторов и собственных значений (один из 10 самых важных алгоритмов 20-го века) основан на QR разложении - Решение переопределенных систем линейных уравнений (задача линейных наименьших квадратов)"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#qr-разложение-и-метод-наименьших-квадратов",
    "href": "lectures/lecture-6/lecture-6.html#qr-разложение-и-метод-наименьших-квадратов",
    "title": "",
    "section": "QR разложение и метод наименьших квадратов",
    "text": "QR разложение и метод наименьших квадратов\n\nПредположим, нам нужно решить\n\n \\Vert A x - b \\Vert_2 \\rightarrow \\min_x, \nгде A имеет размер n \\times m, n \\geq m.\n\nТогда мы факторизуем\n\n A = Q R, \nи используем уравнение для псевдообратной матрицы в случае матрицы A полного ранга:\n x = A^{\\dagger}b = (A^*A)^{-1}A^*b = ((QR)^*(QR))^{-1}(QR)^*b = (R^*Q^*QR)^{-1}R^*Q^*b = R^{-1}Q^*b. \nтаким образом, x можно найти из\nR x = Q^*b\n\nЗаметим, что это квадратная система линейных уравнений с нижней треугольной матрицей. Какова сложность решения этой системы?"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#существование-qr-разложения",
    "href": "lectures/lecture-6/lecture-6.html#существование-qr-разложения",
    "title": "",
    "section": "Существование QR разложения",
    "text": "Существование QR разложения\nТеорема. Каждая прямоугольная матрица размера n \\times m имеет QR разложение.\nСуществует несколько способов доказать это и вычислить его:\n\nТеоретический: с использованием матриц Грама и разложения Холецкого\nГеометрический: с использованием ортогонализации Грама-Шмидта\nПрактический: с использованием преобразований Хаусхолдера/Гивенса\n\n\nДоказательство с использованием разложения Холецкого\nЕсли у нас есть представление вида\nA = QR,\nтогда A^* A = ( Q R)^* (QR)  = R^* (Q^* Q) R = R^* R, матрица A^* A называется матрицей Грама, и её элементы являются скалярными произведениями столбцов матрицы A.\n\n\nДоказательство с использованием разложения Холецкого (полный ранг по столбцам)\n\nПредположим, что A имеет полный ранг по столбцам. Тогда легко показать, что A^* A положительно определена:\n\n (A^* A y, y) = (Ay, Ay) = \\Vert Ay \\Vert^2  &gt; 0, \\quad y\\not = 0. \n\nСледовательно, A^* A = R^* R всегда существует.\nТогда матрица Q = A R^{-1} является унитарной:\n\n (A R^{-1})^* (AR^{-1})= R^{-*} A^* A R^{-1} = R^{-*} R^* R R^{-1} = I. \n\n\nДоказательство с использованием разложения Холецкого (случай неполного ранга)\n\nКогда матрица n \\times m не имеет полного ранга по столбцам, она называется матрицей неполного ранга.\nQR разложение, однако, также существует.\nДля любой матрицы неполного ранга существует последовательность матриц полного ранга по столбцам A_k такая, что A_k \\rightarrow A (почему?).\nКаждая A_k может быть разложена как A_k = Q_k R_k.\nМножество всех унитарных матриц компактно, поэтому существует сходящаяся подпоследовательность Q_{n_k} \\rightarrow Q (почему?), и Q^* A_k \\rightarrow Q^* A = R, которая является треугольной."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#устойчивость-qr-разложения-через-разложение-холецкого",
    "href": "lectures/lecture-6/lecture-6.html#устойчивость-qr-разложения-через-разложение-холецкого",
    "title": "",
    "section": "Устойчивость QR разложения через разложение Холецкого",
    "text": "Устойчивость QR разложения через разложение Холецкого\n\nИтак, простейший способ вычислить QR разложение:\n\nA^* A = R^* R,\nи\nQ = A R^{-1}.\n\nЭто плохая идея с точки зрения численной устойчивости. Давайте рассмотрим пример (для подматрицы матрицы Гильберта).\n\n\nimport numpy as np\nn = 20\nr = 9\na = [[1.0 / (i + j + 0.5) for i in range(r)] for j in range(n)]\na = np.array(a)\nq, Rmat = np.linalg.qr(a)\ne = np.eye(r)\nprint('Built-in QR orth', np.linalg.norm(np.dot(q.T, q) - e))\ngram_matrix = a.T.dot(a)\nRmat1 = np.linalg.cholesky(gram_matrix)\n#q1 = np.dot(a, np.linalg.inv(Rmat1.T))\nq1 = np.linalg.solve(Rmat1, a.T).T\nprint('Via Cholesky:', np.linalg.norm(np.dot(q1.T, q1) - e))\n\nBuilt-in QR orth 7.668461578388947e-16\nVia Cholesky: 0.9966594169095181"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#второй-способ-ортогонализация-грама-шмидта",
    "href": "lectures/lecture-6/lecture-6.html#второй-способ-ортогонализация-грама-шмидта",
    "title": "",
    "section": "Второй способ: ортогонализация Грама-Шмидта",
    "text": "Второй способ: ортогонализация Грама-Шмидта\n\nQR разложение - это математический способ записи процесса ортогонализации Грама-Шмидта.\nИмея последовательность векторов a_1, \\ldots, a_m, мы хотим найти ортогональный базис q_1, \\ldots, q_m такой, что каждый a_i является линейной комбинацией этих векторов.\n\nГрам-Шмидт: 1. q_1 := a_1/\\Vert a_1 \\Vert 2. q_2 := a_2 - (a_2, q_1) q_1, \\quad q_2 := q_2/\\Vert q_2 \\Vert 3. q_3 := a_3 - (a_3, q_1) q_1 - (a_3, q_2) q_2, \\quad q_3 := q_3/\\Vert q_3 \\Vert 4. И так далее\nЗаметим, что преобразование от Q к A имеет треугольную структуру, так как из k-го вектора мы вычитаем только предыдущие. Это следует из того факта, что произведение треугольных матриц является треугольной матрицей."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#модифицированный-метод-грама-шмидта",
    "href": "lectures/lecture-6/lecture-6.html#модифицированный-метод-грама-шмидта",
    "title": "",
    "section": "Модифицированный метод Грама-Шмидта",
    "text": "Модифицированный метод Грама-Шмидта\n\nМетод Грама-Шмидта может быть очень неустойчивым (т.е. полученные векторы не будут ортогональными, особенно если q_k имеет малую норму).\nЭто называется потерей ортогональности.\nСуществует решение, называемое модифицированным методом Грама-Шмидта. Вместо того, чтобы делать\n\nq_k := a_k - (a_k, q_1) q_1 - \\ldots - (a_k, q_{k-1}) q_{k-1}\nмы делаем это пошагово. Сначала устанавливаем q_k := a_k и ортогонализуем последовательно:\n\n   q_k := q_k - (q_k, q_1)q_1, \\quad q_k := q_{k} - (q_k,q_2)q_2, \\ldots\n\n\nВ точной арифметике это одно и то же. В арифметике с плавающей точкой это абсолютно разные вещи!\nОбратите внимание, что сложность составляет \\mathcal{O}(nm^2) операций\n\n\nn = 100 \na = np.random.rand(n)\nb = a + 1e-9*np.random.randn(n)\na = a/np.linalg.norm(a)\nc = b - np.dot(b, a)*a\nc = c/np.linalg.norm(c)\nprint(np.dot(c, a))\nc = c - np.dot(c, a)*a\nc = c/np.linalg.norm(c)\nprint(np.dot(c, a))\n\n-1.3558882440423137e-07\n5.551115123125783e-17"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#qr-разложение-почти-практический-способ",
    "href": "lectures/lecture-6/lecture-6.html#qr-разложение-почти-практический-способ",
    "title": "",
    "section": "QR разложение: (почти) практический способ",
    "text": "QR разложение: (почти) практический способ\n\nЕсли A = QR, то\n\n R = Q^* A, \nи нам нужно найти определенную ортогональную матрицу Q, которая приводит матрицу к верхнетреугольной форме.\n- Для простоты мы будем искать матрицу n \\times n такую, что\n Q^* A = \\begin{bmatrix} * & * & *  \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ & 0_{(n-m) \\times m} \\end{bmatrix} \n\nМы будем делать это по столбцам.\n\nСначала мы находим матрицу Хаусхолдера H_1 = (I - 2 uu^{\\top}) такую, что (проиллюстрируем на матрице 4 \\times 3)\n\n H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & * & * \\\\ 0 & * & * \\end{bmatrix} \nЗатем,\n H_2 H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ 0 & 0 & * \\end{bmatrix}, \nгде\n H_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & H'_2, \\end{bmatrix} \nи H'_2 - это матрица Хаусхолдера размера 3 \\times 3.\nНаконец,\n H_3 H_2 H_1 A = \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\\\ 0 & 0 & 0 \\end{bmatrix}, \nВы можете попробовать реализовать это самостоятельно, это просто."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#qr-разложение-реальная-жизнь",
    "href": "lectures/lecture-6/lecture-6.html#qr-разложение-реальная-жизнь",
    "title": "",
    "section": "QR разложение: реальная жизнь",
    "text": "QR разложение: реальная жизнь\n\nВ реальности, поскольку это разложение плотной матрицы, алгоритм следует реализовывать в терминах блоков (почему?).\nВместо использования преобразования Хаусхолдера, мы используем блочное преобразование Хаусхолдера вида\n\nH = I - 2UU^*, \nгде U^* U = I.\n\nЭто позволяет нам использовать операции BLAS-3."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#qr-разложение-выявляющее-ранг",
    "href": "lectures/lecture-6/lecture-6.html#qr-разложение-выявляющее-ранг",
    "title": "",
    "section": "QR-разложение, выявляющее ранг",
    "text": "QR-разложение, выявляющее ранг\n\nQR-разложение также может быть использовано для вычисления (численного) ранга матрицы, см. Rank-Revealing QR Factorizations and the Singular Value Decomposition, Y. P. Hong; C.-T. Pan\nЭто делается с помощью так называемого разложения, выявляющего ранг.\nОно основано на представлении\n\nP A = Q R,\nгде P - матрица перестановки (она переставляет столбцы), а R имеет блочную форму\nR = \\begin{bmatrix} R_{11} & R_{12} \\\\ 0 & R_{22}\\end{bmatrix}.\n\nЦель состоит в том, чтобы найти P такую, что норма R_{22} мала, тогда вы можете определить численный ранг, взглянув на неё.\nОценка: \\sigma_{r+1} \\leq \\Vert R_{22} \\Vert_2 (проверьте, почему)."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#резюме",
    "href": "lectures/lecture-6/lecture-6.html#резюме",
    "title": "",
    "section": "Резюме",
    "text": "Резюме\n\nLU и QR разложения могут быть вычислены с помощью прямых методов за конечное количество операций.\nА как насчет формы Шура и SVD?\nОни не могут быть вычислены с помощью прямых методов (почему?) и могут быть вычислены только с помощью итерационных методов.\nХотя итерационные методы все равно имеют ту же сложность \\mathcal{O}(n^3) в арифметике с плавающей точкой благодаря быстрой сходимости."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#форма-шура",
    "href": "lectures/lecture-6/lecture-6.html#форма-шура",
    "title": "",
    "section": "Форма Шура",
    "text": "Форма Шура\n\nНапомним, что каждая матрица может быть записана в форме Шура\n\nA = Q T Q^*,\nгде T - верхнетреугольная матрица, а Q - унитарная матрица, и это разложение дает собственные значения матрицы (они находятся на диагонали T).\n\nПервым и основным алгоритмом для вычисления формы Шура является QR алгоритм."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#qr-алгоритм",
    "href": "lectures/lecture-6/lecture-6.html#qr-алгоритм",
    "title": "",
    "section": "QR алгоритм",
    "text": "QR алгоритм\n\nQR алгоритм был независимо предложен в 1961 году Кублановской и Фрэнсисом.\n Не путайте QR алгоритм и QR разложение! \nQR разложение - это представление матрицы, тогда как QR алгоритм использует QR разложение для вычисления собственных значений!"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#путь-к-qr-алгоритму",
    "href": "lectures/lecture-6/lecture-6.html#путь-к-qr-алгоритму",
    "title": "",
    "section": "Путь к QR алгоритму",
    "text": "Путь к QR алгоритму\n\nРассмотрим уравнение\n\nA = Q T Q^*,\nи перепишем его в форме\n Q T = A Q. \n\nСлева мы можем видеть QR разложение матрицы AQ.\nМы можем использовать это для вывода итерации с фиксированной точкой для формы Шура, также известной как QR алгоритм."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#вывод-qr-алгоритма-как-итерации-с-фиксированной-точкой",
    "href": "lectures/lecture-6/lecture-6.html#вывод-qr-алгоритма-как-итерации-с-фиксированной-точкой",
    "title": "",
    "section": "Вывод QR алгоритма как итерации с фиксированной точкой",
    "text": "Вывод QR алгоритма как итерации с фиксированной точкой\nМы можем записать итерационный процесс\n\n    Q_{k+1} R_{k+1} = A Q_k, \\quad Q_{k+1}^* A = R_{k+1} Q^*_k\n\nВведем\nA_k = Q^* _k A Q_k = Q^*_k Q_{k+1} R_{k+1} = \\widehat{Q}_k R_{k+1}\nи новое приближение имеет вид\nA_{k+1} = Q^*_{k+1} A Q_{k+1} = ( Q_{k+1}^* A = R_{k+1} Q^*_k)  = R_{k+1} \\widehat{Q}_k.\nТаким образом, мы приходим к стандартной форме QR алгоритма.\nИтоговые формулы записываются в классической форме QRRQ:\n\nНачинаем с A_0 = A.\nВычисляем QR разложение матрицы A_k = Q_k R_k.\nПолагаем A_{k+1} = R_k Q_k.\n\nПовторяем итерации, пока A_k не станет достаточно треугольной (например, норма поддиагональной части достаточно мала)."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#что-насчет-сходимости-и-сложности",
    "href": "lectures/lecture-6/lecture-6.html#что-насчет-сходимости-и-сложности",
    "title": "",
    "section": "Что насчет сходимости и сложности",
    "text": "Что насчет сходимости и сложности\nУтверждение\nМатрицы A_k унитарно подобны A\nA_k = Q^*_{k-1} A_{k-1} Q_{k-1} = (Q_{k-1} \\ldots Q_1)^* A (Q_{k-1} \\ldots Q_1)\nи произведение унитарных матриц является унитарной матрицей.\n\nСложность каждого шага составляет \\mathcal{O}(n^3), если выполняется общее QR разложение.\nМы надеемся, что A_k будет очень близка к треугольной матрице для достаточно большого k.\n\n\nimport numpy as np\nn = 40\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\nniters = 10000\nfor k in range(niters):\n    q, rmat = np.linalg.qr(a)\n    a = rmat.dot(q)\nprint('Leading 3x3 block of a:')\nprint(a[:4, :4])\n\nLeading 3x3 block of a:\n[[ 3.08733278e-01  3.08447671e+00  1.39093779e-02 -9.15979870e-02]\n [-3.05714157e+00  2.90031089e-01  1.49436447e-01 -4.41658920e-02]\n [-2.25478613e-21 -1.27296314e-20  5.50815465e-01  3.04169557e+00]\n [-1.45411659e-20 -7.68894785e-21 -3.00877077e+00  5.09624202e-01]]"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#сходимость-и-сложность-qr-алгоритма",
    "href": "lectures/lecture-6/lecture-6.html#сходимость-и-сложность-qr-алгоритма",
    "title": "",
    "section": "Сходимость и сложность QR алгоритма",
    "text": "Сходимость и сложность QR алгоритма\n\nСходимость QR алгоритма происходит от наибольших собственных значений к наименьшим.\nДля одного собственного значения требуется как минимум 2-3 итерации.\nКаждый шаг состоит из одной QR факторизации и одного произведения матриц, в результате сложность \\mathcal{O}(n^3).\n\nВ: означает ли это, что общая сложность \\mathcal{O}(n^4)?\nО: к счастью, нет.\n\nМы можем ускорить QR алгоритм, используя сдвиги, так как A_k - \\lambda I имеет те же векторы Шура.\nМы обсудим эти детали позже"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#сходимость-qr-алгоритма",
    "href": "lectures/lecture-6/lecture-6.html#сходимость-qr-алгоритма",
    "title": "",
    "section": "Сходимость QR алгоритма",
    "text": "Сходимость QR алгоритма\nВсегда ли сходится QR алгоритм?\nПриведите пример."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#контрпример",
    "href": "lectures/lecture-6/lecture-6.html#контрпример",
    "title": "",
    "section": "Контрпример",
    "text": "Контрпример\nДля матрицы A = \\begin{bmatrix} 0 & 1 \\\\\n                                  1 & 0 \\end{bmatrix}\nмы имеем A_k = A."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#связь-с-ортогональной-итерацией",
    "href": "lectures/lecture-6/lecture-6.html#связь-с-ортогональной-итерацией",
    "title": "",
    "section": "Связь с ортогональной итерацией",
    "text": "Связь с ортогональной итерацией\nВ предыдущей лекции мы рассмотрели степенной метод, который представляет собой A^k v – приближение собственного вектора.\nQR алгоритм вычисляет (неявно) QR-разложение матрицы A^k:\nA^k = A \\cdot \\ldots \\cdot A = Q_1 R_1 Q_1 R_1 \\ldots = Q_1 Q_2 R_2 Q_2 R_2 \\ldots (R_2 R_1) = \\ldots (Q_1 Q_2 \\ldots Q_k) (R_k \\ldots R_1)."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#несколько-слов-о-svd",
    "href": "lectures/lecture-6/lecture-6.html#несколько-слов-о-svd",
    "title": "",
    "section": "Несколько слов о SVD",
    "text": "Несколько слов о SVD\n\nИ последнее, но не менее важное: сингулярное разложение матрицы.\n\nA = U \\Sigma V^*.\n\nМы можем вычислить его через собственное разложение\n\nA^* A = V^* \\Sigma^2 V,\nи/или\nAA^* = U^* \\Sigma^2 U\nс помощью QR алгоритма, но это плохая идея (см. матрицы Грама).\n\nМы обсудим методы вычисления SVD позже."
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#итоги",
    "href": "lectures/lecture-6/lecture-6.html#итоги",
    "title": "",
    "section": "Итоги",
    "text": "Итоги\n\nQR разложение и алгоритм Грама-Шмидта, приведение к более простой форме с помощью преобразований Хаусхолдера\nРазложение Шура и QR алгоритм"
  },
  {
    "objectID": "lectures/lecture-6/lecture-6.html#следующие-шаги",
    "href": "lectures/lecture-6/lecture-6.html#следующие-шаги",
    "title": "",
    "section": "Следующие шаги",
    "text": "Следующие шаги\n\nЭффективная реализация QR алгоритма и его сходимость\nЭффективное вычисление SVD: 4 алгоритма\nДополнительные применения SVD\n\n\nВопросы?\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#краткий-обзор-предыдущей-лекции",
    "href": "lectures/lecture-8/lecture-8.html#краткий-обзор-предыдущей-лекции",
    "title": "",
    "section": "Краткий обзор предыдущей лекции",
    "text": "Краткий обзор предыдущей лекции\nSVD и алгоритмы для его вычисления: разделяй-и-властвуй, QR, Якоби, бисекция."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#сегодняшняя-лекция",
    "href": "lectures/lecture-8/lecture-8.html#сегодняшняя-лекция",
    "title": "",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\nСегодня мы кратко погрузимся в рандомизированную линейную алгебру.\nХорошим источником для чтения является (https://arxiv.org/pdf/2002.01387.pdf)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#алгоритмы-бывают-детерминированными-и-рандомизированными",
    "href": "lectures/lecture-8/lecture-8.html#алгоритмы-бывают-детерминированными-и-рандомизированными",
    "title": "",
    "section": "Алгоритмы бывают детерминированными и рандомизированными",
    "text": "Алгоритмы бывают детерминированными и рандомизированными\nВсе вычисления, которые мы рассматривали до сегодняшнего дня, были детерминированными.\nОднако снижение сложности может быть достигнуто с помощью рандомизированных (стохастических) вычислений.\nПример: рандомизированное умножение матриц."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#оказывается-мы-можем-проверить-если-a-b-c-за-mathcalon2-операций.",
    "href": "lectures/lecture-8/lecture-8.html#оказывается-мы-можем-проверить-если-a-b-c-за-mathcalon2-операций.",
    "title": "",
    "section": "Оказывается, мы можем проверить, если $ A B = C$ за \\mathcal{O}(n^2) операций.",
    "text": "Оказывается, мы можем проверить, если $ A B = C$ за \\mathcal{O}(n^2) операций.\nДано: A, B, C — матрицы n \\times n.\nЗадача: проверить, если $ A B = C$.\nКак?"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#ответ-дает-алгоритм-фрейвальдса",
    "href": "lectures/lecture-8/lecture-8.html#ответ-дает-алгоритм-фрейвальдса",
    "title": "",
    "section": "Ответ дает Алгоритм Фрейвальдса",
    "text": "Ответ дает Алгоритм Фрейвальдса\nВ его основе – проверка на равенство матриц путем умножения на случайные векторы!\nСложность составляет k n^2, вероятность ошибки равна \\frac{1}{2^k}."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#а-что-мы-можем-сделать-с-умножением-матриц",
    "href": "lectures/lecture-8/lecture-8.html#а-что-мы-можем-сделать-с-умножением-матриц",
    "title": "",
    "section": "А что мы можем сделать с умножением матриц?",
    "text": "А что мы можем сделать с умножением матриц?\nНо можем ли мы умножать матрицы быстрее, используя идеи рандомизации?"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#можно-построить-быстрое-но-приближенное-умножение-матриц",
    "href": "lectures/lecture-8/lecture-8.html#можно-построить-быстрое-но-приближенное-умножение-матриц",
    "title": "",
    "section": "Можно построить быстрое, но приближенное умножение матриц",
    "text": "Можно построить быстрое, но приближенное умножение матриц\n\nМы знаем, что умножение матриц AB стоит O(mnp) для матриц размеров m \\times p и p \\times n\nМы можем построить аппроксимацию этого произведения, выбирая строки и столбцы множителей\n\nВопрос: как их выбирать?\nОтвет: генерировать вероятности из их норм!\n\nИтак, окончательное выражение аппроксимации\n\n AB \\approx \\sum_{t=1}^k \\frac{1}{kp_{i_t}} A^{(i_t)} B_{(i_t)}, \nгде A^{(i_t)} - столбец A, а B_{(i_t)} - строка B\n\nСнижение сложности с O(mnp) до O(mnk)\n\n\nimport numpy as np\n\nn = 5\np = 10000\nm = 5\nA = np.random.randn(n, p)\nB = np.random.randn(p, m)\nC = A @ B\n\ndef randomized_matmul(A, B, k):\n    p1 = A.shape[1]\n    p = np.linalg.norm(A, axis=0) * np.linalg.norm(B, axis=1)\n    p = p\n    p = p.ravel() / p.sum()\n    n = A.shape[1]\n    p = np.ones(p1)\n    p = p/p.sum()\n    idx = np.random.choice(np.arange(n), (k,), False, p)\n    #d = 1 / np.sqrt(k * p[idx])\n    d = 1.0/np.sqrt(k)#np.sqrt(p1)/np.sqrt(k*p[idx])\n    A_sketched = A[:, idx]*np.sqrt(p1)/np.sqrt(k)#* d[None, :]\n    B_sketched = B[idx, :]*np.sqrt(p1)/np.sqrt(k) #* d[:, None]\n    C = A_sketched @ B_sketched\n    #print(d)\n    return C\n\ndef randomized_matmul_topk(A, B, K):\n    \n    norm_mult = np.linalg.norm(A,axis=0) * np.linalg.norm(B,axis=1)\n    top_k_idx = np.sort(np.argsort(norm_mult)[::-1][:K])\n    \n    A_top_k_cols = A[:, top_k_idx]\n    B_top_k_rows = B[top_k_idx, :]\n\n    C_approx = A_top_k_cols @ B_top_k_rows\n    return C_approx\n\nnum_items = 4000\nC_appr_samples = randomized_matmul(A, B, num_items)\n#print(C_appr_samples, 'appr')\n#print(C, 'true')\nC_appr_topk = randomized_matmul_topk(A, B, num_items)\nprint(np.linalg.norm(C_appr_topk - C, 2) / np.linalg.norm(C, 2))\nprint(np.linalg.norm(C_appr_samples - C, 2) / np.linalg.norm(C, 2))\n\n0.47962255089465017\n1.8999756830536187"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#оценка-ошибки-аппроксимации-потребует-знания-теории-вероятностей",
    "href": "lectures/lecture-8/lecture-8.html#оценка-ошибки-аппроксимации-потребует-знания-теории-вероятностей",
    "title": "",
    "section": "Оценка ошибки аппроксимации потребует знания теории вероятностей",
    "text": "Оценка ошибки аппроксимации потребует знания теории вероятностей\n \\mathbb{E} [\\|AB - CR\\|^2_F] = \\frac{1}{k} \\left(\\sum_{i=1}^n \\| A^{(i)} \\|_2 \\| B_{(i)} \\|_2\\right)^2   - \\frac{1}{k}\\|AB\\|_F^2 \n\nВозможны другие вероятности выборки\nИспользуйте аппроксимацию  AB \\approx ASD(SD)^\\top B  = ACC^{\\top}B можно заменить выборку и масштабирование другой матрицей, которая\n\nуменьшает размерность\nдостаточно точно аппроксимирует\n\n\nВопрос: какие матрицы можно использовать?"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#след-матрицы-тоже-можно-оценить",
    "href": "lectures/lecture-8/lecture-8.html#след-матрицы-тоже-можно-оценить",
    "title": "",
    "section": "След матрицы тоже можно оценить",
    "text": "След матрицы тоже можно оценить\nМногие задачи могут быть записаны в форме оценки следа:\n\\mathrm{Tr}(A) = \\sum_{i} A_{ii}.\nМожем ли мы вычислить след матрицы, если у нас есть доступ только к произведениям матрицы на вектор?"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#есть-две-оценки-одна-с-меньшей-дисперсией-другая-с-большей",
    "href": "lectures/lecture-8/lecture-8.html#есть-две-оценки-одна-с-меньшей-дисперсией-другая-с-большей",
    "title": "",
    "section": "Есть две оценки – одна с меньшей дисперсией, другая с большей",
    "text": "Есть две оценки – одна с меньшей дисперсией, другая с большей\nРандомизированные оценщики следа могут быть вычислены по следующей формуле:\n\\mathrm{Tr}(A) = E_w w^* A w, \\quad E ww^* = 1\nДля выборки мы берем k независимых образцов w_k, получаем случайную величину X_k и усредняем результаты.\nОценщик следа Жирара: Выборка w \\sim N(0, 1)\nТогда, \\mathrm{Var} X_k = \\frac{2}{k} \\sum_{i, j=1}^n \\vert A_{ij} \\vert^2 = \\frac{2}{k} \\Vert A \\Vert^2_F\nОценщик следа Хатчинсона: Пусть w будет случайным вектором Радемахера (т.е. элементы выбираются из равномерного распределения.\nОн дает оценщик с минимальной дисперсией."
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#можно-строить-более-тонкие-оценки-на-основе-внутренней-размерности",
    "href": "lectures/lecture-8/lecture-8.html#можно-строить-более-тонкие-оценки-на-основе-внутренней-размерности",
    "title": "",
    "section": "Можно строить более тонкие оценки на основе внутренней размерности",
    "text": "Можно строить более тонкие оценки на основе внутренней размерности\nДисперсия следа может быть оценена в терминах внутренней размерности (intdim) для симметричных положительно определенных матриц.\nОна определяется как \\mathrm{intdim}(A) = \\frac{\\mathrm{Tr}(A)}{\\Vert A \\Vert_F}. Легко показать, что\n1 \\leq \\mathrm{intdim}(A) \\leq ?.\nТогда вероятность большого отклонения может быть оценена как\nP( \\vert \\overline{X}_k - \\mathrm{Tr}(A) \\vert \\geq t \\mathrm{Tr}(A)) \\leq \\frac{2}{k \\mathrm{intdim}(A) t^2}"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#лучшие-оценки-для-симметричных-положительно-определенных-матриц",
    "href": "lectures/lecture-8/lecture-8.html#лучшие-оценки-для-симметричных-положительно-определенных-матриц",
    "title": "",
    "section": "Лучшие оценки для симметричных положительно определенных матриц",
    "text": "Лучшие оценки для симметричных положительно определенных матриц\nЕсли A - симметричная положительно определенная матрица, то\nP(\\overline{X}_k \\geq \\tau \\mathrm{Tr}(A) ) \\leq \\exp\\left(-1/2 \\mathrm{intdim}(A) (\\sqrt{\\tau} - 1)^2)\\right) \nАналогичное неравенство справедливо и для нижней границы.\nЭта оценка намного лучше.\nИнтересное (и часто упускаемое из виду) свойство стохастического оценщика заключается в том, что он поставляется со стохастической оценкой дисперсии (из выборки!)\nПредупреждение: нам все равно нужно \\varepsilon^{-2} выборок, чтобы достичь точности \\varepsilon при использовании независимых выборок.\n\nПример результата, где применялось – надо читать исходную статью (original paper)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#где-еще-помогают-стохастические-методы",
    "href": "lectures/lecture-8/lecture-8.html#где-еще-помогают-стохастические-методы",
    "title": "",
    "section": "Где еще помогают стохастические методы?",
    "text": "Где еще помогают стохастические методы?\n\nSVD\nЛинейные системы"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#рандомизированный-svd-halko-et-al-2011",
    "href": "lectures/lecture-8/lecture-8.html#рандомизированный-svd-halko-et-al-2011",
    "title": "",
    "section": "Рандомизированный SVD (Halko et al, 2011)",
    "text": "Рандомизированный SVD (Halko et al, 2011)\n\nНапоминание о постановке задачи\n\n A \\approx U\\Sigma V^\\top, \nгде A имеет размер m \\times n, U имеет размер m \\times k и V имеет размер n \\times k.\n\nМы уже знаем, что сложность аппроксимации ранга k составляет O(mnk)\nКак мы можем уменьшить эту сложность?\nПредположим, что мы знаем ортогональную матрицу Q размера m \\times k такую, что\n\nA \\approx Q Q^{\\top}A \n\nДругими словами, столбцы Q представляют ортогональный базис в пространстве столбцов матрицы A\nТогда следующие детерминированные шаги могут дать факторы U, \\Sigma и V, соответствующие SVD матрицы A\n\nФормируем матрицу B = Q^{\\top}A размера k \\times n\nВычисляем SVD малой матрицы B = \\hat{U}\\Sigma V^{\\top}\nОбновляем левые сингулярные векторы U = Q\\hat{U}\n\nЕсли k \\ll \\min(m, n), то эти шаги можно выполнить быстро\nЕсли Q образует точный базис в пространстве столбцов A, то U, \\Sigma и V также будут точными!\nИтак, как составить матрицу Q?\n\n\nРандомизированная аппроксимация базиса в пространстве столбцов A\n\nОсновной подход\n\nСгенерировать k + p гауссовых векторов размера m и сформировать матрицу G\nВычислить Y = AG\nВычислить QR-разложение Y и использовать полученную матрицу Q как аппроксимацию базиса\n\nПараметр p называется параметром переизбыточности и нужен для улучшения аппроксимации ведущих k левых сингулярных векторов\nВычисление Y может быть выполнено параллельно\nЗдесь нам нужна только функция умножения матрицы A на вектор, а не её элементы в виде 2D массива - концепция черного ящика!\nВместо гауссовой случайной матрицы можно использовать более структурированную, но все еще случайную матрицу, которую можно быстро умножить на A\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn = 1000\nk = 100\nm = 200\n# Lowrank matrix\nA = np.random.randn(n, k)\nB = np.random.randn(k, m)\nA = A @ B\n\n# Random matrix\n# A = np.random.randn(n, m)\n\ndef randomized_svd(A, rank, p):\n    m, n = A.shape\n    G = np.random.randn(n, rank + p)\n    Y = A @ G\n    Q, _ = np.linalg.qr(Y)\n    B = Q.T @ A\n    u, S, V = np.linalg.svd(B)\n    U = Q @ u\n    return U, S, V\n\nrank = 100\np = 5\nU, S, V = randomized_svd(A, rank, p)\nprint(\"Error from randomized SVD\", np.linalg.norm(A - U[:, :rank] * S[None, :rank] @ V[:rank, :]))\nplt.semilogy(S[:rank] / S[0], label=\"Random SVD\")\nu, s, v = np.linalg.svd(A)\nprint(\"Error from exact SVD\", np.linalg.norm(A - u[:, :rank] * s[None, :rank] @ v[:rank, :]))\nplt.semilogy(s[:rank] / s[0], label=\"Exact SVD\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\nError from randomized SVD 2.159802807908629e-11\nError from exact SVD 1.3779850607705733e-11\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as spsplin\n# More details about Facebook package for computing randomized SVD is here: https://research.fb.com/blog/2014/09/fast-randomized-svd/ \nimport fbpca\nn = 1000\nm = 200\nA = np.random.randn(n, m)\nk = 10\np = 10\n%timeit spsplin.svds(A, k=k)\n%timeit randomized_svd(A, k, p)\n%timeit fbpca.pca(A, k=k, raw=False) \n\n60.5 ms ± 11.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n8.07 ms ± 3.32 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n3.09 ms ± 177 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nМожно получить нетривиальные оценки сходимости\nУсредненная ошибка представленного алгоритма, где k - целевой ранг, а p - параметр избыточной выборки, следующая - в норме Фробениуса\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_F \\leq \\left( 1 + \\frac{k}{p-1} \\right)^{1/2}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^2_j \\right)^{1/2}  \n\nв спектральной норме\n\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_2 \\leq \\left( 1 + \\sqrt{\\frac{k}{p-1}} \\right)\\sigma_{k+1} + \\frac{e\\sqrt{k+p}}{p}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^2_j \\right)^{1/2} \nМатематическое ожидание берется относительно случайной матрицы G, сгенерированной в методе, описанном выше.\nСравните эти верхние границы с теоремой Экарта-Янга. Хороши ли эти границы?"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#умножив-несколько-раз-можно-оценить-больше-сингулярных-значений",
    "href": "lectures/lecture-8/lecture-8.html#умножив-несколько-раз-можно-оценить-больше-сингулярных-значений",
    "title": "",
    "section": "Умножив несколько раз, можно оценить больше сингулярных значений",
    "text": "Умножив несколько раз, можно оценить больше сингулярных значений\n\nОсновная идея: степенной метод\nЕсли A = U \\Sigma V^\\top, то $A^{(q)} = (AA{})qA = U {2q+1}V$, где q - некоторое небольшое натуральное число, например 1 или 2\nЗатем мы делаем выборку из A^{(q)}, а не из A\n\n Y = (AA^{\\top})^qAG \\qquad Q, R = \\mathtt{qr}(Y) \n\nОсновная причина: если сингулярные значения A убывают медленно, то сингулярные значения A^{(q)} будут убывать быстрее\n\n\nn = 1000\nm = 200\nA = np.random.randn(n, m)\ns = np.linalg.svd(A, compute_uv=False)\nAq = A @ A.T @ A\nsq = np.linalg.svd(Aq, compute_uv=False)\nplt.semilogy(s / s[0], label=\"$A$\")\nplt.semilogy(sq / sq[0], label=\"$A^{(1)}$\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#потеря-точности-из-за-ошибок-округления-но-мы-сможем-ее-исправить",
    "href": "lectures/lecture-8/lecture-8.html#потеря-точности-из-за-ошибок-округления-но-мы-сможем-ее-исправить",
    "title": "",
    "section": "Потеря точности из-за ошибок округления, но мы сможем ее исправить",
    "text": "Потеря точности из-за ошибок округления, но мы сможем ее исправить\nНаивное составление A^{(q)} приводит к росту числа обусловленности и потере точности\nQ: как мы можем бороться с этой проблемой?\nA: sequential orthogonalization!\n\ndef more_accurate_randomized_svd(A, rank, p, q):\n    m, n = A.shape\n    G = np.random.randn(n, rank + p)\n    Y = A @ G\n    Q, _ = np.linalg.qr(Y)\n    for i in range(q):\n        W = A.T @ Q\n        W, _ = np.linalg.qr(W)\n        Q = A @ W\n        Q, _ = np.linalg.qr(Q)\n    B = Q.T @ A\n    u, S, V = np.linalg.svd(B)\n    U = Q @ u\n    return U, S, V\n\nn = 1000\nm = 200\nA = np.random.randn(n, m)\n\nrank = 100\np = 20\nU, S, V = randomized_svd(A, rank, p)\nprint(\"Error from randomized SVD\", np.linalg.norm(A - U[:, :rank] * S[None, :rank] @ V[:rank, :]))\nplt.semilogy(S[:rank] / S[0], label=\"Random SVD\")\n\nUq, Sq, Vq = more_accurate_randomized_svd(A, rank, p, 5)\nprint(\"Error from more accurate randomized SVD\", np.linalg.norm(A - Uq[:, :rank] * Sq[None, :rank] @ Vq[:rank, :]))\nplt.semilogy(Sq[:rank] / Sq[0], label=\"Accurate random SVD\")\n\nu, s, v = np.linalg.svd(A)\nprint(\"Error from exact SVD\", np.linalg.norm(A - u[:, :rank] * s[None, :rank] @ v[:rank, :]))\nplt.semilogy(s[:rank] / s[0], label=\"Exact SVD\")\nplt.legend(fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylabel(\"$\\sigma_i / \\sigma_0$\", fontsize=16)\n_ = plt.xlabel(\"Index of singular value\", fontsize=16)\n\nError from randomized SVD 286.99760873015225\nError from more accurate randomized SVD 250.2388642432797\nError from exact SVD 249.3503301291079\n\n\n\n\n\n\n\n\n\n\n%timeit spsplin.svds(A, k=k)\n%timeit fbpca.pca(A, k=k, raw=False)\n%timeit randomized_svd(A, k, p) \n%timeit more_accurate_randomized_svd(A, k, p, 1)\n%timeit more_accurate_randomized_svd(A, k, p, 2)\n%timeit more_accurate_randomized_svd(A, k, p, 5)\n\n347 ms ± 60.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n82.3 ms ± 6.93 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n68.7 ms ± 4.99 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n118 ms ± 6.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n176 ms ± 13.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n352 ms ± 43.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nЗдесь тоже есть теорема сходимости\nПредставленный выше метод обеспечивает следующую верхнюю границу\n \\mathbb{E}\\|A - QQ^{\\top}A \\|_2 \\leq \\left[\\left( 1 + \\sqrt{\\frac{k}{p-1}} \\right)\\sigma^{2q+1}_{k+1} + \\frac{e\\sqrt{k+p}}{p}\\left( \\sum_{j=k+1}^{\\min(m, n)} \\sigma^{2(2q+1)}_j \\right)^{1/2}\\right]^{1/(2q+1)} \nРассмотрим наихудший случай, когда в данной матрице не существует структуры низкого ранга.\nВопрос: какова степень субоптимальности по отношению к теореме Экарта-Янга?\n\n\nРезюме по рандомизированному SVD\n\nЭффективный метод для получения приближенного SVD\nПрост в реализации\nМожет быть расширен до однопроходного метода, где матрица A нужна только для построения Q\nТребует только умножение матрицы на вектор с целевой матрицей"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#метод-качмажа-kaczmarz-method-для-решения-линейных-систем-позволяет-приближенно-решать-не-считая-даже-все-строки",
    "href": "lectures/lecture-8/lecture-8.html#метод-качмажа-kaczmarz-method-для-решения-линейных-систем-позволяет-приближенно-решать-не-считая-даже-все-строки",
    "title": "",
    "section": "Метод Качмажа (Kaczmarz method) для решения линейных систем позволяет приближенно решать, не считая даже все строки",
    "text": "Метод Качмажа (Kaczmarz method) для решения линейных систем позволяет приближенно решать, не считая даже все строки\n\nМы уже обсудили, как решать переопределенные линейные системы Ax = f в смысле наименьших квадратов\n\nпсевдообратная матрица\nQR разложение\n\nЕще один подход основан на итеративных проекциях, известный как метод Качмажа или алгебраический метод реконструкции в области вычислительной томографии\nВместо решения всех уравнений, выбираем одно случайно, которое имеет вид\n\na^{\\top}_i x = f_i,\nи имея приближение x_k, пытаемся найти x_{k+1} как\nx_{k+1} = \\arg \\min_x \\frac12 \\Vert x - x_k \\Vert^2_2, \\quad \\mbox{s.t.} \\quad  a^{\\top}_i x = f_i.\n\nПростой анализ дает\n\nx_{k+1} = x_k - \\frac{(a_i, x_k) - f_i}{(a_i, a_i)} a_i. \n\nЭто недорогое обновление, но анализ довольно сложный.\nВ этом методе можно узнать стохастический градиентный спуск с определенным размером шага, равным \\frac{1}{\\|a_i\\|_2^2} для каждого образца"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#теорема-сходимости-была-доказана-очень-недавно",
    "href": "lectures/lecture-8/lecture-8.html#теорема-сходимости-была-доказана-очень-недавно",
    "title": "",
    "section": "Теорема сходимости была доказана очень недавно",
    "text": "Теорема сходимости была доказана очень недавно\n\nПредположим, что мы генерируем i согласно распределению по всем доступным индексам пропорционально нормам строк, т.е. \\mathbb{P}[i = k] = \\frac{\\|a_k\\|_2^2}{\\| A \\|^2_F}. Этот метод называется рандомизированным методом Качмажа (RKM)\nПочему стратегия выборки здесь важна?\nИсследование наилучшей выборки представлено здесь\nЕсли переопределенная линейная система совместна, то\n\n \\mathbb{E}[\\|x_{k+1} - x^*\\|^2_2] \\leq \\left(1 - \\frac{1}{\\kappa^2_F(A)}\\right) \\mathbb{E}[\\|x_{k} - x^*\\|^2_2], \nгде \\kappa_F(A) = \\frac{\\| A \\|_F}{\\sigma_{\\min}(A)} и \\sigma_{\\min}(A) - минимальное ненулевое сингулярное число матрицы A. Этот результат был представлен в работе (Strohmer and Vershynin, 2009)\n\nЕсли переопределенная линейная система несовместна, то\n\n \\mathbb{E}[\\|x_{k+1} - x^*\\|^2_2] \\leq \\left(1 - \\frac{1}{\\kappa^2_F(A)}\\right) \\mathbb{E}[\\|x_{k} - x^*\\|^2_2] + \\frac{\\|r^*\\|_2^2}{\\| A \\|^2_F}, \nгде r^* = Ax^* - f"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#хотелось-бы-поговорить-о-скетчинге-но-это-уже-другая-история",
    "href": "lectures/lecture-8/lecture-8.html#хотелось-бы-поговорить-о-скетчинге-но-это-уже-другая-история",
    "title": "",
    "section": "Хотелось бы поговорить о скетчинге, но это уже другая история",
    "text": "Хотелось бы поговорить о скетчинге, но это уже другая история\n\nВыборка определенной строки может рассматриваться как частный случай более общего подхода, называемого скетчингом\nИдея: заменить матрицу A другой матрицей SA, где матрица SA имеет значительно меньшее количество строк, но сохраняет некоторые важные свойства матрицы A\nВозможные варианты:\n\nслучайная проекция\nслучайный выбор строк\n\nПример: задача линейных наименьших квадратов \\|Ax - b\\|_2^2 \\to \\min_x преобразуется в \\| (SA)y - Sb \\|_2^2 \\to \\min_y, и мы ожидаем, что x \\approx y\nРешатель Blendenpick основан на этой идее и превосходит по производительности процедуру LAPACK\nБолее подробную информацию см. в работе Sketching as a Tool for Numerical Linear Algebra автора D. Woodruff\n\n\nРезюме по рандомизированным методам решения линейных систем\n\nСемейство методов, простых в использовании\nОсобенно полезны в задачах с потоковыми данными\nСуществуют теоретические границы сходимости\nМножество интерпретаций в различных областях (SGD в глубоком обучении, ART в вычислительной томографии)\n\n\n\nРезюме по рандомизированному матричному умножению\n\nПростой метод для получения приближенного результата\nМожет использоваться, если высокая точность не критична\nОсобенно полезен для больших плотных матриц"
  },
  {
    "objectID": "lectures/lecture-8/lecture-8.html#следующая-лекция",
    "href": "lectures/lecture-8/lecture-8.html#следующая-лекция",
    "title": "",
    "section": "Следующая лекция",
    "text": "Следующая лекция\n\nМы начинаем изучение разреженной и/или структурированной численной линейной алгебры.\n\nВопросы?"
  },
  {
    "objectID": "files/sparse_la.html",
    "href": "files/sparse_la.html",
    "title": "Потеря разреженности",
    "section": "",
    "text": "import numpy as np\nimport scipy.sparse as sps\nimport timeit\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nnp.random.seed(42)\n\nn_values = np.linspace(200, 2000, dtype=int)\ndensity = 50\n\n# Заранее создаём и храним матрицы и векторы\nsparse_matrices = []\ndense_matrices = []\nvectors = []\n\nfor n in n_values:\n    A_sparse = sps.random(n, n, density=density/n**2, format='csr', random_state=42)\n    A_dense  = A_sparse.toarray()\n    x        = np.random.rand(n)\n    \n    sparse_matrices.append(A_sparse)\n    dense_matrices.append(A_dense)\n    vectors.append(x)\n\nsparse_times = []\ndense_times = []\n\npbar = tqdm(range(len(n_values)))\n\nfor i in pbar:\n    pbar.set_description(f\"Размер матрицы {n_values[i]}x{n_values[i]}\")\n    n = n_values[i]\n    A_sparse = sparse_matrices[i]\n    A_dense  = dense_matrices[i]\n    x        = vectors[i]\n\n    # Прогрев (не меряем это время, просто вызываем 1 раз)\n    A_sparse.dot(x)\n    A_dense.dot(x)\n\n    # Параметры timeit\n    number_runs = 200   # В каждом \"повторе\" делаем 200 умножений\n    repeat_runs = 5     # Сколько серий замеров делаем\n\n    setup_code = \"\"\"\nimport numpy as np\nimport scipy.sparse as sps\nfrom __main__ import A_sparse, A_dense, x\n\"\"\"\n\n    # Собираем многократные замеры и берём медиану\n    times_sparse = timeit.repeat(stmt=\"A_sparse.dot(x)\",\n                                 setup=setup_code,\n                                 repeat=repeat_runs,\n                                 number=number_runs)\n    times_dense  = timeit.repeat(stmt=\"A_dense.dot(x)\",\n                                 setup=setup_code,\n                                 repeat=repeat_runs,\n                                 number=number_runs)\n\n    median_sparse = np.median(times_sparse) / number_runs\n    median_dense  = np.median(times_dense)  / number_runs\n\n    sparse_times.append(median_sparse)\n    dense_times.append(median_dense)\n\n\n\n\n\n# Построение графика\nplt.figure(figsize=(6, 4))\nplt.plot(n_values, sparse_times, marker='o', label='CSR')\nplt.plot(n_values, dense_times, marker='x', label='Dense')\nplt.yscale('log')\nplt.grid(linestyle=':')\nplt.xlabel('Размер матрицы (N)')\nplt.ylabel('Медианное время матвека (с)')\nplt.title(f'Сравнение времени матвека, nnz={density}')\nplt.ylim(2e-6,1e-3)\nplt.legend()\nplt.savefig('sparse_fixed_nnz.pdf')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport scipy.sparse as sps\nimport timeit\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nnp.random.seed(42)\n\nn_values = np.linspace(200, 2000, dtype=int)\ndensity = 0.01\n\n# Заранее создаём и храним матрицы и векторы\nsparse_matrices = []\ndense_matrices = []\nvectors = []\n\nfor n in n_values:\n    A_sparse = sps.random(n, n, density=density, format='csr', random_state=42)\n    A_dense  = A_sparse.toarray()\n    x        = np.random.rand(n)\n    \n    sparse_matrices.append(A_sparse)\n    dense_matrices.append(A_dense)\n    vectors.append(x)\n\nsparse_times = []\ndense_times = []\n\npbar = tqdm(range(len(n_values)))\n\nfor i in pbar:\n    pbar.set_description(f\"Размер матрицы {n_values[i]}x{n_values[i]}\")\n    n = n_values[i]\n    A_sparse = sparse_matrices[i]\n    A_dense  = dense_matrices[i]\n    x        = vectors[i]\n\n    # Прогрев (не меряем это время, просто вызываем 1 раз)\n    A_sparse.dot(x)\n    A_dense.dot(x)\n\n    # Параметры timeit\n    number_runs = 200   # В каждом \"повторе\" делаем 200 умножений\n    repeat_runs = 5     # Сколько серий замеров делаем\n\n    setup_code = \"\"\"\nimport numpy as np\nimport scipy.sparse as sps\nfrom __main__ import A_sparse, A_dense, x\n\"\"\"\n\n    # Собираем многократные замеры и берём медиану\n    times_sparse = timeit.repeat(stmt=\"A_sparse.dot(x)\",\n                                 setup=setup_code,\n                                 repeat=repeat_runs,\n                                 number=number_runs)\n    times_dense  = timeit.repeat(stmt=\"A_dense.dot(x)\",\n                                 setup=setup_code,\n                                 repeat=repeat_runs,\n                                 number=number_runs)\n\n    median_sparse = np.median(times_sparse) / number_runs\n    median_dense  = np.median(times_dense)  / number_runs\n\n    sparse_times.append(median_sparse)\n    dense_times.append(median_dense)\n\n\n\n\n\n# Построение графика\nplt.figure(figsize=(6, 4))\nplt.plot(n_values, sparse_times, marker='o', label='CSR')\nplt.plot(n_values, dense_times, marker='x', label='Dense')\nplt.yscale('log')\nplt.grid(linestyle=':')\nplt.xlabel('Размер матрицы (N)')\nplt.ylabel('Медианное время матвека (с)')\nplt.title(f'Сравнение времени матвека, плотность={density}')\nplt.ylim(2e-6,1e-3)\nplt.legend()\nplt.savefig('sparse_fixed_density.pdf')\nplt.show()\n\n\n\n\n\n\n\n\n\nПотеря разреженности\n\nimport numpy as np\nimport scipy.sparse as sp\nimport scipy.sparse.linalg as spla\nimport matplotlib.pyplot as plt\n\n# 1. Создание разреженной матрицы\n# Создадим матрицу, где LU разложение без перестановок приведет к заполнению.\n# Пример: \"стреловидная\" матрица (arrowhead matrix)\nn = 10\nA_dense = np.diag(np.random.rand(n) + 1)  # Диагональ\nA_dense[0, 1:] = np.random.rand(n - 1) * 0.5\nA_dense[1:, 0] = np.random.rand(n - 1) * 0.5\nA = sp.csc_matrix(A_dense) # Преобразуем в разреженный формат CSC (Compressed Sparse Column)\n\nprint(\"Исходная разреженная матрица A (ненулевые элементы):\")\nprint(A)\n\n# 2. LU разложение без перестановок (естественный порядок)\n# Используем splu (SuperLU) из scipy.sparse.linalg\n# permc_spec=\"NATURAL\" означает отсутствие перестановок столбцов\nlu_natural = spla.splu(A, permc_spec=\"NATURAL\", diag_pivot_thresh=0) # diag_pivot_thresh=0 отключает частичный пивотинг\n\nL_natural = lu_natural.L\nU_natural = lu_natural.U\n\n# 3. LU разложение с перестановками для минимизации заполнения\n# Используем COLAMD (Column Approximate Minimum Degree) - популярный алгоритм\n# splu по умолчанию использует перестановки для уменьшения заполнения\nlu_permuted = spla.splu(A, diag_pivot_thresh=0) # По умолчанию permc_spec='COLAMD'\n\nL_permuted = lu_permuted.L\nU_permuted = lu_permuted.U\n# Важно: Факторы L и U соответствуют матрице P @ A @ Q, где P и Q - матрицы перестановок\n# lu_permuted.perm_r содержит индексы перестановки строк (P)\n# lu_permuted.perm_c содержит индексы перестановки столбцов (Q)\n\n# 4. Визуализация разреженности\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\nfig.suptitle(\"Влияние перестановок на заполненность при LU-разложении\", fontsize=16)\n\n# Исходная матрица\naxs[0, 0].spy(A, markersize=5)\naxs[0, 0].set_title(f\"Исходная A ({A.nnz} ненулевых)\")\naxs[0, 0].set_xticks([])\naxs[0, 0].set_yticks([])\n\n\n# Факторы без перестановок\naxs[0, 1].spy(L_natural, markersize=5)\naxs[0, 1].set_title(f\"L (без перестановок) ({L_natural.nnz} ненулевых)\")\naxs[0, 1].set_xticks([])\naxs[0, 1].set_yticks([])\n\n\naxs[0, 2].spy(U_natural, markersize=5)\naxs[0, 2].set_title(f\"U (без перестановок) ({U_natural.nnz} ненулевых)\")\naxs[0, 2].set_xticks([])\naxs[0, 2].set_yticks([])\n\n\n# Переставленная матрица (для наглядности, хотя splu работает с ней неявно)\nP = sp.csc_matrix((np.ones(n), (lu_permuted.perm_r, np.arange(n))))\nQ = sp.csc_matrix((np.ones(n), (np.arange(n), lu_permuted.perm_c)))\nA_permuted = P @ A @ Q\n\naxs[1, 0].spy(A_permuted, markersize=5)\naxs[1, 0].set_title(f\"P @ A @ Q ({A_permuted.nnz} ненулевых)\")\naxs[1, 0].set_xticks([])\naxs[1, 0].set_yticks([])\n\n\n# Факторы с перестановками\naxs[1, 1].spy(L_permuted, markersize=5)\naxs[1, 1].set_title(f\"L (с COLAMD) ({L_permuted.nnz} ненулевых)\")\naxs[1, 1].set_xticks([])\naxs[1, 1].set_yticks([])\n\n\naxs[1, 2].spy(U_permuted, markersize=5)\naxs[1, 2].set_title(f\"U (с COLAMD) ({U_permuted.nnz} ненулевых)\")\naxs[1, 2].set_xticks([])\naxs[1, 2].set_yticks([])\n\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\nplt.savefig('sparse_permutation.pdf')\nplt.show()\n\nprint(\"\\nСравнение количества ненулевых элементов:\")\nprint(f\"Исходная матрица A: {A.nnz}\")\nprint(f\"L (без перестановок): {L_natural.nnz}\")\nprint(f\"U (без перестановок): {U_natural.nnz}\")\nprint(f\"Сумма L+U (без перестановок): {L_natural.nnz + U_natural.nnz - n}\") # Вычитаем диагональ L, которая вся единицы\nprint(\"-\" * 20)\nprint(f\"L (с COLAMD): {L_permuted.nnz}\")\nprint(f\"U (с COLAMD): {U_permuted.nnz}\")\nprint(f\"Сумма L+U (с COLAMD): {L_permuted.nnz + U_permuted.nnz - n}\")\n\nИсходная разреженная матрица A (ненулевые элементы):\n  (0, 0)    1.8776782503720555\n  (1, 0)    0.28666311355271434\n  (2, 0)    0.06125074516763307\n  (3, 0)    0.20815901357243471\n  (4, 0)    0.14911718572511817\n  (5, 0)    0.4377389771128421\n  (6, 0)    0.2536100905730013\n  (7, 0)    0.28875627305162155\n  (8, 0)    0.1176452024641696\n  (9, 0)    0.3995317737956705\n  (0, 1)    0.16470247073023336\n  (1, 1)    1.535850677660174\n  (0, 2)    0.4023472696302903\n  (2, 2)    1.4785209479554453\n  (0, 3)    0.4455542569562675\n  (3, 3)    1.7728187724647264\n  (0, 4)    0.3091274855628439\n  (4, 4)    1.7410960035518377\n  (0, 5)    0.2777895963540577\n  (5, 5)    1.7042765323899776\n  (0, 6)    0.33594337379731914\n  (6, 6)    1.6899009560005396\n  (0, 7)    0.08153032873935356\n  (7, 7)    1.5153959971781843\n  (0, 8)    0.04367183810471542\n  (8, 8)    1.9264755359754915\n  (0, 9)    0.41007513748355745\n  (9, 9)    1.8906414097273678\n\n\n\n\n\n\n\n\n\n\nСравнение количества ненулевых элементов:\nИсходная матрица A: 28\nL (без перестановок): 55\nU (без перестановок): 55\nСумма L+U (без перестановок): 100\n--------------------\nL (с COLAMD): 19\nU (с COLAMD): 19\nСумма L+U (с COLAMD): 28"
  },
  {
    "objectID": "files/qr_alg_exercise.html",
    "href": "files/qr_alg_exercise.html",
    "title": "QR алгоритм + форма Гессенберга",
    "section": "",
    "text": "В этом упражнении вам необходимо написать функцию qr_iteration, которая будет выполнять один шаг QR-итерации.\nВходные данные: - A_k: матрица, которую нужно привести к треугольной форме\nВыходные данные: - A_{k+1}: матрица, к которой применена одна итерация QR-алгоритма\ndef qr_iteration(A):\n    \"\"\"Perform one step towards Schur form using QR iteration\"\"\"\n    return A\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import animation\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# ----------------------------\n# Parameters and Setup\n# ----------------------------\nN_FRAMES = 120  # total number of frames\nHOLD_FRAMES = 10  # frames to hold original images\nFPS = 20\nFILENAME = \"matrix_animation.mp4\"\nDPI = 200\nMATRIX_SIZE = 128  # size of the matrices\n\n# Style parameters\nLABELS_FONTSIZE = 10\nFIGSIZE = (8, 4)  # Adjusted for 2 images instead of 3\n\ndef load_image_as_matrix(url, size=(MATRIX_SIZE, MATRIX_SIZE)):\n    \"\"\"Load and preprocess image from URL into a matrix\"\"\"\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content)).convert('L')  # Convert to grayscale\n    img = img.resize(size)\n    return np.array(img)\n\n# Load two images\nurl0 = \"https://raw.githubusercontent.com/MerkulovDaniil/optim/refs/heads/master/assets/Notebooks/fmin_hopfield_1.webp\"\nurl1 = 'https://raw.githubusercontent.com/MerkulovDaniil/optim/refs/heads/master/assets/Notebooks/fmin_hopfield_0.webp'\n\n\n# Generate matrices from images\nA1 = load_image_as_matrix(url0)\nA2 = load_image_as_matrix(url1)\n\n# Store initial matrices for reference\nA1_init = A1.copy()\nA2_init = A2.copy()\n\ndef qr_iteration(A):\n    \"\"\"Perform one step towards Schur form using QR iteration\"\"\"\n    return A\n\n# ----------------------------\n# Set up the plot\n# ----------------------------\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=FIGSIZE, layout='tight')\n\n# Initial heatmaps\nim1 = ax1.imshow(A1, cmap='gray')\nim2 = ax2.imshow(A2, cmap='gray')\n\n# Set titles\nax1.set_title('Рус', fontsize=LABELS_FONTSIZE)\nax2.set_title('Ящер', fontsize=LABELS_FONTSIZE)\nplt.tight_layout()\n\n# Remove ticks\nfor ax in [ax1, ax2]:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Animation update function\ndef update(frame):\n    global A1, A2\n    \n    # Hold original images for HOLD_FRAMES frames\n    if frame &lt; HOLD_FRAMES:\n        return im1, im2\n    \n    # After hold period, perform Schur iterations\n    A1 = qr_iteration(A1)\n    A2 = qr_iteration(A2)\n    \n    im1.set_array(A1)\n    im2.set_array(A2)\n    return im1, im2\n\n# ----------------------------\n# Create and save the animation\n# ----------------------------\nprint(\"Generating animation...\")\nani = FuncAnimation(fig, update, frames=N_FRAMES, interval=100, blit=True)\n\n# Optimize video writing\nwriter = animation.FFMpegWriter(\n    fps=FPS,\n    metadata=dict(artist='Matrix Animation'),\n    bitrate=-1,\n    codec='h264',\n    extra_args=['-preset', 'ultrafast', '-crf', '28', '-pix_fmt', 'yuv420p', '-tune', 'animation']\n)\n\nwith tqdm(total=100, desc=\"Saving animation\") as pbar:\n    plt.tight_layout()\n    ani.save(FILENAME, writer=writer, dpi=DPI,\n             progress_callback=lambda i, n: pbar.update(100/n))\n\nplt.close()\n\nGenerating animation...\n\n\n\n\n\n/var/folders/7m/3rbdnx5n5sz625f3l87m91cc0000gn/T/ipykernel_55670/3608339251.py:100: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n\n\n\nQR алгоритм + форма Гессенберга\nВ этом упражнении мы сравним классический QR-алгоритм и QR-алгоритм с использованием формы Гессенберга. Вам необходимо реализовать две функции: naive_qr_iterations и hessenberg_qr_iterations.\ndef naive_qr_iterations(A, iterations):\n    \"\"\"Performs a fixed number of naive QR iterations.\"\"\"\n    A_k = A.copy()\n    for _ in range(iterations):\n        \n    return A_k # Return the result to ensure computation isn't optimized away\n\ndef hessenberg_qr_iterations(A, iterations):\n    \"\"\"Performs Hessenberg reduction and then a fixed number of QR iterations.\"\"\"\n    # 1. Reduce to Upper Hessenberg form\n    H = \n\n    # 2. Perform QR iterations on the Hessenberg matrix\n    H_k = H.copy()\n    for _ in range(iterations):\n        \n    return H_k # Return the result\n\nimport numpy as np\nimport scipy.linalg\nimport timeit\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm  # Optional: for progress bar\n\n# --- Configuration ---\n# Matrix sizes to test\nn_values = np.linspace(3, 90, 15, dtype=int)\n# Number of QR iterations to perform within the timed function\n# Note: This is *not* for full convergence, but to represent the iterative part.\n# The main benefit of Hessenberg is reducing the cost *per iteration*.\nNUM_ITERATIONS = 10\n# timeit parameters (mimicking -n 5 -r 5)\nTIMEIT_NUMBER = 9  # Number of executions within one repeat loop\nTIMEIT_REPEAT = 9  # Number of times to repeat the timing loop\n\n# --- Algorithm Implementations ---\n\ndef naive_qr_iterations(A, iterations):\n    \"\"\"Performs a fixed number of naive QR iterations.\"\"\"\n    A_k = A.copy()\n    for _ in range(iterations):\n        Q, R = np.linalg.qr(A_k)\n        A_k = R @ Q # Note: RQ instead of QR for eigenvalue algorithm\n    return A_k # Return the result to ensure computation isn't optimized away\n\ndef hessenberg_qr_iterations(A, iterations):\n    \"\"\"Performs Hessenberg reduction and then a fixed number of QR iterations.\"\"\"\n    # 1. Reduce to Upper Hessenberg form\n    H = \n\n    # 2. Perform QR iterations on the Hessenberg matrix\n    H_k = H.copy()\n    for _ in range(iterations):\n       \n    return H_k # Return the result\n\n# --- Timing Loop ---\nnaive_times = []\nhessenberg_times = []\n\nprint(f\"Starting timing comparison for n = {n_values}\")\nprint(f\"Using {NUM_ITERATIONS} QR iterations.\")\nprint(f\"Timeit settings: number={TIMEIT_NUMBER}, repeat={TIMEIT_REPEAT}\")\n\nfor n in tqdm(n_values, desc=\"Matrix Sizes\"):\n    # Create a random matrix for this size\n    matrix = np.random.rand(n, n)\n\n    # --- Time Naive QR ---\n    setup_code = f\"\"\"\nimport numpy as np\nfrom __main__ import naive_qr_iterations, matrix, NUM_ITERATIONS\n# Ensure matrix is defined in the setup scope for timeit\nmatrix_copy = matrix.copy()\n    \"\"\"\n    stmt_code = \"naive_qr_iterations(matrix_copy, NUM_ITERATIONS)\"\n\n    # timeit.repeat runs the setup once per repeat, then the statement 'number' times.\n    # It returns a list of total times for each repeat loop.\n    results_naive = timeit.repeat(\n        stmt=stmt_code,\n        setup=setup_code,\n        number=TIMEIT_NUMBER,\n        repeat=TIMEIT_REPEAT\n    )\n    # Get the best time and average it over the number of executions per loop\n    # This mimics the typical output of %timeit (best of 'r' repeats)\n    best_time_naive = min(results_naive) / TIMEIT_NUMBER\n    naive_times.append(best_time_naive)\n\n    # --- Time Hessenberg + QR ---\n    setup_code_hess = f\"\"\"\nimport numpy as np\nimport scipy.linalg\nfrom __main__ import hessenberg_qr_iterations, matrix, NUM_ITERATIONS\n# Ensure matrix is defined in the setup scope for timeit\nmatrix_copy = matrix.copy()\n    \"\"\"\n    stmt_code_hess = \"hessenberg_qr_iterations(matrix_copy, NUM_ITERATIONS)\"\n\n    results_hess = timeit.repeat(\n        stmt=stmt_code_hess,\n        setup=setup_code_hess,\n        number=TIMEIT_NUMBER,\n        repeat=TIMEIT_REPEAT\n    )\n    # Get the best time and average it\n    best_time_hess = min(results_hess) / TIMEIT_NUMBER\n    hessenberg_times.append(best_time_hess)\n\n    # Optional: print intermediate results\n    # print(f\"n={n}: Naive Time = {best_time_naive:.6f}s, Hessenberg Time = {best_time_hess:.6f}s\")\n\nprint(\"\\nTiming complete.\")\nprint(\"Naive Times:\", naive_times)\nprint(\"Hessenberg Times:\", hessenberg_times)\n\n# --- Plotting Results ---\nplt.figure(figsize=(10, 6))\nplt.plot(n_values, naive_times, 'o-', label=f'Naive QR ({NUM_ITERATIONS} iterations)')\nplt.plot(n_values, hessenberg_times, 's-', label=f'Hessenberg + QR ({NUM_ITERATIONS} iterations)')\n\n\nplt.xlabel(\"Matrix Size (n)\")\nplt.ylabel(f\"Average Execution Time (s) (best of {TIMEIT_REPEAT} repeats of {TIMEIT_NUMBER} loops)\")\nplt.title(\"QR Algorithm Performance Comparison\")\nplt.legend()\nplt.grid(True, which=\"both\", ls=\"--\")\nplt.yscale('log') # Use log scale for time as it grows quickly\nplt.xticks(n_values) # Ensure ticks are at the tested n values\n\nplt.tight_layout()\nplt.show()\n\nStarting timing comparison for n = [ 3  9 15 21 27 34 40 46 52 58 65 71 77 83 90]\nUsing 10 QR iterations.\nTimeit settings: number=9, repeat=9\n\n\nMatrix Sizes: 100%|██████████| 15/15 [00:02&lt;00:00,  5.14it/s]\n\n\n\nTiming complete.\nNaive Times: [0.0001225694444049926, 0.00011121300000619765, 0.00013676855562355032, 0.00017768055557907146, 0.00020818988893248994, 0.00029009255558776204, 0.0003725463332860575, 0.0004923055555789486, 0.0006390741111519876, 0.0008034213333303342, 0.0016623888888918576, 0.0020328148888842813, 0.0023104953334041056, 0.003200666666594366, 0.0034901897778480714]\nHessenberg Times: [0.0001085277778353581, 0.00012573611113313946, 0.00014839344445742123, 0.00016394899992317532, 0.0002040741111260205, 0.00025551388888642477, 0.00031028244443910406, 0.00039233333336596843, 0.0004637870000199958, 0.0005670601110776059, 0.0011991250000088864, 0.0013367638889071208, 0.0017542175555718131, 0.001469842555606091, 0.00201773144443804]\n\n\n\n\n\n\n\n\n\nПопробуйте увеличить размерность матрицы до 95 и посмотрите на результат. Объясните полученный эффект.\n\n\nСдвиги в степенном методе\n\nimport numpy as np\nnp.random.seed(42)\n\ndef generate_matrix(n, lambda_min, lambda_max):\n    # Создаем диагональную матрицу с равномерно распределенными собственными значениями\n    eigenvalues = np.linspace(lambda_min, lambda_max, n)\n    Lambda = np.diag(eigenvalues)\n    \n    # Генерируем случайную унитарную матрицу через QR-разложение\n    random_matrix = np.random.randn(n, n)\n    Q, _ = np.linalg.qr(random_matrix)\n    \n    # Формируем матрицу A = Q Λ Q^T\n    A = Q @ Lambda @ Q.T\n    \n    return A, eigenvalues, Q\n\ndef power_method(A, max_iter=1000, tol=1e-10):\n    n = A.shape[0]\n    x = np.random.randn(n)\n    x = x / np.linalg.norm(x)\n    \n    lambda_old = 0\n    for i in range(max_iter):\n        # Степенной метод\n        y = A @ x\n        lambda_new = np.dot(x, y)\n        x = y / np.linalg.norm(y)\n        \n        # Проверка сходимости\n        if abs(lambda_new - lambda_old) &lt; tol:\n            return lambda_new, x, i+1\n        lambda_old = lambda_new\n    \n    return lambda_new, x, max_iter\n\n# Параметры\nn = 5\nlambda_min = 1\nlambda_max = 10\n\n# Генерация матрицы\nA, true_eigenvalues, Q = generate_matrix(n, lambda_min, lambda_max)\n\nprint(\"Сгенерированная матрица A:\")\nprint(A)\nprint(\"\\nИстинные собственные значения:\")\nprint(true_eigenvalues)\n\n# Классический степенной метод\nlambda_max, v_max, iters_max = power_method(A)\nprint(\"\\nКлассический степенной метод:\")\nprint(f\"Найденное максимальное по модулю с.з.: {lambda_max:.6f}\")\nprint(f\"Количество итераций: {iters_max}\")\n\n# Степенной метод со сдвигом\nshift = 0\n\nA_shifted = A - shift * np.eye(n)\nlambda_min, v_min, iters_min = power_method(A_shifted)\nprint(f\"\\nСтепенной метод со сдвигом {shift}:\")\nprint(f\"Найденное с.з.: {lambda_min:.6f}\")\nprint(f\"Количество итераций: {iters_min}\")\n\nСгенерированная матрица A:\n[[ 6.36317478e+00 -3.03735322e-01  4.28467805e-01 -6.48432807e-01\n  -1.97941462e+00]\n [-3.03735322e-01  4.25676121e+00 -5.52679334e-03  1.35642825e+00\n   1.14184196e+00]\n [ 4.28467805e-01 -5.52679334e-03  7.85143322e+00 -2.43095386e+00\n   1.08761411e+00]\n [-6.48432807e-01  1.35642825e+00 -2.43095386e+00  6.19441545e+00\n   1.66061869e+00]\n [-1.97941462e+00  1.14184196e+00  1.08761411e+00  1.66061869e+00\n   2.83421534e+00]]\n\nИстинные собственные значения:\n[ 1.    3.25  5.5   7.75 10.  ]\n\nКлассический степенной метод:\nНайденное максимальное по модулю с.з.: 10.000000\nКоличество итераций: 44\n\nСтепенной метод со сдвигом 0:\nНайденное с.з.: 10.000000\nКоличество итераций: 67"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html",
    "href": "seminars/seminar-2/seminar-2.html",
    "title": "Matrix Norms Calculation",
    "section": "",
    "text": "Matrix norms, unitary matrices\nimport numpy as np"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#householder-martices",
    "href": "seminars/seminar-2/seminar-2.html#householder-martices",
    "title": "Matrix Norms Calculation",
    "section": "Householder Martices",
    "text": "Householder Martices\nHouseholder matrix is the matrix of the form: H \\equiv H(v) = I - 2 vv^*, where v is an n \\times 1 column and v^* v = 1.\nIt is also a reflection:  Hx = x - 2(v^* x) v\nAttention! If it does not work, remember about vector norm"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#build-your-own-from-a-vector",
    "href": "seminars/seminar-2/seminar-2.html#build-your-own-from-a-vector",
    "title": "Matrix Norms Calculation",
    "section": "Build your own from a vector",
    "text": "Build your own from a vector\n\ndef build_householder(v):\n    # v - vector of size n\n    v = v/np.linalg.norm(v)\n    a = np.identity(v.size)\n    \n    H = a - 2*np.outer(v, v)\n    \n    return H\n\n\nv = np.random.normal(size=(3))\nprint(v)\nh = build_householder(v)\nprint(h)"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#see-how-it-reflects-vectors",
    "href": "seminars/seminar-2/seminar-2.html#see-how-it-reflects-vectors",
    "title": "Matrix Norms Calculation",
    "section": "See how it reflects vectors",
    "text": "See how it reflects vectors\n\nv = np.random.normal(size=(3))\nh = build_householder(v)\n\nx = np.random.normal(size=(3))\nprint(x)\nprint(np.matmul(h, x))\n\n\nv = np.array([0,  1,  -1])\nh = build_householder(v)\nx = np.array([0,  1,  0])\nprint(np.round(h , decimals=2))\nprint(x)\nprint(np.round(h @ x.T, decimals=2))"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#optional-check-that-it-indeed-is-also-a-reflection",
    "href": "seminars/seminar-2/seminar-2.html#optional-check-that-it-indeed-is-also-a-reflection",
    "title": "Matrix Norms Calculation",
    "section": "Optional: check that it indeed is also a reflection",
    "text": "Optional: check that it indeed is also a reflection\n Hx = x - 2(v^* x) v\n\nv = np.random.normal(size=(3))\nv = v/np.linalg.norm(v)\nh = build_householder(v)\n\nx = np.random.normal(size=(3))\n\nv = np.array([0,  1,  -1])\nv = v/np.linalg.norm(v)\nh = build_householder(v)\nx = np.array([0,  1,  0])\n\nhx = h @ x.T\n\nreflected =  x - 2 * np.dot(v.T,x) * v\n\nprint(\"initial vector: \", x)\nprint(\"transofrmed by matrix: \", hx)\nprint(\"reflected by vector: \", reflected)"
  },
  {
    "objectID": "seminars/seminar-2/seminar-2.html#check-unitarity",
    "href": "seminars/seminar-2/seminar-2.html#check-unitarity",
    "title": "Matrix Norms Calculation",
    "section": "Check unitarity",
    "text": "Check unitarity\n\nUse numpy tools to check if the matrix is unitary using formula U* x U = I\n\nn = 3\n\n\nv = np.random.normal(size=(n))\nh = build_householder(v)\n\nprint(np.round(h @ h, decimals=2))\n\n\ndef householder_transform(A):\n    \"\"\"\n    Transforms the matrix A into an upper triangular matrix using Householder reflections.\n    \n    Parameters:\n        A (numpy.ndarray): The matrix to be transformed.\n    \n    Returns:\n        R (numpy.ndarray): The upper triangular matrix after applying Householder transformations.\n    \"\"\"\n    A = A.copy()\n    m, n = A.shape\n    for j in range(min(m, n)):\n        # Create the vector x for the current column\n        x = A[j:, j]\n        \n        # Calculate the norm of x and the Householder vector v\n        norm_x = np.linalg.norm(x)\n        if norm_x == 0:\n            continue\n        sign = -1 if x[0] &lt; 0 else 1\n        v = x.copy()\n        v[0] += sign * norm_x  # Adjust the first element of v for the reflection\n        v /= np.linalg.norm(v)  # Normalize v\n        \n        # Apply the Householder transformation to A[j:, j:]\n        A[j:, j:] -= 2 * np.outer(v, v @ A[j:, j:])\n    \n    return A\n\n# Example matrix\nA = np.array([\n    [4, 1, -2, 2],\n    [1, 2, 0, 1],\n    [-2, 0, 3, -2],\n    [2, 1, -2, -1]\n], dtype=float)\n\nR = householder_transform(A)\nprint(\"Upper triangular matrix R:\\n\", R)\n\n\n\nBonus task: check that it also preserves the norm. You can check it for your own custom norm if you created one!\n \\frac{\\Vert x - \\widehat{x} \\Vert}{\\Vert x \\Vert} \\leq \\varepsilon. \n \\frac{\\Vert y - \\widehat{y} \\Vert}{\\Vert y \\Vert } = \\frac{\\Vert U ( x - \\widehat{x}) \\Vert}{\\Vert U  x\\Vert}  \\leq \\varepsilon. \n\nv = np.random.normal(size=(n))\nv = v/np.linalg.norm(v)\nh = build_householder(v)\nepsilon = 0.001\n\nx = np.random.normal(size=(n))\nx_hat = x + epsilon * np.random.normal(size=(n)) # approximaton of x\ny = x - x_hat                                    # error of approximation\n\ninitial_error = np.linalg.norm(y)/np.linalg.norm(x)\ntransformed_error = np.linalg.norm(h @ y.T)/np.linalg.norm(h @ x.T)\n        \nprint(\"initial error:     \", initial_error)\nprint(\"transformed error: \", transformed_error)"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7.html",
    "href": "seminars/seminar-7/seminar-7.html",
    "title": "Recall determinant definition",
    "section": "",
    "text": "import numpy as np\nimport itertools\nfrom scipy.linalg import lu\nimport time\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7.html#determinant-of-triangular-matrix",
    "href": "seminars/seminar-7/seminar-7.html#determinant-of-triangular-matrix",
    "title": "Recall determinant definition",
    "section": "Determinant of triangular matrix",
    "text": "Determinant of triangular matrix\nDeterminant of an upper-(lower-)triangular matrix M is equal to \\prod_{i=1}^{n}M_{i,i}.\nProof: row expansion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Вычислительная линейная алгебра",
    "section": "",
    "text": "Вычислительная линейная алгебра\n\nКурс для 1-го курса бакалавриата ФПМИ МФТИ в рамках программы “AI360”. 1 лекция + 1 семинар в неделю.\nОценка за курс складывается из двух компонент в равном соотношении:\n\nТесты, проводимые на семинарах.\nОценка за проект.\n\n\nYour browser does not support the video tag.\n\n\n\n\n\n\n\n\n№\nЛекция\nСеминар\n\n\n\n\n1\nАрифметика чисел с плавающей точкой. Векторные нормы. Устойчивость\nЧисла с плавающей точкой, основы линейной алгебры - векторы, матрицы🎥 Видео💿 Скачать\n\n\n2\nИерархия памяти, умножение матриц, алгоритм Штрассена\nНормы матриц, унитарные матрицы✍️ Записи🎥 Видео💿 Скачать\n\n\n3\nРанг матрицы, скелетное разложение, SVD\nРанг метрица, скелетное разложение, SVD, LoRA ✍️ Записи🎥 Видео💿 Скачать\n\n\n4\nЛинейные системы\nSVD, PCA 👨‍💻 Eigenfaces ✍️ Записи🎥 Видео 💿 Скачать\n\n\n5\nСобственные значения и собственные векторы\nЛинейные системы, Собственные значения и собственные векторы ✍️ Записи🎥 Видео💿 Скачать\n\n\n6\nОбзор разложений матриц. Вычисление QR-разложения\nПроцесс Грама-Шмидта + QR-разложение  ✍️ Записи 👨‍💻 Упражнение PageRank 👨‍💻 Упражнение Modified Gram-Schmidt 🎥 Видео💿 Скачать\n\n\n7\nПроблема собственных значений симметричных матриц и SVD\nРазложение Шура. QR-алгоритм и свойства его сходимости. Форма Гессенберга. Как вычислять сингулярные числа, если мы умеем вычислять собственные значения?  ✍️ Записи 👨‍💻 Упражнение на QR-алгоритм 🎥 Видео💿 Скачать\n\n\n8\nРандомизированная линейная алгебра\nРандомизированное SVD, рандомизированное умножение матриц  👨‍💻 Практика  ✍️ Записи 🎥 Видео💿 Скачать\n\n\n9\nОт плотной к разреженной линейной алгебре\nПрактика с разреженными линейными матрицами  👨‍💻 Практика ✍️ Записи 🎥 Видео💿 Скачать\n\n\n10\nВведение в итерационные методы\nГрадиентный спуск + многочлены Чебышёва  ✍️ Записи 🎥 Видео💿 Скачать\n\n\n11\nКрутые итерационные методы\nМетод сопряженных градиентов\n\n\n12\nИтерационные методы и предобуславливатели\nShampoo\n\n\n13\nСтруктурированные матрицы, БПФ, свертки, матрицы Тёплица\nПрактика с матрично-векторными операциями\n\n\n\n\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Иван Валерьевич Оселедец\n                    \n                    Лектор\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Максимович Меркулов\n                    \n                    Семинарист\n                  \n                \n              \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#выбор-темы",
    "href": "projects.html#выбор-темы",
    "title": "",
    "section": "Выбор темы",
    "text": "Выбор темы\n🗓 10 апреля 2025. 🦄 4 балла\nВыбрать команду для проекта. Размер команды - 4-5 человек.\nК этому моменту необходимо согласовать с семинаристом/лектором тему проекта и высокоуровневое описание процесса работы над ним. Шаблон \\LaTeX для работы представлен здесь, выполнять этапы проекта надо в нём. Рекомендуется использовать VSCode/Cursor с расширением LaTeX Workshop. Так же доступна версия в overleaf. Допускается выполнение проекта в Typst, если вы умеете, но требования ниже будут предъявляться точно такие же (например, корректное цитирование). В дальнейшем необходимо использовать данный шаблон, дополняя его по мере продвижения в проекте. Все материалы и ссылки по теме удобно собирать в notion, чтобы при нашем обсуждении они были под рукой.\n\nВозможные темы\nГлавное требование к теме проекта - вам должно быть прикольно его делать, тема должна вас живо интересовать. Второе требование - он должен быть связан с вычислительной линейной алгеброй (хотя бы как-то 🙂). Тему проекта необходимо придумать/ найти/ выбрать самостоятельно.\n\nВзять недавно (можно смело смотреть по keywords SVD, Schur decomposition, low-rank и т.д. за последние несколько лет) опубликованную статью на конференции NeurIPS, ICML, ICLR. Детально разобраться. Воспроизвести. Попробовать на других данных/моделях/методах. Возможно, предложить посмотреть/протестировать что-нибудь новое.\nПрекрасным примером проекта может быть экстремальное выполнение классической задачи. Например,\n\nВычисление матричного умножения огромных матриц (здесь, вероятно, придется рассмотреть стохастические методы)\nВычисление максимального сингулярного числа огромной матрицы\nВычисление спектра огромной матрицы\nВычисление QR разложения огромной матрицы\nВычисление SVD огромной матрицы\nВычисление псевдообратной матрицы\nБыстрая оценка собственных значений огромной квадратной матрицы\n\nВ рамках такого проекта предлагается очень понятная постановка задачи и творческая свобода исследовать любые современные подходы к её решению - стохастические, требующие специальный формат входов, и т.д.\nВ статье Old Optimizer, New Norm: An Anthology авторы делают упор на построение разных методов оптимизации для современных задач машинного обучения с помощью решения специальной задачи оптимизации. Важно, что авторы предлагают единый взгляд на построение разных методов с помощью использования разных операторных норм. В рамках предлагается исследовать другие операторные нормы (например, ядерную), которые можно использовать для построения методов оптимизации и сравнить методы теоретически или численно. Можно вот изучить пост автора по теме вывода одного из популярных сегодня оптимизаторов для LLM.\nDecomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset - в статье авторы исследуют как матричные разложения могут помочь в задаче отделения фона от объекта. Похожих статей много.\nВ нашей статье мы рассматриваем метод квантизации больших языковых моделей с помощью разложения матрицы весов на два фактора, формирующих представление Кашина. Однако, некоторые унитарные матрицы плохо подходят для построения таких разложений. В рамках проекта предлагается исследовать возможные объяснения этого феномена.Можно посмотреть наш недавний доклад.\nНа этой странице чуть позже появятся возможные интересные статьи, которые можно взять в качестве темы проекта.\n\nНа этом этапе необходимо четко сформулировать решаемую задачу. С большой вероятностью, в этом пункте нужно написать задачу оптимизации или другую решаемую математическую задачу.\nОбратите внимание, что если в качестве проекта вы работаете с существующей статьей вы сначала должны написать свою задачу в рамках проекта - разобраться в чём-то, воспроизвести, повторить численные результаты, придумать другие численные эксперименты с другими моделями. А после этого необходимо так же привести в наиболее простом виде решаемую задачу из статьи. На этом этапе и далее важно не присваивать себе чужие заслуги. Постарайтесь избегать формулировок вида: мы решаем задачу (вместо этого можно написать авторы статьи решают задачу), мы хотим исследовать проблему/ предложить метод оптимизации/ применить численный метод или матричное разложение/ проанализировать сходимость, устойчивость (вместо этого авторы статьи предлагают/ исследуют и т.д.)\n\n\nФормат сдачи\nЗагрузить в таблицу в соответствующее поле pdf с названием темы и абстрактом. Убедитесь, что на этом этапе вы удалили остальные пункты из шаблона выше. Пдфка должна выглядеть как-то так:\n\n\n\nExample of this stage of the project\n\n\n\n\nКритерии оценивания\n\n0 баллов - файл не загружен вовремя/ файл загружен в другом формате\n1-2 балла - файл загружен вовремя, тема не согласована или решаемая задача сформулирована не чётко\n3-4 балла - файл загружен вовремя, тема согласована, решаемая проблема ясна и сформулирована чётко"
  },
  {
    "objectID": "projects.html#project-proposal",
    "href": "projects.html#project-proposal",
    "title": "",
    "section": "Project proposal",
    "text": "Project proposal\n🗓 25 апреля 2025. 🦄 16 баллов\nНа этом этапе необходимо понять научный ландшафт вокруг постановки задачи. Для этого постарайтесь, чтобы после литературного обзора были ясны ответы на следующие вопросы:\n\nКакие результаты были достигнуты в похожих формулировках?\nЕсть ли более простые формулировки и результаты в схожей тематике?\nНа какие результаты необходимо существенно опираться?\nКакие исследования обуславливают актуальность рассматриваемой задачи?\nС какими источниками необходимо ознакомиться для существенного понимания задачи?\nЕсть ли в открытом доступе код для воспроизведение экспериментов для рассматриваемой задачи?\n\nМожно использовать поиск в интернете, поиск по google scholar, поиск по perplexity.ai, поиск по ссылкам в статье. В идеале ссылаться на рецензируемые опубликованные статьи монографии. Однако, при необходимости, можно ссылаться на статьи на arxiv, блогпосты и другие источники, существенные и авторитетные для задачи. Цитирование необходимо делать с помощью bibtex. Пример откуда его брать приведён ниже:\n\n\n\nЗдесь показан один из способов найти bibtex для цитирования\n\n\nПосле литературного обзора необходимо собрать воедино всё, что было раньше, спланировать и провести фазу прототипирования. На мой взгляд - это важнейший этап проекта. Тут нужно очень четко определить куда и как двигаться, какие тропы уже пройдены другими людьми. Обратите внимание на следующие аспекты:\n\nНазвание проекта.\nAbstract (краткое описание проекта в один абзац).\nОписание проекта (обратите внимание на конкретность постановки задачи и её реалистичность).\nOutcomes - опишите, что конкретно будет выходом Вашего проекта (код, теорема, численные эксперименты, телеграм бот, веб сайт, приложение, рассказ).\nЛитературный обзор.\nДетальный план работ. Ясно, что в процессе выполнения проекта он будет меняться, однако наличие плана здесь лучше его отсутствия.\nМетрики качества. По возможности, приведите формальные и измеряемые показатели, по которым можно оценивать Ваше решение проект - это могут быть конкретные метрики качества алгоритмов, соц. опрос, логическое доказательство и т.д. Основная задача этого пункта - договориться на берегу о том, как мы сможем объективно оценить работу, проведенную в проекте. Обратите внимание, что результат проекта может быть “отрицательным” в том смысле, что мы собрались исследовать применение метода к какому-то классу задач и у нас не получилось. Это абсолютно нормально, тогда нужно будет просто описать этот процесс (мы попробовали и не вышло, но зато вот такое вот интересное наблюдали).\nОтчёт о фазе прототипирования. На этом этапе необходимо максимально широкими мазками приступить к работе над проектом. Если есть существующий код - нужно его запустить, представить результаты ваших экспериментов, показать проблемы, с которыми вы столкнулись. Попытаться предпринять первые шаги к решению проекта. Сделать что-нибудь с наскока. Совсем идеально показать какой-нибудь прототип (если это применимо к проекту).\n\n\nФормат сдачи\nЗагрузить в таблицу в соответствующее поле обновленный pdf с добавленным литературным обзором и project proposal с учётом фидбека по предыдущему этапу.\n\n\nКритерии оценивания\nБаллы будут сниматься в следующих случаях (список не полный):\n\nМенее 5 релевантных источников по теме.\nРабота с источниками была проведена поверхностно, ссылки добавлены ради ссылок, а не ради сути.\nВ результате литературного обзора совершенно не понятно, какое место занимает проект на научном ландшафте.\n\nНе учтены комментарии по предыдущему этапу, если они были.\nПлан работ не реалистичный, очень поверхностный. Обратите внимание, что тяжело уверенно планировать творческие задачи (доказать теорему). Здесь лучше писать чуть более специфично (например, попробовать доказать/ обобщить доказательство из другого источника).\nНет метрик качества.\nНет литературного обзора.\nНе написан чёткий выход (outcomes) проекта.\nНет отчёта о фазе прототипирования.\nИзображения в proposal низкого качества плохо подписаны."
  },
  {
    "objectID": "projects.html#первый-черновик-постера",
    "href": "projects.html#первый-черновик-постера",
    "title": "",
    "section": "Первый черновик постера",
    "text": "Первый черновик постера\n🗓 5 мая 2025. 🦄 10 баллов\nНа этом этапе необходимо подготовить черновик постера в латехе с разделением на разделы, объяснением - что в каком разделе где будет. Для удобства приведен 📝 \\LaTeX шаблон с 📜 примером.\nНа основании результатов проделанной работы также необходимо определить план публикации вашего проекта. Это может быть:\n\nстатья в журнале\nстатья на конференции\nстатья для летней школы\nдоклад на конференции\nпубликация на вашем сайте\nстатья в блоге\nвидео на YouTubeканале и т.д.\n\nВ частности, этот план содержит конкретные даты, выбранный журнал, куда это будет подаваться.\n\nФормат сдачи\nЗагрузить в таблицу в соответствующее поле черновик постера в pdf и план публикации с учётом фидбека по предыдущим этапам.\n\n\nКритерии оценивания\nБаллы будут сниматься в следующих случаях (список не полный):\n\nЧерновик постера отсутствует или выполнен не в латехе\nСтруктура постера нелогична или отсутствует четкое разделение на разделы\nНе представлен план публикации проекта\nНе учтены комментарии по предыдущим этапам, если они были"
  },
  {
    "objectID": "projects.html#постер",
    "href": "projects.html#постер",
    "title": "",
    "section": "Постер",
    "text": "Постер\n🗓 13 мая 2025. 🦄 20 баллов\nК этому этапу должен быть готов финальный постер в латехе с результатами проекта. Подведение итогов, публичная защита проекта. Оценивается выступление студента и качество представленных результатов. Выступление должно быть понятным, структурированным, интересным.\n\nФормат сдачи\nЗагрузить в таблицу в соответствующее поле финальный постер в pdf и подготовиться к публичной защите проекта.\n\n\nКритерии оценивания\nБаллы будут сниматься в следующих случаях (список не полный): * Финальный постер не соответствует требованиям или отсутствует * Выступление неструктурировано, непонятно или неинтересно * Результаты проекта представлены некачественно или неполно * Не учтены комментарии по предыдущим этапам\nВсе дедлайны понимаются как 23:59:59 по Московскому времени."
  },
  {
    "objectID": "seminars/seminar-7/seminar-7 (unsolved).html",
    "href": "seminars/seminar-7/seminar-7 (unsolved).html",
    "title": "Recall determinant definition",
    "section": "",
    "text": "import numpy as np\nimport itertools\nfrom scipy.linalg import lu\nimport time\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "seminars/seminar-7/seminar-7 (unsolved).html#determinant-of-triangular-matrix",
    "href": "seminars/seminar-7/seminar-7 (unsolved).html#determinant-of-triangular-matrix",
    "title": "Recall determinant definition",
    "section": "Determinant of triangular matrix",
    "text": "Determinant of triangular matrix\nDeterminant of an upper-(lower-)triangular matrix M is equal to \\prod_{i=1}^{n}M_{i,i}.\nProof: row expansion"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html",
    "href": "seminars/seminar-1/seminar-1.html",
    "title": "",
    "section": "",
    "text": "0.1+0.2\n\n0.30000000000000004\n0.4+0.5\n\n0.9\nФиксированная и плавающая точка, векторные нормы и понятие устойчивости алгоритмов."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#почему-0.1-0.2-0.3",
    "href": "seminars/seminar-1/seminar-1.html#почему-0.1-0.2-0.3",
    "title": "",
    "section": "🧐 Почему 0.1 + 0.2 != 0.3?",
    "text": "🧐 Почему 0.1 + 0.2 != 0.3?\nЧисла с плавающей запятой в Python хранятся в формате IEEE 754, который использует бинарное представление чисел. Однако не все десятичные дроби можно точно представить в двоичной системе. Например:\n\nДесятичное 0.1 в двоичном формате представляется бесконечной дробью:\n0.0001100110011001100110011001100110011... (повторяется бесконечно)\nТо же самое касается 0.2:\n0.001100110011001100110011001100110011... (тоже бесконечная дробь)\n\nТак как компьютер работает с ограниченной точностью, он усекает эти дроби, оставляя только конечное число битов. Это приводит к небольшим ошибкам округления.\n\n🧮 Доказательство в Python\nЕсли вывести точное двоичное представление 0.1, 0.2 и 0.3, можно увидеть разницу:\n\nfrom decimal import Decimal\n\nprint(Decimal(0.1))  # 0.1000000000000000055511151231257827021181583404541015625\nprint(Decimal(0.2))  # 0.200000000000000011102230246251565404236316680908203125\nprint(Decimal(0.3))  # 0.299999999999999988897769753748434595763683319091796875\n\n0.1000000000000000055511151231257827021181583404541015625\n0.200000000000000011102230246251565404236316680908203125\n0.299999999999999988897769753748434595763683319091796875\n\n\nТеперь сложим 0.1 + 0.2:\n\nprint(Decimal(0.1) + Decimal(0.2))  \n\n0.3000000000000000166533453694\n\n\nА 0.3 на самом деле:\n\nprint(Decimal(0.3))  \n\n0.299999999999999988897769753748434595763683319091796875\n\n\nОни не равны из-за разницы в последних разрядах!\n\n\n🤔 Как правильно сравнивать?\nПоскольку числа с плавающей запятой содержат небольшие ошибки округления, их нельзя сравнивать напрямую с ==. Вместо этого используют погрешность (epsilon):\n\n(0.1+0.2)==0.3\n\nFalse\n\n\n\nimport math\n\na = 0.1 + 0.2\nb = 0.3\n\nprint(math.isclose(a, b, rel_tol=1e-9)) \n\nTrue\n\n\nФункция math.isclose() проверяет, находятся ли два числа достаточно близко друг к другу с учетом заданной относительной ошибки."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#числа-с-фиксированной-точкой",
    "href": "seminars/seminar-1/seminar-1.html#числа-с-фиксированной-точкой",
    "title": "",
    "section": "Числа с фиксированной точкой",
    "text": "Числа с фиксированной точкой\nЧисло с фиксированной точкой состоит из 1-битного знака, m-битного целого и n-битного дробного числа: \n\\text{decimal} =\n(-1)^{\\text{sign}} \\times\n\\Big(\n\\sum_{i=0}^{m-1} \\text{integer}[i] \\cdot base^{m-1-i} +\n\\sum_{i=0}^{n-1} \\text{fractional}[i] \\cdot base^{-i-1}\n\\Big)\n\n\nдиапазон [-2^m + 2^{-n}, 2^m - 2^{-n}]\nразрешение 2^{-n}\nобщее количество бит m + n + 1\n\n\ndef binary_fixed_point_to_decimal(x, m=8, n=8):\n    \"\"\"\n    x - binary string of size 1 + m + n\n    m - size of an integer part\n    n - sze of a fractional part\n    \"\"\"\n    sign_part, integer_part, fractional_part = x[0], x[1:m+1], x[m+1:m+n+1]\n    sign_value = (-1) ** int(sign_part)\n    integer_value = sum([\n        int(v) * 2 ** i\n        for i, v in enumerate(integer_part[::-1])\n    ])\n    fractional_value = sum([\n        int(v) * 2 ** -(i + 1)\n        for i, v in enumerate(fractional_part)\n    ])\n    return sign_value * (integer_value + fractional_value)\n\nm, n = 8, 8\nx = '00000010100100000'\nprint(binary_fixed_point_to_decimal(x, m, n) == 5.125)\n\nTrue\n\n\n\nx = '11111111111111111' # Insert a string corresponding to a minimal possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == -(2 ** m - 2 ** (-n)))\n\nTrue\n\n\n\nx = '01111111111111111' # Insert a string corresponding to a maximal possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == 2 ** m - 2 ** (-n))\n\nTrue\n\n\n\nx = '00000000000000001' # Insert a string corresponding to an absolute minimal but nonzero possible value\nprint(binary_fixed_point_to_decimal(x, m, n) == 2 ** (-n))\n\nTrue\n\n\nОсновной недостаток чисел с фиксированной точкой — ограниченный диапазон и разрешение. Например, для 8-битного формата: - диапазон [-128, 127] - разрешение 2^{-7} = 0.0078125\nЭто означает, что для представления очень больших или очень маленьких чисел потребуется больше бит.\nДля 32-битного формата: - диапазон [-2^{31}, 2^{31} - 1] - разрешение 2^{-23} \\approx 1.192 \\times 10^{-7}\nЭто означает, что для представления очень больших или очень маленьких чисел потребуется больше бит."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#числа-с-плавающей-точкой",
    "href": "seminars/seminar-1/seminar-1.html#числа-с-плавающей-точкой",
    "title": "",
    "section": "Числа с плавающей точкой",
    "text": "Числа с плавающей точкой\nЧисла с плавающей точкой состоят из 1-битного знака, m-битного экспоненты и n-битного мантиссы:\n\n\\text{decimal} =\n(-1)^{\\text{sign}} \\times\nbase^{\\Big(\\sum_{i=0}^{m-1} \\text{exponent}[i] \\cdot base^{m-1-i} - (2^{m-1} - 1)\\Big)}\n\\times\n\\Big(1 + \\sum_{i=0}^{n-1} \\text{mantissa}[i] \\cdot base^{-i-1}\\Big)\n\n\nзначения экспоненты, равные 0 и 1, зарезервированы для специальных чисел: NaN, бесконечность и т.д.\nобщее количество бит m + n + 1\n\nHalf (float16) vs Single (float32) vs and Double (float32) Точность\n\nfloat16 - 16 bit total: 1 for a sign, m = 5 for exponent and n = 10 for mantissa\nfloat32 - 32 bits total: 1 for a sign, m = 8 for exponent and n = 23 for mantissa\nfloat64 - 64 bits total: 1 for a sign, m = 11 for exponent and n = 52 for mantissa\n\n\ndef binary_floating_point_to_decimal(x, m=8, n=23):\n    \"\"\"\n    x - binary string of size 1 + m + n\n    m - size of an exponent part\n    n - sze of a mantissa part\n    \"\"\"\n    sign_part, exponent_part, mantissa_part = x[0], x[1:m+1], x[m+1:n+m+1]\n    sign_value = (-1) ** int(sign_part)\n\n    mantissa_value = 1\n    for i, v in enumerate(mantissa_part):\n        mantissa_value += int(v) * (2 ** -(i + 1))\n\n    exponent_value = 0\n    for i, v in enumerate(exponent_part):\n        exponent_value += int(v) * 2 ** i\n    exponent_value -= (2 ** (m - 1) - 1)\n        \n    return sign_value * (2 ** exponent_value) * mantissa_value\n\nm, n = 8, 23\nx = '01000000101001000000000000000000'\nprint(binary_floating_point_to_decimal(x, m, n) == 5.125)\n\nTrue"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#ошибки-округления",
    "href": "seminars/seminar-1/seminar-1.html#ошибки-округления",
    "title": "",
    "section": "Ошибки округления",
    "text": "Ошибки округления\nИз-за того, что представления с плавающей точкой являются лишь приближениями к действительным числам, могут возникать ошибки округления.\nНапример, рассмотрим простой алгоритм суммирования, где x_i — числа с плавающей точкой:\n\nf(x) = x_1 + x_2 + ... + x_n\n\nРеализуйте простой алгоритм из лекции (добавляя по одному):\n[!] Установите n равным 1000 и все x_i равными 0.1.\n\ntotal = 0.0\nfor _ in range(100000):\n    total += 0.1\n\nprint(\"Expected result: 10000.0\")\nprint(f\"Actual result:\", total)\n\nExpected result: 10000.0\nActual result: 10000.000000018848\n\n\nРеализуйте алгоритм Кахана из лекции и проверьте возникающую ошибку.\n[!] Установите n равным 1000 и все x_i равными 0.1.\n\ns = 0\nc = 0\nfor i in range(1000):\n    y = 0.1 - c\n    t = s + y\n    c = (t - s) - y\n    s = t\n\nprint(\"Expected result: 100.0\")\nprint(f\"Actual result:\", s)\n\nExpected result: 100.0\nActual result: 100.0\n\n\nОбъяснение: значение 0.1 не может быть точно представлено в двоичной системе, поэтому оно становится приближением. Когда это приближение добавляется по одному, малые ошибки округления накапливаются, что приводит к конечному результату, немного меньшему, чем 100.0."
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#векторы-и-векторные-нормы",
    "href": "seminars/seminar-1/seminar-1.html#векторы-и-векторные-нормы",
    "title": "",
    "section": "Векторы и векторные нормы",
    "text": "Векторы и векторные нормы\nВ NLA мы обычно работаем не с числами, а с векторами, которые являются просто массивами чисел размера n.\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])\n\nprint(f'Size of the x vector is {len(x)}')\nprint(f'Type of the vector elements is {type(x[0])}')\n\nSize of the x vector is 5\nType of the vector elements is &lt;class 'numpy.int64'&gt;\n\n\nКак видите, этот вектор содержит только целые значения. Теперь преобразуйте их в тип float32.\n\nx = x.astype(np.float32)\nprint(f'Type of the vector elements is {type(x[0])}')\n\nType of the vector elements is &lt;class 'numpy.float32'&gt;\n\n\nДля измерения малости вектора используется его норма \\|x\\|.\nСамый важный класс норм — p-нормы: \n\\|x\\|_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}\n Примеры p-норм: - Манхэттенское расстояние или L_1 норма - когда p=1 - Евклидова норма или L_2 норма - когда p=2 - Бесконечная норма, или норма Чебышева - когда p=+\\infty: $ |x|_{} = _i | x_i|$\nПосчитайте нормы для вектора x:\nПодсказка: используйте np.linalg.norm\n\nprint('L1 norm:', np.linalg.norm(x, 1))\nprint('L2 norm:', np.linalg.norm(x, 2))\nprint('Chebyshev norm:', np.linalg.norm(x, np.inf))\n\nL1 norm: 15.0\nL2 norm: 7.4161983\nChebyshev norm: 5.0\n\n\nЕдиничный диск для p-нормы — это множество точек таких, что \\|x\\|_p = 1.\nВизуализируйте единичный диск для следующих p-норм: p \\in (0.25, 0.75, 1.0, 2.0, 5.0, \\infty)\nПодсказка: y = \\pm (1 - |x|^p)^{1/p}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef unit_disk(p):\n    x = np.linspace(-1, 1, 201)\n    y = (1 - np.abs(x) ** p) ** (1 / p)\n    x = np.hstack([x, x[1:][::-1], x[0]])\n    y = np.hstack([y, -y[1:][::-1], y[0]])\n    return x, y\n\nplt.figure(figsize=(4, 4))\nplt.axis('equal')\nfor p in (0.25, 0.5, 1.0, 2.0, 5.0, np.inf):\n    x, y = unit_disk(p)\n    plt.plot(x, y, label=f'$p$={p}')\nplt.legend(loc=1)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#устойчивость",
    "href": "seminars/seminar-1/seminar-1.html#устойчивость",
    "title": "",
    "section": "Устойчивость",
    "text": "Устойчивость\nПредположим, у нас есть вектор x, функция f(x), и алгоритм \\text{alg}(x) для приближения функции. Тогда алгоритм называется устойчивым в прямом направлении, если для некоторого малого \\varepsilon\n\n\\|\\text{alg}(x) - f(x)\\|  \\leq \\varepsilon\n\n[Задание] Проверьте суммирующие алгоритмы, упомянутые ранее (простой и Кахана) на устойчивость в прямом направлении.\nПусть x_i = 0.1 и n = 100. \nf(x) = \\sum_{i=1}^{100} x_i, \\;\\;\nx_i = 0.1\n Запишите ошибку, возникающую в каждом шаге суммирования: \n\\text{error}[i] = |0.1 \\cdot i - \\text{alg}(x)|\n\n\nfrom matplotlib import pyplot as plt\nN = 10000\n\n# Naive\ntotal = 0.0\nerror_naive = []\nfor i in range(N):\n    total += 0.1\n    refer = (i + 1) / 10\n    error_naive.append(np.abs(refer - total))\n\n# Kahan\ns = 0\nc = 0\nerror_kahan = []\nfor i in range(N):\n    y = 0.1 - c\n    t = s + y\n    c = (t - s) - y\n    s = t\n    error_kahan.append(np.abs(c))\n\nplt.figure(figsize=(8, 4))\nplt.title(r'Forward stability of summation algorithms $\\varepsilon(n)$')\nplt.plot(error_naive, label='Naive')\nplt.plot(error_kahan, label='Kahan')\nplt.ylabel(r'$\\varepsilon$', rotation=0)\nplt.xlabel(r'$n$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWhat do you see?"
  },
  {
    "objectID": "seminars/seminar-1/seminar-1.html#простая-но-очень-важная-идея-умножения-матриц",
    "href": "seminars/seminar-1/seminar-1.html#простая-но-очень-важная-идея-умножения-матриц",
    "title": "",
    "section": "Простая, но очень важная идея умножения матриц",
    "text": "Простая, но очень важная идея умножения матриц\nПредположим, у вас есть следующее выражение: \nb = A_1 A_2 A_3 x,\n где A_1, A_2, A_3 \\in \\mathbb{R}^{3 \\times 3} - случайные квадратные плотные матрицы, а x \\in \\mathbb{R}^n - вектор. Вам нужно вычислить b. Какой способ лучше всего использовать?\nA_1 A_2 A_3 x (слева направо) \\left(A_1 \\left(A_2 \\left(A_3 x\\right)\\right)\\right) (справа налево) Не имеет значения Результаты первых двух вариантов не будут одинаковыми.\n\nimport numpy as np\n\n# Function to create a random square matrix of size n\ndef create_random_matrix(n):\n    return np.random.rand(n, n)\n\n# Define the size of the matrices\nn = 200\n\n# Create a list of 3 random matrices\nmatrices = [create_random_matrix(n) for _ in range(3)]\ny = np.random.rand(n, 1)  # y is a vector\n\n# Function to compute the expression in a given order\ndef compute_expression(matrices, y, reverse=False):\n    result = y\n    if reverse:\n        # Start with y and multiply with each matrix from right to left\n        for matrix in reversed(matrices):\n            result = matrix @ result\n    else:\n        # Start with the first matrix and multiply each next matrix from left to right\n        result = matrices[0]\n        for matrix in matrices[1:]:\n            result = result @ matrix\n        # Finally multiply with y\n        result = result @ y\n    return result\n\n# Time the expression from left to right\nprint(\"💎 From left to right\")\n%timeit compute_expression(matrices, y)\n\n# Time the expression from right to left\nprint(\"💎 From right to left\")\n%timeit compute_expression(matrices, y, reverse=True)\n\n💎 From left to right\n2.23 ms ± 636 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n💎 From right to left\n56.3 µs ± 3.95 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html",
    "href": "seminars/seminar-4/seminar-4.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#four-fundamental-subspaces",
    "href": "seminars/seminar-4/seminar-4.html#four-fundamental-subspaces",
    "title": "",
    "section": "Four Fundamental Subspaces",
    "text": "Four Fundamental Subspaces\nLet A = \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix}\n\nFind the column space \\text{col}(A) and null space \\text{null}(A) of A.\nDetermine if \\vec{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\in \\text{col}(A).\nDetermine if \\vec{u} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\text{null}(A).\n\n\ndef swap(matrix, row1, row2):\n    \"\"\"Swap two rows in a matrix.\"\"\"\n    matrix[[row1, row2]] = matrix[[row2, row1]]\n    return matrix\n\ndef scale(matrix, row, scalar):\n    \"\"\"Multiply all entries in a specified row by a scalar.\"\"\"\n    matrix[row] *= scalar\n    return matrix\n\ndef replace(matrix, row1, row2, scalar):\n    \"\"\"Replace a row by itself plus a scalar multiple of another row.\"\"\"\n    matrix[row1] += scalar * matrix[row2]\n    return matrix\n\n\nM = np.array([[1, -1, 0], [2, 1, 0], [-1, 3, 0]], dtype=float)\n\n\nM1 = replace(M.copy(), 1, 0, -2)\nM2 = replace(M1, 2, 0, 1)\nM3 = scale(M2, 1, 1/3)\nM4 = replace(M3, 2, 1, -2)\n\nprint(\"Reduced Matrix M4:\")\nprint(M4)\n\n\nM_augm = np.array([[1, -1, 1], [2, 1, 2], [-1, 3, 1]], dtype=float)\nM\n\n\n# Row Reduction Example\nM1 = replace(M_augm.copy(), 1, 0, -2)\nM2 = replace(M1, 2, 0, 1)\nM3 = scale(M2, 1, 1/3)\nM4 = replace(M3, 2, 1, -2)\nM4\n\nA = \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & -1 \\\\ 0 & 3 \\\\ 0 & 2 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\sim\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \n\\mathrm{im}(A) = \\mathrm{col}(A) = \\mathrm{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 3 \\end{bmatrix} \\right\\}\n\\mathrm{im}(A^T) =  \\mathrm{col}(A^T) = \\mathrm{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\right\\}\n\\mathrm{ker}(A) = \\mathrm{null}(A) = \\{ 0 \\}\n\\mathrm{ker}(A^T) = \\mathrm{null}(A^T) =  \\mathrm{span}\\left\\{ \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right\\}"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#skeleton-decomposition",
    "href": "seminars/seminar-4/seminar-4.html#skeleton-decomposition",
    "title": "",
    "section": "Skeleton decomposition",
    "text": "Skeleton decomposition\nA = \\sum_{\\alpha = 1}^r U_{\\alpha} V_{\\alpha}^T\n##Low-rank Approximation\n\nimg = Image.open('./sk_campus_img.jpg').convert('L')\nimg_array = np.array(img)\noriginal_shape = img_array.shape\n\nplt.figure(figsize=(8, 4))\nplt.imshow(img_array, cmap='gray')\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n\nU, S, Vt = np.linalg.svd(img_array, full_matrices=False)  # economy SVD\nU.shape, S.shape, Vt.shape\n\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.semilogy(S / S[0])\nplt.xlabel(r\"$k$\")\nplt.ylabel(r\"$\\sigma_k / \\sigma_1$\")\nplt.title(r\"$\\sigma_k / \\sigma_1$\")\nplt.grid()\n\n\ncumulative_energy = np.cumsum(S**2) / np.sum(S**2)\nplt.subplot(1, 2, 2)\nplt.plot(cumulative_energy)\nplt.xlabel(r\"$k$\")\nplt.ylabel(r\"Cumulative Energy\")\nplt.title(r\"$(\\sum_{i=1}^k \\sigma_i) / \\sum_{i=0}^n \\sigma_i)$\")\nplt.grid()\n\nplt.show()\n\n\ndef reconstruct_image(k):\n    return (U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :])\n\n\nranks = [5, 20, 50, 100, original_shape[1]]\nplt.figure(figsize=(15, 5))\n\nfor i, rank in enumerate(ranks, 1):\n    plt.subplot(1, len(ranks), i)\n    recon_img = reconstruct_image(rank)\n    plt.imshow(recon_img, cmap='gray')\n    plt.title(f'Rank {rank}') if rank!= original_shape[1] else plt.title(f'Original Image')\n    plt.axis('off')\n\nplt.suptitle(\"Low-Rank Approximations of Image\")\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#svd-and-its-applications",
    "href": "seminars/seminar-4/seminar-4.html#svd-and-its-applications",
    "title": "",
    "section": "SVD and its Applications",
    "text": "SVD and its Applications\nSingular Value Decomposition (SVD) is a versatile tool in numerical linear algebra, implemented in many programming languages, typically relying on the LAPACK (Linear Algebra Package) library written in Fortran for its underlying computations.\n\nGeometric interpretation\n\ndef plot_transformed_circle_and_vectors(A, plot_singular_vectors=False, singular_values=None, singular_vectors=None,\n                                        circle_color='black', vector_colors=['blue', 'deeppink'],\n                                        singular_vector_colors=['red', 'green'],\n                                        singular_labels=[r'$\\sigma_1 u_1$', r'$\\sigma_2 u_2$'],\n                                        label_offset=0.2, xlim=(-8, 8), ylim=(-8, 8)):\n    theta = np.linspace(0, 2 * np.pi, 300)\n    unit_circle = np.vstack((np.cos(theta), np.sin(theta)))\n\n    transformed_circle = A @ unit_circle\n\n    plt.plot(transformed_circle[0, :], transformed_circle[1, :], color=circle_color, alpha=0.5)\n\n    e1_transformed = A @ np.array([1, 0])\n    e2_transformed = A @ np.array([0, 1])\n\n    for i, vec in enumerate([e1_transformed, e2_transformed]):\n        color = vector_colors[i]\n        plt.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color=color)\n\n    if plot_singular_vectors and singular_values is not None and singular_vectors is not None:\n        for i, (sigma, vec) in enumerate(zip(singular_values, singular_vectors.T)):\n            vec_scaled = sigma * vec\n            color = singular_vector_colors[i]\n            label = singular_labels[i]\n            plt.quiver(0, 0, vec_scaled[0], vec_scaled[1], angles='xy', scale_units='xy', scale=1, color=color)\n            plt.text(vec_scaled[0] * (1 + label_offset), vec_scaled[1] * (1 + label_offset), label, color=color, fontsize=12)\n\n    plt.axvline(x=0, color='black', lw=1)\n    plt.axhline(y=0, color='black', lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n\n\nA = np.array([[1, 3], [4, 5]])\n\nU, D, Vt = np.linalg.svd(A)\n\nprint('Unit circle (before transformation):')\nplot_transformed_circle_and_vectors(np.eye(2), xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\nplt.show()\n\nprint('1st rotation by V (right singular vectors):')\nplot_transformed_circle_and_vectors(Vt.T, xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\nplt.show()\n\nprint('Scaling by D:')\nscaling_matrix = np.diag(D) @ Vt\nplot_transformed_circle_and_vectors(scaling_matrix, xlim=(-8, 8), ylim=(-8, 8))\nplt.show()\n\nprint('2nd rotation by U (final transformation by A):')\nfinal_transformation = U @ np.diag(D) @ Vt\nplot_transformed_circle_and_vectors(final_transformation, xlim=(-8, 8), ylim=(-8, 8))\nplt.show()\n\n\nA = np.array([[1, 3], [4, 5]])\n\nU, D, Vt = np.linalg.svd(A)\n\nprint(\"Transformed unit circle, basis vectors, and singular vectors:\")\nplot_transformed_circle_and_vectors(A,\n                                    plot_singular_vectors=True,\n                                    singular_values=D,\n                                    singular_vectors=U,\n                                    singular_labels=[r'$\\sigma_1 u_1$', r'$\\sigma_2 u_2$'])\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#applications-in-image-reconstruction",
    "href": "seminars/seminar-4/seminar-4.html#applications-in-image-reconstruction",
    "title": "",
    "section": "Applications in Image Reconstruction",
    "text": "Applications in Image Reconstruction\nMatrix completion, commonly used for filling missing data, can be applied to image and recommendation systems. A well-known example is movie recommendation systems, where a ratings matrix is often only partially filled, as users have not rated every movie. To provide accurate recommendations, we aim to predict these missing ratings.\nThis task is feasible because user ratings tend to follow patterns, meaning the ratings matrix is often low-rank; only a limited amount of information is needed to approximate it well.\nA similar approach applies to images, where pixel values often depend on neighboring pixels, making low-rank approximations effective for reconstructing images with missing or corrupted data."
  },
  {
    "objectID": "seminars/seminar-4/seminar-4.html#svd-in-facial-recognition-eigenfaces",
    "href": "seminars/seminar-4/seminar-4.html#svd-in-facial-recognition-eigenfaces",
    "title": "",
    "section": "SVD in Facial Recognition: Eigenfaces",
    "text": "SVD in Facial Recognition: Eigenfaces\nThe “Eigenfaces for Recognition” paper introduced a novel approach to facial recognition. Unlike earlier methods that focused on detecting individual features (e.g., eyes or nose), Eigenfaces uses SVD to extract and encode essential information from face images. This encoding allows for efficient comparisons between faces by compressing the most relevant facial information into a low-dimensional representation. This method paved the way for data-driven approaches in face recognition, relying on similarities within the encoded space rather than feature-by-feature comparison.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\n\nfrom sklearn.datasets import fetch_lfw_people\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom sklearn.datasets import fetch_lfw_people\n\n\nlfw_dataset = fetch_lfw_people(min_faces_per_person=100)\nX, y, target_names = lfw_dataset.data, lfw_dataset.target, lfw_dataset.target_names\nh, w = lfw_dataset.images.shape[1:3]\nprint(f\"Number of samples: {X.shape[0]}, Image size: {h}x{w}, Unique classes: {len(target_names)}\")\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n\nU, S, VT = np.linalg.svd(X_train, full_matrices=False)\n\nnum_components = 100\nface_space = VT[:num_components, :]\n\nX_train_transformed = X_train @ face_space.T\nX_test_transformed = X_test @ face_space.T\n\nplt.figure(figsize=(8, 4))\nplt.semilogy(np.arange(len(S)), S / S[0], marker=\"o\")\nplt.title(\"$\\sigma_k / \\sigma_1$\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"$\\sigma_k / \\sigma_1$\")\nplt.grid()\nplt.show()\n\n\ndef plot_reconstructed_images(original, transformed, face_space, h, w, index=0):\n    reconstructed = transformed[index] @ face_space\n    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n    ax[0].imshow(original[index].reshape(h, w), cmap=\"gray\")\n    ax[0].set_title(\"Original Image\")\n    ax[1].imshow(reconstructed.reshape(h, w), cmap=\"gray\")\n    ax[1].set_title(\"Reconstructed Image\")\n    plt.show()\n\nplot_reconstructed_images(X_train, X_train_transformed, face_space, h, w, index=0)\n\nclf_knn = KNeighborsClassifier().fit(X_train_transformed, y_train)\nclf_mlp = MLPClassifier(hidden_layer_sizes=(1024,), batch_size=256, early_stopping=True).fit(X_train_transformed, y_train)\n\ny_pred_knn = clf_knn.predict(X_test_transformed)\ny_pred_mlp = clf_mlp.predict(X_test_transformed)\nprint(\"k-NN Classifier Report:\\n\", classification_report(y_test, y_pred_knn, target_names=target_names))\nprint(\"MLP Classifier Report:\\n\", classification_report(y_test, y_pred_mlp, target_names=target_names))\n\nIt might seem discouraging, but it’s worthwhile to check if the data is imbalanced and go through the steps again (exercise)."
  },
  {
    "objectID": "seminars/seminar-11/seminar-11.html",
    "href": "seminars/seminar-11/seminar-11.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport scipy.sparse\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nfrom sklearn.utils.extmath import randomized_svd\nThe best low-rank approximation can be computed by SVD.\nTheorem: Let r &lt; \\text{rank}(A), A_r = U_r \\Sigma_r V_r^*. Then\n\\min_{\\text{rank}(B)=r} \\|A - B\\|_2 = \\|A - A_r\\|_2 = \\sigma_{r+1}.\nThe same holds for \\|\\cdot\\|_F, but \\|A - A_r\\|_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{\\min (n,m)}^2}."
  },
  {
    "objectID": "seminars/seminar-11/seminar-11.html#low-rank-and-sparse-decomposition",
    "href": "seminars/seminar-11/seminar-11.html#low-rank-and-sparse-decomposition",
    "title": "",
    "section": "Low-rank and sparse decomposition",
    "text": "Low-rank and sparse decomposition\nA_r = U_r \\Sigma_r V_r^* S = A - A_r Ax = A_r x + Sx = U_r \\Sigma_r V_r^*x + Sx\nFor A: n \\times n and rank truncation r&lt;n:\nComplexity of Ax: \\mathcal{O}(n^2)\nComplexity of A_rx: \\mathcal{O}(nr)\nComplexity of Sx: \\mathcal{O}(nnz(S))\nIt becomes effective with:\nr&lt;&lt;n\nnnz(S)&lt;&lt;n^2\n\ndef decompose_matrix_with_sparse_correction_optimized(A, rank, threshold=1e-3):\n    U, sigma, Vt = randomized_svd(A, n_components=rank, n_iter=5, random_state=None)\n    U_r = U[:, :rank]\n    Sigma_r = sigma[:rank]\n    Vt_r = Vt[:rank, :]\n    B = (U_r * Sigma_r) @ Vt_r\n    S_dense = A - B\n    S_dense[np.abs(S_dense) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_dense)\n    return U_r, Sigma_r, Vt_r, S_sparse\n\ndef optimized_multiply(U, Sigma, Vt, S, x):\n    temp = Vt @ x\n    temp = Sigma * temp\n    Bx = U @ temp\n    Sx = S @ x\n    return Bx + Sx\n\n\n# Parameters\nsizes = np.arange(200, 4000, 200)\nthreshold = 0.6\nmax_rank = 0.05\n\n\nmses = []\nexact_times = []\napprox_times = []\nsvd_times = []\n\n\nfor i, n in enumerate(sizes):\n    A = np.random.rand(n, n)\n    x = np.random.rand(n)\n    rank = int(n*max_rank)\n    start_time = time.time()\n    U_r, Sigma_r, Vt_r, S_sparse = decompose_matrix_with_sparse_correction_optimized(A, rank, threshold)\n\n    svd_times.append(time.time() - start_time)\n\n    # Measure time for exact multiplication\n    start_time = time.time()\n    exact_result = A @ x\n    exact_times.append(time.time() - start_time)\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(sizes),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(sizes, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Size Grows\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(sizes, exact_times, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(sizes, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nn = 3000\nthreshold = 0.5\nmax_ranks = np.arange(0.0, 1.0, 0.05)\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\n\nU, Sigma, Vt, S = decompose_matrix_with_sparse_correction_optimized(A, n, 0)\n\n\n\nfor i, max_rank in enumerate(max_ranks):\n    rank = max(int(n*max_rank), 1)\n    U_r = U[:, :rank]\n    Sigma_r = Sigma[:rank]\n    Vt_r = Vt[:rank, :]\n    B = (U_r * Sigma_r) @ Vt_r\n    S_dense = A - B\n    S_dense[np.abs(S_dense) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_dense)\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(max_ranks, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Rank is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(max_ranks, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(max_ranks, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nn = 3200\nthresholds = np.arange(0.0, 1.0, 0.05)\nrank = 200\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\nU_r, Sigma_r, Vt_r, S = decompose_matrix_with_sparse_correction_optimized(A, rank, 0)\n\nfor i, threshold in enumerate(thresholds):\n    S_sparse = S.toarray()\n    S_sparse[np.abs(S_sparse) &lt; threshold] = 0\n    S_sparse = scipy.sparse.csr_matrix(S_sparse)\n\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks),\" sparsity: \", S_sparse.nnz/n/n)\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Density is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(thresholds, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nmses = []\napprox_times = []\nsvd_times = []\n\nA = np.random.rand(n, n)\nx = np.random.rand(n)\nstart_time = time.time()\nexact_result = A @ x\nexact_time = time.time() - start_time\nU_r, Sigma_r, Vt_r, S = decompose_matrix_with_sparse_correction_optimized(A, rank, 0)\n\nfor i, threshold in enumerate(thresholds):\n    S_sparse = S.toarray()\n    S_sparse[np.abs(S_sparse) &lt; threshold] = 0\n    #S_sparse = scipy.sparse.csr_matrix(S_sparse)\n\n\n    # Measure time for decomposition-based multiplication\n    start_time = time.time()\n    approx_result = optimized_multiply(U_r, Sigma_r, Vt_r, S_sparse, x)\n    approx_times.append(time.time() - start_time)\n\n    mse = np.mean(((exact_result - approx_result)/exact_result) ** 2)\n    mses.append(mse)\n\n    print(\"Step \", i, \"of\", len(max_ranks))\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, mses, marker='o', label=\"MSE vs Rank\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE Growth as Density is Reduced\")\nplt.grid(True)\nplt.legend()\n\n# Plot Time Comparison\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, np.ones(len(approx_times))*exact_time, label=\"Exact Multiplication Time\", marker='o')\nplt.plot(thresholds, approx_times, label=\"Approximate Multiplication Time\", marker='x')\n#plt.plot(sizes, svd_times, label=\"SVD Time\", marker='o')\nplt.xlabel(\"Size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Time Comparison: Exact vs Approximate Multiplication\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html",
    "href": "seminars/seminar-5/Seminar-5.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\nimport scipy"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html#outline",
    "href": "seminars/seminar-5/Seminar-5.html#outline",
    "title": "",
    "section": "Outline",
    "text": "Outline\n\nLU decomposition as a sum of rank one matrices\nLU Decomposition with Pivoting\nOverdetermined Linear Systems (Least Squares) 3.1 Geometry of Normal Equation  3.2 Pseudoinverse  3.3 Pseudoinverse via SVD  3.4 QR approach  3.5 Linear Regression\n\nIn many applications, we encounter overdetermined systems of linear equations, where there are more equations than unknowns, represented as\n Ax = b. \nThese systems often lack an exact solution.\nTo address this, we can use the least squares method, which finds an approximate solution by minimizing the residual error:\n x = \\arg\\min_x \\| Ax - b \\|_2"
  },
  {
    "objectID": "seminars/seminar-5/Seminar-5.html#overdetermined-linear-systems",
    "href": "seminars/seminar-5/Seminar-5.html#overdetermined-linear-systems",
    "title": "",
    "section": "Overdetermined Linear Systems",
    "text": "Overdetermined Linear Systems\nMany applications lead to unsolvable linear equations Ax = b, where the number of equations is greater, than the number of unknowns.\nThe least squares method chooses the solution as\n x = \\arg\\min_x \\| Ax - b \\|_2\n\nThe Normal Equation\nThe solution to the least squares problem satisfies the normal equation:\n A^T A x = A^T b \nWhen $ A $ has full column rank, the matrix $ A^T A $ is positive definite, allowing us to solve this equation efficiently with Cholesky decomposition:\n A^T A = R^T R \nwhere $ R $ is an upper triangular matrix (and $ R^T $ is lower triangular). This leads to the following set of equations:\n\n\\begin{align}\n    R^T y &= A^T b \\\\\n    R x &= y\n\\end{align}\n\nHowever, solving the normal equations directly can be numerically unstable, especially for larger problems. This approach is generally safe for small problems, but for stability, other methods are recommended. If the pivots in Gaussian elimination are small, LU decomposition may fail, whereas Cholesky decomposition remains stable in these cases.\n\n# Cholesky Decomposition Method\ndef leastsq_chol(A, b):\n    R = scipy.linalg.cholesky(A.T @ A)\n    w = scipy.linalg.solve_triangular(R, A.T @ b, trans='T')\n    return scipy.linalg.solve_triangular(R, w)\n\n\n\nThe Pseudoinverse\nIn linear algebra, not all matrices have an inverse, particularly when a system of equations has no solution or many solutions. The Moore-Penrose pseudoinverse offers a way to find an approximate solution that minimizes error, even when a unique solution doesn’t exist.\nIn the lecture, the pseudoinverse is defined as:\n A^{\\dagger} = \\lim_{\\alpha \\to 0} (A^T A + \\alpha I)^{-1} A^T \nAlternatively,\n A^{\\dagger} = \\lim_{\\alpha \\to 0} A^T (A A^T + \\alpha I)^{-1} \nThese limits exist even if $ (A^T A)^{-1} $ or $ (A AT){-1} $ do not. Later, we will see how this relates to Tikhonov regularization.\nIf $ A $ has full column rank, the pseudoinverse simplifies to:\n A^{\\dagger} = (A^T A)^{-1} A^T \n\n\nComputing the Pseudoinverse Using SVD\nAnother way to compute the pseudoinverse is through the Singular Value Decomposition (SVD) $ A = U V^T $:\n A^{\\dagger} = V \\Sigma^{\\dagger} U^T \nIn this approach, we invert the diagonal entries of $ $ where possible.\nWith the pseudoinverse $ A^{} $, we can write the solution to $ Ax = b $ as:\n x = A^{\\dagger} b \n\n\nExample: Solving a System Using Pseudoinverse\nLet’s now consider the toy matrix and system of equations:\n \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x = \\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} \n\n# Pseudo-inverse Method using Matrix Inversion\ndef leastsq_pinv(A, b):\n    return np.linalg.inv(A.T @ A) @ A.T @ b\n\n\n# Pseudo-inverse Method using SVD\ndef leastsq_pinv_svd(A, b):\n    U, S, Vh = scipy.linalg.svd(A)\n    S_plus = np.zeros(A.shape).T\n    S_plus[:S.shape[0], :S.shape[0]] = np.linalg.inv(np.diag(S))\n    return Vh.T @ S_plus @ U.T @ b\n\nLet’s now return to our toy matrix and consider the system\n\\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x =\\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} \n\nA = np.array([[1., -1.], [2., 1.], [-1., 3.]])\nb = np.array([[0.], [3.], [2.]])\n\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\n\nA_pinv = np.linalg.pinv(A)\nx_sol = A_pinv @ b\nprint(\"\\n Solution by using pseudoinverse: \\n\", x_sol)\n\n\nx_1 = np.linspace(-3, 3, 100)\nx_2_1 = x_1\nx_2_2 = 3. - 2. * x_1\nx_2_3 =(2. - x_1) / 3\n\nplt.plot(x_1, x_2_1)\nplt.plot(x_1, x_2_2)\nplt.plot(x_1, x_2_3)\nplt.scatter(x_sol[0], x_sol[1], c='r')\n\nplt.xlim(0., 2.)\nplt.ylim(-0.5, 2.)\nplt.show()\n\n\n\nQR Decomposition for Solving Least Squares Problem\nIf $ A $ has full column rank, a QR decomposition exists for $ A $:\n A = QR \nThis allows us to rewrite the normal equations as:\n R^T Q^T Q R x = R^T Q^T b \nSince $ Q^T Q = I $, this further simplifies to:\n R x = Q^T b. \n\n# QR Decomposition Method\ndef leastsq_qr(A, b):\n    Q, R = scipy.linalg.qr(A, mode='economic')\n    return scipy.linalg.solve_triangular(R, Q.T @ b)\n\n\n\nLinear Regression\n\ndef SyntheticData(x, corr_col=False, noise=0.1, num_points=100):\n    A = np.random.randn(num_points, len(x) - 1)\n    A = np.hstack((A, np.ones((num_points, 1))))\n    if corr_col and len(x) &gt; 2:\n        A[:, 2] = A[:, 1] + np.random.rand(num_points) * noise * 1e-4\n    noise = np.random.randn(num_points, 1) * noise\n    b = A @ x.reshape((-1, 1)) + noise\n    return A, b\n\n\nx_true = np.array([2.0, 1.0])\n\nA, b = SyntheticData(x_true, num_points=1000)\n\nx_chol = leastsq_chol(A, b)\nx_pinv = leastsq_pinv(A, b)\nx_svd = leastsq_pinv_svd(A, b)\nx_qr = leastsq_qr(A, b)\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\nprint(\"True coefficients:\\n\", x_true)\nprint(\"Cholesky solution:\\n\", x_chol.flatten())\nprint(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\nprint(\"SVD solution:\\n\", x_svd.flatten())\nprint(\"QR solution:\\n\", x_qr.flatten())\n\nplt.figure(figsize=(10, 6))\nplt.scatter(A[:, 0], b, label='Data Points', color='blue', alpha=0.5)\n\nplt.plot(A[:, 0], A @ x_true, label='True Line', color='green')\nplt.plot(A[:, 0], A @ x_chol, label='Cholesky Fit', color='red')\nplt.plot(A[:, 0], A @ x_pinv, label='Pseudo-inverse Fit', color='purple')\nplt.plot(A[:, 0], A @ x_svd, label='SVD Fit', color='orange')\nplt.plot(A[:, 0], A @ x_qr, label='QR Fit', color='cyan')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('1D Linear Regression')\nplt.show()\n\nFinally, let’s consider the case where $ A $ does not have full column rank.\n\nx_true = np.array([2.0, 3.0, -1.0, 1.0])\n\nA, b = SyntheticData(x_true, corr_col=True, num_points=1000)\n\nx_chol = leastsq_chol(A, b)\nx_pinv = leastsq_pinv(A, b)\nx_svd = leastsq_pinv_svd(A, b)\nx_qr = leastsq_qr(A, b)\n\nprint(\"Condition number of A:\\n\", np.linalg.cond(A))\n\nprint(\"True coefficients:\\n\", x_true)\nprint(\"Cholesky solution:\\n\", x_chol.flatten())\nprint(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\nprint(\"SVD solution:\\n\", x_svd.flatten())\nprint(\"QR solution:\\n\", x_qr.flatten())\n\nIn such cases Tikhonov regularization is more effective for finding stable solutions.\nExercise Show that the problem\n \\min \\|x'\\|_2 \\quad \\text{s.t. } x' = \\arg\\min_x \\| Ax - b \\|_2^2 \nis equivalent to the following regularization problem:\n \\min_x \\| Ax - b \\|_2^2 + \\lambda \\|x\\|_2^2. \nNote that the analytical solution is available in this case as well:\n x^* = (A^T A + \\lambda I)^{-1}  X^T b.\nThe analytical solutions described earlier involve inverting the matrix A^T A (or A^T A + \\lambda I), which is computationally expensive. This brings us to iterative methods, which are generally more efficient and have become the primary approach for numerous applications.\nGradient descent is one of the most widely used optimization methods. It is important to note that the objective function (e.g., the loss function, which is the residual in our case: \\mathcal{L}(x) = \\|Ax - b\\|_2^2) should be differentiable with respect to the unknown $ x $. Using gradient descent, the weight vector at each step can be updated as follows:\n x^{k+1} = x^k - \\beta_k \\nabla \\mathcal{L}(x^k) \nExercise Compute the gradient of \\mathcal{L}_{\\lambda}(x) = \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2.\nTo further improve efficiency, one could use stochastic gradient descent (SGD), which computes the gradient over a randomly selected subset of data points, rather than the full dataset A.\nThese ideas will be explored further in the second homework assignment… stay tuned!"
  },
  {
    "objectID": "files/qr_exercise.html",
    "href": "files/qr_exercise.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\n\nn = 100\na = np.random.rand(n)\nb = a + 1e-9 * np.random.randn(n)\n\n# Сначала нормируем вектор a\na = a / np.linalg.norm(a)\n\n# Убираем из b проекцию на a (попытка сделать c ортогональным к a)\nc = b - np.dot(b, a) * a\nc = c / np.linalg.norm(c)\n\n# Скалярное произведение c и a пока не идеально мало:\nprint(np.dot(c, a))\n\n# Делаем повторную ортогонализацию:\n# Снова вычитаем проекцию c на a\nc = c - np.dot(c, a) * a\nc = c / np.linalg.norm(c)\n\n# Теперь скалярное произведение гораздо меньше!\nprint(np.dot(c, a))\n\n-2.7671064037337878e-08\n-6.591949208711867e-17"
  },
  {
    "objectID": "files/qr_exercise.html#упражнение",
    "href": "files/qr_exercise.html#упражнение",
    "title": "",
    "section": "Упражнение",
    "text": "Упражнение\nВ этом упражнении мы реализуем классический и модифицированный процессы Грама-Шмидта для построения QR-разложения матрицы.\nВход: матрица A \\in \\mathbb{R}^{n \\times n} Выход: матрицы Q и R такие, что A = QR, при этом Q ортогональная, а R верхнетреугольная. Мы сфокусируемся на матрице Q, потому что имея её, мы можем легко получить R по формуле R = Q^T A.\n\nЧисленная неустойчивость. Классический процесс Грама-Шмидта (CGS) может приводить к большим погрешностям при вычислении ортонормированной системы векторов, особенно при плохой обусловленности исходной матрицы. В машинной арифметике возникает потеря ортогональности.\nМодифицированный процесс (MGS). Идея заключается в том, чтобы последовательно вычитать проекции на уже построенные ортонормированные вектора, шаг за шагом. Формально, если q_1, \\dots, q_{k-1} уже построены, то для k-го вектора: \nq_k := a_k, \\quad\nq_k := q_k - (q_k,\\,q_1)\\,q_1, \\quad\nq_k := q_k - (q_k,\\,q_2)\\,q_2, \\quad \\dots\n В точной арифметике это эквивалентно классическому процессу, но в машинной арифметике MGS, как правило, даёт более устойчивое решение.\nСложность. Оба алгоритма имеют асимптотическую сложность порядка O(n^2 m), где m \\times n — размерность исходной матрицы. Для квадратной матрицы n \\times n сложность O(n^3).\nQR-разложение. По готовой ортонормированной матрице Q можно найти верхнетреугольную R, например, умножив Q^T A. Тогда A = Q R.\n\n\nimport numpy as np\n\ndef hilbert_matrix(n):\n    H = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(n):\n            H[i, j] = 1.0 / (i + j + 1)\n    return H\n\ndef generate_matrix(n, type='random'):\n    if type == 'random':\n        A = np.random.randn(n, n)\n    elif type == 'hilbert':\n        A = hilbert_matrix(n)\n    return A\n\ndef classical_gs(A):\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    for k in range(n):\n        # Берём исходный столбец\n        v = A[:, k].copy()\n        \n        # Считаем все скалярные произведения \\alpha_j = (q_j, v) заранее\n        # (здесь q_j уже ортонормированы из предыдущих шагов)\n        alphas = np.array([np.dot(Q[:, j], v) for j in range(k)])\n        \n        # Вычитаем сумму \\sum_j \\alpha_j * q_j\n        for j in range(k):\n            v -= alphas[j] * Q[:, j]\n            ### YOUR CODE HERE\n        \n         ### YOUR CODE HERE\n        Q[:, k] = np.zeros_like(v) # WRONG!\n    return Q\n\ndef modified_gs(A):\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    for k in range(n):\n        v = A[:, k].copy()\n        # В МОДИФИЦИРОВАННОМ алгоритме на каждом шаге j\n        # берём текущее \"свежее\" v\n        ### YOUR CODE HERE\n        for j in range(k):\n            pass\n        Q[:, k] = np.zeros_like(v) # WRONG!\n    return Q\n\n\n\n# Пример: проверяем ортогонализацию на матрице Гильберта\nn = 10\nA = generate_matrix(n, type='hilbert')\n\nQ_cgs = classical_gs(A)\nQ_mgs = modified_gs(A)\n\n# Оцениваем \"ошибку ортогональности\" как ||Q^T Q - I|| по Фробениусу\nerr_cgs = np.linalg.norm(Q_cgs.T @ Q_cgs - np.eye(n), ord='fro')\nerr_mgs = np.linalg.norm(Q_mgs.T @ Q_mgs - np.eye(n), ord='fro')\n\n# Фробениусова норма единичной матрицы I (это sqrt(n))\nnorm_I_fro = np.linalg.norm(np.eye(n), ord='fro')\n\nrel_cgs_fro = err_cgs / norm_I_fro\nrel_mgs_fro = err_mgs / norm_I_fro\n\nprint(\"Абсолютная ошибка (CGS) =\", err_cgs)\nprint(\"Абсолютная ошибка (MGS) =\", err_mgs)\nprint(\"Относительная ошибка (CGS) =\", rel_cgs_fro)\nprint(\"Относительная ошибка (MGS) =\", rel_mgs_fro)\n\nАбсолютная ошибка (CGS) = 3.4653383412436143\nАбсолютная ошибка (MGS) = 8.763548618214397e-05\nОтносительная ошибка (CGS) = 1.095836202143963\nОтносительная ошибка (MGS) = 2.7712774019178854e-05"
  },
  {
    "objectID": "files/randomized_la.html",
    "href": "files/randomized_la.html",
    "title": "Рандомизированное матричное умножение",
    "section": "",
    "text": "import numpy as np\n\ndef freivalds_check(A, B, C, k=2):\n    \"\"\"\n    Проверяет равенство A @ B = C\n    с помощью алгоритма Фрейвалдса k раз.\n    Возвращает True (скорее всего) или False (точно).\n    \"\"\"\n    n = A.shape[0]\n    for _ in range(k):\n        # Случайный вектор (элементы -1 или 1)\n        r = np.random.randint(0, 2, size=(n,)) * 2 - 1\n        \n        Br = B @ r      # O(n^2)\n        C_r = C @ r     # O(n^2)\n        ABr = A @ Br    # O(n^2)\n        \n        if not np.allclose(ABr, C_r, atol=1e-8):\n            return False  # Точно не равны\n    return True           # Скорее всего равны\n\n# Демонстрация\nn = 500\nA = np.random.randn(n, n)\nB = np.random.randn(n, n)\nC = A @ B  # Истинное произведение\n\nprint(\"Проверяем истинное равенство A*B и C:\")\nprint(freivalds_check(A, B, C, k=3))\n\n# Немного подпортим C\nC_perturbed = C.copy()\nC_perturbed[0,0] += 1e-1\n\nprint(\"Проверяем заведомо неверное равенство:\")\nprint(freivalds_check(A, B, C_perturbed, k=3))\n\nПроверяем истинное равенство A*B и C:\nTrue\nПроверяем заведомо неверное равенство:\nFalse\n\n\n\nРандомизированное матричное умножение\n\nimport numpy as np\n\ndef randomized_matrix_multiplication(A, B, k):\n    \"\"\"Приближенное матричное умножение AB с сэмплированием.\"\"\"\n    m, n = A.shape\n    n_b, p = B.shape\n    if n != n_b:\n        raise ValueError(\"Несовместимые размеры матриц\")\n\n    # Вычисляем нормы для вероятностей (можно упростить до p_i = 1/n)\n    col_norms_A = np.linalg.norm(A, axis=0)\n    row_norms_B = np.linalg.norm(B, axis=1)\n    probs = col_norms_A * row_norms_B\n    sum_probs = np.sum(probs)\n    if sum_probs == 0: # Если одна из матриц нулевая\n         return np.zeros((m, p))\n    probs /= sum_probs\n\n    # Можно использовать равномерное распределение p_i = 1/n\n    # probs = np.ones(n) / n\n\n    C_approx = np.zeros((m, p))\n    indices = np.random.choice(n, size=k, p=probs)\n    # Если используем равномерное распределение:\n    # indices = np.random.choice(n, size=k)\n    # probs_selected = np.ones(k) / n\n\n    probs_selected = probs[indices]\n\n    for t in range(k):\n        idx = indices[t]\n        p_i = probs_selected[t]\n        if p_i &gt; 1e-9: # Избегаем деления на ноль\n            A_col = A[:, idx:idx+1] # A^{(i_t)} как столбец (m x 1)\n            B_row = B[idx:idx+1, :] # B_{(i_t)} как строка (1 x p)\n            C_approx += (1.0 / (k * p_i)) * (A_col @ B_row)\n\n    return C_approx\n\n# Пример использования\nm, n, p = 50, 100, 60\nA = np.random.rand(m, n)\nB = np.random.rand(n, p)\nk = 500 # Количество сэмплов\n\nC_exact = A @ B\nC_approx = randomized_matrix_multiplication(A, B, k)\n\nerror = np.linalg.norm(C_exact - C_approx, 'fro') / np.linalg.norm(C_exact, 'fro')\nprint(f\"Относительная ошибка Фробениуса: {error:.4f}\")\n\nОтносительная ошибка Фробениуса: 0.0377\n\n\n\nimport numpy as np\n\ndef randomized_matmul(A, B, k):\n    \"\"\"\n    Приближённое умножение матриц A (m x p) и B (p x n).\n    Возвращает матрицу C ~ A @ B.\n    \"\"\"\n    m, p = A.shape\n    p2, n = B.shape\n    assert p == p2\n    \n    # Вероятности берём пропорционально нормам столбцов A * строк B\n    col_norms = np.linalg.norm(A, axis=0)  # размер p\n    row_norms = np.linalg.norm(B, axis=1)  # размер p\n    weights = col_norms * row_norms\n    weights = weights / np.sum(weights)\n    \n    # Сэмплим k индексов без возвращения\n    idx = np.random.choice(p, size=k, replace=False, p=weights)\n    \n    # Формируем сумму\n    C_approx = np.zeros((m, n))\n    for i in idx:\n        # A^{(i)} -- i-ый столбец A, B_{(i)} -- i-ая строка B\n        # Учитываем масштабирование 1/(k * p_i)\n        C_approx += (1.0 / (k * weights[i])) * np.outer(A[:, i], B[i, :])\n    \n    return C_approx\n\n# Тест\nm, p, n = 100, 500, 100\nA = np.random.randn(m, p)\nB = np.random.randn(p, n)\nC_true = A @ B\n\n# k намного меньше p\nk = 500\nC_approx = randomized_matmul(A, B, k)\n\nerror_rel = np.linalg.norm(C_true - C_approx, 'fro') / np.linalg.norm(C_true, 'fro')\nprint(f\"Относительная ошибка (F-норма): {error_rel:.4f}\")\n\nОтносительная ошибка (F-норма): 0.0902\n\n\n\n\nОценка следа матрицы\n\nimport numpy as np\n\ndef hutchinson_trace_estimator(A, k):\n    \"\"\"Оценка следа Tr(A) методом Хатчинсона.\"\"\"\n    n = A.shape[0]\n    trace_estimates = []\n    for _ in range(k):\n        w = np.random.choice([-1.0, 1.0], size=(n, 1))\n        trace_estimate = (w.T @ A @ w).item() # w^T A w\n        trace_estimates.append(trace_estimate)\n    return np.mean(trace_estimates)\n\ndef girard_trace_estimator(A, k):\n    \"\"\"Оценка следа Tr(A) методом Жирарда.\"\"\"\n    n = A.shape[0]\n    trace_estimates = []\n    for _ in range(k):\n        w = np.random.randn(n, 1) # w ~ N(0, I)\n        trace_estimate = (w.T @ A @ w).item() # w^T A w\n        trace_estimates.append(trace_estimate)\n    return np.mean(trace_estimates)\n\n# Пример использования\nn = 100\n# Создаем симметричную положительно определенную матрицу\nA_sym = np.random.rand(n, n)\nA_sym = 0.5 * (A_sym + A_sym.T)\nA_sym = A_sym + n * np.eye(n) # Гарантируем положительную определенность\n\nexact_trace = np.trace(A_sym)\nk = 100 # Количество векторов для сэмплирования\n\nhutchinson_est = hutchinson_trace_estimator(A_sym, k)\ngirard_est = girard_trace_estimator(A_sym, k)\n\nprint(f\"Точный след: {exact_trace:.4f}\")\nprint(f\"Оценка Хатчинсона (k={k}): {hutchinson_est:.4f}, Ошибка: {abs(hutchinson_est - exact_trace):.4f}\")\nprint(f\"Оценка Жирарда (k={k}): {girard_est:.4f}, Ошибка: {abs(girard_est - exact_trace):.4f}\")\n\n# Оценка intdim\nnorm_f_sq = np.linalg.norm(A_sym, 'fro')**2\nintdim = (exact_trace**2) / norm_f_sq\nprint(f\"Внутренняя размерность (intdim): {intdim:.4f} (макс. {n})\")\n\nТочный след: 10055.8264\nОценка Хатчинсона (k=100): 10048.4577, Ошибка: 7.3687\nОценка Жирарда (k=100): 10072.2824, Ошибка: 16.4561\nВнутренняя размерность (intdim): 99.7131 (макс. 100)\n\n\n\nimport numpy as np\n\ndef hutchinson_trace_estimator(A_mv, n, k=10):\n    \"\"\"\n    Оценивает Tr(A), не зная A явно, \n    но имея функцию A_mv(x), вычисляющую A@x.\n    \n    Параметры:\n    -----------\n    A_mv : function\n        Функция, принимающая на вход x и возвращающая A@x.\n    n : int\n        Размерность матрицы A (n x n).\n    k : int\n        Число случайных векторов для усреднения.\n    \"\"\"\n    estimates = []\n    for _ in range(k):\n        # Случайный радемахеровский вектор\n        r = np.random.randint(0, 2, size=n)*2 - 1\n        Ar = A_mv(r)\n        estimates.append(r @ Ar)  # скаляр w^T (A w)\n    return np.mean(estimates)\n\n# Пример: матрица A и явный A_mv\nn = 30\nA = np.random.randn(n, n)\nA = 0.5*(A + A.T)  # Сделаем её симметричной для наглядности\ntrue_trace = np.trace(A)\n\n# Создаём функцию A_mv\ndef A_mv(x):\n    return A @ x\n\nk = 8000\ntrace_hat = hutchinson_trace_estimator(A_mv, n, k)\nprint(f\"Истинный след:      {true_trace:.2f}\")\nprint(f\"Оценка Хатчинсона: {trace_hat:.2f}\")\n\nИстинный след:      -9.50\nОценка Хатчинсона: -9.18\n\n\n\nimport numpy as np\n\ndef girard_trace_estimator(A_mv, n, k=10):\n    \"\"\"\n    Оценивает Tr(A), используя гауссовские случайные векторы.\n    \"\"\"\n    estimates = []\n    for _ in range(k):\n        w = np.random.randn(n)  # гауссовский вектор\n        Aw = A_mv(w)\n        estimates.append(w @ Aw)\n    return np.mean(estimates)\n\ndef intrinsic_dimension(A):\n    # Предполагаем, что A задана явно, чтобы просто продемонстрировать\n    trace_val = np.trace(A)\n    fro_norm = np.linalg.norm(A, 'fro')\n    return trace_val / fro_norm\n\n# Демонстрация\nn = 200\nA = np.random.randn(n, n)\nA = A @ A.T  # делаем A = A^T A, чтобы получилась ПД матрица\ntrue_trace = np.trace(A)\n\ndef A_mv(x):\n    return A @ x\n\nk = 30\ntrace_est_girard = girard_trace_estimator(A_mv, n, k)\nidim = intrinsic_dimension(A)\n\nprint(f\"Истинный след:          {true_trace:.2f}\")\nprint(f\"Оценка Жирара (k={k}):  {trace_est_girard:.2f}\")\nprint(f\"Intrinsic dimension:    {idim:.2f}\")\n\nИстинный след:          39814.18\nОценка Жирара (k=30):  40171.33\nIntrinsic dimension:    9.99\n\n\n\n\nРандомизированный SVD\n\nimport numpy as np\n\ndef randomized_svd(A, rank, oversampling=10):\n    \"\"\"Вычисляет приближенное SVD ранга 'rank'.\"\"\"\n    m, n = A.shape\n    ell = rank + oversampling # Целевой ранг + доп. сэмплирование\n\n    # Шаг 1: Генерируем случайную матрицу\n    Omega = np.random.randn(n, ell)\n\n    # Шаг 2: Вычисляем Y = A * Omega\n    Y = A @ Omega\n\n    # Шаг 3: Находим ортонормированный базис Q для Y (QR разложение)\n    Q, _ = np.linalg.qr(Y) # Q имеет размер (m x ell)\n\n    # Шаг 4: Проецируем A на базис Q: B = Q^T * A\n    B = Q.T @ A # B имеет размер (ell x n)\n\n    # Шаг 5: Вычисляем SVD для маленькой матрицы B\n    U_tilde, s, Vh = np.linalg.svd(B, full_matrices=False) # U_tilde (ell x ell), s (ell,), Vh (ell x n)\n\n    # Шаг 6: Формируем U для исходной матрицы A\n    U = Q @ U_tilde # U имеет размер (m x ell)\n\n    # Шаг 7: Обрезаем до нужного ранга k\n    U_k = U[:, :rank]\n    s_k = s[:rank]\n    Vh_k = Vh[:rank, :]\n\n    return U_k, s_k, Vh_k\n\n# Пример использования\nm, n = 1000, 500\nA = np.random.rand(m, n) @ np.random.rand(n, n) # Создаем матрицу с падающими сингулярными числами\n\ntarget_rank = 10\n\nU_r, s_r, Vh_r = randomized_svd(A, target_rank)\nA_approx_r = U_r @ np.diag(s_r) @ Vh_r\n\n# Для сравнения - точное SVD\nU_exact, s_exact, Vh_exact = np.linalg.svd(A, full_matrices=False)\nA_approx_exact_k = U_exact[:, :target_rank] @ np.diag(s_exact[:target_rank]) @ Vh_exact[:target_rank, :]\n\nerror_rand = np.linalg.norm(A - A_approx_r, 'fro') / np.linalg.norm(A, 'fro')\nerror_exact_k = np.linalg.norm(A - A_approx_exact_k, 'fro') / np.linalg.norm(A, 'fro')\n\nprint(f\"Ранг приближения: {target_rank}\")\nprint(f\"Ошибка рандомизированного SVD: {error_rand:.4e}\")\nprint(f\"Ошибка точного SVD ранга k:     {error_exact_k:.4e}\")\nprint(f\"Первые 5 синг. чисел (рандом): {s_r[:5]}\")\nprint(f\"Первые 5 синг. чисел (точно):  {s_exact[:5]}\")\n\nРанг приближения: 10\nОшибка рандомизированного SVD: 1.4632e-02\nОшибка точного SVD ранга k:     1.4126e-02\nПервые 5 синг. чисел (рандом): [88663.64610265   107.31285841   104.0445584    102.02981015\n   101.02648135]\nПервые 5 синг. чисел (точно):  [88663.91400357   138.4687148    135.01731642   134.03101164\n   132.74498632]\n\n\n\nimport numpy as np\n\ndef randomized_svd(A, k, p=5):\n    \"\"\"\n    Возвращает приближение SVD матрицы A: A ~ U @ np.diag(S) @ V.\n    k - желаемый ранг, p - оверсэмплинг.\n    \"\"\"\n    m, n = A.shape\n    # 1. Случайная матрица G\n    G = np.random.randn(n, k + p)\n    # 2. Y = A G\n    Y = A @ G  # размер (m x (k+p))\n    # 3. QR-разложение\n    Q, _ = np.linalg.qr(Y, mode='reduced')  # Q: (m x (k+p))\n    # 4. B = Q^T A\n    B = Q.T @ A  # размер ((k+p) x n)\n    # 5. Точное SVD матрицы B\n    u_hat, s, v = np.linalg.svd(B, full_matrices=False)\n    # 6. U = Q u_hat\n    U = Q @ u_hat  # размер (m x (k+p))\n    # Обрежем до ранга k\n    return U[:, :k], s[:k], v[:k, :]\n\n# Демонстрация\nm, n, k = 1000, 300, 50\nA = np.random.randn(m, k) @ np.random.randn(k, n)  # матрица ранга k (точно)\nA += 0.01*np.random.randn(m, n)  # чуть портим\n\nU_approx, S_approx, V_approx = randomized_svd(A, k=50, p=30)\nA_approx = U_approx @ np.diag(S_approx) @ V_approx\n\nerror_fro = np.linalg.norm(A - A_approx, 'fro') / np.linalg.norm(A, 'fro')\nprint(f\"Относительная ошибка (F-регион): {error_fro:.4e}\")\n\nОтносительная ошибка (F-регион): 2.0224e-03\n\n\n\n\nKaczmarz algorithm\n\nimport numpy as np\n\ndef randomized_kaczmarz(A, b, iters=1000):\n    \"\"\"\n    Решает систему A x = b методом Kaczmarz.\n    Возвращает приблизительное решение x.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    row_norms = np.sum(A*A, axis=1)\n    row_norms_sum = np.sum(row_norms)\n    \n    for _ in range(iters):\n        # Выбираем строку i случайно, пропорц. ее норме\n        i = np.random.choice(m, p=row_norms/row_norms_sum)\n        a_i = A[i, :]\n        residual = a_i.dot(x) - b[i]\n        x = x - (residual / (a_i.dot(a_i))) * a_i\n    return x\n\n# Демонстрация\nm, n = 500, 300\nA = np.random.randn(m, n)\nx_true = np.random.randn(n)\nb = A @ x_true\n\nx_est = randomized_kaczmarz(A, b, iters=5000)\nerror = np.linalg.norm(x_est - x_true)/np.linalg.norm(x_true)\nprint(f\"Относительная ошибка в решении: {error:.4e}\")\n\nОтносительная ошибка в решении: 7.3028e-02"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html",
    "href": "files/Eigenfaces_exercise.html",
    "title": "",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#load-the-dataset",
    "href": "files/Eigenfaces_exercise.html#load-the-dataset",
    "title": "",
    "section": "Load the dataset",
    "text": "Load the dataset\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_lfw_people #lol\nfrom sklearn.preprocessing import StandardScaler\n\ndataset     = fetch_lfw_people(min_faces_per_person=100, resize=0.4)\nA           = dataset['data']\nlabels      = dataset['target']\nclasses     = dataset['target_names']\nlabel_names = np.array([classes[label] for label in labels])\nprint('🤖: Dataset contains {} points in {}-dimensional space'.format(*A.shape))\n\ndef plot_gallery(dataset, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    n_samples, h, w = dataset.images.shape\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    images, titles = dataset[\"images\"], dataset[\"target\"]\n    titles = [dataset[\"target_names\"][title] for title in titles]\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\nplot_gallery(dataset)\n\n🤖: Dataset contains 1140 points in 1850-dimensional space"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-normalize-the-data-to-have-zero-mean",
    "href": "files/Eigenfaces_exercise.html#task-normalize-the-data-to-have-zero-mean",
    "title": "",
    "section": "Task: normalize the data to have zero mean",
    "text": "Task: normalize the data to have zero mean\n\n### YOU CODE HERE\nA_std = ..."
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-calculate-svd-of-normalized-matrix",
    "href": "files/Eigenfaces_exercise.html#task-calculate-svd-of-normalized-matrix",
    "title": "",
    "section": "Task: Calculate SVD of normalized matrix",
    "text": "Task: Calculate SVD of normalized matrix\n\nA_{std} = U \\Sigma V^\\top\n\n\n### YOU CODE HERE\nu, sigmas, vt = ..."
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-plot-eigenfaces",
    "href": "files/Eigenfaces_exercise.html#task-plot-eigenfaces",
    "title": "",
    "section": "Task: plot eigenfaces",
    "text": "Task: plot eigenfaces\n\ndef plot_eigenfaces(dataset=dataset, u=u, sigmas=sigmas, vt=vt, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    n_samples, h, w = dataset.images.shape\n    ### YOU CODE HERE\n    projections = ...\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        \n        plt.imshow(projections[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(f\"Eigenface #{i+1}\", size=12)\n        plt.xticks(())\n        plt.yticks(())\n      \nplot_eigenfaces()"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#task-plot-reconstructions",
    "href": "files/Eigenfaces_exercise.html#task-plot-reconstructions",
    "title": "",
    "section": "Task: plot reconstructions",
    "text": "Task: plot reconstructions\n\ndef plot_projections(rank = 20, dataset=dataset, u=u, sigmas=sigmas, vt=vt, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    n_samples, h, w = dataset.images.shape\n    ### YOU CODE HERE\n    projections = ...\n    reconstructions = ...\n\n    images, titles = dataset[\"images\"], dataset[\"target\"]\n    titles = [dataset[\"target_names\"][title] for title in titles]\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    print(f\"Rank {rank} compression\")\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        ### YOU CODE HERE\n        plt.imshow(reconstructions[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(f\"{titles[i]}\", size=12)\n        plt.xticks(())\n        plt.yticks(())\n      \nplot_projections()\n\nRank 20 compression"
  },
  {
    "objectID": "files/Eigenfaces_exercise.html#plot-cumulative-variance-by-each-individual-component-graph",
    "href": "files/Eigenfaces_exercise.html#plot-cumulative-variance-by-each-individual-component-graph",
    "title": "",
    "section": "Plot cumulative variance by each individual component graph",
    "text": "Plot cumulative variance by each individual component graph\n\n### YOUR CODE HERE\ntotal_variance = ...\nvariance_explained = [(i / total_variance)*100 for i in sorted(sigmas, reverse=True)]\ncumulative_variance_explained = np.cumsum(variance_explained)\n\n### YOUR CODE HERE\nn_sigmas = ...\nxs = [0.5 + i for i in range(n_sigmas)]\nplt.bar(xs, variance_explained, alpha=0.5, align='center',\n        label='Individual explained variance')\nplt.step(xs, cumulative_variance_explained, where='mid',\n         label='Cumulative explained variance')\nplt.ylabel('Explained variance')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\n# plt.xticks(np.arange(A_std.shape[1]+1))\nplt.show()"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#представление-чисел-с-фиксированной-точкой",
    "href": "lectures/lecture-1/lecture-1.html#представление-чисел-с-фиксированной-точкой",
    "title": "Представление чисел",
    "section": "Представление чисел с фиксированной точкой",
    "text": "Представление чисел с фиксированной точкой\n\nЧисла с фиксированной точкой - простейший способ представления вещественных чисел в цифровом виде\n\nТакже известен как формат Qm.n, где:\n\nm бит для целой части\nn бит для дробной части\n\n\nОсновные свойства:\n\nДиапазон: [-(2^m), 2^m - 2^{-n}]\nРазрешение: 2^{-n} (наименьшая представимая разница)\nХранение: всего m + n + 1 бит (включая знаковый бит)\n\nОграничения:\n\nФиксированный диапазон представимых чисел\nКомпромисс между диапазоном (m) и точностью (n)\nНеэффективно для представления очень больших или очень малых чисел"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-формула",
    "href": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-формула",
    "title": "Представление чисел",
    "section": "Числа с плавающей точкой: формула",
    "text": "Числа с плавающей точкой: формула\nf = (-1)^s 2^{(p-b)} \\left( 1 + \\frac{d_1}{2} + \\frac{d_2}{2^2}  + \\ldots + \\frac{d_m}{2^m}\\right),\nгде s \\in \\{0, 1\\} - знаковый бит, d_i \\in \\{0, 1\\} - m-битная мантисса, p \\in \\mathbb{Z}; 0 \\leq p \\leq 2^e, e - e-битная экспонента, обычно определяемая как 2^e - 1\nМожно представить как равномерную m-битную сетку между двумя последовательными степенями числа 2."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#простые-примеры",
    "href": "lectures/lecture-1/lecture-1.html#простые-примеры",
    "title": "Представление чисел",
    "section": "Простые примеры",
    "text": "Простые примеры\nСуществует много способов записать число в научной нотации, но всегда есть единственное нормализованное представление с ровно одной ненулевой цифрой слева от десятичной точки.\nFor example: - 0.232 \\times 10^3 = 23.2 \\times 10^1 = 2.32 \\times 10^2 = \\ldots - 01001 = 1.001 \\times 2^3 = \\ldots\nПример 1: Каково нормализованное представление числа 00101101.101?\n0.0001101001110 = 1.110100111 \\times 2^{-4}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#умножение-более-подробно",
    "href": "lectures/lecture-1/lecture-1.html#умножение-более-подробно",
    "title": "Представление чисел",
    "section": "Умножение более подробно",
    "text": "Умножение более подробно\nРассмотрим два числа с плавающей точкой x, y, у которых экспоненты и дробные части равны x_e, y_e и x_m, y_m соответственно. Результат обычного умножения чисел с плавающей точкой будет:\n\n\\operatorname{Mul}(x, y)=\\left(1+x_m\\right) \\cdot 2^{x_e} \\cdot\\left(1+y_m\\right) \\cdot 2^{y_e}=\\left(1+x_m+y_m+x_m \\cdot y_m\\right) \\cdot 2^{x_e+y_e}\n\nНедавние статьи: Addition is all you need пытаются доказать, что мы можем заменить это умножение сложением и все равно получить обучаемые нейронные сети (требует проверки)."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#фиксированная-и-плавающая-точка",
    "href": "lectures/lecture-1/lecture-1.html#фиксированная-и-плавающая-точка",
    "title": "Представление чисел",
    "section": "Фиксированная и плавающая точка",
    "text": "Фиксированная и плавающая точка\nВ: Каковы преимущества/недостатки чисел с фиксированной и плавающей точкой?\nA: In most cases, they work just fine.\n\nHowever, fixed point represents numbers within specified range and controls absolute accuracy.\nFloating point represent numbers with relative accuracy, and is suitable for the case when numbers in the computations have varying scale (i.e., 10^{-1} and 10^{5}).\nIn practice, if speed is of no concern, use float32 or float64 (на самом деле, нет!)"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-на-логарифмической-шкале",
    "href": "lectures/lecture-1/lecture-1.html#числа-с-плавающей-точкой-на-логарифмической-шкале",
    "title": "Представление чисел",
    "section": "Числа с плавающей точкой на логарифмической шкале",
    "text": "Числа с плавающей точкой на логарифмической шкале\nВизуализируем, как числа с плавающей точкой распределены на вещественной прямой:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define parameters for a small floating-point system\nb = 2  # base\ne_max = 2  # maximum exponent\nmantissa_bits = 2  # number of bits for mantissa\n\n# Generate all possible combinations of exponent and mantissa\nexponents = range(-e_max, e_max + 1)\nmantissas = np.linspace(0, 1 - 2**(-mantissa_bits), 2**mantissa_bits)\n\n# Calculate floating-point numbers\nfp_numbers = []\nfor e in exponents:\n    for m in mantissas:\n        fp_numbers.append((1 + m) * b**(e))\n\n# Sort the numbers for proper visualization\nfp_numbers.sort()\n\n# Create figure\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the floating-point numbers\nax.scatter(fp_numbers, [1] * len(fp_numbers), marker='|', s=100, color='blue')\nax.set_ylim(0.5, 1.5)\nax.set_yticks([])  # Remove y-axis ticks\n\nax.set_title(r'Distribution of Floating-Point Numbers (base=$%d$, max_exp=$%d$, mantissa_bits=$%d$)' % (b, e_max, mantissa_bits))\nax.set_xlabel('Value')\n\n# Add text explanation\nplt.figtext(0.1, -0.05, \n    r\"Base (b) = $%d$, Max exponent = $%d$, Mantissa bits = $%d$\" % (b, e_max, mantissa_bits) + \"\\n\" +\n    r\"Numbers are of the form $(1 + x_m) \\cdot 2^e$, where $x_m$ is the fraction and $e$ is the exponent.\",\n    wrap=True)\n#plt.tight_layout()\n#plt.show()\n\nText(0.1, -0.05, 'Base (b) = $2$, Max exponent = $2$, Mantissa bits = $2$\\nNumbers are of the form $(1 + x_m) \\\\cdot 2^e$, where $x_m$ is the fraction and $e$ is the exponent.')"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#ieee-754",
    "href": "lectures/lecture-1/lecture-1.html#ieee-754",
    "title": "Представление чисел",
    "section": "IEEE 754",
    "text": "IEEE 754\nВ современных компьютерах представление чисел с плавающей точкой контролируется стандартом IEEE 754, который был опубликован в 1985 году, и до этого момента разные компьютеры по-разному работали с числами с плавающей точкой.\nIEEE 754 имеет: - Представление чисел с плавающей точкой (как описано выше), (-1)^s \\times c \\times b^q. - Две бесконечности, +\\infty и -\\infty - Два нуля: +0 и -0 - Два вида NaN: тихий NaN (qNaN) и сигнальный NaN (sNaN) - qNaN не вызывает исключение на уровне модуля операций с плавающей точкой (FPU), пока вы не проверите результат вычислений - значение sNaN вызывает исключение от FPU при использовании соответствующей переменной. Этот тип NaN может быть полезен для целей инициализации - C++11 предлагает стандартный интерфейс для создания различных NaN - Правила округления - Правила для \\frac{0}{0}, \\frac{1}{-0}, \\ldots\nВозможные значения определяются с помощью - основания b - точности p - количество цифр - максимально возможного значения e_{\\max}\nи имеют следующие ограничения - $ 0 c b^p - 1$ - 1 - e_{\\max} \\leq q + p - 1 \\leq e_{\\max}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#одинарная-и-двойная-точность",
    "href": "lectures/lecture-1/lecture-1.html#одинарная-и-двойная-точность",
    "title": "Представление чисел",
    "section": "Одинарная и двойная точность",
    "text": "Одинарная и двойная точность\nДва стандартных формата, называемые binary32 и binary64 (также называемые форматами одинарной и двойной точности). В последнее время формат binary16 играет важную роль в обучении глубоких нейронных сетей.\n\n\n\nНазвание\nОбщее название\nОснование\nРазряды\nEmin\nEmax\n\n\n\n\nbinary16\nполовинная точность\n2\n11\n-14\n+ 15\n\n\nbinary32\nодинарная точность\n2\n24\n-126\n+ 127\n\n\nbinary64\nдвойная точность\n2\n53\n-1022\n+1023"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#примеры",
    "href": "lectures/lecture-1/lecture-1.html#примеры",
    "title": "Представление чисел",
    "section": "Примеры",
    "text": "Примеры\n\nДля числа +0\n\nsign равен 0\nexponent равен 00000000000\nfraction состоит из нулей\n\nДля числа -0\n\nsign равен 1\nexponent равен 00000000000\nfraction состоит из нулей\n\nДля +бесконечности\n\nsign равен 0\nexponent равен 11111111111\nfraction состоит из нулей\n\n\nВопрос: что насчет -бесконечности и NaN?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#точность-и-память",
    "href": "lectures/lecture-1/lecture-1.html#точность-и-память",
    "title": "Представление чисел",
    "section": "Точность и память",
    "text": "Точность и память\nОтносительная точность одинарной точности составляет 10^{-7}-10^{-8}, в то время как для двойной точности - 10^{-14}-10^{-16}.\n Важное замечание 1:  float16 занимает 2 байта, float32 занимает 4 байта, float64, или двойная точность, занимает 8 байт.\n Важное замечание 2:  Только эти два типа чисел с плавающей точкой поддерживаются аппаратно (float32 и float64) + на GPU/TPU поддерживаются различные типы с плавающей точкой.\n Важное замечание 3:  Вам следует использовать двойную точность в вычислительной науке и инженерии, и float32/float16 на GPU/в науке о данных.\nСейчас для больших моделей float16 становится все более надежным."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#как-формат-представления-чисел-влияет-на-обучение-нейронных-сетей-нс",
    "href": "lectures/lecture-1/lecture-1.html#как-формат-представления-чисел-влияет-на-обучение-нейронных-сетей-нс",
    "title": "Представление чисел",
    "section": "Как формат представления чисел влияет на обучение нейронных сетей (НС)?",
    "text": "Как формат представления чисел влияет на обучение нейронных сетей (НС)?\n\nВеса в слоях (полносвязных, сверточных, функциях активации) могут храниться с разной точностью\nВажно повышать энергоэффективность устройств, используемых для обучения НС\nПроект DeepFloat от Facebook демонстрирует, как переработать операции с плавающей точкой для обеспечения эффективности при обучении НС, подробнее см. в этой статье\nВлияние представления вещественных чисел на градиенты функций активации\nОбычно первая цифра равна единице\nСубнормальные числа имеют первую цифру 0 для представления нулей и чисел, близких к нулю\nСубнормальные числа заполняют промежуток между положительными и отрицательными числами\nУ них есть проблемы с производительностью, часто по умолчанию обнуляются\n\n\n\nИ на кривых обучения\n\n\nГрафики взяты из этой статьи"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#bfloat16-brain-floating-point",
    "href": "lectures/lecture-1/lecture-1.html#bfloat16-brain-floating-point",
    "title": "Представление чисел",
    "section": "bfloat16 (Brain Floating Point)",
    "text": "bfloat16 (Brain Floating Point)\n\nЭтот формат занимает 16 бит\n\n1 бит для sign\n8 бит для exponent\n7 бит для fraction\n\n\nУсеченный формат одинарной точности из стандарта IEEE\nВ чем разница между float32 и float16?\nЭтот формат используется в Intel FPGA, Google TPU, Xeon CPUs и других платформах"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#tensor-float-от-nvidia-блог-пост-об-этом-формате",
    "href": "lectures/lecture-1/lecture-1.html#tensor-float-от-nvidia-блог-пост-об-этом-формате",
    "title": "Представление чисел",
    "section": "Tensor Float от Nvidia (блог пост об этом формате)",
    "text": "Tensor Float от Nvidia (блог пост об этом формате)\n\nСравнение с другими форматами\n\n\n\nРезультаты\n\n\n\nPyTorch и Tensorflow с поддержкой этого формата доступны в Nvidia NCG"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#альтернатива-стандарту-ieee-754",
    "href": "lectures/lecture-1/lecture-1.html#альтернатива-стандарту-ieee-754",
    "title": "Представление чисел",
    "section": "Альтернатива стандарту IEEE 754",
    "text": "Альтернатива стандарту IEEE 754\nПроблемы в IEEE 754: - переполнение в бесконечность или ноль - множество различных NaN - невидимые ошибки округления - точность либо очень высокая, либо очень низкая - subnormal numbers – числа между 0 и минимальным возможным представимым числом, т.е. significand начинается с нуля\nКонцепция posits может заменить числа с плавающей точкой, см. эту статью\n\n\nпредставляют числа с некоторой точностью, но обеспечивают пределы изменения\nнет переполнений!\nпример представления числа\n\n\n\nДемонстрация точности деления\n\nimport random\nimport numpy as np\n\nc = np.float32(0.925924589693)\nprint(c)\na = np.float32(1.786875867457e-2)\nb = np.float32(c / a)\nprint('{0:10.16f}'.format(b))\nprint(abs(a * b - c)/abs(c))\n\n0.9259246\n51.8180694580078125\n0.0\n\n\n\n\nКвадратный корень\n\na = np.float64(1e-100)\nb = np.sqrt(a)\nprint(b.dtype)\nprint('{0:10.64f}'.format(abs(b * b - a)/abs(a)))\n\nfloat64\n0.0000000000000000000000000000000000000000000000000000000000000000\n\n\n\n\nДемонстрация точности экспоненты\n\na = np.float32(0.000001)\nb = np.exp(a)\nprint(b.dtype)\nprint((np.log(b) - a)/a)\n\nfloat32\n-0.046326134"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#еще-примерчики",
    "href": "lectures/lecture-1/lecture-1.html#еще-примерчики",
    "title": "Представление чисел",
    "section": "Еще примерчики",
    "text": "Еще примерчики\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create x points in the range [-2e-15, 2e-15]\nx = np.linspace(-2e-15, 2e-15, 100)\n\n# Compute log(1+x)/x - 1 using method (a) and (b), being careful about x=0\ny_a = np.zeros_like(x)\ny_b = np.zeros_like(x)\nnonzero = x != 0\ny_a[nonzero] = (np.log(x[nonzero]+1))/x[nonzero] - 1  # (a)\ny_b[nonzero] = (np.log(x[nonzero]+1))/((1+x[nonzero])-1) - 1  # (b\n\n# Create the plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n\n# Plot for method (a)\nax1.plot(x, y_a, 'b-')\nax1.grid(True)\nax1.set_xlabel('x')\nax1.set_ylabel('log(1+x)/x - 1')\nax1.set_title('Method (a): (log(1+x))/x - 1')\n\n# Plot for method (b)\nax2.plot(x, y_b, 'r-')\nax2.grid(True)\nax2.set_xlabel('x')\nax2.set_ylabel('log(1+x)/x - 1')\nax2.set_title('Method (b): (log(1+x))/((1+x)-1) - 1')\n\n# Adjust layout and add a main title\nplt.tight_layout()\nfig.suptitle('Comparison of two methods for computing log(1+x)/x - 1 in double precision', fontsize=16)\nplt.subplots_adjust(top=0.88)\n\nplt.show()\n\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_10237/1439480773.py:12: RuntimeWarning: invalid value encountered in divide\n  y_b[nonzero] = (np.log(x[nonzero]+1))/((1+x[nonzero])-1) - 1  # (b"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#итоги-демо",
    "href": "lectures/lecture-1/lecture-1.html#итоги-демо",
    "title": "Представление чисел",
    "section": "Итоги демо",
    "text": "Итоги демо\n\nДля некоторых значений обратные функции дают точные ответы\nОтносительная accuracy должна сохраняться благодаря IEEE standard\nНе выполняется для многих современных GPU\nБольше деталей про адаптацию IEEE 754 standard для GPU можно найти здесь"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#потеря-значимых-цифр",
    "href": "lectures/lecture-1/lecture-1.html#потеря-значимых-цифр",
    "title": "Представление чисел",
    "section": "Потеря значимых цифр",
    "text": "Потеря значимых цифр\n\nМногие операции приводят к потере цифр loss of significance\nНапример, плохая идея вычитать два больших близких числа, разность будет иметь меньше правильных цифр\nЭто связано с алгоритмами и их свойствами (forward/backward stability), которые мы обсудим позже"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#summation-algorithm",
    "href": "lectures/lecture-1/lecture-1.html#summation-algorithm",
    "title": "Представление чисел",
    "section": "Summation algorithm",
    "text": "Summation algorithm\nОднако ошибки округления могут зависеть от алгоритма.\n\nРассмотрим простейшую задачу: даны n чисел с плавающей точкой x_1, \\ldots, x_n\nВычислить их сумму\n\nS = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.\n\nПростейший алгоритм - складывать числа одно за другим\nКакова реальная ошибка такого алгоритма?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#наивный-алгоритм",
    "href": "lectures/lecture-1/lecture-1.html#наивный-алгоритм",
    "title": "Представление чисел",
    "section": "Наивный алгоритм",
    "text": "Наивный алгоритм\ny_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.\n\nWorst-case ошибка пропорциональна \\mathcal{O}(n), в то время как mean-squared ошибка равна \\mathcal{O}(\\sqrt{n}).\nKahan algorithm дает границу worst-case ошибки \\mathcal{O}(1) (т.е. независимо от n).\n Можете ли вы найти алгоритм лучше?"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#суммирование-кахана",
    "href": "lectures/lecture-1/lecture-1.html#суммирование-кахана",
    "title": "Представление чисел",
    "section": "Суммирование Кахана",
    "text": "Суммирование Кахана\nСледующий алгоритм дает ошибку 2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2), где \\varepsilon - это machine precision.\n\nПричина потери значимости при суммировании заключается в операциях с числами разного порядка\nОсновная идея Kahan summation состоит в том, чтобы отслеживать маленькие ошибки и аккумулировать их в отдельной переменной\nЭтот подход называется compensated summation\n\n\nimport math\nimport numpy as np\nfrom numba import jit as numba_jit\n\nn = 10 ** 6\nsm = 1e-10\nx = np.ones(n, dtype=np.float32) * sm\nx[0] = 1.0\ntrue_sum = 1.0 + (n - 1)*sm\napprox_sum = np.sum(x)\nmath_fsum = math.fsum(x)\n\n@numba_jit(nopython=True)\ndef kahan_sum_numba(x):\n    s = np.float32(0.0)\n    c = np.float32(0.0)\n    for i in range(len(x)):\n        y = x[i] - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\ndef simple_sum(x):\n    s = 0.0\n    for i in range(len(x)):\n        s += x[i]\n    return s\n\nk_sum_numba = kahan_sum_numba(x)\nsimple_sum_result = simple_sum(x)\n\nprint('Error in np sum: {0:3.1e}'.format(approx_sum - true_sum))\nprint('Error in simple sum: {0:3.1e}'.format(simple_sum_result - true_sum))\nprint('Error in Kahan sum Numba: {0:3.1e}'.format(k_sum_numba - true_sum))\nprint('Error in math fsum: {0:3.1e}'.format(math_fsum - true_sum))\n\nError in np sum: 1.7e-06\nError in simple sum: -1.0e-04\nError in Kahan sum Numba: 1.7e-08\nError in math fsum: 1.3e-12"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#итого",
    "href": "lectures/lecture-1/lecture-1.html#итого",
    "title": "Представление чисел",
    "section": "Итого",
    "text": "Итого\n\nНеобходимо быть очень осторожным с числами с плавающей точкой, так как они могут давать неверные результаты из-за ошибок округления.\nДля многих стандартных алгоритмов стабильность хорошо изучена и проблемы могут быть легко обнаружены."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#векторы",
    "href": "lectures/lecture-1/lecture-1.html#векторы",
    "title": "Представление чисел",
    "section": "Векторы",
    "text": "Векторы\n\nВ численной линейной алгебре мы обычно работаем не с числами, а с векторами\nНапомним, что вектор в фиксированном базисе размера n может быть представлен как одномерный массив с n числами\nКак правило, он рассматривается как матрица размера n \\times 1 (вектор-столбец)\n\nПример: Многочлены степени \\leq n образуют линейное пространство. Многочлен $ x^3 - 2x^2 + 1$ может быть представлен как вектор \\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix} в базисе \\{x^3, x^2, x, 1\\}"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#норма-вектора",
    "href": "lectures/lecture-1/lecture-1.html#норма-вектора",
    "title": "Представление чисел",
    "section": "Норма вектора",
    "text": "Норма вектора\n\nВекторы обычно предоставляют (приближенное) описание физического (или какого-либо другого) объекта\nОдин из главных вопросов - насколько точным является приближение (1%, 10%)\nПриемлемое представление, конечно, зависит от конкретного применения. Например:\n\nВ дифференциальных уравнениях в частных производных точности 10^{-5} - 10^{-10} являются типичным случаем\nВ приложениях, основанных на данных, иногда ошибка в 80\\% допустима, так как полезный сигнал искажен сильным шумом"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#расстояния-и-нормы",
    "href": "lectures/lecture-1/lecture-1.html#расстояния-и-нормы",
    "title": "Представление чисел",
    "section": "Расстояния и нормы",
    "text": "Расстояния и нормы\n\nНорма - это количественная мера малости вектора, которая обычно обозначается как \\Vert x \\Vert.\n\nНорма должна удовлетворять определенным свойствам:\n\n\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (неравенство треугольника)\nЕсли \\Vert x \\Vert = 0, то x = 0\n\nРасстояние между двумя векторами определяется как\n d(x, y) = \\Vert x - y \\Vert. \nСтандартные нормы Наиболее известной и широко используемой нормой является евклидова норма:\n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\nкоторая соответствует расстоянию в нашей реальной жизни. Если векторы имеют комплексные элементы, мы используем их модуль."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#p-норма",
    "href": "lectures/lecture-1/lecture-1.html#p-норма",
    "title": "Представление чисел",
    "section": "p-норма",
    "text": "p-норма\nЕвклидова норма, или 2-норма, является подклассом важного класса p-норм:\n \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}. \nСуществуют два очень важных частных случая: - Норма Чебышева, определяется как элемент с максимальным абсолютным значением:\n \\Vert x \\Vert_{\\infty} = \\max_i | x_i| \n\n\nL_1 норма (или манхэттенское расстояние), которая определяется как сумма модулей элементов x:\n\n \\Vert x \\Vert_1 = \\sum_i |x_i|"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#эквивалентность-норм",
    "href": "lectures/lecture-1/lecture-1.html#эквивалентность-норм",
    "title": "Представление чисел",
    "section": "Эквивалентность норм",
    "text": "Эквивалентность норм\nВсе нормы эквивалентны в том смысле, что\n C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_* \nдля некоторых положительных констант C_1(n), C_2(n), x \\in \\mathbb{R}^n для любой пары норм \\Vert \\cdot \\Vert_* и \\Vert \\cdot \\Vert_{**}. Эквивалентность норм по существу означает, что если вектор мал в одной норме, то он мал и в другой норме. Однако константы могут быть большими."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#вычисление-норм-в-python",
    "href": "lectures/lecture-1/lecture-1.html#вычисление-норм-в-python",
    "title": "Представление чисел",
    "section": "Вычисление норм в Python",
    "text": "Вычисление норм в Python\nПакет NumPy содержит все необходимое для вычисления норм: функция np.linalg.norm\n\nimport numpy as np\nn = 100\na = np.random.randn(n)\nb = a + 1e-5 * np.random.normal((n,))\nprint('Relative error in L1 norm:', np.linalg.norm(a - b, 1) / np.linalg.norm(b, 1))\nprint('Relative error in L2 norm:', np.linalg.norm(a - b) / np.linalg.norm(b))\nprint('Relative error in Chebyshev norm:', np.linalg.norm(a - b, np.inf) / np.linalg.norm(b, np.inf))\n\nRelative error in L1 norm: 0.0013743710119635821\nRelative error in L2 norm: 0.0010294800327283588\nRelative error in Chebyshev norm: 0.0004017056903068766"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#единичные-шары-в-различных-нормах",
    "href": "lectures/lecture-1/lecture-1.html#единичные-шары-в-различных-нормах",
    "title": "Представление чисел",
    "section": "Единичные шары в различных нормах",
    "text": "Единичные шары в различных нормах\n\nЕдиничный шар - это множество точек, таких что \\Vert x \\Vert \\leq 1\nДля евклидовой нормы единичный шар является обычным кругом\nДля других норм единичные шары выглядят совершенно по-разному\n\n\n\nimport matplotlib.pyplot as plt\np = 0.5 # Which norm do we use\nM = 10000 # Number of sampling points\nb = []\nfor i in range(M):\n    a = np.random.randn(2, 1)\n    if np.linalg.norm(a[:, 0], p) &lt;= 1:\n        b.append(a[:, 0])\nb = np.array(b)\nplt.plot(b[:, 0], b[:, 1], '.')\nplt.axis('equal')\nplt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))\n\nText(0.5, 1.0, 'Unit disk in the p-th norm, $p=0.5$')"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#что-такое-устойчивый-алгоритм",
    "href": "lectures/lecture-1/lecture-1.html#что-такое-устойчивый-алгоритм",
    "title": "Представление чисел",
    "section": "Что такое устойчивый алгоритм?",
    "text": "Что такое устойчивый алгоритм?\nИ мы завершаем лекцию понятием устойчивости.\n\nПусть x - объект (например, вектор)\nПусть f(x) - функция (функционал), которую вы хотите вычислить\n\nУ вас также есть численный алгоритм alg(x), который фактически вычисляет приближение к f(x).\nАлгоритм называется устойчивым, если\n\\Vert \\text{alg}(x) - f(x) \\Vert  \\leq \\varepsilon \nАлгоритм называется обратно устойчивым, если для любого x существует близкий вектор x + \\delta x такой, что\n\\text{alg}(x) = f(x + \\delta x)\nи \\Vert \\delta x \\Vert мало."
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#классический-пример",
    "href": "lectures/lecture-1/lecture-1.html#классический-пример",
    "title": "Представление чисел",
    "section": "Классический пример",
    "text": "Классический пример\nКлассическим примером является решение систем линейных уравнений с помощью метода Гаусса, который похож на LU-разложение (подробнее позже)\nРассмотрим матрицу Гильберта с элементами\nA = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.\nИ рассмотрим линейную систему\nAx = f.\n\nimport numpy as np\nn = 100\na = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] # Hilbert matrix\nA = np.array(a)\n#rhs =  np.random.randn(n)\nrhs = np.ones(n)\nsol = np.linalg.solve(A, rhs)\nprint(np.linalg.norm(A @ sol - rhs)/np.linalg.norm(rhs))\nplt.plot(sol)\n\n1.3614802098870779e-07\n\n\n\n\n\n\n\n\n\n\nrhs = np.ones(n)\nsol = np.linalg.solve(A, rhs)\nprint(np.linalg.norm(A @ sol - rhs)/np.linalg.norm(rhs))\n#plt.plot(sol)\n\n0.0018351191"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#больше-примеров-неустойчивости",
    "href": "lectures/lecture-1/lecture-1.html#больше-примеров-неустойчивости",
    "title": "Представление чисел",
    "section": "Больше примеров неустойчивости",
    "text": "Больше примеров неустойчивости\nКак вычислить следующие функции численно устойчивым способом?\n\n\\log(1 - \\tanh^2(x))\n\\text{SoftMax}(x)_j = \\dfrac{e^{x_j}}{\\sum\\limits_{i=1}^n e^{x_i}}\n\n\nu = 30\neps = 1e-6\nprint(\"Исходная функция:\", np.log(1 - np.tanh(u)**2))\neps_add = np.log(1 - np.tanh(u)**2 + eps)\nprint(\"Попытка улучшить стабильность добавлением малой константы:\", eps_add)\nprint(\"Использование более численно стабильной формы:\", np.log(4) - 2 * np.log(np.exp(-u) + np.exp(u)))\n\nИсходная функция: -inf\nПопытка улучшить стабильность добавлением малой константы: -13.815510557964274\nИспользование более численно стабильной формы: -58.61370563888011\n\n\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_10237/1002619719.py:3: RuntimeWarning: divide by zero encountered in log\n  print(\"Исходная функция:\", np.log(1 - np.tanh(u)**2))\n\n\n\nn = 5\nx = np.random.normal(size=(n,))\nx[0] = 1000\nprint(np.exp(x) / np.sum(np.exp(x)))\nprint(np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x))))\n\n[nan  0.  0.  0.  0.]\n[1. 0. 0. 0. 0.]\n\n\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_7697/1311588733.py:4: RuntimeWarning: overflow encountered in exp\n  print(np.exp(x) / np.sum(np.exp(x)))\n/var/folders/x_/_k8z8m6s2qxc_j4gwz6fpvmm0000gp/T/ipykernel_7697/1311588733.py:4: RuntimeWarning: invalid value encountered in divide\n  print(np.exp(x) / np.sum(np.exp(x)))"
  },
  {
    "objectID": "lectures/lecture-1/lecture-1.html#основные-выводы",
    "href": "lectures/lecture-1/lecture-1.html#основные-выводы",
    "title": "Представление чисел",
    "section": "Основные выводы",
    "text": "Основные выводы\n\nЧисла с плавающей точкой (двойная, одинарная точность, количество байт), ошибки округления\nНормы как меры малости, используются для вычисления точности\nНормы: 1, p и евклидова\nПрямая/обратная ошибка (и устойчивость алгоритмов) (в следующих лекциях)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#краткий-обзор-предыдущей-лекции",
    "href": "lectures/lecture-7/lecture-7.html#краткий-обзор-предыдущей-лекции",
    "title": "Метод Якоби: сходимость",
    "section": "Краткий обзор предыдущей лекции",
    "text": "Краткий обзор предыдущей лекции\n\nQR разложение и алгоритм Грама-Шмидта\nРазложение Шура и QR-алгоритм (основы)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#план-на-сегодня",
    "href": "lectures/lecture-7/lecture-7.html#план-на-сегодня",
    "title": "Метод Якоби: сходимость",
    "section": "План на сегодня",
    "text": "План на сегодня\nСегодня мы поговорим о:\n\nАлгоритмах для симметричных задач на собственные значения\n\nQR алгоритм (более подробно)\nРазделяй и властвуй\nМетод бисекции\n\nАлгоритмах вычисления SVD"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#вычисление-формы-шура",
    "href": "lectures/lecture-7/lecture-7.html#вычисление-формы-шура",
    "title": "Метод Якоби: сходимость",
    "section": "Вычисление формы Шура",
    "text": "Вычисление формы Шура\n\nНапомним, что мы пытаемся избежать сложности \\mathcal{O}(n^3) для каждой итерации.\nИдея состоит в том, чтобы придать матрице более простую структуру, чтобы каждый шаг QR алгоритма стал дешевле.\nВ случае произвольной матрицы мы можем использовать форму Хессенберга."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#форма-хессенберга",
    "href": "lectures/lecture-7/lecture-7.html#форма-хессенберга",
    "title": "Метод Якоби: сходимость",
    "section": "Форма Хессенберга",
    "text": "Форма Хессенберга\nМатрица A называется матрицей в форме Хессенберга, если\na_{ij} = 0, \\quad \\mbox{если } i \\geq j+2.\nH = \\begin{bmatrix} * & * & * & * & * \\\\ * & * & * & * & * \\\\ 0 & * & * & * & *\\\\ 0 & 0 & * & * & *\\\\ 0 & 0 & 0 & * & * \\\\ \\end{bmatrix}."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#приведение-произвольной-матрицы-к-форме-хессенберга",
    "href": "lectures/lecture-7/lecture-7.html#приведение-произвольной-матрицы-к-форме-хессенберга",
    "title": "Метод Якоби: сходимость",
    "section": "Приведение произвольной матрицы к форме Хессенберга",
    "text": "Приведение произвольной матрицы к форме Хессенберга\n\nПрименяя отражения Хаусхолдера, мы можем привести любую матрицу к форме Хессенберга\n\nU^* A U = H\n\nЕдинственное отличие от разложения Шура состоит в том, что мы должны отобразить первый столбец в вектор с двумя ненулевыми элементами, причем первый элемент не изменяется.\nВычислительная сложность такого приведения составляет \\mathcal{O}(n^3) операций.\nВ форме Хессенберга вычисление одной итерации QR алгоритма требует \\mathcal{O}(n^2) операций (например, используя вращения Гивенса, как?), и форма Хессенберга сохраняется при QR итерации (проверьте почему)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#симметричный-эрмитов-случай",
    "href": "lectures/lecture-7/lecture-7.html#симметричный-эрмитов-случай",
    "title": "Метод Якоби: сходимость",
    "section": "Симметричный (эрмитов) случай",
    "text": "Симметричный (эрмитов) случай\n\nВ симметричном случае у нас A = A^*, тогда H = H^* и верхняя форма Хессенберга становится трехдиагональной матрицей.\nДалее мы будем говорить о случае симметричной трехдиагональной формы.\nЛюбая симметричная (эрмитова) матрица может быть приведена к трехдиагональной форме с помощью отражений Хаусхолдера.\nКлючевой момент заключается в том, что трехдиагональная форма сохраняется при QR алгоритме, и стоимость одного шага может быть снижена до \\mathcal{O}(n)!"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-алгоритм-итерации",
    "href": "lectures/lecture-7/lecture-7.html#qr-алгоритм-итерации",
    "title": "Метод Якоби: сходимость",
    "section": "QR алгоритм: итерации",
    "text": "QR алгоритм: итерации\n\nИтерации QR алгоритма имеют следующий вид:\n\nA_k = Q_k R_k, \\quad A_{k+1} = R_k Q_k.\n\nЕсли A_0 = A является  трехдиагональной симметричной матрицей , эта форма сохраняется при QR алгоритме.\n\nДавайте посмотрим..\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Generate a random tridiagonal matrix\n\nn = 20\nrng = np.random.RandomState(0)\nd = rng.normal(size=(n,))\n\nrng = np.random.RandomState(1)\nsub_diag = rng.normal(size=(n-1,))\n\nmat = np.diag(d) + np.diag(sub_diag, -1) + np.diag(sub_diag, 1)\nmat1 = np.abs(mat)\nmat1 = mat1/np.max(mat1.flatten())\nplt.spy(mat)\nq, r = np.linalg.qr(mat)\nplt.figure()\nb = r.dot(q)\nb[abs(b) &lt;= 1e-12] = 0\nplt.spy(b)\n#plt.figure()\n#plt.imshow(np.abs(r.dot(q)))\nb[0, :]\n\nArray([-1.47168069, -0.75157939,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ],      dtype=float64)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#трехдиагональная-форма",
    "href": "lectures/lecture-7/lecture-7.html#трехдиагональная-форма",
    "title": "Метод Якоби: сходимость",
    "section": "Трехдиагональная форма",
    "text": "Трехдиагональная форма\n\nВ трехдиагональной форме вам не нужно вычислять матрицу Q: вам нужно вычислить только трехдиагональную часть, которая появляется после итераций\n\nA_k = Q_k R_k, \\quad A_{k+1}  = R_k Q_k,\nв случае, когда A_k = A^*_k и также является трехдиагональной.\n\nТакая матрица определяется \\mathcal{O}(n) параметрами; вычисление QR более сложное, но можно вычислить A_{k+1} напрямую без вычисления Q_k.\nЭто называется неявным QR-шагом."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#теорема-о-неявной-qr-итерации",
    "href": "lectures/lecture-7/lecture-7.html#теорема-о-неявной-qr-итерации",
    "title": "Метод Якоби: сходимость",
    "section": "Теорема о неявной QR-итерации",
    "text": "Теорема о неявной QR-итерации\nВсе неявные QR-алгоритмы основаны на следующей теореме:\nПусть\nQ^* A Q = H\nявляется неприводимой верхней матрицей Хессенберга. Тогда первый столбец матрицы Q определяет все остальные её столбцы. Его можно найти из уравнения\nA Q = Q H."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#сходимость-qr-алгоритма",
    "href": "lectures/lecture-7/lecture-7.html#сходимость-qr-алгоритма",
    "title": "Метод Якоби: сходимость",
    "section": "Сходимость QR-алгоритма",
    "text": "Сходимость QR-алгоритма\n\nСходимость QR-алгоритма является очень деликатным вопросом (см. Е. Е. Тыртышников, “Краткое введение в численный анализ” для подробностей).\n\nРезюме. Если у нас есть разложение вида\nA = X \\Lambda X^{-1}, \\quad A = \\begin{bmatrix}A_{11} & A_{12} \\\\ A_{21} & A_{22}\\end{bmatrix}\nи\n\n\\Lambda = \\begin{bmatrix} \\Lambda_1 & 0 \\\\\n0 & \\Lambda_2 \\end{bmatrix}, \\quad \\lambda(\\Lambda_1)=\\{\\lambda_1,\\dots,\\lambda_m\\}, \\ \\lambda(\\Lambda_2)=\\{\\lambda_{m+1},\\dots,\\lambda_r\\},\n\nи существует разрыв между собственными значениями \\Lambda_1 и \\Lambda_2 (|\\lambda_1|\\geq \\dots \\geq |\\lambda_m| &gt; |\\lambda_{m+1}| \\geq\\dots \\geq |\\lambda_r| &gt;0), тогда блок A^{(k)}_{21} матрицы A_k в QR-итерации стремится к нулю со скоростью\n\\Vert A^{(k)}_{21} \\Vert \\leq  C q^k, \\quad q = \\left| \\frac{\\lambda_{m+1}}{\\lambda_{m}}  \\right |,\nгде m - размер \\Lambda_1.\nТаким образом, нам нужно увеличить разрыв! Это можно сделать с помощью QR-алгоритма со сдвигами."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#qr-алгоритм-со-сдвигами",
    "href": "lectures/lecture-7/lecture-7.html#qr-алгоритм-со-сдвигами",
    "title": "Метод Якоби: сходимость",
    "section": "QR-алгоритм со сдвигами",
    "text": "QR-алгоритм со сдвигами\nA_{k} - s_k I = Q_k R_k, \\quad A_{k+1} = R_k Q_k + s_k I\nСкорость сходимости для версии со сдвигом тогда равна\n\\left| \\frac{\\lambda_{m+1} - s_k}{\\lambda_{m} - s_k}  \\right |,\nгде \\lambda_m - m-е по величине собственное значение матрицы по модулю. - Если сдвиг близок к собственному значению, то скорость сходимости лучше. - Существуют различные стратегии выбора сдвигов. - Введение сдвигов - это общая стратегия для улучшения сходимости итерационных методов нахождения собственных значений. - В следующих слайдах мы проиллюстрируем, как выбрать сдвиг на более простом алгоритме."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#сдвиги-и-степенной-метод",
    "href": "lectures/lecture-7/lecture-7.html#сдвиги-и-степенной-метод",
    "title": "Метод Якоби: сходимость",
    "section": "Сдвиги и степенной метод",
    "text": "Сдвиги и степенной метод\n\nВспомним степенной метод для вычисления собственных значений.\n\nx_{k+1} := A x_k, \\quad x_{k+1} := \\frac{x_{k+1}}{\\Vert x_{k+1} \\Vert}.\n\nОн сходится к собственному вектору, соответствующему наибольшему по модулю собственному значению.\nСходимость может быть очень медленной.\nПопробуем использовать стратегию сдвигов. Если мы сдвигаем матрицу как\n\n  A := A - \\lambda_k I\nто соответствующее собственное значение становится маленьким (а нам нужно большое). - Это не то, что мы хотели!"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#метод-обратной-итерации-и-метод-итерации-рэлея",
    "href": "lectures/lecture-7/lecture-7.html#метод-обратной-итерации-и-метод-итерации-рэлея",
    "title": "Метод Якоби: сходимость",
    "section": "Метод обратной итерации и метод итерации Рэлея",
    "text": "Метод обратной итерации и метод итерации Рэлея\n\nЧтобы сделать малое собственное значение большим, нам нужно обратить матрицу, и это дает нам метод обратной итерации\n\nx_{k+1} = (A - \\lambda I)^{-1} x_k,\nгде \\lambda - сдвиг, который является приближением к собственному значению, которое мы хотим найти.\n\nКак и для степенного метода, сходимость является линейной.\nДля ускорения сходимости можно использовать метод итерации Рэлея (обратная итерация с адаптивными сдвигами), который задается выбором адаптивного сдвига:\n\nx_{k+1} = (A - \\lambda_k I)^{-1} x_k,\n\\lambda_k = \\frac{(Ax_k, x_k)}{(x_k, x_k)}\n\nВ симметричном случае A = A^* сходимость локально кубическая, а в остальных случаях - локально квадратичная."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#сингулярные-значения-и-собственные-значения-1",
    "href": "lectures/lecture-7/lecture-7.html#сингулярные-значения-и-собственные-значения-1",
    "title": "Метод Якоби: сходимость",
    "section": "Сингулярные значения и собственные значения (1)",
    "text": "Сингулярные значения и собственные значения (1)\n\nТеперь поговорим о сингулярных значениях и собственных значениях.\nSVD\n\nA = U \\Sigma V^*\nсуществует для любой матрицы.\n\nЭто также можно рассматривать как приведение данной матрицы к диагональной форме с помощью двусторонних унитарных преобразований:\n\n\\Sigma = U^* A V.\n\nС помощью двусторонних преобразований Хаусхолдера мы можем привести любую матрицу к бидиагональной форме B (как?)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#сингулярные-значения-и-собственные-значения-2",
    "href": "lectures/lecture-7/lecture-7.html#сингулярные-значения-и-собственные-значения-2",
    "title": "Метод Якоби: сходимость",
    "section": "Сингулярные значения и собственные значения (2)",
    "text": "Сингулярные значения и собственные значения (2)\n\nНеявный QR алгоритм (со сдвигами) дает способ вычисления собственных значений (и формы Шура).\nНо мы не можем применить QR алгоритм непосредственно к бидиагональной матрице, так как она не диагонализируема в общем случае.\nОднако задача вычисления SVD может быть сведена к задаче о собственных значениях симметричной матрицы двумя способами:\n\n\nРаботать с трехдиагональной матрицей\n\nT = B^* B\n\nРаботать с расширенной матрицей\n\nT = \\begin{bmatrix} 0 & B \\\\ B^* & 0 \\end{bmatrix}\n\nСлучай 1 хорош, если вы не формируете T напрямую!\nТаким образом, задача вычисления сингулярных значений может быть сведена к задаче вычисления собственных значений симметричной трехдиагональной матрицы."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#алгоритмы-для-задачи-о-собственных-значениях-симметричных-матриц",
    "href": "lectures/lecture-7/lecture-7.html#алгоритмы-для-задачи-о-собственных-значениях-симметричных-матриц",
    "title": "Метод Якоби: сходимость",
    "section": "Алгоритмы для задачи о собственных значениях симметричных матриц",
    "text": "Алгоритмы для задачи о собственных значениях симметричных матриц\nРассмотрены: - QR алгоритм: “золотой стандарт” вычисления собственных значений\nСледующие слайды: - Метод “разделяй и властвуй” - Метод бисекции - Метод Якоби"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#разделяй-и-властвуй",
    "href": "lectures/lecture-7/lecture-7.html#разделяй-и-властвуй",
    "title": "Метод Якоби: сходимость",
    "section": "Разделяй и властвуй",
    "text": "Разделяй и властвуй\n\nПредположим, у нас есть трехдиагональная матрица, и мы разделяем ее на два блока:\n\nT = \\begin{bmatrix} T'_1 & B \\\\ B^{\\top} & T'_2 \\end{bmatrix}\n\nМы можем записать матрицу T как\n\nT = \\begin{bmatrix} T_1 & 0 \\\\ 0 & T_2 \\end{bmatrix} + b_m v v^*\nгде vv^* - матрица ранга 1, v = (0,\\dots,0,1,1,0,\\dots,0)^T.\n\nПредположим, мы уже разложили T_1 и T_2:\n\nT_1 = Q_1 \\Lambda_1 Q^*_1, \\quad T_2 = Q_2 \\Lambda_2 Q^*_2\n\nТогда (проверьте),\n\n\\begin{bmatrix} Q^*_1 & 0 \\\\ 0 & Q^*_2 \\end{bmatrix} T\\begin{bmatrix} Q_1 & 0 \\\\ 0 & Q_2 \\end{bmatrix} = D + \\rho u u^{*}, \\quad D = \\begin{bmatrix} \\Lambda_1 & 0 \\\\ 0 & \\Lambda_2\\end{bmatrix}\n\nТо есть мы свели задачу к проблеме вычисления собственных значений  диагональной плюс низкоранговой матрицы"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#диагональная-плюс-низкоранговая-матрица",
    "href": "lectures/lecture-7/lecture-7.html#диагональная-плюс-низкоранговая-матрица",
    "title": "Метод Якоби: сходимость",
    "section": "Диагональная плюс низкоранговая матрица",
    "text": "Диагональная плюс низкоранговая матрица\nВычисление собственных значений матрицы\nD + \\rho u u^* \nявляется непростой задачей.\nХарактеристический многочлен имеет вид\n\\det(D + \\rho uu^* - \\lambda I) = \\det(D - \\lambda I)\\det(I + \\rho (D - \\lambda I)^{-1} uu^*) = 0.\nТогда (докажите!!)\n\\det(I + \\rho (D - \\lambda I)^{-1} uu^*) = 1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{d_i - \\lambda} = 0\nПодсказка: найдите \\det(I + w u^*), используя тот факт, что \\text{det}(C) = \\prod_{i=1}^n\\lambda_i(C) и \\text{trace}(C) = \\sum_{i=1}^n \\lambda_i."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#характеристическое-уравнение",
    "href": "lectures/lecture-7/lecture-7.html#характеристическое-уравнение",
    "title": "Метод Якоби: сходимость",
    "section": "Характеристическое уравнение",
    "text": "Характеристическое уравнение\n1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{d_i - \\lambda} = 0\nКак найти корни?\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlm = np.array([1, 2, 3, 4])\nM = len(lm)\nD = np.array(lm)\na = np.min(lm)\nb = np.max(lm)\nt = np.linspace(-1, 6, 1000)\nu = 0.5 * np.ones(M)\nrho = 1\ndef fun(lam):\n    return 1 + rho * np.sum(u**2/(D - lam))\nres = [fun(lam) for lam in t]\nplt.figure(figsize=(10,8))\nplt.plot(t, res, 'k')\n#plt.plot(np.zeros_like(t))\nplt.ylim([-6, 6])\nplt.tight_layout()\nplt.yticks(fontsize=24)\nplt.xticks(fontsize=24)\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nФункция имеет только один корень на отрезке [d_i, d_{i+1}]\nМы, кстати, доказали теорему о перемежаемости Коши (что происходит с собственными значениями при возмущении ранга 1)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#как-найти-корень",
    "href": "lectures/lecture-7/lecture-7.html#как-найти-корень",
    "title": "Метод Якоби: сходимость",
    "section": "Как найти корень",
    "text": "Как найти корень\n\nМетод Ньютона не сработает (нарисуйте картинку с касательной линией).\nЗаметим, что метод Ньютона - это просто аппроксимация функции f(\\lambda) линейной функцией.\nГораздо лучшей аппроксимацией является гипербола:\n\nf(\\lambda) \\approx c_0 + \\frac{c_1}{d_i - \\lambda} + \\frac{c_2}{d_{i+1} - \\lambda}.\n\nЧтобы подобрать коэффициенты, нам нужно вычислить f(\\lambda) и f'(\\lambda) в конкретной точке.\nПосле этого аппроксимацию можно получить, решив квадратное уравнение"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#важные-проблемы",
    "href": "lectures/lecture-7/lecture-7.html#важные-проблемы",
    "title": "Метод Якоби: сходимость",
    "section": "Важные проблемы",
    "text": "Важные проблемы\n\nВо-первых, стабильность: этот метод был заброшен на долгое время из-за нестабильности вычисления собственных векторов.\nВ рекурсии нам нужно вычислить собственные векторы матрицы D + \\rho uu^*.\nТочное выражение для собственных векторов просто (давайте проверим!)\n\n(D - \\alpha_i I)^{-1}u, где \\alpha_i - вычисленный корень.\n\nПричина нестабильности:\n\nесли \\alpha_i и \\alpha_{i+1} близки, то соответствующие собственные векторы коллинеарны, но они должны быть ортогональными\nесли \\alpha_i и \\alpha_{i+1} близки, то они близки к d_i, поэтому матрицы D - \\alpha_i I и D - \\alpha_{i+1} I близки к сингулярным"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#теорема-лёвнера",
    "href": "lectures/lecture-7/lecture-7.html#теорема-лёвнера",
    "title": "Метод Якоби: сходимость",
    "section": "Теорема Лёвнера",
    "text": "Теорема Лёвнера\n\nРешение пришло в виде использования странной теоремы Лёвнера:\n\nЕсли \\alpha_i и d_i удовлетворяют теореме о перемежаемости\nd_n &lt; \\alpha_n &lt; \\ldots &lt; d_{i+1} &lt; \\alpha_{i+1} \\ldots\nТо существует вектор \\widehat{u} такой, что \\alpha_i являются точными собственными значениями матрицы\n\\widehat{D} = D + \\widehat{u} \\widehat{u}^*.\n\nТаким образом, сначала вычисляются собственные значения, затем вычисляется \\widehat{u} и только потом собственные векторы."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#разделяй-и-властвуй-и-быстрый-мультипольный-метод",
    "href": "lectures/lecture-7/lecture-7.html#разделяй-и-властвуй-и-быстрый-мультипольный-метод",
    "title": "Метод Якоби: сходимость",
    "section": "Разделяй и властвуй и Быстрый Мультипольный Метод",
    "text": "Разделяй и властвуй и Быстрый Мультипольный Метод\nВ вычислениях метода “разделяй и властвуй” нам необходимо вычислять суммы вида\nf(\\lambda) = 1 + \\rho \\sum_{i=1}^n \\frac{|u_i|^2}{(d_i - \\lambda)},\nи делать это как минимум для n точек.\n\nСложность тогда составляет \\mathcal{O}(n^2), как и для QR алгоритма.\nМожем ли мы сделать её \\mathcal{O}(n \\log n)?\nОтвет - да, но нам придётся заменить точные вычисления приближёнными с помощью Быстрого Мультипольного Метода.\n\nДавайте объясним немного подробнее…"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#еще-несколько-алгоритмов",
    "href": "lectures/lecture-7/lecture-7.html#еще-несколько-алгоритмов",
    "title": "Метод Якоби: сходимость",
    "section": "Еще несколько алгоритмов",
    "text": "Еще несколько алгоритмов\n\nСовершенно другой подход основан на бисекции.\nДля матрицы A ее инерция определяется как тройка\n\n(\\nu, \\zeta, \\pi),\nгде \\nu - количество отрицательных, \\zeta - нулевых и \\pi - положительных собственных значений.\n\nЕсли X невырожденная, то\n\nInertia(A) = Inertia(X^* A X)"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#бисекция-с-помощью-метода-гаусса",
    "href": "lectures/lecture-7/lecture-7.html#бисекция-с-помощью-метода-гаусса",
    "title": "Метод Якоби: сходимость",
    "section": "Бисекция с помощью метода Гаусса",
    "text": "Бисекция с помощью метода Гаусса\n\nДля заданного z мы можем выполнить исключение Гаусса:\n\nA - zI = L D L^*,\nи инерцию диагональной матрицы тривиально вычислить.\n\nТаким образом, если мы хотим найти все собственные значения в интервале a, b\nИспользуя инерцию, мы можем легко подсчитать количество собственных значений в интервале.\nИллюстрация: если Inertia(A)=(5,0,2) и после сдвига Inertia(A-zI)=(4,0,3), z\\in[a,b], то мы знаем, что \\lambda(A)\\in[a,z]."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#метод-якоби",
    "href": "lectures/lecture-7/lecture-7.html#метод-якоби",
    "title": "Метод Якоби: сходимость",
    "section": "Метод Якоби",
    "text": "Метод Якоби\n\nВспомним, что такое вращения Якоби (Гивенса)\nВ плоскости они соответствуют ортогональной матрице 2 \\times 2 вида\n\n\\begin{pmatrix} \\cos \\phi & \\sin \\phi \\\\ -\\sin \\phi & \\cos \\phi \\end{pmatrix},\nи в n-мерном случае мы выбираем две переменные i и j и выполняем вращение."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#метод-якоби-продолжение",
    "href": "lectures/lecture-7/lecture-7.html#метод-якоби-продолжение",
    "title": "Метод Якоби: сходимость",
    "section": "Метод Якоби (продолжение)",
    "text": "Метод Якоби (продолжение)\n\nИдея метода Якоби заключается в минимизации суммы квадратов внедиагональных элементов:\n\n\\Gamma(A) = \\mathrm{off}( U^* A U), \\quad \\mathrm{off}^2(X) = \\sum_{i \\ne j} \\left|X_{ij}\\right|^2 = \\|X \\|^2_F - \\sum\\limits_{i=1}^n x^2_{ii}.\nпутем применения последовательных вращений Якоби U для обнуления внедиагональных элементов.\n\nКогда “опорный элемент” выбран, его легко исключить.\nОсновной вопрос заключается в том, каков порядок проходов, которые мы должны сделать (т.е. в каком порядке исключать элементы).\nЕсли мы всегда исключаем наибольшие внедиагональные элементы, метод имеет квадратичную сходимость.\nНа практике используется циклический порядок (т.е., (1, 2), (1, 3), \\ldots, (2, 3), \\ldots)."
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#jacobi-итоги",
    "href": "lectures/lecture-7/lecture-7.html#jacobi-итоги",
    "title": "Метод Якоби: сходимость",
    "section": "Jacobi: итоги",
    "text": "Jacobi: итоги\nМетод Якоби был первым численным методом для вычисления собственных значений, предложенным в 1846 году.\n\nБольшая константа\nОчень точный (высокая относительная погрешность для малых собственных значений)\nХорошие возможности параллелизации"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#итоги-по-этой-части",
    "href": "lectures/lecture-7/lecture-7.html#итоги-по-этой-части",
    "title": "Метод Якоби: сходимость",
    "section": "Итоги по этой части",
    "text": "Итоги по этой части\n\nМножество алгоритмов для вычисления решения задачи на собственные значения:\n\nQR\nРазделяй и властвуй\nМетод бисекции\nМетод Якоби"
  },
  {
    "objectID": "lectures/lecture-7/lecture-7.html#следующая-лекция",
    "href": "lectures/lecture-7/lecture-7.html#следующая-лекция",
    "title": "Метод Якоби: сходимость",
    "section": "Следующая лекция",
    "text": "Следующая лекция\n\nМы начинаем разреженную и/или структурированную численную линейную алгебру."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#краткий-обзор-предыдущей-части",
    "href": "lectures/lecture-10/lecture-10.html#краткий-обзор-предыдущей-части",
    "title": "Вопросы?",
    "section": "Краткий обзор предыдущей части",
    "text": "Краткий обзор предыдущей части\n\nРаспределенная память для огромных плотных матриц\nФорматы разреженных матриц (COO, CSR, CSC)\nПроизведение матрицы на вектор\nНеэффективность обработки разреженных матриц\nПодходы к уменьшению промахов кэша"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#прямые-методы-для-разреженных-матриц-lu-разложение",
    "href": "lectures/lecture-10/lecture-10.html#прямые-методы-для-разреженных-матриц-lu-разложение",
    "title": "Вопросы?",
    "section": "Прямые методы для разреженных матриц: LU разложение",
    "text": "Прямые методы для разреженных матриц: LU разложение\n\nПочему разреженные линейные системы можно решать быстрее, какова методика?\nВ LU разложении матрицы A множители L и U также могут быть разреженными:\n\nA = L U\n\nИ решение линейных систем с разреженными треугольными матрицами очень простое.\n\n Обратите внимание, что обратная матрица к разреженной матрице не является разреженной! \n\nimport numpy as np\nimport scipy.sparse as spsp\nn = 4\nex = np.ones(n);\n#a = spsp.spdiags(np.vstack((ex,  np.random.rand(n), np.random.rand(n))), [-1, 0, 1], n, n, 'csr'); \na = spsp.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \n\n#a = spsp.spdiags(np.vstack((-ex,  2*ex, -ex), np.random.rand(n)), [-1, 0, 1], n, n, 'csr'); \n\nb = np.array(np.linalg.inv(a.toarray()))\nprint(a.toarray())\nprint(b)\nnp.linalg.svd(b[:3, 4:])[1]\n\n[[ 2. -1.  0.  0.]\n [-1.  2. -1.  0.]\n [ 0. -1.  2. -1.]\n [ 0.  0. -1.  2.]]\n[[0.8 0.6 0.4 0.2]\n [0.6 1.2 0.8 0.4]\n [0.4 0.8 1.2 0.6]\n [0.2 0.4 0.6 0.8]]\n\n\narray([], dtype=float64)"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#и-множители",
    "href": "lectures/lecture-10/lecture-10.html#и-множители",
    "title": "Вопросы?",
    "section": "И множители…",
    "text": "И множители…\nL и U обычно разреженные. В трехдиагональном случае они даже бидиагональные!\n\nfrom scipy.sparse.linalg import splu\nT = splu(a.tocsc())#, permc_spec=\"NATURAL\")\nprint(T.L.toarray())\n\n[[ 1.   0.   0.   0. ]\n [ 0.   1.   0.   0. ]\n [-0.5 -0.5  1.   0. ]\n [ 0.  -0.5 -0.5  1. ]]\n\n\nИнтересно отметить, что splu без параметра permc_spec будет создавать перестановки, которые не дадут бидиагональный множитель:\n\nfrom scipy.sparse.linalg import splu\nT = splu(a.tocsc())\nprint(T.L.todense())\nprint(T.perm_c)\n\n[[ 1.          0.          0.          0.          0.          0.\n   0.        ]\n [ 0.          1.          0.          0.          0.          0.\n   0.        ]\n [ 0.          0.          1.          0.          0.          0.\n   0.        ]\n [ 0.          0.          0.          1.          0.          0.\n   0.        ]\n [ 0.          0.          0.          0.          1.          0.\n   0.        ]\n [ 0.          0.          0.          0.          0.19991324  1.\n   0.        ]\n [ 0.81628124  0.14222853 -0.47418756  0.43359091 -0.00845846  0.23571107\n   1.        ]]\n[0 1 2 3 5 4 6]"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#d-case",
    "href": "lectures/lecture-10/lecture-10.html#d-case",
    "title": "Вопросы?",
    "section": "2D-case",
    "text": "2D-case\nFrom a matrix that comes as a discretization of a two-dimensional problem everything is much worse:\n\nimport scipy as sp\nimport scipy.sparse as spsp\nimport scipy.sparse.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 128\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = spsp.csc_matrix(A)\nT = spsp.linalg.splu(A)#, permc_spec=\"NATURAL\")\nplt.spy(T.L, marker='.', color='k', markersize=8)\nT.L\n#plt.spy(A, marker='.', color='k', markersize=8)\n\n&lt;Compressed Sparse Column sparse matrix of dtype 'float64'\n    with 610620 stored elements and shape (16384, 16384)&gt;\n\n\n\n\n\n\n\n\n\nДля правильной перестановки в двумерном случае количество ненулевых элементов в множителе L растет как \\mathcal{O}(N \\log N). Но сложность составляет \\mathcal{O}(N^{3/2})."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#основная-задача-как-сделать-множители-l-и-u-максимально-разреженными",
    "href": "lectures/lecture-10/lecture-10.html#основная-задача-как-сделать-множители-l-и-u-максимально-разреженными",
    "title": "Вопросы?",
    "section": "Основная задача: как сделать множители L и U максимально разреженными?",
    "text": "Основная задача: как сделать множители L и U максимально разреженными?"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#основной-инструмент-для-анализа-разреженности-множителей-теория-графов",
    "href": "lectures/lecture-10/lecture-10.html#основной-инструмент-для-анализа-разреженности-множителей-теория-графов",
    "title": "Вопросы?",
    "section": "Основной инструмент для анализа разреженности множителей: теория графов",
    "text": "Основной инструмент для анализа разреженности множителей: теория графов\n\nКоличество ненулевых элементов в LU-разложении имеет глубокую связь с теорией графов.\nПакет networkx может использоваться для визуализации графов, имея только матрицу смежности.\nОн даже может в некоторой степени восстановить структуру графа.\n\n\nimport networkx as nx\nimport scipy as sp\nn = 3\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(sp.sparse.kron(lp1, e), e) + sp.sparse.kron(sp.sparse.kron(e, lp1), e) + sp.sparse.kron(sp.sparse.kron(e, e), lp1)\nA = spsp.csc_matrix(A)\nG = nx.Graph(A)\nnx.draw(G, pos=nx.spectral_layout(G))#, node_size=1)"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#что-такое-заполнение",
    "href": "lectures/lecture-10/lecture-10.html#что-такое-заполнение",
    "title": "Вопросы?",
    "section": "Что такое заполнение?",
    "text": "Что такое заполнение?\n\nЗаполнение матрицы - это те элементы, которые изменяются с нулевого на ненулевое значение во время выполнения алгоритма.\nЗаполнение различно для разных перестановок. Поэтому перед факторизацией нам нужно найти такое переупорядочение, которое даёт наименьшее заполнение.\n\nПример\nA = \\begin{bmatrix} * & * & * & * & *\\\\ * & * & 0 & 0 & 0 \\\\ * & 0  & * & 0 & 0 \\\\ * & 0 & 0& * & 0 \\\\ * & 0 & 0& 0 & * \\end{bmatrix} \nЕсли мы исключаем элементы сверху вниз, то получим плотную матрицу. Однако мы могли бы сохранить разреженность, если бы исключение выполнялось снизу вверх."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#пример-плотных-множителей-после-lu-разложения",
    "href": "lectures/lecture-10/lecture-10.html#пример-плотных-множителей-после-lu-разложения",
    "title": "Вопросы?",
    "section": "Пример плотных множителей после LU-разложения",
    "text": "Пример плотных множителей после LU-разложения\nДля матрицы A=A^*&gt;0 мы вычисляем её разложение Холецкого A = LL^*.\nМножитель L может быть плотным, даже если A разреженная:\n\n\\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} =\n\\begin{bmatrix} * &  &  &  \\\\ * & * &  &  \\\\ * & * & * &  \\\\ * & * & * & * \\end{bmatrix}\n\\begin{bmatrix} * & * & * & * \\\\  & * & * & * \\\\  &  & * & * \\\\  &  &  & * \\end{bmatrix}\n\nКак сделать множители разреженными, т.е. минимизировать заполнение?"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#почему-перестановки-могут-уменьшить-заполнение-вот-пример",
    "href": "lectures/lecture-10/lecture-10.html#почему-перестановки-могут-уменьшить-заполнение-вот-пример",
    "title": "Вопросы?",
    "section": "Почему перестановки могут уменьшить заполнение? Вот пример…",
    "text": "Почему перестановки могут уменьшить заполнение? Вот пример…\nНам нужно найти перестановку индексов так, чтобы множители были разреженными, т.е. мы строим разложение Холецкого для PAP^\\top, где P - матрица перестановки.\nДля примера из предыдущего слайда\n\nP \\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} P^\\top =\n\\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\ * & * & * & * \\end{bmatrix} =\n\\begin{bmatrix} * &  &  &  \\\\  & * &  &  \\\\  &  & * &  \\\\ * & * & * & * \\end{bmatrix}\n\\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\  & &  & * \\end{bmatrix}\n\nгде\n\nP = \\begin{bmatrix}  &  &  & 1 \\\\  &  & 1 &  \\\\  & 1 &  &  \\\\ 1 &  &  &  \\end{bmatrix}\n\n\nФорма матрицы в виде стрелки дает разреженные множители в LU-разложении\n\n\nimport numpy as np\nimport scipy.sparse as spsp\nimport scipy.sparse.linalg as spsplin\nimport scipy.linalg as splin\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nA = spsp.coo_matrix((np.random.randn(10), ([0, 0, 0, 0, 1, 1, 2, 2, 3, 3], \n                                           [0, 1, 2, 3, 0, 1, 0, 2, 0, 3])))\nprint(\"Original matrix\")\nplt.spy(A)\nplt.show()\nlu = spsplin.splu(A.tocsc(), permc_spec=\"NATURAL\")\nprint(\"L factor\")\nplt.spy(lu.L)\nplt.show()\nprint(\"U factor\")\nplt.spy(lu.U)\nplt.show()\nprint(\"Column permutation:\", lu.perm_c)\nprint(\"Row permutation:\", lu.perm_r)\n\nOriginal matrix\n\n\n\n\n\n\n\n\n\nL factor\n\n\n\n\n\n\n\n\n\nU factor\n\n\n\n\n\n\n\n\n\nColumn permutation: [0 1 2 3]\nRow permutation: [1 3 2 0]"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#блочная-версия-подходящей-структуры-разреженности-структура-в-виде-стрелки",
    "href": "lectures/lecture-10/lecture-10.html#блочная-версия-подходящей-структуры-разреженности-структура-в-виде-стрелки",
    "title": "Вопросы?",
    "section": "Блочная версия подходящей структуры разреженности (структура в виде стрелки)",
    "text": "Блочная версия подходящей структуры разреженности (структура в виде стрелки)\n\nPAP^\\top = \\begin{bmatrix} A_{11} &  & A_{13} \\\\  & A_{22} & A_{23} \\\\ A_{31} & A_{32} & A_{33}\\end{bmatrix}\n\nтогда\n\nPAP^\\top = \\begin{bmatrix} A_{11} & 0 & 0 \\\\ 0 & A_{22} & 0 \\\\ A_{31} & A_{32} & A_{33} - A_{31}A_{11}^{-1} A_{13} - A_{32}A_{22}^{-1}A_{23} \\end{bmatrix} \\begin{bmatrix}  I & 0 & A_{11}^{-1}A_{13} \\\\ 0 & I & A_{22}^{-1}A_{23} \\\\ 0 & 0 & I\\end{bmatrix}\n\n\nБлок $ A_{33} - A_{31}A_{11}^{-1} A_{13} - A_{32}A_{22}^{-1}A_{23}$ является дополнением Шура для блочно-диагональной матрицы \\begin{bmatrix} A_{11} & 0 \\\\ 0 & A_{22} \\end{bmatrix}\nМы сводим задачу к решению систем линейных уравнений меньшего размера с матрицами A_{11} и A_{22}"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#что-мы-можем-сделать-для-минимизации-заполнения",
    "href": "lectures/lecture-10/lecture-10.html#что-мы-можем-сделать-для-минимизации-заполнения",
    "title": "Вопросы?",
    "section": "Что мы можем сделать для минимизации заполнения?",
    "text": "Что мы можем сделать для минимизации заполнения?\n\nПереупорядочение строк и столбцов разреженной матрицы с целью уменьшения количества ненулевых элементов в множителях L и U называется минимизацией заполнения (fill-in).\nК сожалению, эта статья Роуза и Тарьяна 1975 года доказывает, что задача минимизации заполнения является NP-полной.\nНо существует множество эвристик:\n\nВыбор ведущего элемента по Марковицу - упорядочение по произведению ненулевых элементов в столбце и строке с ограничением устойчивости\nУпорядочение по минимальной степени - упорядочение по степени вершины\nАлгоритм Катхилла-Макки (и обратный Катхилла-Макки) - переупорядочение для минимизации ширины ленты (не использует представление графа).\nВложенное разбиение: разделение графа на две части с минимальным числом вершин на разделителе (набор вершин, удаляемых после разделения графа на два отдельных связных графа).  Сложность алгоритма зависит от размера разделителя графа. Для одномерного лапласиана разделитель содержит только 1 вершину, в двумерном случае - \\sqrt{N} вершин.\n\n\n\nКак найти перестановку?\n\nКлючевая идея приходит из теории графов\nРазреженную матрицу можно рассматривать как матрицу смежности определенного графа: вершины (i, j) соединены, если соответствующий элемент матрицы ненулевой.\n\n\n\nПример\nГрафы матриц \\begin{bmatrix} * & * & * & * \\\\ * & * &  &  \\\\ * &  & * &  \\\\ * &  &  & * \\end{bmatrix} и \\begin{bmatrix} * &  &  & *  \\\\  & * &  & * \\\\  &  & * & * \\\\ * & * & * & * \\end{bmatrix} имеют следующий вид:\n и \n\nПочему второй порядок лучше, чем первый?"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#минимизация-верхней-границы-заполнения-выбор-ведущего-элемента-по-марковицу",
    "href": "lectures/lecture-10/lecture-10.html#минимизация-верхней-границы-заполнения-выбор-ведущего-элемента-по-марковицу",
    "title": "Вопросы?",
    "section": "Минимизация верхней границы заполнения: выбор ведущего элемента по Марковицу",
    "text": "Минимизация верхней границы заполнения: выбор ведущего элемента по Марковицу\n\nУниверсальный подход к упорядочиванию элементов разреженной матрицы, которые будут исключены\nПоказатель Марковица для каждого ненулевого элемента с индексами (i, j) вычисляется как (r_i - 1)(c_j - 1), где r_i - количество ненулевых элементов в i-й строке, а c_j - количество ненулевых элементов в j-м столбце\nЭто значение является верхней границей заполнения после исключения элемента (i, j). Почему?\nМы можем упорядочить элементы относительно этих значений, выбрать элемент с минимальным значением, исключить его и обновить матрицу. А как насчет устойчивости?\nЗатем пересчитать эти значения и повторить процедуру\nЭтот метод дает нам перестановки строк и столбцов и разреженные множители\nОсновной недостаток - эффективное отслеживание количества ненулевых элементов в каждой строке и столбце после обновления матрицы без полного пересчета\nБолее подробную информацию можно найти в книге Direct Methods for Sparse Matrices by I. S. Duff, A. M. Erisman, and J. K. Reid\nДе-факто стандартный подход при решении задач линейного программирования (LP) и их MILP-модификаций"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#что-если-рассматривать-только-соседей-получаем-упорядочение-по-минимальной-степени",
    "href": "lectures/lecture-10/lecture-10.html#что-если-рассматривать-только-соседей-получаем-упорядочение-по-минимальной-степени",
    "title": "Вопросы?",
    "section": "Что если рассматривать только соседей? Получаем упорядочение по минимальной степени!",
    "text": "Что если рассматривать только соседей? Получаем упорядочение по минимальной степени!\n\nИдея заключается в исключении строк и/или столбцов с меньшим количеством ненулевых элементов, обновлении заполнения и повторении процесса. Как это связано с выбором ведущего элемента по Марковицу?\nЭффективная реализация является проблемой (добавление/удаление элементов).\nТекущий лидер - “приближенная минимальная степень” от Аместоя, Дэвиса, Даффа.\nЭто неоптимально даже для задач с двумерными дифференциальными уравнениями в частных производных\nПакет SciPy sparse использует подход минимального упорядочения для различных матриц (A^{\\top}A, A + A^{\\top})"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#но-в-этих-методах-мы-игнорируем-знание-о-хорошей-структуре-для-разреженного-lu-давайте-явно-используем-его-в-методе",
    "href": "lectures/lecture-10/lecture-10.html#но-в-этих-методах-мы-игнорируем-знание-о-хорошей-структуре-для-разреженного-lu-давайте-явно-используем-его-в-методе",
    "title": "Вопросы?",
    "section": "Но в этих методах мы игнорируем знание о хорошей структуре для разреженного LU! Давайте явно используем его в методе!",
    "text": "Но в этих методах мы игнорируем знание о хорошей структуре для разреженного LU! Давайте явно используем его в методе!"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#как-формализовать-сведение-к-блочной-форме-со-стрелкой",
    "href": "lectures/lecture-10/lecture-10.html#как-формализовать-сведение-к-блочной-форме-со-стрелкой",
    "title": "Вопросы?",
    "section": "Как формализовать сведение к блочной форме со стрелкой?",
    "text": "Как формализовать сведение к блочной форме со стрелкой?\nОпределение. Сепаратор в графе G - это множество S вершин, удаление которых оставляет как минимум две несвязные компоненты.\nСепаратор S дает следующий порядок для графа G с N вершинами: - Найти сепаратор S, удаление которого оставляет связные компоненты T_1, T_2, \\ldots, T_k - Пронумеровать вершины S от N − |S| + 1 до N - Рекурсивно пронумеровать вершины каждой компоненты: T_1 от 1 до |T_1|, T_2 от |T_1| + 1 до |T_1| + |T_2| и т.д. - Если компонента достаточно мала, нумерация в этой компоненте произвольна\n\nСепаратор и блочная структура со стрелкой: пример\nСепаратор для двумерной матрицы Лапласа\n\nA_{2D} = I \\otimes A_{1D} + A_{1D} \\otimes I, \\quad A_{1D} = \\mathrm{tridiag}(-1, 2, -1),\n\nвыглядит следующим образом\n \nКак только мы пронумеровали сначала индексы в \\alpha, затем в \\beta и индексы сепаратора в \\sigma, мы получаем следующую матрицу\n\nPAP^\\top = \\begin{bmatrix} A_{\\alpha\\alpha} &  & A_{\\alpha\\sigma} \\\\  & A_{\\beta\\beta} & A_{\\beta\\sigma} \\\\ A_{\\sigma\\alpha} & A_{\\sigma\\beta} & A_{\\sigma\\sigma}\\end{bmatrix}\n\nкоторая имеет структуру стрелки.\n\nТаким образом, задача нахождения перестановки была сведена к задаче нахождения сепаратора графа!"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#метод-рекурсивного-сведения-к-блочной-структуре-со-стрелкой-вложенное-разбиение",
    "href": "lectures/lecture-10/lecture-10.html#метод-рекурсивного-сведения-к-блочной-структуре-со-стрелкой-вложенное-разбиение",
    "title": "Вопросы?",
    "section": "Метод рекурсивного сведения к блочной структуре со стрелкой – Вложенное разбиение",
    "text": "Метод рекурсивного сведения к блочной структуре со стрелкой – Вложенное разбиение\n\nДля блоков A_{\\alpha\\alpha}, A_{\\beta\\beta} мы продолжаем разбиение рекурсивно.\nКогда рекурсия завершена, нам нужно исключить блоки A_{\\sigma\\alpha} и A_{\\sigma\\beta}.\nЭто делает блок в позиции A_{\\sigma\\sigma}\\in\\mathbb{R}^{n\\times n} плотным.\n\nВычисление разложения Холецкого для этого блока стоит \\mathcal{O}(n^3) = \\mathcal{O}(N^{3/2}), где N = n^2 - общее количество узлов.\nТаким образом, сложность составляет \\mathcal{O}(N^{3/2})"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#пакеты-для-вложенного-разбиения",
    "href": "lectures/lecture-10/lecture-10.html#пакеты-для-вложенного-разбиения",
    "title": "Вопросы?",
    "section": "Пакеты для вложенного разбиения",
    "text": "Пакеты для вложенного разбиения\n\nMUltifrontal Massively Parallel sparse direct Solver (MUMPS)\nPardiso\nUmfpack как часть SuiteSparse\n\nВсе они имеют интерфейсы для C/C++, Fortran и Matlab\n\nКраткое описание вложенного разбиения\n\nНумерация: найти сепаратор.\nПарадигма “разделяй и властвуй”\nРекурсивная обработка двух подмножеств вершин после разделения\nВ теории вложенное разбиение дает оптимальную сложность.\nНа практике оно превосходит другие методы только для очень больших задач."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#сепараторы-на-практике",
    "href": "lectures/lecture-10/lecture-10.html#сепараторы-на-практике",
    "title": "Вопросы?",
    "section": "Сепараторы на практике",
    "text": "Сепараторы на практике\n\nВычисление сепараторов - нетривиальная задача.\nЭвристики разбиения графов являются активной областью исследований на протяжении многих лет, часто мотивированные разбиением для параллельных вычислений.\n\nСуществующие подходы:\n\nСпектральное разбиение (использует собственные векторы матрицы Лапласа графа) - подробнее ниже\nГеометрическое разбиение (для сеток с заданными координатами вершин) обзор и анализ\nИтеративный обмен ((Kernighan-Lin, 1970), (Fiduccia-Matheysses, 1982)\nПоиск в ширину (Lipton, Tarjan 1979)\nМногоуровневое рекурсивное бисекционирование (эвристика, в настоящее время наиболее практичная) (обзор и статья). Пакет для такого рода разбиения называется METIS, написан на C и доступен здесь"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#один-из-способов-построения-сепараторов-спектральное-разбиение-графа",
    "href": "lectures/lecture-10/lecture-10.html#один-из-способов-построения-сепараторов-спектральное-разбиение-графа",
    "title": "Вопросы?",
    "section": "Один из способов построения сепараторов – спектральное разбиение графа",
    "text": "Один из способов построения сепараторов – спектральное разбиение графа\n\nИдея спектрального разбиения восходит к Мирославу Фидлеру, который изучал связность графов (статья).\nНам нужно разделить вершины на два множества.\nРассмотрим маркировку вершин значениями +1/-1 и стоимость\n\nE_c(x) = \\sum_{j} \\sum_{i \\in N(j)} (x_i - x_j)^2, \\quad N(j) \\text{ обозначает множество соседей узла } j. \n\nНам нужно сбалансированное разбиение, поэтому\n\n\\sum_i x_i =  0 \\quad \\Longleftrightarrow \\quad x^\\top e = 0, \\quad e = \\begin{bmatrix}1 & \\dots & 1\\end{bmatrix}^\\top,\nи поскольку у нас метки +1/-1, имеем\n\\sum_i x^2_i = n \\quad \\Longleftrightarrow \\quad \\|x\\|_2^2 = n."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#лапласиан-графа",
    "href": "lectures/lecture-10/lecture-10.html#лапласиан-графа",
    "title": "Вопросы?",
    "section": "Лапласиан графа",
    "text": "Лапласиан графа\nСтоимость E_c может быть записана как (проверьте почему)\nE_c = (Lx, x)\nгде L - это лапласиан графа, который определяется как симметричная матрица с\nL_{ii} = \\mbox{степень вершины $i$},\nL_{ij} = -1, \\quad \\mbox{если $i \\ne j$ и существует ребро},\nи 0 в остальных случаях.\n\nСумма элементов в строках L равна нулю, поэтому существует собственное значение 0, которому соответствует тривиальный собственный вектор из всех единиц.\nСобственные значения неотрицательны (почему?)."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#разбиение-как-задача-оптимизации",
    "href": "lectures/lecture-10/lecture-10.html#разбиение-как-задача-оптимизации",
    "title": "Вопросы?",
    "section": "Разбиение как задача оптимизации",
    "text": "Разбиение как задача оптимизации\n\nМинимизация E_c с указанными ограничениями приводит к разбиению, которое пытается минимизировать количество рёбер в сепараторе, сохраняя при этом сбалансированность разбиения.\nТеперь мы ослабляем целочисленное квадратичное программирование до непрерывного квадратичного программирования\n\nE_c(x) = (Lx, x)\\to \\min_{\\substack{x^\\top e =0, \\\\ \\|x\\|_2^2 = n}}"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#от-вектора-фидлера-к-разделителю",
    "href": "lectures/lecture-10/lecture-10.html#от-вектора-фидлера-к-разделителю",
    "title": "Вопросы?",
    "section": "От вектора Фидлера к разделителю",
    "text": "От вектора Фидлера к разделителю\n\nРешение задачи минимизации даётся собственным вектором (называемым вектором Фидлера), соответствующим второму наименьшему собственному значению лапласиана графа. Действительно,\n\n\n    \\min_{\\substack{x^\\top e =0, \\\\ \\|x\\|_2^2 = n}} (Lx, x) = n \\cdot \\min_{{x^\\top e =0}} \\frac{(Lx, x)}{(x, x)} = n \\cdot \\min_{{x^\\top e =0}} R(x), \\quad R(x) \\text{ это отношение Рэлея}\n\n\nПоскольку e является собственным вектором, соответствующим наименьшему собственному значению, на пространстве x^\\top e =0 мы получаем второе минимальное собственное значение.\nЗнак x_i указывает на разбиение.\nВ вычислениях нам нужно выяснить, как найти это второе минимальное собственное значение — мы по крайней мере знаем о степенном методе, но он находит наибольшее. Мы обсудим итерационные методы для задач на собственные значения позже в нашем курсе.\nЭто основная цель итерационных методов для крупномасштабных линейных задач, и она может быть достигнута с помощью нескольких произведений матрицы на вектор.\n\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport networkx as nx\nkn = nx.read_gml('karate.gml')\nprint(\"Number of vertices = {}\".format(kn.number_of_nodes()))\nprint(\"Number of edges = {}\".format(kn.number_of_edges()))\nnx.draw_networkx(kn, node_color=\"red\") #Draw the graph\n\nNumber of vertices = 34\nNumber of edges = 78\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as spsplin\nLaplacian = nx.laplacian_matrix(kn).asfptype()\nplt.spy(Laplacian, markersize=5)\nplt.title(\"Graph laplacian\")\nplt.axis(\"off\")\nplt.show()\neigval, eigvec = spsplin.eigsh(Laplacian, k=3, which=\"SM\")\nprint(\"The 2 smallest eigenvalues =\", eigval)\n\n\n\n\n\n\n\n\nThe 2 smallest eigenvalues = [2.52665709e-17 4.68525227e-01 9.09247664e-01]\n\n\n\n#plt.scatter(np.arange(len(eigvec[:, 1])), np.sign(eigvec[:, 1]))\nplt.scatter(np.arange(len(eigvec[:, 1])), eigvec[:, 1])\n\nplt.show()\nprint(\"Sum of elements in Fiedler vector = {}\".format(np.sum(eigvec[:, 1].real)))\n\n\n\n\n\n\n\n\nSum of elements in Fiedler vector = -8.493206138382448e-15\n\n\n\nnx.draw_networkx(kn, node_color=np.sign(eigvec[:, 1]))\n\n\n\n\n\n\n\n\n\nРезюме по демонстрации\n\nЗдесь мы вызываем функцию SciPy sparse для нахождения фиксированного числа собственных значений (и собственных векторов), которые являются наименьшими (возможны и другие варианты)\nДетали используемого метода мы обсудим в ближайшее время\nВектор Фидлера дает простое разделение графа\nЧтобы разделить граф на более чем две части, вы должны использовать собственные векторы лапласиана как векторы признаков и запустить какой-нибудь алгоритм кластеризации, например, k-средних"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#от-вектора-фидлера-к-разделителю-1",
    "href": "lectures/lecture-10/lecture-10.html#от-вектора-фидлера-к-разделителю-1",
    "title": "Вопросы?",
    "section": "От вектора Фидлера к разделителю",
    "text": "От вектора Фидлера к разделителю\n\nЭлементы собственного вектора v, соответствующего второму наименьшему собственному значению лапласиана, указывают на разбиение вершин\nЕсли мы выберем некоторое малое положительное \\tau &gt;0, то мы можем разделить вершины на три группы\n\nv_i &lt; -\\tau\nv_i \\in [-\\tau, \\tau]\nv_i &gt; \\tau\n\nПосле этого разделитель состоит из вершин, соответствующих элементам v таким, что v_i \\in [-\\tau, \\tau]\nРазмер разделителя можно регулировать величиной \\tau\nРаспределение элементов в v важно для определения размера разделителя\n\n\nВектор Фидлера и алгебраическая связность графа\nОпределение. Алгебраическая связность графа - это второе наименьшее собственное значение матрицы Лапласиана.\nУтверждение. Алгебраическая связность графа больше 0 тогда и только тогда, когда граф является связным."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#практические-проблемы",
    "href": "lectures/lecture-10/lecture-10.html#практические-проблемы",
    "title": "Вопросы?",
    "section": "Практические проблемы",
    "text": "Практические проблемы\nРекурсивное вычисление бисекции является дорогостоящим.\nВ качестве альтернативы обычно вычисляют многоуровневую бисекцию, которая состоит из 3 фаз.\n\nОгрубление графа: Из данного графа мы объединяем вершины в более крупные узлы и получаем последовательности графов G_1, \\ldots, G_m.\nНа грубом уровне мы выполняем высококачественную бисекцию\nЗатем мы выполняем разукрупнение: мы распространяем разделение от G_k к G_{k-1} и улучшаем качество разделения с помощью алгоритмов локальной оптимизации (уточнение)."
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#практические-проблемы-2",
    "href": "lectures/lecture-10/lecture-10.html#практические-проблемы-2",
    "title": "Вопросы?",
    "section": "Практические проблемы (2)",
    "text": "Практические проблемы (2)\n\nПосле вычисления перестановки нам необходимо реализовать исключение, используя эффективные вычислительные ядра.\nЕсли в процессе исключения мы сможем сгруппировать элементы в блоки, мы сможем использовать вычисления BLAS-3.\nЭто делается с помощью суперузловых структур данных:\nЕсли соседние строки имеют одинаковую структуру разреженности, их можно хранить в блоках:\nТакже мы можем использовать такую структуру в эффективных вычислениях!\n\nПодробности в этом обзоре"
  },
  {
    "objectID": "lectures/lecture-10/lecture-10.html#основные-выводы",
    "href": "lectures/lecture-10/lecture-10.html#основные-выводы",
    "title": "Вопросы?",
    "section": "Основные выводы",
    "text": "Основные выводы\n\nРазреженные матрицы и упорядочивание графов\nУпорядочивание важно для заполнения при LU-разложении: подробнее в следующей лекции\nВыбор ведущего элемента по Марковицу и упорядочивание минимальной степени\nМетоды упорядочивания из пакета SciPy sparse\nСепараторы и как они помогают в минимизации заполнения\nИдея вложенного разбиения\nВектор Фидлера и спектральное разбиение на части"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#краткое-содержание-предыдущих-лекцийсеминаров",
    "href": "lectures/lecture-2/lecture-2.html#краткое-содержание-предыдущих-лекцийсеминаров",
    "title": "Visualization",
    "section": "Краткое содержание предыдущих лекций/семинаров",
    "text": "Краткое содержание предыдущих лекций/семинаров\n\nАрифметика с плавающей точкой и связанные с ней проблемы\nУстойчивые алгоритмы: обратная и прямая устойчивость\nВажнейшие матричные нормы: спектральная и норма Фробениуса\nУнитарные матрицы сохраняют эти нормы\nСуществуют два “базовых” класса унитарных матриц: матрицы Хаусхолдера и Гивенса"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#примеры-пиковой-производительности",
    "href": "lectures/lecture-2/lecture-2.html#примеры-пиковой-производительности",
    "title": "Visualization",
    "section": "Примеры пиковой производительности",
    "text": "Примеры пиковой производительности\nFlops –– операции с плавающей точкой в секунду.\nGiga = 2^{30} \\approx 10^9,\nTera = 2^{40} \\approx 10^{12},\nPeta = 2^{50} \\approx 10^{15},\nExa = 2^{60} \\approx 10^{18}\nКакова пиковая производительность:\n\nСовременного CPU\nСовременного GPU\nСамого мощного суперкомпьютера в мире?\n\n\nТактовая частота процессора и производительность в флопс\nFLOPS = сокеты * (ядра на сокет) * (количество тактов в секунду) * (количество операций с плавающей точкой за такт).\n\nОбычно количество сокетов = 1\nКоличество ядер обычно 2 или 4\n\nКоличество тактов в секунду - это знакомая тактовая частота\nКоличество операций с плавающей точкой за такт зависит от конкретного процессора\n\n\nСовременный CPU (Intel Core i7) –– 400 Гфлопс\nСовременный GPU Nvidia DGX H100 – зависит от точности!\nСамый мощный суперкомпьютер в мире –– 1.102 Экзафлопс/с –– пиковая производительность"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#умножение-матрицы-на-вектор-matvec",
    "href": "lectures/lecture-2/lecture-2.html#умножение-матрицы-на-вектор-matvec",
    "title": "Visualization",
    "section": "Умножение матрицы на вектор (matvec)",
    "text": "Умножение матрицы на вектор (matvec)\nУмножение матрицы A размера n\\times n на вектор x размера n\\times 1 (y=Ax):\n\ny_{i} = \\sum_{j=1}^n a_{ij} x_j\n\nтребует n^2 умножений и n(n-1) сложений. Таким образом, общая сложность составляет 2n^2 - n =  \\mathcal{O}(n^2)"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#насколько-плохо-mathcalon2",
    "href": "lectures/lecture-2/lecture-2.html#насколько-плохо-mathcalon2",
    "title": "Visualization",
    "section": "Насколько плохо \\mathcal{O}(n^2)?",
    "text": "Насколько плохо \\mathcal{O}(n^2)?\n\nПусть A - матрица попарного гравитационного взаимодействия между планетами в галактике.\nЧисло планет в средней галактике составляет 10^{11}, поэтому размер этой матрицы 10^{11} \\times 10^{11}.\nЧтобы моделировать эволюцию во времени, мы должны умножать эту матрицу на вектор на каждом временном шаге.\nСамые мощные суперкомпьютеры выполняют около 10^{16} операций с плавающей точкой в секунду (флопс), поэтому время, необходимое для умножения матрицы A на вектор, составляет примерно\n\n\\begin{align*}\n\\frac{(10^{11})^2 \\text{ операций}}{10^{16} \\text{ флопс}} = 10^6 \\text{ сек} \\approx 11.5 \\text{ дней}\n\\end{align*}\nдля одного временного шага. Если бы мы могли умножать со сложностью \\mathcal{O}(n), мы бы получили\n\\begin{align*}\n\\frac{10^{11} \\text{ операций}}{10^{16} \\text{ флопс}} = 10^{-5} \\text{ сек}.\n\\end{align*}\nВот видео на YouTube, которое иллюстрирует столкновение двух галактик, смоделированное алгоритмом со сложностью \\mathcal{O}(n \\log n):\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"7HF5Oy8IMoM\")"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#можем-ли-мы-побить-mathcalon2",
    "href": "lectures/lecture-2/lecture-2.html#можем-ли-мы-побить-mathcalon2",
    "title": "Visualization",
    "section": "Можем ли мы побить \\mathcal{O}(n^2)?",
    "text": "Можем ли мы побить \\mathcal{O}(n^2)?\n\nВ общем случае НЕТ.\nДело в том, что у нас есть входные данные размера \\mathcal{O}(n^2), поэтому нет способа быть быстрее для произвольной матрицы.\nК счастью, мы можем быть быстрее для определенных типов матриц. Вот несколько примеров:\n\nПростейший пример - матрица, состоящая из всех единиц, которую можно легко умножить, используя только n-1 сложений. Эта матрица имеет ранг один. В более общем случае мы можем быстро умножать матрицы малого ранга (или матрицы, имеющие блоки малого ранга)\nРазреженные матрицы (содержат \\mathcal{O}(n) ненулевых элементов)\nСтруктурированные матрицы:\n\nФурье\nЦиркулянтные\nТеплицевы\nГанкелевы"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#произведение-матриц",
    "href": "lectures/lecture-2/lecture-2.html#произведение-матриц",
    "title": "Visualization",
    "section": "Произведение матриц",
    "text": "Произведение матриц\nРассмотрим композицию двух линейных операторов:\n\ny = Bx\nz = Ay\n\nТогда z = Ay = A B x = C x, где C - это произведение матриц."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#произведение-матрицы-на-матрицу-мм-классика",
    "href": "lectures/lecture-2/lecture-2.html#произведение-матрицы-на-матрицу-мм-классика",
    "title": "Visualization",
    "section": "Произведение матрицы на матрицу (ММ): классика",
    "text": "Произведение матрицы на матрицу (ММ): классика\nОпределение. Произведение матрицы A размера n \\times k и матрицы B размера k \\times m - это матрица C размера n \\times m с элементами \n   c_{ij} = \\sum_{s=1}^k a_{is} b_{sj}, \\quad i = 1, \\ldots, n, \\quad j = 1, \\ldots, m\n\nДля m=k=n сложность наивного алгоритма составляет 2n^3 - n^2 = \\mathcal{O}(n^3)."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#обсуждение-матричного-умножения",
    "href": "lectures/lecture-2/lecture-2.html#обсуждение-матричного-умножения",
    "title": "Visualization",
    "section": "Обсуждение матричного умножения",
    "text": "Обсуждение матричного умножения\n\nПроизведение матриц является основой почти всех эффективных алгоритмов в численной линейной алгебре.\nПо сути, все алгоритмы плотной численной линейной алгебры сводятся к последовательности матричных произведений.\nЭффективная реализация матричного умножения снижает сложность численных алгоритмов на тот же коэффициент.\nОднако реализация матричного умножения совсем не проста!"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#эффективная-реализация-матричного-умножения",
    "href": "lectures/lecture-2/lecture-2.html#эффективная-реализация-матричного-умножения",
    "title": "Visualization",
    "section": "Эффективная реализация матричного умножения",
    "text": "Эффективная реализация матричного умножения\nВ1: Легко ли умножать матрицу на матрицу наиболее эффективным способом?"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#ответ-нет-это-непросто",
    "href": "lectures/lecture-2/lecture-2.html#ответ-нет-это-непросто",
    "title": "Visualization",
    "section": "Ответ: нет, это непросто",
    "text": "Ответ: нет, это непросто\nЕсли вы хотите сделать это максимально быстро, используя имеющиеся компьютеры."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#демонстрация",
    "href": "lectures/lecture-2/lecture-2.html#демонстрация",
    "title": "Visualization",
    "section": "Демонстрация",
    "text": "Демонстрация\nДавайте сделаем короткую демонстрацию и сравним процедуру np.dot(), которая в моем случае использует MKL, с написанной вручную подпрограммой умножения матриц на Python, а также с ее версией на numba.\n\nimport numpy as np\ndef matmul(a, b):\n    n = a.shape[0] # размер первой размерности матрицы a\n    k = a.shape[1] # размер второй размерности матрицы a\n    m = b.shape[1] # размер второй размерности матрицы b\n    c = np.zeros((n, m)) # создаем матрицу нулей размера n x m\n    for i in range(n): # проходим по строкам результирующей матрицы\n        for j in range(m): # проходим по столбцам результирующей матрицы\n            for s in range(k): # суммируем произведения элементов\n                c[i, j] += a[i, s] * b[s, j]\n                \n    return c # возвращаем результат умножения матриц\n\n\nimport numpy as np\nfrom numba import jit # Компилятор \"на лету\" для Python, см. http://numba.pydata.org\n\n@jit(nopython=True)\ndef numba_matmul(a, b):\n    n = a.shape[0]\n    k = a.shape[1]\n    m = b.shape[1]\n    c = np.zeros((n, m))\n    for i in range(n):\n        for j in range(m):\n            for s in range(k):\n                c[i, j] += a[i, s] * b[s, j]\n    return c\n\nТеперь мы просто сравним время вычислений.\nПопробуйте угадать ответ.\n\nn = 100\na = np.random.randn(n, n)\nb = np.random.randn(n, n)\n\n%timeit matmul(a, b)\n%timeit numba_matmul(a, b)\n%timeit a @ b\n\n265 ms ± 854 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n572 μs ± 997 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n10.5 μs ± 69.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nIs this answer correct for any dimensions of matrices?\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndim_range = [10*i for i in range(1, 11)]\ntime_range_matmul = []\ntime_range_numba_matmul = []\ntime_range_np = []\nfor n in dim_range:\n    print(\"Dimension = {}\".format(n))\n    a = np.random.randn(n, n)\n    b = np.random.randn(n, n)\n\n    t = %timeit -o -q matmul(a, b)\n    time_range_matmul.append(t.best)\n    t = %timeit -o -q numba_matmul(a, b)\n    time_range_numba_matmul.append(t.best)\n    t = %timeit -o -q np.dot(a, b)\n    time_range_np.append(t.best)\n\nDimension = 10\nDimension = 20\nDimension = 30\nDimension = 40\nDimension = 50\nDimension = 60\nDimension = 70\nDimension = 80\nDimension = 90\nDimension = 100\n\n\n\nplt.plot(dim_range, time_range_matmul, label=\"Matmul\")\nplt.plot(dim_range, time_range_numba_matmul, label=\"Matmul Numba\")\nplt.plot(dim_range, time_range_np, label=\"Numpy\")\nplt.legend(fontsize=18)\nplt.xlabel(\"Dimension\", fontsize=18)\nplt.ylabel(\"Time\", fontsize=18)\nplt.yscale(\"log\")"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#почему-наивная-реализация-медленная",
    "href": "lectures/lecture-2/lecture-2.html#почему-наивная-реализация-медленная",
    "title": "Visualization",
    "section": "Почему наивная реализация медленная?",
    "text": "Почему наивная реализация медленная?\nОна медленная из-за двух проблем:\n\nОна не использует преимущества быстрой памяти (кэша) и архитектуры памяти в целом\nОна не использует доступные возможности параллелизации (особенно важно для GPU)"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#архитектура-памяти",
    "href": "lectures/lecture-2/lecture-2.html#архитектура-памяти",
    "title": "Visualization",
    "section": "Архитектура памяти",
    "text": "Архитектура памяти\n\n\nБыстрая память маленькая\nБольшая память медленная"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#типичные-характеристики-иерархии-памяти",
    "href": "lectures/lecture-2/lecture-2.html#типичные-характеристики-иерархии-памяти",
    "title": "Visualization",
    "section": "Типичные характеристики иерархии памяти",
    "text": "Типичные характеристики иерархии памяти\n\n\n\n\n\n\n\n\n\nТип памяти\nРазмер\nВремя доступа\nПримечания\n\n\n\n\nРегистры CPU\nНесколько КБ\n&lt;1 нс\nСамые быстрые, прямой доступ CPU\n\n\nКэш L1\n32-64 КБ\n1-4 нс\nРазделен на кэш инструкций и данных\n\n\nКэш L2\n256 КБ - 1 МБ\n4-10 нс\nЕдиный кэш\n\n\nКэш L3\n2-32 МБ\n10-20 нс\nОбщий между ядрами CPU\n\n\nОперативная память (RAM)\n8-32 ГБ\n100 нс\nОсновная системная память\n\n\nSSD\n256 ГБ - 2 ТБ\n10-100 мкс\nБыстрое вторичное хранилище\n\n\nЖесткий диск\n1-10 ТБ\n5-10 мс\nСамый медленный, но самый большой\n\n\n\nКлючевые наблюдения: - Время доступа увеличивается примерно в 10 раз на каждом уровне - Размер увеличивается в 10-100 раз на каждом уровне - Эффективное использование более быстрых уровней памяти критично для производительности"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#кэш-линии-и-когерентность-кэша",
    "href": "lectures/lecture-2/lecture-2.html#кэш-линии-и-когерентность-кэша",
    "title": "Visualization",
    "section": "Кэш-линии и когерентность кэша",
    "text": "Кэш-линии и когерентность кэша\n\nКэш-память организована в кэш-линии - блоки фиксированного размера (обычно 64 байта)\nКогда CPU нужны данные, он загружает всю кэш-линию, содержащую эти данные\nЭто эффективно при последовательном доступе к памяти (пространственная локальность)\n\nКогерентность кэша обеспечивает: - Все ядра CPU видят согласованное представление памяти - Когда одно ядро изменяет данные, другие ядра получают уведомление - Предотвращает состояния гонки и несогласованность данных\nПочему это важно для матричных операций: - Последовательный доступ к строкам/столбцам матрицы влияет на использование кэш-линий - Плохое использование кэш-линий = больше кэш-промахов = более низкая производительность\n- Многопоточный код требует когерентных кэшей для корректности"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#архитектура-памяти-gpu",
    "href": "lectures/lecture-2/lecture-2.html#архитектура-памяти-gpu",
    "title": "Visualization",
    "section": "Архитектура памяти GPU",
    "text": "Архитектура памяти GPU\nСовременные GPU имеют иную иерархию памяти по сравнению с CPU:\n\n\n\n\n\n\n\n\n\nТип памяти\nРазмер\nВремя доступа\nПримечания\n\n\n\n\nРегистры\n~4 МБ на SM\n~1 такт\nСамые быстрые, на поток/блок\n\n\nРазделяемая память/Кэш L1\n64-256 КБ на SM\n~20-30 тактов\nОбщая в пределах блока потоков\n\n\nКэш L2\n512КБ - 60МБ\n~200 тактов\nОбщий для всего GPU\n\n\nГлобальная память (VRAM)\n16-80 ГБ\n~400-600 тактов\nОсновная память GPU\n\n\nСистемная память (RAM)\n8-128 ГБ\n&gt;1000 тактов\nПамять CPU, доступ через PCIe\n\n\n\nКлючевые отличия от CPU: - Гораздо более параллельный доступ (тысячи потоков) - Больший файл регистров, но меньшие кэши - Более высокая пропускная способность памяти, но выше латентность - Критически важен объединенный доступ к памяти\nШаблоны доступа к памяти: - Объединенный: потоки в варпе обращаются к последовательной памяти = быстро - С шагом/случайный: потоки обращаются к разрозненной памяти = медленно - Конфликты банков разделяемой памяти могут ограничивать пропускную способность\nЛучшие практики: - Использовать разделяемую память для часто используемых данных - Обеспечивать объединенный доступ к глобальной памяти - Минимизировать передачу данных между CPU и GPU\nПримечание: Последний GPU NVIDIA H100 может иметь до 80ГБ памяти HBM3 VRAM"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#как-сделать-алгоритмы-более-вычислительно-интенсивными",
    "href": "lectures/lecture-2/lecture-2.html#как-сделать-алгоритмы-более-вычислительно-интенсивными",
    "title": "Visualization",
    "section": "Как сделать алгоритмы более вычислительно интенсивными",
    "text": "Как сделать алгоритмы более вычислительно интенсивными\nРеализация в NLA: использовать блочные версии алгоритмов. \nЭтот подход является основой BLAS (Basic Linear Algebra Subroutines), написанной много лет назад на Фортране и до сих пор управляющей вычислительным миром."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#разделим-матрицу-на-блоки-для-иллюстрации-рассмотрим-разделение-на-блочную-матрицу-2-times-2",
    "href": "lectures/lecture-2/lecture-2.html#разделим-матрицу-на-блоки-для-иллюстрации-рассмотрим-разделение-на-блочную-матрицу-2-times-2",
    "title": "Visualization",
    "section": "Разделим матрицу на блоки! Для иллюстрации рассмотрим разделение на блочную матрицу 2 \\times 2:",
    "text": "Разделим матрицу на блоки! Для иллюстрации рассмотрим разделение на блочную матрицу 2 \\times 2:\n\n   A = \\begin{bmatrix}\n         A_{11} & A_{12} \\\\\n         A_{21} & A_{22}\n        \\end{bmatrix}, \\quad B = \\begin{bmatrix}\n         B_{11} & B_{12} \\\\\n         B_{21} & B_{22}\n        \\end{bmatrix}\nТогда,\nAB = \\begin{bmatrix}A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\\\\n            A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\end{bmatrix}.\nЕсли A_{11}, B_{11} и их произведение помещаются в кэш-память (которая составляет 20 Мб (L3) для современного процессора Intel), тогда мы загружаем их в память только один раз."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#blas",
    "href": "lectures/lecture-2/lecture-2.html#blas",
    "title": "Visualization",
    "section": "BLAS",
    "text": "BLAS\nBLAS имеет три уровня: 1. BLAS-1, операции типа c = a + b 2. BLAS-2, операции типа умножения матрицы на вектор 3. BLAS-3, умножение матрицы на матрицу\nВ чем принципиальные различия между ними?\nОсновное различие заключается в соотношении количества операций и входных данных!\n\nBLAS-1: \\mathcal{O}(n) данных, \\mathcal{O}(n) операций\nBLAS-2: \\mathcal{O}(n^2) данных, \\mathcal{O}(n^2) операций\n\nBLAS-3: \\mathcal{O}(n^2) данных, \\mathcal{O}(n^3) операций"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#почему-blas-так-важна-и-актуальна",
    "href": "lectures/lecture-2/lecture-2.html#почему-blas-так-важна-и-актуальна",
    "title": "Visualization",
    "section": "Почему BLAS так важна и актуальна?",
    "text": "Почему BLAS так важна и актуальна?\n\nСовременная реализация базовых операций линейной алгебры\nПредоставляет стандартные имена для операций в любых новых реализациях (например, ATLAS, OpenBLAS, MKL). Вы можете вызвать функцию умножения матрицы на матрицу (GEMM), связать свой код с любой реализацией BLAS, и он будет работать корректно\nФормулировка новых алгоритмов в терминах операций BLAS\nСуществуют обертки для большинства популярных языков"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#пакеты-связанные-с-blas",
    "href": "lectures/lecture-2/lecture-2.html#пакеты-связанные-с-blas",
    "title": "Visualization",
    "section": "Пакеты, связанные с BLAS",
    "text": "Пакеты, связанные с BLAS\n\nATLAS - Автоматически настраиваемое программное обеспечение для линейной алгебры. Оно автоматически адаптируется под конкретную архитектуру системы.\nLAPACK - Пакет линейной алгебры. Предоставляет операции линейной алгебры высокого уровня (например, факторизации матриц), основанные на вызовах подпрограмм BLAS.\nIntel MKL - Математическая библиотека ядра. Предоставляет реализацию BLAS и LAPACK, оптимизированную для процессоров Intel. Доступна в дистрибутиве Anaconda Python:"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#более-быстрые-алгоритмы-умножения-матриц",
    "href": "lectures/lecture-2/lecture-2.html#более-быстрые-алгоритмы-умножения-матриц",
    "title": "Visualization",
    "section": "Более быстрые алгоритмы умножения матриц",
    "text": "Более быстрые алгоритмы умножения матриц\nНапомним, что умножение матрицы на матрицу требует \\mathcal{O}(n^3) операций. Однако хранение требует \\mathcal{O}(n^2).\nВопрос: возможно ли уменьшить количество операций до \\mathcal{O}(n^2)?\nОтвет: поиск алгоритма умножения матриц со сложностью \\mathcal{O}(n^2) все еще не завершен.\n\nАлгоритм Штрассена дает \\mathcal{O}(n^{2.807\\dots}) –– иногда используется на практике\nТекущий мировой рекорд \\mathcal{O}(n^{2.37\\dots}) –– большая константа, непрактичен, основан на алгоритме Копперсмита-Винограда.\nОн улучшил предыдущий рекорд (Уильямс 2012) на 3\\cdot 10^{-7}\nВ статьях до сих пор изучается умножение матриц 3 \\times 3 и интерпретируется с разных сторон (Heule, et. al. 2019)\n\nРассмотрим алгоритм Штрассена подробнее."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#наивное-умножение",
    "href": "lectures/lecture-2/lecture-2.html#наивное-умножение",
    "title": "Visualization",
    "section": "Наивное умножение",
    "text": "Наивное умножение\nПусть A и B - две матрицы размера 2\\times 2. Наивное умножение C = AB\n\n\\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22}  \\end{bmatrix}  =\n\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}  \\end{bmatrix}\n\\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22}  \\end{bmatrix} =\n\\begin{bmatrix}\na_{11}b_{11} + a_{12}b_{21} & a_{11}b_{21} + a_{12}b_{22} \\\\\na_{21}b_{11} + a_{22}b_{21} & a_{21}b_{21} + a_{22}b_{22}\n\\end{bmatrix}\n\nсодержит 8 умножений и 4 сложения."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#алгоритм-штрассена",
    "href": "lectures/lecture-2/lecture-2.html#алгоритм-штрассена",
    "title": "Visualization",
    "section": "Алгоритм Штрассена",
    "text": "Алгоритм Штрассена\nВ работе Gaussian elimination is not optimal (1969) Штрассен обнаружил, что можно вычислить C используя 18 сложений и только 7 умножений: \n\\begin{split}\nc_{11} &= f_1 + f_4 - f_5 + f_7, \\\\\nc_{12} &= f_3 + f_5, \\\\\nc_{21} &= f_2 + f_4, \\\\\nc_{22} &= f_1 - f_2 + f_3 + f_6,\n\\end{split}\n где \n\\begin{split}\nf_1 &= (a_{11} + a_{22}) (b_{11} + b_{22}), \\\\\nf_2 &= (a_{21} + a_{22}) b_{11}, \\\\\nf_3 &= a_{11} (b_{12} - b_{22}), \\\\\nf_4 &= a_{22} (b_{21} - b_{11}), \\\\\nf_5 &= (a_{11} + a_{12}) b_{22}, \\\\\nf_6 &= (a_{21} - a_{11}) (b_{11} + b_{12}), \\\\\nf_7 &= (a_{12} - a_{22}) (b_{21} + b_{22}).\n\\end{split}\n\nК счастью, эти формулы работают даже если a_{ij} и b_{ij}, i,j=1,2 являются блочными матрицами.\nТаким образом, алгоритм Штрассена выглядит следующим образом. - Сначала мы разделяем матрицы A и B размера n\\times n, n=2^d на 4 блока размера \\frac{n}{2}\\times \\frac{n}{2} - Затем мы вычисляем умножения в описанных формулах рекурсивно\nЭто снова приводит нас к идее разделяй и властвуй."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#пример-алгоритма-штрассена",
    "href": "lectures/lecture-2/lecture-2.html#пример-алгоритма-штрассена",
    "title": "Visualization",
    "section": "Пример алгоритма Штрассена",
    "text": "Пример алгоритма Штрассена\nДавайте перемножим две матрицы 2x2, используя метод Штрассена:\nA = \\begin{bmatrix} 2 & 3 \\\\ 4 & 1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 7 \\\\ 6 & 8 \\end{bmatrix}\nВычислим 7 произведений от f_1 до f_7:\n\\begin{align*}\nf_1 &= (2 + 1)(5 + 8) = 3 \\cdot 13 = 39 \\\\\nf_2 &= (4 + 1)(5) = 5 \\cdot 5 = 25 \\\\\nf_3 &= (2)(7 - 8) = 2 \\cdot (-1) = -2 \\\\\nf_4 &= (1)(6 - 5) = 1 \\cdot 1 = 1 \\\\\nf_5 &= (2 + 3)(8) = 5 \\cdot 8 = 40 \\\\\nf_6 &= (4 - 2)(5 + 7) = 2 \\cdot 12 = 24 \\\\\nf_7 &= (3 - 1)(6 + 8) = 2 \\cdot 14 = 28\n\\end{align*}\nТеперь вычислим элементы результирующей матрицы C:\n\\begin{align*}\nc_{11} &= f_1 + f_4 - f_5 + f_7 = 39 + 1 - 40 + 28 = 28 \\\\\nc_{12} &= f_3 + f_5 = -2 + 40 = 38 \\\\\nc_{21} &= f_2 + f_4 = 25 + 1 = 26 \\\\\nc_{22} &= f_1 - f_2 + f_3 + f_6 = 39 - 25 - 2 + 24 = 36\n\\end{align*}\nТаким образом:\nC = \\begin{bmatrix} 28 & 38 \\\\ 26 & 36 \\end{bmatrix}\nВы можете проверить, что это равно результату стандартного матричного умножения!"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#сложность-алгоритма-штрассена",
    "href": "lectures/lecture-2/lecture-2.html#сложность-алгоритма-штрассена",
    "title": "Visualization",
    "section": "Сложность алгоритма Штрассена",
    "text": "Сложность алгоритма Штрассена\n\nКоличество умножений\nПодсчет количества умножений - тривиальная задача. Обозначим через M(n) количество умножений, используемых для перемножения 2 матриц размера n\\times n с использованием концепции “разделяй и властвуй”. Тогда для наивного алгоритма количество умножений равно\n M_\\text{naive}(n) = 8 M_\\text{naive}\\left(\\frac{n}{2} \\right) = 8^2 M_\\text{naive}\\left(\\frac{n}{4} \\right)\n= \\dots = 8^{d-1} M(2) = 8^{d} M(1) = 8^{d} = 8^{\\log_2 n} = n^{\\log_2 8} = n^3 \nТаким образом, даже при использовании идеи “разделяй и властвуй” мы не можем быть лучше чем n^3.\nДавайте посчитаем количество умножений для алгоритма Штрассена:\n M_\\text{strassen}(n) = 7 M_\\text{strassen}\\left(\\frac{n}{2} \\right) = 7^2 M_\\text{strassen}\\left(\\frac{n}{4} \\right)\n= \\dots = 7^{d-1} M(1) = 7^{d} = 7^{\\log_2 n} = n^{\\log_2 7} \n\n\nКоличество сложений\nНет смысла оценивать количество сложений A(n) для наивного алгоритма, так как мы уже получили n^3 умножений. Для алгоритма Штрассена имеем:\n A_\\text{strassen}(n) = 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 \nпоскольку на первом уровне нам нужно сложить матрицы размера \\frac{n}{2}\\times \\frac{n}{2} 18 раз, а затем углубиться для каждого из 7 умножений. Таким образом,\n\n \\begin{split}\nA_\\text{strassen}(n) =& 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 = 7 \\left(7 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 18 \\left( \\frac{n}{4} \\right)^2 \\right) + 18 \\left( \\frac{n}{2} \\right)^2 =\n7^2 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 7\\cdot 18 \\left( \\frac{n}{4} \\right)^2 +  18 \\left( \\frac{n}{2} \\right)^2 = \\\\\n=& \\dots = 18 \\sum_{k=1}^d 7^{k-1} \\left( \\frac{n}{2^k} \\right)^2 = \\frac{18}{4} n^2 \\sum_{k=1}^d \\left(\\frac{7}{4} \\right)^{k-1} = \\frac{18}{4} n^2 \\frac{\\left(\\frac{7}{4} \\right)^d - 1}{\\frac{7}{4} - 1} = 6 n^2 \\left( \\left(\\frac{7}{4} \\right)^d - 1\\right) \\leqslant 6 n^2 \\left(\\frac{7}{4} \\right)^d = 6 n^{\\log_2 7}\n\\end{split}\n \n(поскольку 4^d = n^2 и 7^d = n^{\\log_2 7}).\nАсимптотическое поведение A(n) также может быть найдено из основной теоремы.\n\n\nTotal complexity\nTotal complexity is M_\\text{strassen}(n) + A_\\text{strassen}(n)= 7 n^{\\log_2 7}. Strassen algorithm becomes faster when\n\\begin{align*}\n2n^3 &&gt; 7 n^{\\log_2 7}, \\\\\nn &&gt; 667,\n\\end{align*}\nso it is not a good idea to get to the bottom level of recursion."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#alphatensor",
    "href": "lectures/lecture-2/lecture-2.html#alphatensor",
    "title": "Visualization",
    "section": "AlphaTensor",
    "text": "AlphaTensor\nНедавняя статья AlphaTensor показала, как современное глубокое обучение с подкреплением может быть использовано для получения новых разложений тензоров."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#интерпретация-с-точки-зрения-обучения-с-подкреплением",
    "href": "lectures/lecture-2/lecture-2.html#интерпретация-с-точки-зрения-обучения-с-подкреплением",
    "title": "Visualization",
    "section": "Интерпретация с точки зрения обучения с подкреплением",
    "text": "Интерпретация с точки зрения обучения с подкреплением\nВ обучении с подкреплением агент учится выполнять действия на основе состояния и вознаграждения.\nВ данном случае состоянием является тензор.\nДействие - это вычитание тензора ранга один.\nЕсли в конце получается ненулевой результат, вы получаете вознаграждение.\nЗатем вы выполняете миллионы различных действий и закрепляете хорошие результаты."
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#избранные-результаты",
    "href": "lectures/lecture-2/lecture-2.html#избранные-результаты",
    "title": "Visualization",
    "section": "Избранные результаты",
    "text": "Избранные результаты\n\nЛучшие ранги для определенных размеров матриц\nНовые варианты алгоритма Штрассена 4x4, которые работают быстрее на реальном оборудовании (но только для этого конкретного оборудования!)\nУлучшенное произведение антисимметричной матрицы на вектор"
  },
  {
    "objectID": "lectures/lecture-2/lecture-2.html#итоги-части-про-матричное-умножение",
    "href": "lectures/lecture-2/lecture-2.html#итоги-части-про-матричное-умножение",
    "title": "Visualization",
    "section": "Итоги части про матричное умножение",
    "text": "Итоги части про матричное умножение\n\nМатричное умножение - это основа численной линейной алгебры. Для достижения высокой эффективности необходимо мыслить блоками\nВсе это связано с иерархией компьютерной памяти\nКонцепция блочных алгоритмов"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#предыдущая-лекция",
    "href": "lectures/lecture-4/lecture-4.html#предыдущая-лекция",
    "title": "Итоги",
    "section": "Предыдущая лекция",
    "text": "Предыдущая лекция\n\nРанг матрицы\nСкелетное разложение\nНизкоранговая аппроксимация\nСингулярное разложение (SVD)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#сегодняшняя-лекция",
    "href": "lectures/lecture-4/lecture-4.html#сегодняшняя-лекция",
    "title": "Итоги",
    "section": "Сегодняшняя лекция",
    "text": "Сегодняшняя лекция\n\nЛинейные системы\nОбратная матрица\nЧисло обусловленности\nМетод Гаусса"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#линейные-системы",
    "title": "Итоги",
    "section": "Линейные системы",
    "text": "Линейные системы\n\nЛинейные системы уравнений являются базовым инструментом в численной линейной алгебре.\nОни встречаются в:\n\nЗадачах линейной регрессии\nДискретизации дифференциальных/интегральных уравнений в частных производных\nЛинеаризации задач нелинейной регрессии\nОптимизации (например, методы Гаусса-Ньютона и Ньютона-Рафсона, условия Каруша-Куна-Такера)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-уравнения-и-матрицы",
    "href": "lectures/lecture-4/lecture-4.html#линейные-уравнения-и-матрицы",
    "title": "Итоги",
    "section": "Линейные уравнения и матрицы",
    "text": "Линейные уравнения и матрицы\n\nИз школы мы знаем о линейных уравнениях.\nСистема линейных уравнений может быть записана в форме\n\n\\begin{align*}\n    &2 x + 3 y = 5\\quad &\\longrightarrow \\quad &2x + 3 y + 0 z = 5\\\\\n    &2 x + 3z = 5\\quad &\\longrightarrow\\quad &2 x + 0 y + 3 z = 5\\\\\n    &x + y = 2\\quad &\\longrightarrow\\quad  & 1 x + 1 y + 0 z = 2\\\\\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#матричная-форма",
    "href": "lectures/lecture-4/lecture-4.html#матричная-форма",
    "title": "Итоги",
    "section": "Матричная форма",
    "text": "Матричная форма\n\n\\begin{pmatrix}\n2 & 3 & 0 \\\\\n2 & 0 & 3 \\\\\n1 & 1 & 0 \\\\\n\\end{pmatrix}\\begin{pmatrix}\nx \\\\\ny \\\\\nz\n\\end{pmatrix} =\n\\begin{pmatrix}\n5 \\\\\n5 \\\\\n2\n\\end{pmatrix}\n\nили просто\n A u = f,  \nгде A - матрица размера 3 \\times 3, а f - правая часть"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#переопределенные-и-недоопределенные-системы-линейных-уравнений",
    "href": "lectures/lecture-4/lecture-4.html#переопределенные-и-недоопределенные-системы-линейных-уравнений",
    "title": "Итоги",
    "section": "Переопределенные и недоопределенные системы линейных уравнений",
    "text": "Переопределенные и недоопределенные системы линейных уравнений\nЕсли система Au = f имеет:\n\nбольше уравнений, чем неизвестных, она называется переопределенной системой (как правило, не имеет решения)\nменьше уравнений, чем неизвестных, она называется недоопределенной системой (решение не единственно, для получения единственного решения необходимы дополнительные предположения)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#существование-решений",
    "href": "lectures/lecture-4/lecture-4.html#существование-решений",
    "title": "Итоги",
    "section": "Существование решений",
    "text": "Существование решений\nРешение системы линейных уравнений с квадратной матрицей A\nA u = f\nсуществует тогда и только тогда, когда * \\det A \\ne 0\nили\n\nматрица A имеет полный ранг."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#масштабы-линейных-систем",
    "href": "lectures/lecture-4/lecture-4.html#масштабы-линейных-систем",
    "title": "Итоги",
    "section": "Масштабы линейных систем",
    "text": "Масштабы линейных систем\nВ разных приложениях типичный размер линейных систем может быть разным.\n\nМалые: n \\leq 10^4 (полная матрица может храниться в памяти, плотная матрица)\nСредние: n = 10^4 - 10^6 (обычно разреженная или структурированная матрица)\nБольшие: n = 10^8 - 10^9 (обычно разреженная матрица + параллельные вычисления)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-большими",
    "href": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-большими",
    "title": "Итоги",
    "section": "Линейные системы могут быть большими",
    "text": "Линейные системы могут быть большими\n\nМы берем непрерывную задачу, дискретизируем ее на сетке с N элементами и получаем линейную систему с матрицей N\\times N.\nПример сетки вокруг самолета A319 (взято с сайта GMSH). \n\nОсновная сложность в том, что эти системы большие: миллионы или миллиарды неизвестных!"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-структурированными",
    "href": "lectures/lecture-4/lecture-4.html#линейные-системы-могут-быть-структурированными",
    "title": "Итоги",
    "section": "Линейные системы могут быть структурированными",
    "text": "Линейные системы могут быть структурированными\n\nХранение N^2 элементов матрицы непозволительно даже для N = 100000.\n\nВ: как работать с такими матрицами?\nО: к счастью, эти матрицы структурированные и требуют хранения только \\mathcal{O}(N) параметров.\n\nНаиболее распространенной структурой являются разреженные матрицы: такие матрицы имеют только \\mathcal{O}(N) ненулевых элементов!\nПример (одна из известных матриц для n = 5):\n\n\n  \\begin{pmatrix}\n  2 & -1 & 0 & 0 & 0 \\\\\n  -1 & 2 & -1 & 0 & 0 \\\\\n  0 & -1 & 2 & -1 & 0 \\\\\n  0 & 0 &-1& 2 & -1  \\\\\n  0 & 0 & 0 & -1 & 2 \\\\\n  \\end{pmatrix}\n\n\nПо крайней мере такие матрицы можно хранить\nТакже можно быстро умножать такую матрицу на вектор\nНо как решать линейные системы?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#основные-вопросы-о-линейных-системах",
    "href": "lectures/lecture-4/lecture-4.html#основные-вопросы-о-линейных-системах",
    "title": "Итоги",
    "section": "Основные вопросы о линейных системах",
    "text": "Основные вопросы о линейных системах\n\nКакую точность мы получаем в решении (из-за ошибок округления)?\nКак мы вычисляем решение? (LU-разложение, метод Гаусса)\nКакова сложность решения линейных систем?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы",
    "title": "Итоги",
    "section": "Как решать линейные системы?",
    "text": "Как решать линейные системы?\nВажно: забудьте про определители и правило Крамера (оно хорошо работает только для матриц 2 \\times 2)!"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы-1",
    "href": "lectures/lecture-4/lecture-4.html#как-решать-линейные-системы-1",
    "title": "Итоги",
    "section": "Как решать линейные системы?",
    "text": "Как решать линейные системы?\nОсновной инструмент - исключение переменных.\n\\begin{align*}\n    &2 y + 3 x = 5 \\quad&\\longrightarrow \\quad &y = 5/2 -  3/2 x \\\\\n    &2 x + 3z = 5 \\quad&\\longrightarrow\\quad &z = 5/3 - 2/3 x\\\\\n    &z + y = 2 \\quad&\\longrightarrow\\quad  & 5/2 + 5/3 - (3/2 + 2/3) x = 2,\\\\\n\\end{align*}\nи так находится x (и все предыдущие).\nЭтот процесс называется методом Гаусса и является одним из наиболее широко используемых алгоритмов."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#метод-гаусса",
    "href": "lectures/lecture-4/lecture-4.html#метод-гаусса",
    "title": "Итоги",
    "section": "Метод Гаусса",
    "text": "Метод Гаусса\nМетод Гаусса состоит из двух шагов: 1. Прямой ход 2. Обратный ход"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#прямой-проход",
    "href": "lectures/lecture-4/lecture-4.html#прямой-проход",
    "title": "Итоги",
    "section": "Прямой проход",
    "text": "Прямой проход\n\nНа прямом проходе мы исключаем x_1:\n\n\n   x_1 = f_1 - (a_{12} x_2 + \\ldots + a_{1n} x_n)/a_{11},\n\nи затем подставляем это в уравнения 2, \\ldots, n.\n\nЗатем мы исключаем x_2 и так далее из второго уравнения.\nВажно, что ведущие элементы (на которые мы делим) не равны 0."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#обратный-проход",
    "href": "lectures/lecture-4/lecture-4.html#обратный-проход",
    "title": "Итоги",
    "section": "Обратный проход",
    "text": "Обратный проход\nНа обратном проходе: - решаем уравнение для x_n - подставляем его в уравнение для x_{n-1} и так далее, пока не вычислим все x_i, i=1,\\ldots, n."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#метод-гаусса-и-lu-разложение",
    "href": "lectures/lecture-4/lecture-4.html#метод-гаусса-и-lu-разложение",
    "title": "Итоги",
    "section": "Метод Гаусса и LU-разложение",
    "text": "Метод Гаусса и LU-разложение\n\nМетод Гаусса - это вычисление одного из важнейших матричных разложений: LU-разложения.\n\nОпределение: LU-разложением квадратной матрицы A называется представление\nA = LU,\nгде - L - нижнетреугольная матрица (элементы строго над диагональю равны нулю) - U - верхнетреугольная матрица (элементы строго под диагональю равны нулю)\nЭто разложение не единственно, поэтому обычно требуют, чтобы матрица L имела единицы на диагонали.\nОсновная цель LU-разложения - это решение линейной системы, потому что\n A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, \nи это сводится к решению двух линейных систем прямой проход\n L y = f, \nи обратный проход\n U x = y. \nСуществует ли LU-разложение всегда?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#сложность-метода-гауссаlu-разложения",
    "href": "lectures/lecture-4/lecture-4.html#сложность-метода-гауссаlu-разложения",
    "title": "Итоги",
    "section": "Сложность метода Гаусса/LU-разложения",
    "text": "Сложность метода Гаусса/LU-разложения\n\nКаждый шаг исключения требует \\mathcal{O}(n^2) операций.\nТаким образом, стоимость наивного алгоритма составляет \\mathcal{O}(n^3).\n\nПодумайте: может ли алгоритм Штрассена помочь здесь?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#блочное-lu-разложение",
    "href": "lectures/lecture-4/lecture-4.html#блочное-lu-разложение",
    "title": "Итоги",
    "section": "Блочное LU-разложение",
    "text": "Блочное LU-разложение\nМы можем попробовать вычислить блочную версию LU-разложения:\n\\begin{pmatrix} A_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{pmatrix} = \\begin{pmatrix} L_{11} & 0 \\\\\nL_{21} & L_{22}\n\\end{pmatrix} \\begin{pmatrix} U_{11} & U_{12} \\\\\n0 & U_{22}\n\\end{pmatrix} \n\nЕсть две основные операции: вычисление LU-разложения половинных матриц + произведение матриц."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#существование-lu-разложения",
    "href": "lectures/lecture-4/lecture-4.html#существование-lu-разложения",
    "title": "Итоги",
    "section": "Существование LU-разложения",
    "text": "Существование LU-разложения\n\nАлгоритм LU-разложения не даст сбой, если мы не делим на ноль на каждом шаге метода Гаусса.\n\nВопрос: когда это так, для какого класса матриц?\nОтвет: это верно для строго регулярных матриц."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#строго-регулярные-матрицы-и-lu-разложение",
    "href": "lectures/lecture-4/lecture-4.html#строго-регулярные-матрицы-и-lu-разложение",
    "title": "Итоги",
    "section": "Строго регулярные матрицы и LU-разложение",
    "text": "Строго регулярные матрицы и LU-разложение\n\nОпределение. Матрица A называется строго регулярной, если все её ведущие главные миноры (т.е. подматрицы, состоящие из первых k строк и k столбцов) невырождены.\nВ этом случае всегда существует LU-разложение. Обратное также верно (проверьте!).\n\nСледствие: Если L - унитреугольная матрица (единицы на диагонали), то LU-разложение единственно. \nДоказательство: Действительно, L_1 U_1 = L_2 U_2 означает L_2^{-1} L_1 = U_2 U_1^{-1}. L_2^{-1} L_1 - нижнетреугольная матрица с единицами на диагонали. U_2 U_1^{-1} - верхнетреугольная матрица. Следовательно, L_2^{-1} L_1 = U_2 U_1^{-1} = I и L_1 = L_2, U_1 = U_2."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#lu-разложение-для-эрмитовых-положительно-определенных-матриц-разложение-холецкого",
    "href": "lectures/lecture-4/lecture-4.html#lu-разложение-для-эрмитовых-положительно-определенных-матриц-разложение-холецкого",
    "title": "Итоги",
    "section": "LU-разложение для эрмитовых положительно определенных матриц (разложение Холецкого)",
    "text": "LU-разложение для эрмитовых положительно определенных матриц (разложение Холецкого)\n\nСтрого регулярные матрицы имеют LU-разложение.\nВажным подклассом строго регулярных матриц является класс эрмитовых положительно определенных матриц\n\nОпределение. Матрица A называется  положительно определенной , если для любого x: \\Vert x \\Vert \\ne 0 выполняется\n\n(x, Ax) &gt; 0.\n - если это выполняется для x \\in \\mathbb{C}^n, то матрица A должна быть эрмитовой - если это выполняется для x \\in \\mathbb{R}^n, то матрица A может быть несимметричной\n\nУтверждение: Эрмитова положительно определенная матрица A является строго регулярной и имеет разложение Холецкого вида\n\nA = RR^*,\nгде R - нижнетреугольная матрица.\n\nДавайте попробуем доказать этот факт (на доске).\nИногда это называют “квадратным корнем” из матрицы.\n\nВычисление LU-разложения\n\nВо многих случаях вычисление LU-разложения один раз - хорошая идея!\nПосле того, как разложение найдено (это требует \\mathcal{O}(n^3) операций), решение линейных систем с L и U требует только \\mathcal{O}(n^2) операций.\n\nПроверьте:\n\nРешение линейных систем с треугольными матрицами простое (почему?).\nКак мы вычисляем множители L и U?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#когда-lu-разложение-не-работает",
    "href": "lectures/lecture-4/lecture-4.html#когда-lu-разложение-не-работает",
    "title": "Итоги",
    "section": "Когда LU-разложение не работает",
    "text": "Когда LU-разложение не работает\n\nЧто происходит, если матрица не является строго регулярной (или ведущие элементы в методе Гаусса очень малы?).\nСуществует классический пример матрицы 2 \\times 2 с плохим LU-разложением.\nРассмотрим матрицу\n\n\n    A = \\begin{pmatrix}\n    \\varepsilon & 1 \\\\\n    1 & 1\n    \\end{pmatrix}\n\n\nЕсли \\varepsilon достаточно мал, мы можем потерпеть неудачу. В отличие от этого, разложение Холецкого всегда стабильно.\n\nДавайте рассмотрим демонстрацию.\n\nimport numpy as np\n\neps = 9.9e-15\na = [[eps, 1],[1.0,  1]]\na = np.array(a)\na0 = a.copy()\nn = a.shape[0]\nL = np.zeros((n, n))\nU = np.zeros((n, n))\nfor k in range(n): #Eliminate one row\n    L[k, k] = 1.0\n    for i in range(k+1, n):\n        L[i, k] = a[i, k]/a[k, k]\n        for j in range(k+1, n):\n            a[i, j] -= L[i, k]*a[k, j]\n    for j in range(k, n):\n        U[k, j] = a[k, j]\nprint('L * U - A:\\n', np.dot(L, U) - a0)\nL\n\nL * U - A:\n [[0. 0.]\n [0. 0.]]\n\n\narray([[1.00000000e+00, 0.00000000e+00],\n       [1.01010101e+14, 1.00000000e+00]])"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#концепция-выбора-ведущего-элемента",
    "href": "lectures/lecture-4/lecture-4.html#концепция-выбора-ведущего-элемента",
    "title": "Итоги",
    "section": "Концепция выбора ведущего элемента",
    "text": "Концепция выбора ведущего элемента\n\nМы можем выполнять выбор ведущего элемента, т.е. переставлять строки и столбцы для максимизации A_{kk}, на который мы делим.\nПростейшая, но эффективная стратегия - это выбор ведущей строки: на каждом шаге выбираем индекс с максимальным по модулю значением и ставим его на диагональ.\nЭто дает нам разложение\n\nA = P L U,\nгде P - это матрица перестановок.\nВопрос: Что делает выбор ведущей строки хорошим?\nОтвет: Он хорош тем, что\n | L_{ij}|&lt;1, \nно элементы U могут вырасти до 2^n! (на практике это встречается очень редко).\n\nМожете ли вы придумать матрицу, где элементы U растут максимально возможным образом?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#устойчивость-линейных-систем",
    "href": "lectures/lecture-4/lecture-4.html#устойчивость-линейных-систем",
    "title": "Итоги",
    "section": "Устойчивость линейных систем",
    "text": "Устойчивость линейных систем\n\nСуществует фундаментальная проблема решения линейных систем, которая не зависит от используемого алгоритма.\nОна возникает, когда элементы матрицы представлены в виде чисел с плавающей точкой или присутствует измерительный шум.\n\nПроиллюстрируем эту проблему на следующем примере.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nn = 40\na = [[1.0/(i + j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\nrhs = np.random.normal(size=(n,)) #Right-hand side\nx = np.linalg.solve(a, rhs) #This function computes LU-factorization and solves linear system\n\n#And check if everything is fine\ner = np.linalg.norm(np.dot(a, x) - rhs) / np.linalg.norm(rhs)\nprint(er)\nplt.plot(x)\nplt.grid(True)\n\n19.87968164147078\n\n\n\n\n\n\n\n\n\n\nКак видите, ошибка растет с увеличением n, и нам нужно выяснить почему.\nВажный момент заключается в том, что это не проблема алгоритма: это проблема представления матрицы в памяти.\nОшибка возникает в момент, когда элементы матрицы вычисляются приближенно."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#вопросы-по-демонстрации",
    "href": "lectures/lecture-4/lecture-4.html#вопросы-по-демонстрации",
    "title": "Итоги",
    "section": "Вопросы по демонстрации",
    "text": "Вопросы по демонстрации\n\nВ чем была проблема в предыдущем примере?\nПочему ошибка растет так быстро?\nИ здесь мы подходим к одному из основных понятий численной линейной алгебры: понятию числа обусловленности матрицы.\n\nНо прежде чем говорить об этом, нам нужно определить обратную матрицу."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#обратная-матрица-определение",
    "href": "lectures/lecture-4/lecture-4.html#обратная-матрица-определение",
    "title": "Итоги",
    "section": "Обратная матрица: определение",
    "text": "Обратная матрица: определение\n\nОбратная матрица к матрице A определяется как матрица X, обозначаемая A^{-1}, такая что\n\n AX = XA = I, \nгде I - единичная матрица (т.е. I_{ij} = 0 при i \\ne j и 1 в противном случае). - Вычисление обратной матрицы связано с решением линейных систем. Действительно, i-й столбец произведения дает\n A x_i = e_i,\nгде e_i - это i-й столбец единичной матрицы. - Таким образом, мы можем применить метод Гаусса для решения этой системы. Более того, если в этом процессе нет деления на ноль (и опорные элементы не зависят от правой части), то систему можно решить."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#обратная-матрица-и-линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#обратная-матрица-и-линейные-системы",
    "title": "Итоги",
    "section": "Обратная матрица и линейные системы",
    "text": "Обратная матрица и линейные системы\nЕсли мы вычислили A^{-1}, то решение линейной системы\nAx = f\nпросто x = A^{-1} f.\nДействительно,\n A(A^{-1} f) = (AA^{-1})f = I f = f."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#ряд-неймана",
    "href": "lectures/lecture-4/lecture-4.html#ряд-неймана",
    "title": "Итоги",
    "section": "Ряд Неймана",
    "text": "Ряд Неймана\n\nЧтобы изучить, почему могут возникать такие большие ошибки в решении (см. пример выше с матрицей Гильберта), нам нужен важный вспомогательный результат.\n\nРяд Неймана:\nЕсли для матрицы F выполняется условие \\Vert F \\Vert &lt; 1, то матрица (I - F) обратима и\n(I - F)^{-1} = I + F + F^2 + F^3 + \\ldots = \\sum_{k=0}^{\\infty} F^k.\nЗаметим, что это матричная версия геометрической прогрессии.\nВопрос: какая норма здесь рассматривается? Какая норма является “наилучшей” в данном случае?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#доказательство",
    "href": "lectures/lecture-4/lecture-4.html#доказательство",
    "title": "Итоги",
    "section": "Доказательство",
    "text": "Доказательство\nДоказательство конструктивно. Сначала докажем, что ряд \\sum_{k=0}^{\\infty} F^k сходится.\nКак и в скалярном случае, имеем\n (I - F) \\sum_{k=0}^N F^k = (I - F^{N+1}) \\rightarrow I, \\quad N \\to +\\infty \nДействительно,\n \\| (I - F^{N+1}) - I\\| = \\|F^{N+1}\\| \\leqslant \\|F\\|^{N+1} \\to 0, \\quad N\\to +\\infty. \nМы также можем оценить норму обратной матрицы:\n \\left\\Vert \\sum_{k=0}^N F^k \\right\\Vert \\leq \\sum_{k=0}^N \\Vert F \\Vert^k \\Vert I \\Vert \\leq \\frac{\\Vert I \\Vert}{1 - \\Vert F \\Vert}"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#малое-возмущение-обратной-матрицы",
    "href": "lectures/lecture-4/lecture-4.html#малое-возмущение-обратной-матрицы",
    "title": "Итоги",
    "section": "Малое возмущение обратной матрицы",
    "text": "Малое возмущение обратной матрицы\n\nИспользуя этот результат, мы можем оценить, как возмущение матрицы влияет на обратную матрицу.\nПредположим, что возмущение E мало в том смысле, что \\Vert A^{-1} E \\Vert &lt; 1.\nТогда\n\n(A + E)^{-1} = \\sum_{k=0}^{\\infty} (-A^{-1} E)^k A^{-1}\nи более того,\n \\frac{\\Vert (A + E)^{-1} - A^{-1} \\Vert}{\\Vert A^{-1} \\Vert} \\leq \\frac{\\Vert A^{-1} \\Vert \\Vert E \\Vert \\Vert I \\Vert}{1 - \\Vert A^{-1} E \\Vert}. \nКак видите, норма обратной матрицы входит в оценку."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#число-обусловленности-линейной-системы",
    "href": "lectures/lecture-4/lecture-4.html#число-обусловленности-линейной-системы",
    "title": "Итоги",
    "section": "Число обусловленности линейной системы",
    "text": "Число обусловленности линейной системы\nРассмотрим возмущенную линейную систему:\n (A + \\Delta A) \\widehat{x} = f + \\Delta f."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#оценки",
    "href": "lectures/lecture-4/lecture-4.html#оценки",
    "title": "Итоги",
    "section": "Оценки",
    "text": "Оценки\n\n\\begin{split}\n\\widehat{x} - x &= (A + \\Delta A)^{-1} (f + \\Delta f) - A^{-1} f =\\\\\n&= \\left((A + \\Delta A)^{-1} - A^{-1}\\right)f + (A + \\Delta A)^{-1} \\Delta f = \\\\\n& = \\Big[ \\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k - I \\Big]A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k \\Big]A^{-1} \\Delta f  \\\\\n&= \\Big[\\sum_{k=1}^{\\infty} (-A^{-1} \\Delta A)^k\\Big] A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k \\Big] A^{-1} \\Delta f,\n\\end{split}\n\nследовательно\n\n\\begin{split}\n\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq\n&\\frac{1}{\\|A^{-1}f\\|} \\Big[ \\frac{\\|A^{-1}\\|\\|\\Delta A\\|}{1 - \\|A^{-1}\\Delta A\\|}\\|A^{-1}f\\| + \\frac{1}{1 - \\|A^{-1} \\Delta A\\|} \\|A^{-1} \\Delta f\\|  \\Big] \\\\\n\\leq & \\frac{\\|A\\|\\|A^{-1}\\|}{1 - \\|A^{-1}\\Delta A\\|} \\frac{\\|\\Delta A\\|}{\\|A\\|} + \\frac{\\|A^{-1}\\|}{1 - \\|A^{-1}\\Delta A\\|} \\frac{\\|\\Delta f\\|}{\\|A^{-1}f\\|}\\\\\n\\end{split}\n\nЗаметим, что \\|AA^{-1}f\\| \\leq \\|A\\|\\|A^{-1}f\\|, поэтому \\| A^{-1} f \\| \\geq \\frac{\\|f\\|}{\\|A\\|}\nТеперь мы готовы получить окончательную оценку\n\n\\begin{split}\n\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq\n&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A^{-1}\\Delta A\\|} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) \\leq \\\\\n\\leq\n&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A\\|\\|A^{-1}\\|\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) = \\\\\n= &\\frac{\\mathrm{cond}(A)}{1 - \\mathrm{cond}(A)\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big)\n\\end{split}\n\nКлючевую роль играет число обусловленности \\mathrm{cond}(A) = \\Vert A \\Vert \\Vert A^{-1} \\Vert."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#число-обусловленности",
    "href": "lectures/lecture-4/lecture-4.html#число-обусловленности",
    "title": "Итоги",
    "section": "Число обусловленности",
    "text": "Число обусловленности\n\nЧем больше число обусловленности, тем меньше цифр мы можем восстановить. Заметим, что число обусловленности различно для разных норм.\nЗаметим, что если \\Delta A = 0, то\n\n \\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq \\mathrm{cond}(A) \\frac{\\|\\Delta f\\|}{\\|f\\|} \n\nСпектральная норма матрицы равна наибольшему сингулярному числу, а сингулярные числа обратной матрицы равны обратным значениям сингулярных чисел.\nТаким образом, число обусловленности в спектральной норме равно отношению наибольшего сингулярного числа к наименьшему.\n\n \\mathrm{cond}_2 (A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#матрица-гильберта-снова",
    "href": "lectures/lecture-4/lecture-4.html#матрица-гильберта-снова",
    "title": "Итоги",
    "section": "Матрица Гильберта (снова)",
    "text": "Матрица Гильберта (снова)\n\nМы также можем попробовать проверить, насколько точна оценка как с единицами в правой части, так и со случайным вектором в правой части.\nРезультаты разительно отличаются\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn = 30\na = [[1.0/(i + j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\n#rhs = np.random.normal(size=n)\nrhs = np.ones(n) #Right-hand side\n#rhs = np.random.randn(n)\n#rhs = np.arange(n)\n#rhs = (-1)**rhs\nf = np.linalg.solve(a, rhs)\n\n#And check if everything is fine\ner = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\ncn = np.linalg.cond(a, 2)\nprint('Error:', er, 'Log Condition number:', np.log10(cn))\n\nu1, s1, v1 = np.linalg.svd(a)\ncf = u1.T@rhs\nplt.plot(u1[:, 20])\n#cf\n#cf/s1\n\nError: 2.2843047554427222e-08 Log Condition number: 19.020186116767867\n\n\n\n\n\n\n\n\n\nА теперь со случайной правой частью…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nn = 100\na = [[1.0/(i - j + 0.5) for i in range(n)] for j in range(n)]\na = np.array(a)\n#rhs = np.random.randn(n) #Right-hand side\n#f = np.linalg.solve(a, rhs)\n\n#And check if everything is fine\n#er = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\n#cn = np.linalg.cond(a)\n#print('Error:', er, 'Condition number:', cn)\n\nu, s, v = np.linalg.svd(a)\n#rhs = np.random.randn(n)\nrhs = np.ones((n,))\nplt.plot(u[:, 0])\nplt.grid(True)\nplt.xlabel(\"Index of vector elements\", fontsize=20)\nplt.ylabel(\"Elements of vector\", fontsize=20)\nplt.xticks(fontsize=18)\n_ = plt.yticks(fontsize=18)\n\n\n\n\n\n\n\n\nМожете ли вы объяснить это?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#переопределенные-линейные-системы",
    "href": "lectures/lecture-4/lecture-4.html#переопределенные-линейные-системы",
    "title": "Итоги",
    "section": "Переопределенные линейные системы",
    "text": "Переопределенные линейные системы\n\nВажный класс задач - это переопределенные линейные системы, когда количество уравнений больше, чем количество неизвестных.\nПростейший пример, который вы все знаете - это линейная аппроксимация, аппроксимация набора 2D точек прямой линией.\nТогда типичный способ - это минимизировать невязку (метод наименьших квадратов)\n\n\\Vert A x - b \\Vert_2 \\rightarrow \\min"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#переопределенная-система-и-матрица-грама",
    "href": "lectures/lecture-4/lecture-4.html#переопределенная-система-и-матрица-грама",
    "title": "Итоги",
    "section": "Переопределенная система и матрица Грама",
    "text": "Переопределенная система и матрица Грама\nУсловие оптимальности - это 0\\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right), где \\nabla обозначает градиент. Следовательно,\n 0 \\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right) = 2(A^*A x - A^*b) = 0. \nТаким образом,\n \\quad A^* A x = A^* b \n\nМатрица A^* A называется матрицей Грама, а система называется нормальным уравнением.\nЭто не лучший способ решения, так как число обусловленности A^* A является квадратом числа обусловленности A (проверьте почему)."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#псевдообратная-матрица",
    "href": "lectures/lecture-4/lecture-4.html#псевдообратная-матрица",
    "title": "Итоги",
    "section": "Псевдообратная матрица",
    "text": "Псевдообратная матрица\n\nМатрица A^* A может быть вырожденной в общем случае.\nПоэтому нам нужно ввести понятие псевдообратной матрицы A^{\\dagger} такой, что решение задачи наименьших квадратов может быть формально записано как\n\nx = A^{\\dagger} b.\n\nМатрица A^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* называется псевдообратной матрицей Мура-Пенроуза для матрицы A.\nЕсли матрица A имеет полный ранг по столбцам, то A^* A невырождена и мы получаем\n\nA^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^*. \n\nЕсли матрица A квадратная и невырожденная, мы получаем стандартную обратную матрицу A:\n\nA^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^* = A^{-1} A^{-*} A^* = A^{-1}\n\nЕсли A имеет линейно зависимые столбцы, то A^\\dagger b дает решение, которое имеет минимальную евклидову норму"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#вычисление-псевдообратной-матрицы-через-svd",
    "href": "lectures/lecture-4/lecture-4.html#вычисление-псевдообратной-матрицы-через-svd",
    "title": "Итоги",
    "section": "Вычисление псевдообратной матрицы через SVD",
    "text": "Вычисление псевдообратной матрицы через SVD\nПусть A = U \\Sigma V^* - SVD разложение матрицы A. Тогда,\nA^{\\dagger} = V \\Sigma^{\\dagger} U^*,\nгде \\Sigma^{\\dagger} состоит из обратных значений ненулевых сингулярных чисел матрицы A. Действительно,\n\\begin{align*}\nA^{\\dagger} &= \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = \\lim_{\\alpha \\rightarrow 0}( \\alpha VV^* + V \\Sigma^2 V^*)^{-1} V \\Sigma U^* \\\\ & = \\lim_{\\alpha \\rightarrow 0}( V(\\alpha I + \\Sigma^2) V^*)^{-1} V \\Sigma U^* = V \\lim_{\\alpha \\rightarrow 0}(\\alpha I + \\Sigma^2)^{-1} \\Sigma U^* = V \\Sigma^{\\dagger} U^*.\n\\end{align*}\n\nМожно проверить, что \\Sigma^{\\dagger} содержит только обратные значения ненулевых сингулярных чисел.\nЕсли сингулярные числа малы, можно пропустить их обращение. Это приведет к решению, которое менее чувствительно к шуму в правой части.\nЧисло обусловленности для евклидовой нормы по-прежнему равно отношению наибольшего и наименьшего ненулевого сингулярного числа."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#канонический-способ-решения-задачи-наименьших-квадратов",
    "href": "lectures/lecture-4/lecture-4.html#канонический-способ-решения-задачи-наименьших-квадратов",
    "title": "Итоги",
    "section": "Канонический способ решения задачи наименьших квадратов",
    "text": "Канонический способ решения задачи наименьших квадратов\nЗаключается в использовании QR разложения.\n\nЛюбая матрица может быть представлена в виде произведения\n\n A = Q R, \nгде Q - унитарная матрица, а R - верхнетреугольная матрица (подробности в следующих лекциях).\n\nТогда, если A имеет полный ранг по столбцам, то\n\n x = A^{\\dagger}b = (A^*A)^{-1}A^*b = ((QR)^*(QR))^{-1}(QR)^*b = (R^*Q^*QR)^{-1}R^*Q^*b = R^{-1}Q^*b.  \n\nТаким образом, нахождение оптимального x эквивалентно решению\n\n Rx = Q^* b. \n\nПоскольку R верхнетреугольная, решение этой линейной системы требует \\mathcal{O}(n^2) операций.\nТакже это более устойчиво, чем использование псевдообратной матрицы напрямую."
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#расширение-системы-до-большей-размерности",
    "href": "lectures/lecture-4/lecture-4.html#расширение-системы-до-большей-размерности",
    "title": "Итоги",
    "section": "Расширение системы до большей размерности",
    "text": "Расширение системы до большей размерности\n\nВместо решения A^* A x = A^* b, введем новую переменную r = Ax - b и получим\n\nA^* r = 0, \\quad r = Ax - b,\nили в блочной форме\n \\begin{pmatrix} 0 & A^* \\\\ A & -I \\end{pmatrix} \\begin{pmatrix} x \\\\ r \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ b \\end{pmatrix}, \nобщий размер системы равен (n + m), а число обусловленности совпадает с числом обусловленности для A - Как определить число обусловленности для прямоугольной матрицы?"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#пример-мнк",
    "href": "lectures/lecture-4/lecture-4.html#пример-мнк",
    "title": "Итоги",
    "section": "Пример МНК",
    "text": "Пример МНК\nРассмотрим двумерный пример. Предположим, что у нас есть линейная модель\ny = ax + b\nи зашумленные данные (x_1, y_1), \\dots (x_n, y_n). Тогда линейная система для коэффициентов будет выглядеть следующим образом\n\n\\begin{split}\na x_1 &+ b &= y_1 \\\\\n&\\vdots \\\\\na x_n &+ b &= y_n \\\\\n\\end{split}\n\nили в матричной форме\n\n\\begin{pmatrix}\nx_1 & 1 \\\\\n\\vdots & \\vdots \\\\\nx_n & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na \\\\\nb\n\\end{pmatrix} =\n\\begin{pmatrix}\ny_1 \\\\\n\\vdots  \\\\\ny_n \\\\\n\\end{pmatrix},\n\nчто представляет собой переопределенную систему.\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na_exact = 1.\nb_exact = 2.\n\nn = 10\nxi = np.arange(n)\nyi = a_exact * xi + b_exact + 2 * np.random.normal(size=n)\n\nA = np.array([xi, np.ones(n)])\ncoef = np.linalg.pinv(A).T.dot(yi) # coef is [a, b]\n\nplt.plot(xi, yi, 'o', label='$(x_i, y_i)$')\nplt.plot(xi, coef[0]*xi + coef[1], label='Least squares')\nplt.legend(loc='best', fontsize=18)\nplt.grid(True)"
  },
  {
    "objectID": "lectures/lecture-4/lecture-4.html#следующая-лекция",
    "href": "lectures/lecture-4/lecture-4.html#следующая-лекция",
    "title": "Итоги",
    "section": "Следующая лекция",
    "text": "Следующая лекция\n\nСобственные векторы и собственные значения\nТеорема Шура\n\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#previous-lecture",
    "href": "lectures/lecture-15/lecture-15.html#previous-lecture",
    "title": "",
    "section": "Previous lecture",
    "text": "Previous lecture\n\nKrylov methods: Arnoldi relation, CG, GMRES\nPreconditioners\n\nJacobi\nGauss-Seidel\nSSOR\nILU and its modifications"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#other-structured-matrices",
    "href": "lectures/lecture-15/lecture-15.html#other-structured-matrices",
    "title": "",
    "section": "Other structured matrices",
    "text": "Other structured matrices\n\nUp to now, we discussed preconditioning only for sparse matrices\nBut iterative methods work well for any matrices that have fast black-box matrix-by-vector product\nImportant class of such matrices are Toeplitz matrices (and Hankel matrices) and their multilevel variants\n\nThey are directly connected to the convolution operation and Fast Fourier Transform."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolution",
    "href": "lectures/lecture-15/lecture-15.html#convolution",
    "title": "",
    "section": "Convolution",
    "text": "Convolution\n\nOne of the key operation in signal processing/machine learning is the convolution of two functions.\nLet x(t) and y(t) be two given functions. Their convolution is defined as\n\n(x * y)(t) = \\int_{-\\infty}^{\\infty} x(\\tau) y(t -  \\tau) d \\tau."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolution-theorem-and-fourier-transform",
    "href": "lectures/lecture-15/lecture-15.html#convolution-theorem-and-fourier-transform",
    "title": "",
    "section": "Convolution theorem and Fourier transform",
    "text": "Convolution theorem and Fourier transform\nA well-known fact: a convolution in the time domain is a product in the frequency domain.\n\nTime-frequency transformation is given by the Fourier transform:\n\n\\widehat{x}(w) = (\\mathcal{F}(x))(w) = \\int_{-\\infty}^{\\infty} e^{i w t} x(t) dt.\n\nThen,\n\n\\mathcal{F}(x * y) = \\mathcal{F}(x) \\mathcal{F}(y).\n\nThus, the “algorithm” for the computation of the convolution can be:\n\n\nCompute Fourier transform of x(t) and y(t).\nCompute their product\nCompute inverse Fourier transform"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#discrete-convolution-operation",
    "href": "lectures/lecture-15/lecture-15.html#discrete-convolution-operation",
    "title": "",
    "section": "Discrete convolution operation",
    "text": "Discrete convolution operation\n(x * y)(t) = \\int_{-\\infty}^{\\infty} x(\\tau) y(t -  \\tau) d \\tau.\nLet us approximate the integral by a quadrature sum on a uniform grid, and store the signal at equidistant points.\nThen we are left with the summation\nz_i = \\sum_{j=0}^{n-1} x_j y_{i - j},\nwhich is called discrete convolution. This can be thought as an application of a filter with coefficients x to a signal y.\nThere are different possible filters for different purposes, but they all utilize the shift-invariant structure."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#discrete-convolution-and-toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#discrete-convolution-and-toeplitz-matrices",
    "title": "",
    "section": "Discrete convolution and Toeplitz matrices",
    "text": "Discrete convolution and Toeplitz matrices\nA discrete convolution can be thought as a matrix-by-vector product:\nz_i = \\sum_{j=0}^{n-1} x_j y_{i - j}, \\Leftrightarrow z = Ax\nwhere the matrix A elements are given as a_{ij} = y_{i-j}, i.e., they depend only on the difference between the row index and the column index."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-matrices-definition",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-matrices-definition",
    "title": "",
    "section": "Toeplitz matrices: definition",
    "text": "Toeplitz matrices: definition\nA matrix is called Toeplitz if its elements are defined as\na_{ij} = t_{i - j}.\n\nA Toeplitz matrix is completely defined by its first column and first row (i.e., 2n-1 parameters).\nIt is a dense matrix, however it is a structured matrix (i.e., defined by \\mathcal{O}(n) parameters).\nAnd the main operation in the discrete convolution is the product of Toeplitz matrix by vector.\nCan we compute it faster than \\mathcal{O}(n^2)?"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-and-circulant-matrix",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-and-circulant-matrix",
    "title": "",
    "section": "Toeplitz and circulant matrix",
    "text": "Toeplitz and circulant matrix\n\nFor a special class of Toeplitz matrices, named circulant matrices the fast matrix-by-vector product can be done.\nA matrix C is called circulant, if\n\nC_{ij} = c_{i - j \\mod n},\ni.e. it periodicaly wraps\nC = \\begin{bmatrix}\nc_0 & c_3 & c_2 & c_1 \\\\\nc_1 & c_0 & c_3 & c_2 \\\\\nc_2 & c_1 & c_0 & c_3 \\\\\nc_3 & c_2 & c_1 & c_0 \\\\\n\\end{bmatrix}.\n\n\nThese matrices have the same eigenvectors, given by the Discrete Fourier Transform (DFT)."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#spectral-theorem-for-circulant-matrices",
    "href": "lectures/lecture-15/lecture-15.html#spectral-theorem-for-circulant-matrices",
    "title": "",
    "section": "Spectral theorem for circulant matrices",
    "text": "Spectral theorem for circulant matrices\nTheorem:\nAny circulant matrix can be represented in the form\nC = \\frac{1}{n} F^* \\Lambda F,\nwhere F is the Fourier matrix with the elements\nF_{kl} = w_n^{kl}, \\quad k, l = 0, \\ldots, n-1, \\quad w_n = e^{-\\frac{2 \\pi i}{n}},\nand matrix \\Lambda = \\text{diag}(\\lambda) is the diagonal matrix and\n\\lambda = F c, \nwhere c is the first column of the circulant matrix C.\nThe proof will be later: now we need to study the FFT matrix."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fourier-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fourier-matrix",
    "title": "",
    "section": "Fourier matrix",
    "text": "Fourier matrix\nThe Fourier matrix is defined as:\n\nF_n =\n\\begin{pmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & w^{1\\cdot 1}_n & w^{1\\cdot 2}_n & \\dots & w^{1\\cdot (n-1)}_n\\\\\n1 & w^{2\\cdot 1}_n & w^{2\\cdot 2}_n & \\dots & w^{2\\cdot (n-1)}_n\\\\\n\\dots & \\dots & \\dots &\\dots &\\dots \\\\\n1 & w^{(n-1)\\cdot 1}_n & w^{(n-1)\\cdot 2}_n & \\dots & w^{(n-1)\\cdot (n-1)}_n\\\\\n\\end{pmatrix},\n\nor equivalently\n F_n = \\{ w_n^{kl} \\}_{k,l=0}^{n-1}, \nwhere\nw_n = e^{-\\frac{2\\pi i}{n}}.\nProperties: * Symmetric (not Hermitian!) * Unitary up to a scaling factor: F_n^* F_n = F_n F_n^* = nI (check this fact). Therefore F_n^{-1} = \\frac{1}{n}F^*_n * Can be multiplied by a vector (called discrete Fourier transform or DFT) with \\mathcal{O}(n \\log n) complexity (called fast Fourier transform or FFT)! FFT helps to analyze spectrum of a signal and, as we will see later, helps to do fast mutiplications with certain types of matrices.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nN = 1000\ndt = 1.0 / 800.0\nx = np.linspace(0.0, N*dt, N)\ny = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x) + 0.2*np.sin(300.0 * 2.0*np.pi*x)\nplt.plot(x, y)\nplt.xlabel('Time')\nplt.ylabel('Signal')\nplt.title('Initial signal')\n\nText(0.5, 1.0, 'Initial signal')\n\n\n\n\n\n\n\n\n\n\nyf = np.fft.fft(y)\nxf = np.linspace(0.0, 1.0/(2.0*dt), N//2)\nplt.plot(xf, 2.0/N * np.abs(yf[0:N//2])) #Note: N/2 to N will give negative frequencies\nplt.xlabel('Frequency')\nplt.ylabel('Amplitude')\nplt.title('Discrete Fourier transform')\n\nText(0.5, 1.0, 'Discrete Fourier transform')"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-fourier-transform-fft",
    "href": "lectures/lecture-15/lecture-15.html#fast-fourier-transform-fft",
    "title": "",
    "section": "Fast Fourier transform (FFT)",
    "text": "Fast Fourier transform (FFT)\nHere we consider a matrix interpretation of the standard Cooley-Tukey algorithm (1965), which has underlying divide and conquer idea. Note that in packages more advanced versions are used.\n\nLet n be a power of 2.\nFirst of all we  permute the rows  of the Fourier matrix such that the first n/2 rows of the new matrix had row numbers 1,3,5,\\dots,n-1 and the last n/2 rows had row numbers 2,4,6\\dots,n.\nThis permutation can be expressed in terms of multiplication by permutation matrix P_n:\n\n\nP_n =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 0 & 1 & 0 &\\dots & 0 & 0 \\\\\n\\vdots & & & & & & \\vdots \\\\\n0 & 0 & 0 & 0 &\\dots & 1 & 0 \\\\\n\\hline\n0 & 1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 0 & 0 & 1 &\\dots & 0 & 0 \\\\\n\\vdots & & & & & & \\vdots \\\\\n0 & 0 & 0 & 0 &\\dots & 0 & 1\n\\end{pmatrix},\n\nHence,\n\nP_n F_n =\n\\begin{pmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & w^{2\\cdot 1}_n & w^{2\\cdot 2}_n & \\dots & w^{2\\cdot (n-1)}_n\\\\\n1 & w^{4\\cdot 1}_n & w^{4\\cdot 2}_n & \\dots & w^{4\\cdot (n-1)}_n\\\\\n\\vdots & & & & \\vdots\\\\\n1 & w^{(n-2)\\cdot 1}_n & w^{(n-2)\\cdot 2}_n & \\dots & w^{(n-2)\\cdot (n-1)}_n\\\\\n\\hline\n1 & w^{1\\cdot 1}_n & w^{1\\cdot 2}_n & \\dots & w^{1\\cdot (n-1)}_n\\\\\n1 & w^{3\\cdot 1}_n & w^{3\\cdot 2}_n & \\dots & w^{3\\cdot (n-1)}_n\\\\           \n\\vdots & & & & \\vdots\\\\\n1 & w^{(n-1)\\cdot 1}_n & w^{(n-1)\\cdot 2}_n & \\dots & w^{(n-1)\\cdot (n-1)}_n\\\\\n\\end{pmatrix},\n\nNow let us imagine that we separated its columns and rows by two parts each of size n/2.\nAs a result we get 2\\times 2 block matrix that has the following form\n\nP_n F_n =\n\\begin{pmatrix}\n\\left\\{w^{2kl}_n\\right\\} & \\left\\{w_n^{2k\\left(\\frac{n}{2} + l\\right)}\\right\\} \\\\\n\\left\\{w_n^{(2k+1)l}\\right\\} & \\left\\{w_n^{(2k+1)\\left(\\frac{n}{2} + l\\right)}\\right\\}\n\\end{pmatrix},\n\\quad k,l = 0,\\dots, \\frac{n}{2}-1.\n\nSo far it does not look like something that works faster :) But we will see that in a minute. Lets have a more precise look at the first block \\left\\{w^{2kl}_n\\right\\}:\n\nw^{2kl}_n = e^{-2kl\\frac{2\\pi i}{n}} = e^{-kl\\frac{2\\pi i}{n/2}} = w^{kl}_{n/2}.\n\nSo this block is exactly twice smaller Fourier matrix F_{n/2}!\n\nThe block \\left\\{w_n^{(2k+1)l}\\right\\} can be written as\n\nw_n^{(2k+1)l} = w_n^{2kl + l} = w_n^{l} w_n^{2kl} = w_n^{l} w_{n/2}^{kl},\n\nwhich can be written as W_{n/2}F_{n/2}, where\nW_{n/2} = \\text{diag}(1,w_n,w_n^2,\\dots,w_n^{n/2-1}).\nDoing the same tricks for the other blocks we will finally get\n\nP_n F_n =\n\\begin{pmatrix}\nF_{n/2} & F_{n/2} \\\\\nF_{n/2}W_{n/2} & -F_{n/2}W_{n/2}\n\\end{pmatrix} =\n\\begin{pmatrix}\nF_{n/2} & 0 \\\\\n0 & F_{n/2}\n\\end{pmatrix}\n\\begin{pmatrix}\nI_{n/2} & I_{n/2} \\\\\nW_{n/2} & -W_{n/2}\n\\end{pmatrix}.\n\n\nThus, we reduced multiplication by F_n to 2 multiplications by F_{n/2} and cheap multiplications by diagonal matrices.\nIf we apply the obtained expressions recursively to F_{n/2}, we will get \\mathcal{O}(n\\log n)  complexity.\n\n\n#FFT vs full matvec\nimport time\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg\n\nn = 10000\nF = sp.linalg.dft(n)\nx = np.random.randn(n)\n\ny_full = F.dot(x)\n\nfull_mv_time = %timeit -q -o F.dot(x)\nprint('Full matvec time =', full_mv_time.average)\n\ny_fft = np.fft.fft(x)\nfft_mv_time = %timeit -q -o np.fft.fft(x)\nprint('FFT time =', fft_mv_time.average)\n\nprint('Relative error =', (np.linalg.norm(y_full - y_fft)) / np.linalg.norm(y_full))\n\nFull matvec time = 0.016554963692857142\nFFT time = 6.18095107428571e-05\nRelative error = 1.5329028805883414e-12"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#circulant-matrices",
    "href": "lectures/lecture-15/lecture-15.html#circulant-matrices",
    "title": "",
    "section": "Circulant matrices",
    "text": "Circulant matrices\nFFT helps to multiply fast by certain types of matrices. We start from a circulant matrix:\n\nC =\n\\begin{pmatrix}\nc_0 & c_{n-1} & c_{n-2} & \\dots & c_1 \\\\\nc_{1} & c_{0} & c_{n-1} & \\dots & c_2 \\\\\nc_{2} & c_{1} & c_0 & \\dots & c_3 \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\nc_{n-1} & c_{n-2} & c_{n-3} & \\dots & c_0\n\\end{pmatrix}\n\nTheorem. Let C be a circulant matrix of size n\\times n and let c be it’s first column , then\n\nC = \\frac{1}{n} F_n^* \\text{diag}(F_n c) F_n\n\nProof. - Consider a number\n\\lambda (\\omega) = c_0 + \\omega c_1 + \\dots + \\omega^{n-1} c_{n-1},\nwhere \\omega is any number such that \\omega^n=1. - Lets multiply \\lambda by 1,\\omega,\\dots, \\omega^{n-1}:\n\n\\begin{split}\n\\lambda & = c_0 &+& \\omega c_1 &+& \\dots &+& \\omega^{n-1} c_{n-1},\\\\\n\\lambda\\omega & = c_{n-1} &+& \\omega c_0 &+& \\dots &+& \\omega^{n-1} c_{n-2},\\\\\n\\lambda\\omega^2 & = c_{n-2} &+& \\omega c_{n-1} &+& \\dots &+& \\omega^{n-1} c_{n-3},\\\\\n&\\dots\\\\\n\\lambda\\omega^{n-1} & = c_{1} &+& \\omega c_{2} &+& \\dots &+& \\omega^{n-1} c_{0}.\n\\end{split}\n\n\nTherefore,\n\n\n\\lambda(\\omega) \\cdot \\begin{pmatrix} 1&\\omega & \\dots& \\omega^{n-1} \\end{pmatrix} =\n\\begin{pmatrix} 1&\\omega&\\dots& \\omega^{n-1} \\end{pmatrix} \\cdot C.\n\n\nWriting this for \\omega = 1,w_n, \\dots, w_n^{n-1} we get\n\n\n\\Lambda F_n = F_n C\n\nand finally\n\nC = \\frac{1}{n} F^*_n \\Lambda F_n, \\quad \\text{where}\\quad \\Lambda = \\text{diag}(F_nc) \\qquad\\blacksquare"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-circulant-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-circulant-matrix",
    "title": "",
    "section": "Fast matvec with circulant matrix",
    "text": "Fast matvec with circulant matrix\n\nRepresentation $C = F^* (F_n c) F_n $ gives us an explicit way to multiply a vector x by C in \\mathcal{O}(n\\log n) operations.\nIndeed,\n\n\nCx = \\frac{1}{n} F_n^* \\text{diag}(F_n c) F_n x = \\text{ifft}\\left( \\text{fft}(c) \\circ \\text{fft}(x)\\right)\n\nwhere \\circ denotes elementwise product (Hadamard product) of two vectors (since \\text{diag}(a)b = a\\circ b) and ifft denotes inverse Fourier transform F^{-1}_n.\n\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg\n\ndef circulant_matvec(c, x):\n    return np.fft.ifft(np.fft.fft(c) * np.fft.fft(x))\n\nn = 5000\nc = np.random.random(n)\nC = sp.linalg.circulant(c)\nx = np.random.randn(n)\n\n\ny_full = C.dot(x)\nfull_mv_time = %timeit -q -o C.dot(x)\nprint('Full matvec time =', full_mv_time.average)\n\n\ny_fft = circulant_matvec(c, x)\nfft_mv_time = %timeit -q -o circulant_matvec(c, x)\nprint('FFT time =', fft_mv_time.average)\n\nprint('Relative error =', (np.linalg.norm(y_full - y_fft)) / np.linalg.norm(y_full))\n\nFull matvec time = 0.003428939464285707\nFFT time = 0.0001001016214142856\nRelative error = 1.307050346126901e-15"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#toeplitz-matrices",
    "title": "",
    "section": "Toeplitz matrices",
    "text": "Toeplitz matrices\nNow we get back to Toeplitz matrices!\n\nT =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{-3}& \\dots & t_{1-n} \\\\\nt_{1} & t_{0} & t_{-1} & t_{-2}& \\dots & t_{2-n} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} &\\dots & t_{3-n} \\\\\nt_{3} & t_{2} & t_1 & t_0 & \\dots & t_{4-n} \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\\\\\nt_{n-1} & t_{n-2} & t_{n-3} & t_{n-4} &\\dots &t_0\n\\end{pmatrix},\n\nor equivalently T_{ij} = t_{i-j}.\nMatvec operation can be written as\n\ny_i = \\sum_{j=1}^n t_{i-j} x_j,\n\nwhich can be interpreted as a discrete convolution of filter t_i and signal x_i. For simplicity the size of the filter t is such that the sizes of the input and output signals are the same. Generally, filter size can be arbitrary.\nFast convolution computation has a variety of applications, for instance, in signal processing or partial differential and integral equations. For instance, here is the smoothing of a signal:\n\nfrom scipy import signal\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nalpha = 0.01\nsig = np.repeat([0., 1., 0.], 100)\nfilt = np.exp(-alpha * (np.arange(100)-50)**2)\nfiltered = signal.convolve(sig, filt, mode='same') / sum(filt)\n\nfig, (ax_orig, ax_filt, ax_filtered) = plt.subplots(3, 1, sharex=True)\nax_orig.plot(sig)\nax_orig.margins(0, 0.1)\nax_filt.plot(filt)\nax_filt.margins(0, 0.1)\nax_filtered.plot(filtered)\nax_filtered.margins(0, 0.1)\n\nax_orig.set_title('Original signal')\nax_filt.set_title('Filter')\nax_filtered.set_title('Convolution')\n\nfig.tight_layout()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-toeplitz-matrix",
    "title": "",
    "section": "Fast matvec with Toeplitz matrix",
    "text": "Fast matvec with Toeplitz matrix\nKey point: the multiplication by a Toeplitz matrix can be reduced to the multiplication by a circulant.\n\nIndeed, every Toeplitz matrix of size n\\times n can be embedded into a Circulant matrix C of size (2n - 1) \\times (2n - 1):\n\n\nC =\n\\begin{pmatrix}\nT & \\dots \\\\\n\\dots & \\dots\n\\end{pmatrix}.\n\n\nThe 3\\times 3 matrix T = \\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} \\\\\nt_{1} & t_{0} & t_{-1} \\\\\nt_{2} & t_{1} & t_0 \\\\\n\\end{pmatrix} can be embedded as follows\n\n\nC =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{2} & t_{1}\\\\\nt_{1} & t_{0} & t_{-1} & t_{-2} & t_{2} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} & t_{-2} \\\\\nt_{-2}& t_{2} & t_{1} & t_0 & t_{-1}  \\\\\nt_{-1} & t_{-2} & t_{2} & t_{1} & t_0  \n\\end{pmatrix}.\n\n\nFor matvec $\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\n=\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} \\\\\nt_{1} & t_{0} & t_{-1} \\\\\nt_{2} & t_{1} & t_0 \\\\\n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n$ we pad vector x with zeros:\n\n\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\star \\\\ \\star \\end{pmatrix} =\n\\begin{pmatrix}\nt_0 & t_{-1} & t_{-2} & t_{2} & t_{1}\\\\\nt_{1} & t_{0} & t_{-1} & t_{-2} & t_{2} \\\\\nt_{2} & t_{1} & t_0 & t_{-1} & t_{-2} \\\\\nt_{-2}& t_{2} & t_{1} & t_0 & t_{-1}  \\\\\nt_{-1} & t_{-2} & t_{2} & t_{1} & t_0  \n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix}=\n\\text{ifft}(\\text{fft}(\\begin{pmatrix} t_0 \\\\ t_{1} \\\\ t_{2} \\\\ t_{-2} \\\\ t_{-1} \\end{pmatrix})\\circ \\text{fft}(\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix})).\n\n\nNote that you do not need to form and store the whole matrix T\nFrom the Cooley-Tukey algorithm follows that the preferable size of circulant matrix is 2^k for some k. You can do it with zero padding of the appropriate size."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#multilevel-toeplitz-matrices",
    "href": "lectures/lecture-15/lecture-15.html#multilevel-toeplitz-matrices",
    "title": "",
    "section": "Multilevel Toeplitz matrices",
    "text": "Multilevel Toeplitz matrices\nThe 2-dimensional convolution is defined as\n\ny_{i_1i_2} = \\sum_{j_1,j_2=1}^n t_{i_1-j_1, i_2-j_2} x_{j_1 j_2}.\n\nNote that x and y are 2-dimensional arrays and T is 4-dimensional. To reduce this expression to matrix-by-vector product we have to reshape x and y into long vectors:\n\n\\text{vec}(x) =\n\\begin{pmatrix}\nx_{11} \\\\ \\vdots \\\\ x_{1n} \\\\ \\hline \\\\ \\vdots \\\\ \\hline \\\\ x_{n1} \\\\ \\vdots \\\\ x_{nn}\n\\end{pmatrix},\n\\quad\n\\text{vec}(y) =\n\\begin{pmatrix}\ny_{11} \\\\ \\vdots \\\\ y_{1n} \\\\ \\hline \\\\ \\vdots \\\\ \\hline \\\\ y_{n1} \\\\ \\vdots \\\\ y_{nn}\n\\end{pmatrix}.\n\nIn this case matrix T is block Toeplitz with Toeplitz blocks: (BTTB)\n\nT =\n\\begin{pmatrix}\nT_0 & T_{-1} & T_{-2} &  \\dots & T_{1-n} \\\\\nT_{1} & T_{0} & T_{-1} & \\dots & T_{2-n} \\\\\nT_{2} & T_{1} & T_0 & \\dots & T_{3-n} \\\\\n\\dots & \\dots & \\dots &  \\dots & \\dots\\\\\nT_{n-1} & T_{n-2} & T_{n-3}  &\\dots &T_0\n\\end{pmatrix},\n\\quad \\text{where} \\quad\nT_k = t_{k, i_2 - j_2}\\quad  \\text{are Toeplitz matrices}"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#fast-matvec-with-multilevel-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#fast-matvec-with-multilevel-toeplitz-matrix",
    "title": "",
    "section": "Fast matvec with multilevel Toeplitz matrix",
    "text": "Fast matvec with multilevel Toeplitz matrix\nTo get fast matvec we need to embed block Toeplitz matrix with Toeplitz blocks into the block circulant matrix with circulant blocks. The analog of \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\star \\\\ \\star \\end{pmatrix} =\n\\text{ifft}(\\text{fft}(\\begin{pmatrix} t_0 \\\\ t_{1} \\\\ t_{2} \\\\ t_{-2} \\\\ t_{-1} \\end{pmatrix})\\circ\\text{fft}(\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 0 \\\\ 0 \\end{pmatrix})). will look like  \\begin{pmatrix} y_{11} & y_{12} & y_{13} & \\star & \\star \\\\\ny_{21} & y_{22} & y_{23} & \\star & \\star \\\\  \ny_{31} & y_{32} & y_{33} & \\star & \\star \\\\\n\\star & \\star & \\star & \\star & \\star \\\\  \n\\star & \\star & \\star & \\star & \\star \\\\  \n\\end{pmatrix} = \\text{ifft2d}(\\text{fft2d}(\\begin{pmatrix} t_{0,0} & t_{1,0} & t_{2,0} & t_{-2,0} & t_{-1,0} \\\\\nt_{0,1} & t_{1,1} & t_{2,1} & t_{-2,1} & t_{-1,1} \\\\  \nt_{0,2} & t_{1,2} & t_{2,2} & t_{-2,2} & t_{-1,2} \\\\\nt_{0,-2} & t_{1,-2} & t_{2,-2} & t_{-2,-2} & t_{-1,-2} \\\\\nt_{0,-1} & t_{1,-1} & t_{2,-1} & t_{-2,-1} & t_{-1,-1}\n\\end{pmatrix}) \\circ \\text{fft2d}(\\begin{pmatrix}x_{11} & x_{12} & x_{13} & 0 & 0 \\\\\nx_{21} & x_{22} & x_{23} & 0 & 0 \\\\  \nx_{31} & x_{32} & x_{33} & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\  \n0 & 0 & 0 & 0 & 0 \\\\  \n\\end{pmatrix})), where fft2d is 2-dimensional fft that consists of one-dimensional transforms, applied first to rows and and then to columns (or vice versa).\n\n# Blurring and Sharpening Lena by convolution\n\nfrom scipy import signal\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib  inline\nfrom scipy import misc\nimport imageio\n\nfilter_size = 5\nfilter_blur = np.ones((filter_size, filter_size)) / filter_size**2\nlena = imageio.imread('./lena512.jpg')\n#lena = misc.face()\n#lena = lena[:, :, 0]\nblurred = signal.convolve2d(lena, filter_blur, boundary='symm', mode='same')\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nax[0, 0].imshow(lena[200:300, 200:300], cmap='gray')\nax[0, 0].set_title('Original Lena')\nax[0, 1].imshow(blurred[200:300, 200:300], cmap='gray')\nax[0, 1].set_title('Blurred Lena')\nax[1, 0].imshow((lena - blurred)[200:300, 200:300], cmap='gray')\nax[1, 0].set_title('Lena $-$ Blurred Lena')\nax[1, 1].imshow(((lena - blurred)*3 + blurred)[200:300, 200:300], cmap='gray')\nax[1, 1].set_title('$3\\cdot$(Lena $-$ Blurred Lena) + Blurred Lena')\nfig.tight_layout()\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In [7], line 8\n      6 get_ipython().run_line_magic('matplotlib', ' inline')\n      7 from scipy import misc\n----&gt; 8 import imageio\n     10 filter_size = 5\n     11 filter_blur = np.ones((filter_size, filter_size)) / filter_size**2\n\nModuleNotFoundError: No module named 'imageio'"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#solving-linear-systems-with-toeplitz-matrix",
    "href": "lectures/lecture-15/lecture-15.html#solving-linear-systems-with-toeplitz-matrix",
    "title": "",
    "section": "Solving linear systems with Toeplitz matrix",
    "text": "Solving linear systems with Toeplitz matrix\n\nConvolution is ok; but what about deconvolution, or solving linear systems with Toeplitz matrices?\n\nT x = f.\n\nFor the periodic case, where T = C is circulant,\n\nwe have the spectral theorem\nC = \\frac{1}{n}F^* \\Lambda F, \\quad C^{-1} = \\frac{1}{n}F^* \\Lambda^{-1} F,\nbut for a general Toeplitz matrices, it is not a trivial question."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#iterative-methods",
    "href": "lectures/lecture-15/lecture-15.html#iterative-methods",
    "title": "",
    "section": "Iterative methods",
    "text": "Iterative methods\n\nNot-a-bad recipe for Toeplitz linear system is to use iterative method (fast matvec is available).\nA good choice for a preconditioner is a circulant matrix."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#circulant-preconditioner",
    "href": "lectures/lecture-15/lecture-15.html#circulant-preconditioner",
    "title": "",
    "section": "Circulant preconditioner",
    "text": "Circulant preconditioner\n\nA natural idea is to use circulants as preconditioners, since they are easy to invert.\nThe first preconditioner was the preconditioner by Raymond Chan and Gilbert Strang, who proposed to take the first column of the matrix and use it to generate the circulant.\nThe second preconditioner is the Tony Chan preconditioner, which is also very natural:\n\nC = \\arg \\min_P \\Vert P - T \\Vert_F.\n\nA simple formula for the entries of C can be derived.\n\n\nimport numpy as np\nimport scipy.linalg\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport scipy as sp\n\nn = 100\nc = np.zeros(n)\nc[0] = -2\nc[1] = 1\nTm = sp.linalg.toeplitz(c, c)\n\n\nc1 = sp.linalg.circulant(c) #Strang preconditioner\nFmat = 1.0/np.sqrt(n) * np.fft.fft(np.eye(n)) #Poor man's Fourier matrix\n\nd2 = np.diag(Fmat.conj().dot(Tm).dot(Fmat))\nc2 = Fmat.dot(np.diag(d2)).dot(Fmat.conj().T)\n\n\nmat = np.linalg.inv(c1).dot(Tm)\nev = np.linalg.eigvals(mat).real\nplt.plot(np.sort(ev), np.ones(n), 'o')\nplt.xlabel('Eigenvalues for Strang preconditioner')\nplt.gca().get_yaxis().set_visible(False)\n\nmat = np.linalg.inv(c2).dot(Tm)\nev = np.linalg.eigvals(mat).real\nplt.figure()\nplt.plot(np.sort(ev), np.ones(n), 'o')\nplt.xlabel('Eigenvalues for T. Chan Preconditioner')\nplt.gca().get_yaxis().set_visible(False)"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#convolutions-in-neural-networks",
    "href": "lectures/lecture-15/lecture-15.html#convolutions-in-neural-networks",
    "title": "",
    "section": "Convolutions in neural networks",
    "text": "Convolutions in neural networks\n\nThe revolution in deep learning and computer vision is related to using Convolutional Neural Networks (CNN)\nThe most famous examples are\n\nAlexNet, 2012\nGoogLeNet, 2014\nVGG, 2015\n\nFurther improvements are based on more advanced tricks like normalizations, skip connections and so on\nMore details will be presented in Deep learning/computer vision courses"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#what-is-convolution-in-nn",
    "href": "lectures/lecture-15/lecture-15.html#what-is-convolution-in-nn",
    "title": "",
    "section": "What is convolution in NN?",
    "text": "What is convolution in NN?\n\nIn neural networks the convolution means not convolution but cross-correlation!\n\n (x \\star y)(t) = \\int_{-\\infty}^{+\\infty} x(\\tau)y(\\tau + t)d \\tau \n\nCompare with the definition of convolution from the first slide\n\n(x * y)(t) = \\int_{-\\infty}^{+\\infty} x(\\tau) y(t -  \\tau) d \\tau.\n\nSource is here\n\nConvolution and cross-correlation are related as\n\n x(t) \\star y(t) = x(-t) * y(t) \n\nHow this operation is performed in neural networks?\n\n\nSource of gif is here\n\nAlso nice presentation about the difference of this operations, detailed comparison and PyTorch examples is here"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#additional-remarks-about-this-operation",
    "href": "lectures/lecture-15/lecture-15.html#additional-remarks-about-this-operation",
    "title": "",
    "section": "Additional remarks about this operation",
    "text": "Additional remarks about this operation\n\nThis operation reflects the relations between the neighbour pixels in image\nMultiple filters/kernels can fit different features of the image\nThis is still linear transformation of the input, but it focus on local properties of data\nIt can be efficiently computed with GPU since the single set of simple instructions have to be applied to multiple data\nThe trained parameters here are filters that is used to produce the output, for batch of 3D images (RGB) they are 4-dimensional tensors"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#how-the-trained-filters-and-result-image-representation-looks-like",
    "href": "lectures/lecture-15/lecture-15.html#how-the-trained-filters-and-result-image-representation-looks-like",
    "title": "",
    "section": "How the trained filters and result image representation looks like",
    "text": "How the trained filters and result image representation looks like\n\nimport torchvision.models as models\n\nvgg16 = models.vgg16(pretrained=True)\nprint(vgg16)\n\n\nfor ch in vgg16.children():\n    features = ch\n    break\nprint(features)\nfor name, param in features.named_parameters(): \n    print(name, param.shape)\n\n\nplt.figure(figsize=(20, 17))\nfor i, filter in enumerate(features[2].weight):\n    plt.subplot(16, 16, i+1)\n    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\nfrom PIL import Image\nimg = Image.open(\"./tiger.jpeg\")\nplt.imshow(img)\nplt.show()\n\n\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimg_ = transform(img)\nimg_ = img_.unsqueeze(0)\nprint(img_.size())\nplt.imshow(img_[0].permute(1, 2, 0))\n\n\n# After the first convolutional layer\noutput1 = features[0](img_)\nprint(output1.size())\nplt.figure(figsize=(20, 17))\nfor i, f_map in enumerate(output1[0]):\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()\n\n\n# After the second convolutional layer\noutput2 = features[2](features[1](output1))\nprint(output2.size())\nplt.figure(figsize=(20, 17))\nfor i, f_map in enumerate(output2[0]):\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()\n\n\noutput3 = features[5](features[4](features[3](output2)))\nprint(output3.size())\nplt.figure(figsize=(20, 20))\nfor i, f_map in enumerate(output3[0]):\n    if i + 1 == 65:\n        break\n    plt.subplot(8, 8, i+1)\n    plt.imshow(f_map.detach(), cmap=\"gray\")\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#image-transformations",
    "href": "lectures/lecture-15/lecture-15.html#image-transformations",
    "title": "",
    "section": "Image transformations",
    "text": "Image transformations\n\nDifferent filters highlight different features of the image\nPooling operation reduces the spatial dimensions and further convolutional layer moves it to channels dimension\nAfter all feature block one gets 512 channels (we start from 3 (RGB)) and significant reduction of the spatial size of image\nSuch operation extracts useful features to help classifier work better\nExactly this property of VGG-type networks is one of the ingredients of style transfer networks, see more details here"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#low-rank-and-fourier-transform",
    "href": "lectures/lecture-15/lecture-15.html#low-rank-and-fourier-transform",
    "title": "",
    "section": "Low-rank and Fourier transform",
    "text": "Low-rank and Fourier transform\nRecent achievements for learning structured matrices include [Monarch matrices] (https://arxiv.org/pdf/2204.00595.pdf) and here Monarch mixer\nMonarch matrix is given as\n\\mathbf{M}=\\left(\\prod_{i=1}^p \\mathbf{P}_i \\mathbf{B}_i\\right) \\mathbf{P}_0\nwhere each \\mathbf{P}_i is related to the ‘base \\sqrt[p]{N}’ variant of the bit-reversal permutation, and \\mathbf{B}_i is a block-diagonal matrix with block size b. Setting b=\\sqrt[p]{N} achieves sub-quadratic compute cost. For example, for p=2, b=\\sqrt{N}, Monarch matrices require O\\left(N^{3 / 2}\\right) compute in sequence length N."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#low-rank-characteristic-of-monarch-matrices",
    "href": "lectures/lecture-15/lecture-15.html#low-rank-characteristic-of-monarch-matrices",
    "title": "",
    "section": "Low-rank characteristic of Monarch matrices",
    "text": "Low-rank characteristic of Monarch matrices\nIf we treat Monarch matrix as a block matrix with m \\times m block, the elements have the form:\nM_{\\ell j k i}=L_{j \\ell k} R_{k j i}\nThis is rank-1 approximation in disguise."
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#small-demo",
    "href": "lectures/lecture-15/lecture-15.html#small-demo",
    "title": "",
    "section": "Small demo",
    "text": "Small demo\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nn = 1024\n\nf = np.fft.fft(np.eye(n))\n\n\nprint('Rectangular block, approximate low rank')\nprint(np.linalg.svd(f[:16, :n//16])[1])\n\nprint('Special low-rank submatrices, exact low-rank')\nm1 = f[::16, ::n//16]\nprint(m1.shape)\nprint(np.linalg.svd(m1)[1])\n\nRectangular block, approximate low rank\n[2.83322231e+01 1.44781645e+01 3.38480168e+00 4.57202919e-01\n 4.42705408e-02 3.33299146e-03 2.03024059e-04 1.02439715e-05\n 4.34189364e-07 1.55704460e-08 4.73047845e-10 1.21164373e-11\n 2.58444552e-13 4.26313612e-15 1.71409604e-15 6.10988185e-16]\nSpecial low-rank submatrices, exact low-rank\n(64, 16)\n[3.20000000e+001 1.35213262e-014 3.69395699e-029 1.29572281e-044\n 1.33038472e-059 1.57392820e-074 1.98967388e-089 5.68668883e-105\n 2.40948199e-120 2.27046791e-135 4.41076069e-150 3.57770948e-166\n 3.18773973e-181 3.45410051e-196 3.80878284e-212 1.03087635e-228]"
  },
  {
    "objectID": "lectures/lecture-15/lecture-15.html#take-home-message",
    "href": "lectures/lecture-15/lecture-15.html#take-home-message",
    "title": "",
    "section": "Take home message",
    "text": "Take home message\n\nToeplitz and circulant matrices\nSpectral theorem\nFFT\nMultilevel Toeplitz matrices\nIntro to convolutional neural networks\n\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In [1], line 5\n      3     styles = open(\"./styles/custom.css\", \"r\").read()\n      4     return HTML(styles)\n----&gt; 5 css_styling()\n\nCell In [1], line 3, in css_styling()\n      2 def css_styling():\n----&gt; 3     styles = open(\"./styles/custom.css\", \"r\").read()\n      4     return HTML(styles)\n\nFile ~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282, in _modified_open(file, *args, **kwargs)\n    275 if file in {0, 1, 2}:\n    276     raise ValueError(\n    277         f\"IPython won't let you open fd={file} by default \"\n    278         \"as it is likely to crash IPython. If you know what you are doing, \"\n    279         \"you can use builtins' open.\"\n    280     )\n--&gt; 282 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: './styles/custom.css'"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#solution-of-linear-systems-and-minimization-of-functionals",
    "href": "lectures/lecture-13/lecture-13.html#solution-of-linear-systems-and-minimization-of-functionals",
    "title": "Questions?",
    "section": "Solution of linear systems and minimization of functionals",
    "text": "Solution of linear systems and minimization of functionals\n\nInstead of solving a linear system, we can minimize the residual:\n\nR(x) = \\Vert A x - f \\Vert^2_2.\n\nThe condition \\nabla R(x) = 0 gives\n\nA^* A x = A^* f,\nthus it has squared condition number, so direct minimization of the residual by standard optimization methods is rarely used.\n\nFor the symmetric positive definite case there is a much simpler functional."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#energy-functional",
    "href": "lectures/lecture-13/lecture-13.html#energy-functional",
    "title": "Questions?",
    "section": "Energy functional",
    "text": "Energy functional\nLet A = A^* &gt; 0, then the following functional\n\\Phi(x) = (Ax, x)  - 2(f, x)\nis called energy functional.\n\nProperties of energy functional\n\nIt is strictly convex (check!)\n\n \\Phi(\\alpha x + (1 - \\alpha)y) &lt; \\alpha \\Phi(x) + (1 - \\alpha) \\Phi(y)\n\nSince it is strictly convex, it has unique local minimum, which is also global\nIts global minimum x_* satisfies\n\nA x_* = f.\nIndeed,\n\\nabla \\Phi = 2(Ax - f).\nand the first order optimality condition \\nabla \\Phi (x_*) = 0 yields\nA x_* = f."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#approximation-of-the-solution-by-a-subspace",
    "href": "lectures/lecture-13/lecture-13.html#approximation-of-the-solution-by-a-subspace",
    "title": "Questions?",
    "section": "Approximation of the solution by a subspace",
    "text": "Approximation of the solution by a subspace\n\nGiven a linear M-dimensional subspace \\{y_1, \\dots, y_M\\}, we want to find an approximate solution in this basis, i.e. \n\nA x \\approx f, \\quad x = x_0 +  \\sum_{k=1}^M c_k y_k,\nwhere c is the vector of coefficients.\n\nIn the symmetric positive definite case we need to minimize\n\n(Ax, x) - 2(f, x)\nsubject to x = x_0 + Y c,\nwhere Y=[y_1,\\dots,y_M] is n \\times M and vector c has length M.\n\nUsing the representation of x, we have the following minimization for c:\n\n\\widehat{\\Phi}(c) = (A Y c, Y c) + 2 (Y^*Ax_0, c) - 2(f, Y c) = (Y^* A Y c, c) - 2(Y^* (f - Ax_0), c).\n\nNote that this is the same functional, but for the Galerkin projection of A\n\nY^* A Y c = Y^* (f - Ax_0) = Y^* r_0,\nwhich is an M \\times M linear system with symmetric positive definite matrix if Y has full column rank.\nBut how to choose Y?"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-random-projection",
    "href": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-random-projection",
    "title": "Questions?",
    "section": "Selection of the subspace: random projection",
    "text": "Selection of the subspace: random projection\n\nWe can generate Y with random numbers and then orthogonalize\nWhat quality of x will we get in this case?\nHow the derived quality relates to the conditioning of the matrix?\nSelection of the proper random matrix is important topic in dimensionality reduction theory and relates to random projection approach\n\n\nimport numpy as np\n\nn = 100\nA = np.random.randn(n, n)\nQ, _ = np.linalg.qr(A)\nk = 70\nA = Q.T @ np.diag([1e-6] * k + list(np.random.rand(n-k))) @ Q\nx_true = np.random.randn(n)\nrhs = A @ x_true\nM = n - k\nY = np.random.randn(n, M)\nA_proj = Y.T @ A @ Y\nrhs_proj = Y.T @ rhs\nprint(A_proj.shape)\nc = np.linalg.solve(A_proj, rhs_proj)\nx_proj = Y @ c\nprint(np.linalg.norm(A @ x_proj - rhs) / np.linalg.norm(rhs))\n\n(30, 30)\n0.00044969726264993147"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-krylov-subspace",
    "href": "lectures/lecture-13/lecture-13.html#selection-of-the-subspace-krylov-subspace",
    "title": "Questions?",
    "section": "Selection of the subspace: Krylov subspace",
    "text": "Selection of the subspace: Krylov subspace\nIn the Krylov subspace we generate the whole subspace from a single vector r_0 = f - Ax_0:\ny_0\\equiv k_0 = r_0, \\quad y_1\\equiv k_1 = A r_0, \\quad y_2\\equiv k_2 = A^2 r_0, \\ldots, \\quad y_{M-1}\\equiv k_{M-1} = A^{M-1} r_0.\nThis gives the Krylov subpace of the M-th order\n\\mathcal{K}_M(A, r_0) = \\mathrm{Span}(r_0, Ar_0, \\ldots, A^{M-1} r_0).\n\nIt is known to be quasi-optimal space given only matrix-vector product operation.\nKey reference here is “On the numerical solution of equation by which are determined in technical problems the frequencies of small vibrations of material systems”, A. N. Krylov, 1931, text in russian"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#solution-x_-lies-in-the-krylov-subspace-x_-in-mathcalk_na-f",
    "href": "lectures/lecture-13/lecture-13.html#solution-x_-lies-in-the-krylov-subspace-x_-in-mathcalk_na-f",
    "title": "Questions?",
    "section": "Solution x_* lies in the Krylov subspace: x_* \\in \\mathcal{K}_n(A, f)",
    "text": "Solution x_* lies in the Krylov subspace: x_* \\in \\mathcal{K}_n(A, f)\n\nAccording to Cayley–Hamilton theorem: p(A) = 0, where p(\\lambda) = \\det(A - \\lambda I)\np(A)f = A^nf + a_1A^{n-1}f + \\ldots + a_{n-1}Af + a_n f = 0\nA^{-1}p(A)f = A^{n-1}f + a_1A^{n-2}f + \\ldots + a_{n-1}f + a_nA^{-1}f = 0\nx_* = A^{-1}f = -\\frac{1}{a_n}(A^{n-1}f + a_1A^{n-2}f + \\ldots + a_{n-1}f)\nThus, x_* \\in \\mathcal{K}_n(A, f)"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#ill-conditioned-of-the-natural-basis",
    "href": "lectures/lecture-13/lecture-13.html#ill-conditioned-of-the-natural-basis",
    "title": "Questions?",
    "section": "Ill-conditioned of the natural basis",
    "text": "Ill-conditioned of the natural basis\nThe natural basis in the Krylov subspace is very ill-conditioned, since\nk_i = A^i r_0 \\rightarrow \\lambda_\\max^i v,\nwhere v is the eigenvector, corresponding to the maximal eigenvalue of A, i.e. k_i become more and more collinear for large i.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.sparse as spsp\n%matplotlib inline\n\nn = 100\nex = np.ones(n);\nA = spsp.spdiags(np.vstack((-ex,  2*ex, -ex)), [-1, 0, 1], n, n, 'csr'); \nf = np.ones(n)\nx0 = np.random.randn(n)\n\nsubspace_order = 10\nkrylov_vectors = np.zeros((n, subspace_order))\nkrylov_vectors[:, 0] = f - A.dot(x0)\nfor i in range(1, subspace_order):\n    krylov_vectors[:, i] = A.dot(krylov_vectors[:, i-1])\n    \ns = np.linalg.svd(krylov_vectors, compute_uv=False)\nprint(\"Condition number = {}\".format(s.max() / s.min()))\n\nCondition number = 497674309.99072236\n\n\nSolution: Compute orthogonal basis in the Krylov subspace."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#good-basis-in-a-krylov-subspace",
    "href": "lectures/lecture-13/lecture-13.html#good-basis-in-a-krylov-subspace",
    "title": "Questions?",
    "section": "Good basis in a Krylov subspace",
    "text": "Good basis in a Krylov subspace\nIn order to have stability, we first orthogonalize the vectors from the Krylov subspace using Gram-Schmidt orthogonalization process (or, QR-factorization).\nK_j = \\begin{bmatrix} r_0 & Ar_0 & A^2 r_0 & \\ldots & A^{j-1} r_0\\end{bmatrix} = Q_j R_j, \nand the solution will be approximated as\nx \\approx x_0 + Q_j c."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#short-way-to-arnoldi-relation",
    "href": "lectures/lecture-13/lecture-13.html#short-way-to-arnoldi-relation",
    "title": "Questions?",
    "section": "Short way to Arnoldi relation",
    "text": "Short way to Arnoldi relation\nStatement. The Krylov matrix K_j satisfies an important recurrent relation (called Arnoldi relation)\nA Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1},\nwhere H_j is upper Hessenberg, and Q_{j+1} = [q_0,\\dots,q_j] has orthogonal columns that spans columns of K_{j+1}.\nLet us prove it (consider j = 3 for simplicity):\nA \\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} = \\begin{bmatrix} k_1 & k_2 & k_3 \\end{bmatrix} = \\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} \\begin{bmatrix} 0 & 0 & \\alpha_0 \\\\ 1 & 0  & \\alpha_1 \\\\ 0 & 1  & \\alpha_2 \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 & 0 & k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2 \\end{bmatrix}, \nwhere \\alpha_s will be selected later. Denote \\widehat{k}_3 = k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2.\nIn the matrix form,\nA K_3 = K_3 Z + \\widehat k_3 e^{\\top}_2,\nwhere Z is the lower shift matrix with the last column (\\alpha_0,\\alpha_1,\\alpha_2)^T, and e_2 is the last column of the identity matrix.\nLet\nK_3 = Q_3 R_3\nbe the QR-factorization. Then,\nA Q_3 R_3 = Q_3 R_3 Z + \\widehat{k}_3 e^{\\top}_2,\n A Q_3 = Q_3 R_3 Z R_3^{-1} + \\widehat{k}_3 e^{\\top}_2 R_3^{-1}.\nNote that\ne^{\\top}_2 R_3^{-1} = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} * & * & * \\\\ 0 & * & * \\\\ 0 & 0 & * \\end{bmatrix}  = \\gamma e^{\\top}_2, and\nR_3 Z R_3^{-1} = \\begin{bmatrix} * & * & * \\\\* & * & * \\\\  0 & * & * \\\\ \\end{bmatrix},\nin the general case it will be an upper Hessenberg matrix H, i.e. a matrix that\nH_{ij} = 0, \\quad \\mbox{if } i &gt; j + 1."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#almost-arnoldi-relation",
    "href": "lectures/lecture-13/lecture-13.html#almost-arnoldi-relation",
    "title": "Questions?",
    "section": "(Almost) Arnoldi relation",
    "text": "(Almost) Arnoldi relation\nLet Q_j be the orthogonal basis in the Krylov subspace, then we have almost the Arnoldi relation\nA Q_j = Q_j H_j +  \\gamma\\widehat{k}_j e^{\\top}_{j-1},\nwhere H_j is an upper Hessenberg matrix, and\n\\widehat{k}_j = k_j - \\sum_{s=0}^{j-1} \\alpha_s k_s.\nWe select \\alpha_s in such a way that\nQ^*_j \\widehat{k}_j = 0.\nThen, \\widehat{k}_j = h_{j, j-1} q_j, where q_j is the last column of Q_{j+1}."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#arnoldi-relation-final-formula",
    "href": "lectures/lecture-13/lecture-13.html#arnoldi-relation-final-formula",
    "title": "Questions?",
    "section": "Arnoldi relation: final formula",
    "text": "Arnoldi relation: final formula\nWe have\nA Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1}.\n\nThis is the crucial formula for the efficient generation of such subspaces.\nFor non-symmetric case, it is just modified Gram-Schmidt.\nFor the symmetric case, we have a much simpler form (Lanczos process)."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#lanczos-process",
    "href": "lectures/lecture-13/lecture-13.html#lanczos-process",
    "title": "Questions?",
    "section": "Lanczos process",
    "text": "Lanczos process\nIf A = A^*, then\nQ^*_j A Q_j = H_j, \nthus H_j is hermitian, and thus it is tridiagonal, H_j = T_j.\nThis gives a short-term recurrence relation to generate the Arnoldi vectors q_j without full orthogonalization."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#lanczos-process-2",
    "href": "lectures/lecture-13/lecture-13.html#lanczos-process-2",
    "title": "Questions?",
    "section": "Lanczos process (2)",
    "text": "Lanczos process (2)\n A Q_j = Q_j T_j + t_{j, j-1} q_j e^{\\top}_{j-1}.\nIn order to get q_j, we need to compute just the last column of\nt_{j, j-1} q_j = (A Q_j - Q_j T_j) e_{j-1} = A q_{j-1} - t_{j-1, j-1} q_{j-1} - t_{j-2, j-1} q_{j-2}. \nThe coefficients \\alpha_j = t_{j-1, j-1} and \\beta_j = t_{j-2, j-1} can be recovered from orthogonality constraints\n(q_j, q_{j-1}) = 0, \\quad (q_j, q_{j-2}) = 0\nAll the other constraints will be satisfied automatically!!\nAnd we only need to store two vectors to get the new one."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#from-direct-lanczos-method-to-the-conjugate-gradient",
    "href": "lectures/lecture-13/lecture-13.html#from-direct-lanczos-method-to-the-conjugate-gradient",
    "title": "Questions?",
    "section": "From direct Lanczos method to the conjugate gradient",
    "text": "From direct Lanczos method to the conjugate gradient\nWe can now get from the Lanczos recurrence to the famous conjugate gradient method.\nWe have for A = A^* &gt; 0\nA Q_j = Q_j T_j + T_{j, j-1} q_j.\nRecall that when we minimize energy functional in basis Y we get a system Y^* A Y c = Y^* f,. Here Y = Q_j, so the approximate solution of Ax \\approx f with x_j = x_0 + Q_j c_j can be found by solving a small system\nQ^*_j A Q_j c_j = T_j c_j = Q^*_j r_0 .\nSince f is the first Krylov subspace, then Note!!! (recall what the first column in Q_j is)\nQ^*_j r_0  = \\Vert r_0 \\Vert_2 e_0 = \\gamma e_0.\nWe have a tridiagonal system of equations for c:\nT_j c_j = \\gamma e_0\nand x_j = Q_j c_j.\nWe could stop at this point, but we want short recurrent formulas instead of solving linear system with matrix T_j at each step.\nDerivation of the following update formulas is not required on the oral exam!\n\nSince A is positive definite, T_j is also positive definite, and it allows an LU decomposition\nT_j = L_j U_j, where L_j is a bidiagonal matrix with ones on the diagonal, U_j is a upper bidiagonal matrix.\n\n T_j = \\begin{bmatrix} a_1 & b_1 &  & \\\\ b_1 & a_2 & b_2 & \\\\ & \\ddots & \\ddots & \\ddots & \\\\ & & b_{j-1} & a_{j-1} & b_j \\\\ & & & b_j & a_j \\end{bmatrix} = \\begin{bmatrix} 1 & &  & \\\\ c_1 & 1 &  & \\\\ & \\ddots & \\ddots &  & \\\\ & & c_{j-1} & 1 & \\\\ & & & c_j & 1 \\end{bmatrix} \\begin{bmatrix} d_1 & b_1 &  & \\\\ & d_1 & b_2 & \\\\ & & \\ddots & \\ddots & \\\\ & & & d_{j-1} & b_j \\\\ & & & & d_j \\end{bmatrix} \n\nWe need to define one subdiagonal in L (with elements c_1, \\ldots, c_{j-1}), main diagonal of U_j (with elements d_0, \\ldots, d_{j-1} and superdiagonal of U_j (with elements b_1, \\ldots, b_{j-1}).\nThey have convenient recurrences:\n\nc_i = b_i/d_{i-1}, \\quad d_i = \\begin{cases} a_1, & \\mbox{if } i = 1, \\\\\na_i - c_i b_i, & \\mbox{if } i &gt; 1. \\end{cases}\n\nFor the solution we have\n\nx_j = Q_j T^{-1}_j \\gamma e_0  = \\gamma Q_j (L_j U_j)^{-1} e_0  = \\gamma Q_j U^{-1}_j L^{-1}_j e_0.\n\nWe introduce two new quantities:\n\nP_j = Q_j U^{-1}_j, \\quad z_j = \\gamma L^{-1}_j e_0.\n\nNow we have the following equation for x_j:\n\n x_j = P_j z_j\n\nDue to the recurrence relations, we have\n\nP_j = \\begin{bmatrix} P_{j-1} & p_j \\end{bmatrix}, \nand\nz_j = \\begin{bmatrix} z_{j-1} \\\\ \\xi_{j} \\end{bmatrix}.\n\nFor p_j and \\xi_j we have short-term recurrence relations (due to bidiagonal structure)\n\np_j = \\frac{1}{d_j}\\left(q_j - b_j p_{j-1} \\right), \\quad \\xi_j = -c_j \\xi_{j-1}.\n\nThus, we arrive at short-term recurrence for x_j:\n\nx_j = P_j z_j = P_{j-1} z_{j-1} + \\xi_j p_j = x_{j-1} + \\xi_j p_j.\nand q_j are found from the Lanczos relation (see slides above).\n\nThis method for solving linear systems is called a direct Lanczos method. It is closely related to the conjugate gradient method."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#direct-lanczos-method",
    "href": "lectures/lecture-13/lecture-13.html#direct-lanczos-method",
    "title": "Questions?",
    "section": "Direct Lanczos method",
    "text": "Direct Lanczos method\nWe have the direct Lanczos method, where we store\np_{j-1}, q_j, x_{j-1} to get a new estimate of x_j.\nThe main problem is with q_j: we have the three-term recurrence, but in the floating point arithmetic the orthogonality is can be lost, leading to numerical errors.\nLet us do some demo.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy as sp\nimport scipy.sparse as spsp\nfrom scipy.sparse import csc_matrix\n\nn = 128\nex = np.ones(n);\nA = spsp.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \nrhs = np.ones(n)\n\nnit = 64\nq1 = rhs/np.linalg.norm(rhs)\nq2 = A.dot(q1)\nq2 = q2 - np.dot(q2, q1)*q1\nq2 = q2/np.linalg.norm(q2)\nqall = [q1, q2]\nfor i in range(nit):\n    qnew = A.dot(qall[-1])\n    qnew = qnew - np.dot(qnew, qall[-1])*qall[-1]\n    qnew = qnew/np.linalg.norm(qnew)\n    qnew = qnew - np.dot(qnew, qall[-2])*qall[-2]\n    qnew = qnew/np.linalg.norm(qnew)\n    qall.append(qnew)\nqall_mat = np.vstack(qall).T\nprint(np.linalg.norm(qall_mat.T.dot(qall_mat) - np.eye(qall_mat.shape[1])))\n\n1.9605915654183865"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#conjugate-gradient-method",
    "href": "lectures/lecture-13/lecture-13.html#conjugate-gradient-method",
    "title": "Questions?",
    "section": "Conjugate gradient method",
    "text": "Conjugate gradient method\nInstead of q_j (last vector in the modified Gram-Schmidt process), it is more convenient to work with the residual\nr_j = f - A x_j.\nThe resulting recurrency has the form\nx_j = x_{j-1} + \\alpha_{j-1} p_{j-1}\nr_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}\np_j = r_j + \\beta_j p_{j-1}.\nHence the name conjugate gradient: to the gradient r_j we add a conjugate direction p_j.\nWe have orthogonality of residuals (check!):\n(r_i, r_j) = 0, \\quad i \\ne j\nand A-orthogonality of conjugate directions (check!):\n (A p_i, p_j) = 0,\nwhich can be checked from the definition.\nThe equations for \\alpha_j and \\beta_j can be now defined explicitly from these two properties."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#cg-final-formulas",
    "href": "lectures/lecture-13/lecture-13.html#cg-final-formulas",
    "title": "Questions?",
    "section": "CG final formulas",
    "text": "CG final formulas\nWe have (r_{j}, r_{j-1}) = 0 = (r_{j-1} - \\alpha_{j-1} A p_{j-1}, r_{j-1}),\nthus\n\\alpha_{j-1} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, r_{j-1})} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, p_{j-1} - \\beta_{j-1}p_{j-2})} = \\frac{(r_{j-1}, r_{j-1})}{(A p_{j-1}, p_{j-1})}.\nIn the similar way, we have\n\\beta_{j-1} = \\frac{(r_j, r_j)}{(r_{j-1}, r_{j-1})}.\nRecall that\nx_j = x_{j-1} + \\alpha_{j-1} p_{j-1}\nr_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}\np_j = r_j + \\beta_j p_{j-1}.\nOnly one matrix-by-vector product per iteration."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#cg-derivation-overview",
    "href": "lectures/lecture-13/lecture-13.html#cg-derivation-overview",
    "title": "Questions?",
    "section": "CG derivation overview",
    "text": "CG derivation overview\n\nWant to find x_* in Krylov subspace\nBut natural basis is ill-conditioned, therefore we need orthogonalization\nDerive recurrent equation for sequential orthogonalization of the Krylov subspace basis\n\nArnoldi process for non-symmetric matrix\nLanczos process for symmetrix matrix\n\nClever re-writing of these formulas gives short recurrence"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#some-history",
    "href": "lectures/lecture-13/lecture-13.html#some-history",
    "title": "Questions?",
    "section": "Some history",
    "text": "Some history\nMore details here: https://www.siam.org/meetings/la09/talks/oleary.pdf\nWhen Hestenes worked on conjugate bases in 1936, he was advised by a Harvard professor that it was too obvious for publication - CG doesn’t work on slide rules.  - CG has little advantage over Gauss elimination for computation with calculators. - CG is not well suited for a room of human “computers” – too much data exchange."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#properties-of-the-cg-method",
    "href": "lectures/lecture-13/lecture-13.html#properties-of-the-cg-method",
    "title": "Questions?",
    "section": "Properties of the CG method",
    "text": "Properties of the CG method\n\nWe need to store 3 vectors.\nSince it generates A-orthogonal sequence p_1, \\ldots, p_N, after n steps it should stop (i.e., p_{N+1} = 0.)\nIn practice it does not have this property in finite precision, thus after its invention in 1952 by Hestens and Stiefel it was labeled unstable.\nIn fact, it is a brilliant iterative method."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#a-optimality",
    "href": "lectures/lecture-13/lecture-13.html#a-optimality",
    "title": "Questions?",
    "section": "A-optimality",
    "text": "A-optimality\nEnergy functional can be written as\n(Ax, x) - 2(f, x) = (A (x - x_*), (x - x_*)) - (Ax _*, x_*),\nwhere A x_* = f. Up to a constant factor,\n (A(x - x_*), (x -x_*)) = \\Vert x - x_* \\Vert^2_A\nis the A-norm of the error."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#convergence",
    "href": "lectures/lecture-13/lecture-13.html#convergence",
    "title": "Questions?",
    "section": "Convergence",
    "text": "Convergence\nThe CG method computes x_k that minimizes the energy functional over the Krylov subspace, i.e. x_k = p(A)f, where p is a polynomial of degree k+1, so\n\\Vert x_k - x_* \\Vert_A  =  \\inf\\limits_{p} \\Vert \\left(p(A) - A^{-1}\\right) f \\Vert_A. \nUsing eigendecomposition of A we have\nA = U \\Lambda U^*, \\quad  g = U^* f, and\n\\Vert x - x_* \\Vert^2_A = \\displaystyle{\\inf_p} \\Vert \\left(p(\\Lambda) - \\Lambda^{-1}\\right) g \\Vert_\\Lambda^2 = \\displaystyle{\\inf_p}\n\\displaystyle{\\sum_{i=1}^n} \\frac{(\\lambda_i p(\\lambda_i) - 1)^2 g^2_i}{\\lambda_i} = \\displaystyle{\\inf_{q, q(0) = 1}} \\displaystyle{\\sum_{i=1}^n} \\frac{q(\\lambda_i)^2 g^2_i}{\\lambda_i}\nSelection of the optimal q depends on the eigenvalue distribution."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#absolute-and-relative-error",
    "href": "lectures/lecture-13/lecture-13.html#absolute-and-relative-error",
    "title": "Questions?",
    "section": "Absolute and relative error",
    "text": "Absolute and relative error\nWe have\n\\Vert x - x_* \\Vert^2_A \\leq \\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} \\inf_{q, q(0)=1} \\max_{j} q({\\lambda_j})^2\nThe first term is just\n\\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} = (A^{-1} f, f) = \\Vert x_* \\Vert^2_A.\nAnd we have relative error bound\n\\frac{\\Vert x - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})|,\nso if matrix has only 2 different eigenvalues, then there exists a polynomial of degree 2 such that q({\\lambda_1}) =q({\\lambda_2})=0, so in this case CG converges in 2 iterations.\n\n\nIf eigenvalues are clustered and there are l outliers, then after first \\mathcal{O}(l) iterations CG will converge as if there are no outliers (and hence the effective condition number is smaller).\nThe intuition behind this fact is that after \\mathcal{O}(l) iterations the polynomial has degree more than l and thus is able to zero l outliers. \n\nLet us find another useful upper-bound estimate of convergence. Since\n\n\\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})| \\leq \\inf_{q, q(0)=1} \\max_{\\lambda\\in[\\lambda_\\min,\\lambda_\\max]} |q({\\lambda})|\n\nThe last term is just the same as for the Chebyshev acceleration, thus the same upper convergence bound holds:\n\\frac{\\Vert x_k - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\gamma \\left( \\frac{\\sqrt{\\mathrm{cond}(A)}-1}{\\sqrt{\\mathrm{cond}(A)}+1}\\right)^k."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#finite-termination-clusters",
    "href": "lectures/lecture-13/lecture-13.html#finite-termination-clusters",
    "title": "Questions?",
    "section": "Finite termination & clusters",
    "text": "Finite termination & clusters\n\nIf A has only m different eigenvalues, CG converges in m iterations (proof in the blackboard).\nIf A has m “clusters” of eigenvalues, CG converges cluster-by-cluster.\n\nAs a result, better convergence than Chebyshev acceleration, but slightly higher cost per iteration."
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#summary",
    "href": "lectures/lecture-13/lecture-13.html#summary",
    "title": "Questions?",
    "section": "Summary",
    "text": "Summary\nCG is the method of choice for symmetric positive definite systems:\n\n\\mathcal{O}(n) memory\nSquare root of condition number in the estimates\nAutomatic ignoring of the outliers/clusters\nA-optimality property"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#non-linear-conjugate-gradient-method",
    "href": "lectures/lecture-13/lecture-13.html#non-linear-conjugate-gradient-method",
    "title": "Questions?",
    "section": "Non-linear conjugate gradient method",
    "text": "Non-linear conjugate gradient method\n\nCG minimizes the energy functional, which is quadratic in x\nCG formulas were used as starting point in developing methods to minimize arbitrary convex function\nMost popular CG extensions (so-called non-linear CG method) are\n\nHestenes-Stiefel method\nPolak-Ribiere method - original paper in French\nFletcher–Reeves method"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#non-symmetric-systems-and-the-generalized-minimal-residual-method-gmres-y.-saad-m.-schultz-1986",
    "href": "lectures/lecture-13/lecture-13.html#non-symmetric-systems-and-the-generalized-minimal-residual-method-gmres-y.-saad-m.-schultz-1986",
    "title": "Questions?",
    "section": "Non-symmetric systems and the generalized minimal residual method (GMRES) (Y. Saad, M. Schultz, 1986)",
    "text": "Non-symmetric systems and the generalized minimal residual method (GMRES) (Y. Saad, M. Schultz, 1986)\nBefore we discussed symmetric positive definite systems. What happens if A is non-symmetric?\nWe can still orthogonalize the Krylov subspace using Arnoldi process, and get\nA Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1}.\nLet us rewrite the latter expression as\n A Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1} = Q_{j+1} \\widetilde H_j, \\quad \\widetilde H_j =\n\\begin{bmatrix} h_{0,0} & h_{0,1} & \\dots & h_{0,j-2} & h_{0,j-1} \\\\ h_{1,0} & h_{1,1} & \\dots & h_{1,j-2} & h_{1,j-1} \\\\ 0& h_{2,2} &  \\dots & h_{2,j-2} & h_{2,j-1} \\\\\n0& 0 & \\ddots & \\vdots & \\vdots  \\\\\n0& 0 &  & h_{j,j-1} & h_{j-1,j-1} \\\\ 0& 0 & \\dots & 0 & h_{j,j-1}\\end{bmatrix}\nThen, if we need to minimize the residual over the Krylov subspace, we have\nx_j = x_0 + Q_j c_j \nand x_j has to be selected as\n \\Vert A x_j - f \\Vert_2 =  \\Vert A Q_j c_j - r_0 \\Vert_2 \\rightarrow \\min_{c_j}.\nUsing the Arnoldi recursion, we have\n \\Vert Q_{j+1} \\widetilde H_j c_j -  r_0 \\Vert_2 \\rightarrow \\min_{c_j}.\nUsing the orthogonal invariance under multiplication by unitary matrix, we get\n \\Vert \\widetilde H_j c_j - \\gamma e_0 \\Vert_2 \\rightarrow \\min_{c_j},\nwhere we have used that Q^*_{j+1} r_0 = \\gamma e_0, \\gamma = \\Vert r_0 \\Vert\n\nThis is just a linear least squares with (j+1) equations and j unknowns.\nThe matrix is also upper Hessenberg, thus its QR factorization can be computed in a very cheap way.\nThis allows the computation of c_j. This method is called GMRES (generalized minimal residual)"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#summary-of-the-gmres",
    "href": "lectures/lecture-13/lecture-13.html#summary-of-the-gmres",
    "title": "Questions?",
    "section": "Summary of the GMRES",
    "text": "Summary of the GMRES\n\nMinimizes the residual directly\nNo normal equations\nMemory grows with the number of iterations as \\mathcal{O}(nj), so restarts typically implemented (just start GMRES from the new initial guess).\n\n\nimport scipy.sparse.linalg as la\nfrom scipy.sparse import csc_matrix, csr_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n%matplotlib inline\nn = 150\nex = np.ones(n);\nlp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \ne = sp.sparse.eye(n)\nA = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\nA = csr_matrix(A)\nrhs = np.ones(n * n)\n\nplt.figure(figsize=(10, 5))\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nfor restart in [5, 40, 200]:\n    hist = []\n    def callback(rk):\n        hist.append(np.linalg.norm(rk) / np.linalg.norm(rhs))\n    st = time.time()\n    sol = la.gmres(A, rhs, x0=np.zeros(n*n), maxiter=200, restart=restart, callback=callback, tol=1e-16)\n    current_time = time.time() - st\n    ax1.semilogy(np.array(hist), label='rst={}'.format(restart))\n    ax2.semilogy([current_time * i / len(hist) for i in range(len(hist))], np.array(hist), label='rst={}'.format(restart))\n    \n\nax1.legend(loc='best')\nax2.legend(loc='best')\nax1.set_xlabel(\"Number of outer iterations\", fontsize=20)\nax2.set_xlabel(\"Time, sec\", fontsize=20)\nax1.set_ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nax2.set_ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nplt.sca(ax1)\nplt.yticks(fontsize=20)\nplt.sca(ax2)\nplt.yticks(fontsize=20)\nf.tight_layout()\n\n&lt;Figure size 1000x500 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport scipy.sparse.linalg as la\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Example from http://www.caam.rice.edu/~embree/39961.pdf\n\nA = np.array([[1, 1, 1],\n              [0, 1, 3],\n              [0, 0, 1]]\n            )\nrhs = np.array([2, -4, 1])\nx0 = np.zeros(3)\n\nfor restart in [1, 2, 3]:\n    hist = []\n    def callback(rk):\n        hist.append(np.linalg.norm(rk)/np.linalg.norm(rhs))\n    _ = la.gmres(A, rhs, x0=x0, maxiter=20, restart=restart, callback=callback)\n    plt.semilogy(np.array(hist), label='rst={}'.format(restart))\nplt.legend(fontsize=22)\nplt.xlabel(\"Number of outer iterations\", fontsize=20)\nplt.ylabel(r\"$\\frac{||r_k||_2}{||rhs||_2}$\", fontsize=20)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "lectures/lecture-13/lecture-13.html#next-lecture",
    "href": "lectures/lecture-13/lecture-13.html#next-lecture",
    "title": "Questions?",
    "section": "Next lecture",
    "text": "Next lecture\n\nIterative methods continued (BiCG, MINRES)\nPreconditioners"
  }
]