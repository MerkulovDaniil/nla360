---
title: QR-разложение и разложение Шура.
author: Даня Меркулов
institute: МФТИ. AI360
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back6.jpeg}
---

# PageRank

## PageRank

Рассмотрим простой пример. Предположим, что у нас есть 4 веб-сайта с некоторыми ссылками. Наша цель — понять, насколько важен каждый из этих сайтов. Очевидно, что мы можем переформулировать эту проблему в терминах ориентированных графов. Здесь каждый узел представляет веб-сайт, а каждое ребро описывает ссылку с одного сайта на другой.

::::{.columns}
:::{.column width=50%}
![](PR1.pdf)
:::
:::{.column width=50%}
![](PR2.pdf)
:::
::::

## PageRank

::::{.columns}

:::{.column width=50%}
![](PR2.pdf)
:::

:::{.column width=50%}
Таким образом, мы можем ввести матрицу переходов $A$:
$$
A = \begin{bmatrix}
0 & \frac12 & 1 & \frac13 \\
1 & 0 & 0 & \frac13 \\
0 & 0 & 0 & \frac13 \\
0 & \frac12 & 0 & 0
\end{bmatrix}
$$

. . .

Давайте введём вектор PageRank $\mathbf{x}$, который описывает важность каждого веб-сайта. 
$$
\mathbf{x} = (x_1, x_2, x_3, x_4)^\text{T},\text{ где $x_i$ - важность $i$-го веб-сайта}
$$

. . .

Предположим, что начальная важность равномерно распределена между всеми узлами. Тогда: 
$$
\mathbf{x}^0 = (0.25, 0.25, 0.25, 0.25)^\text{T}
$$
Каждая входящая ссылка увеличивает важность узла. Таким образом, этот обновление может быть записано как умножение матрицы на вектор:

$$\mathbf{x}^1 = A \cdot \mathbf{x}^0 = (0.46, 0.33, 0.08, 0.125)^\text{T}$$
:::
::::

## PageRank

::::{.columns}

:::{.column width=50%}
![](PR2.pdf)
:::

:::{.column width=50%}
Повторяя те же операции, мы можем легко увидеть сходимость:

$$\mathbf{x}^2 = A \cdot \mathbf{x}^1 = (0.29, 0.50, 0.04, 0.17)^\text{T}$$

$$\mathbf{x}^3 = A \cdot \mathbf{x}^2 = (0.35, 0.35, 0.06, 0.25)^\text{T}$$

$$\dots$$

$$\mathbf{x}^{14} = A \cdot \mathbf{x}^{13} = (0.33, 0.40, 0.07, 0.20)^\text{T}$$

$$\mathbf{x}^{15} = A \cdot \mathbf{x}^{14} = (0.33, 0.40, 0.07, 0.20)^\text{T}$$

Выполните упражнение [ \faPython PageRank](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/PageRank_exercise_ru.ipynb).
:::
::::

# QR-разложение

## QR-разложение матрицы

:::: {.columns}
::: {.column width="60%"}
![](QR.pdf)
:::

::: {.column width="40%"}
Для произвольной матрицы $A \in \mathbb{R}^{m \times n}$ существует разложение вида:
$$
A = QR,
$$
где $Q \in \mathbb{R}^{m \times m}$ - ортогональная матрица ($Q^TQ = QQ^T = I$), а $R \in \mathbb{R}^{m \times n}$ - верхнетреугольная матрица. В случае, когда $m > n$, матрица $Q$ может быть усечена до размера $m \times n$ с сохранением ортогональности столбцов. Если зафиксировать все диагональные элементы матрицы $R$, то разложение становится единственным.
:::
::::

## Процесс Грама-Шмидта

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS1.pdf)

## Процесс Грама-Шмидта {.noframenumbering} 

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS2.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS3.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS4.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS5.pdf)

## Процесс Грама-Шмидта

:::: {.columns}
::: {.column width="20%"}

![](GS5.pdf)

![](Projection.pdf)

:::

::: {.column width="80%"}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

. . .

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.
$$
\begin{aligned}
\uncover<+->{ d_0 &= u_0 \\ }
\uncover<+->{ d_1 &= u_1 - \pi_{d_0}(u_1) \\ }
\uncover<+->{ d_2 &= u_2 - \pi_{d_0}(u_2) - \pi_{d_1}(u_2) \\ }
\uncover<+->{ &\vdots \\ }
\uncover<+->{ d_k &= u_k - \sum\limits_{i=0}^{k-1}\pi_{d_i}(u_k) }
\end{aligned}
$$

. . .

$$
d_k = u_k + \sum\limits_{i=0}^{k-1}\beta_{ik} d_i \qquad \beta_{ik} = - \dfrac{\langle d_i, u_k \rangle}{\langle d_i, d_i \rangle}
$$ {#eq-GS}
:::
::::

## QR-разложение с помощью процесса Грама-Шмидта

:::: {.columns}
:::{.column width="45%"}
Пусть дана матрица $A \in \mathbb{R}^{m \times n}$, у которой столбцы - это векторы
$$
A = [\,u_1 \;\; u_2 \;\;\dots\;\; u_n\,].
$$

Если мы последовательно применим процесс Грамма-Шмидта к этим столбцам:

1. На шаге $k$ мы удаляем из столбца $u_k$ проекции на все ранее полученные ортогональные векторы $d_1, \dots, d_{k-1}$.
2. Нормируем результат и получаем ортонормированный вектор $q_k = \dfrac{d_k}{\|d_k\|}$.

. . .

Таким образом образуется ортонормированный набор столбцов
$$
Q = [\,q_1 \;\; q_2 \;\;\dots\;\; q_n\,].
$$
:::

:::{.column width="55%"}
А коэффициенты, которые появляются при разложении каждого $u_k$ по векторам $q_1, \dots, q_k$, образуют верхнетреугольную матрицу $R$:
$$
u_k 
= \underbrace{\langle q_1,\, u_k \rangle}_{r_{1k}} q_1 
+ \underbrace{\langle q_2,\, u_k \rangle}_{r_{2k}} q_2 
+ \dots 
+ \underbrace{\langle q_k,\, u_k \rangle}_{r_{k\,k}} q_k.
$$

Все элементы $r_{ij} = 0$ при $j < i$, и в итоге получаем **QR-разложение**:
$$
A = QR, 
$$
где $Q$ - ортонормированная (ортогональная) матрица, а $R$ - верхняя треугольная.
:::
::::

## Модификация процесса Грама-Шмидта

* Процесс Грама-Шмидта может быть **численно неустойчив**, то есть векторы могут перестать быть ортогональными в машинной арифметике, особенно если норма $q_k$ мала. Это явление называется **потерей ортогональности**.
  
* Существует метод **modified Gram-Schmidt** (MGS). Вместо того, чтобы сразу вычитать все проекции:
  $$
  q_k := a_k \;-\;(a_k,q_1)q_1 \;-\;\dots\;-\;(a_k,q_{k-1})q_{k-1},
  $$
  мы делаем это **поэтапно**:
  $$
  \begin{aligned}
    q_k &:= a_k,\\
    q_k &:= q_k - (q_k,\,q_1)\,q_1,\\
    q_k &:= q_k - (q_k,\,q_2)\,q_2,\\
    &\;\;\vdots
  \end{aligned}
  $$
  
* В точной арифметике результат совпадает со стандартной процедурой Грама-Шмидта, но в машинной арифметике это даёт **совершенно другой** (намного более устойчивый) результат.

* Сложность модифицированного процесса Грама-Шмидта составляет $\mathcal{O}(n^2m)$ операций.

## Упражнение {.t}

Выполните упражнение [\faPython Модифицированный процесс Грама-Шмидта](https://colab.research.google.com/github/MerkulovDaniil/nla360/blob/main/files/qr_exercise.ipynb).



## Алгоритм Хаусхолдера для построения QR-разложения

:::: {.columns}
::: {.column width="50%"}
* Используя полученное свойство мы можем сделать произвольную матрицу $A$ нижней треугольной:
    $$
    H_2 H_1 A = \begin{bmatrix} \times & \times & \times & \times \\ 0 & \times & \times & \times  \\  0 & 0 & \boldsymbol{\times} & \times\\ 0 &0 & \boldsymbol{\times} & \times  \\ 0 &0 & \boldsymbol{\times} & \times \end{bmatrix}
    $$

* Затем находим $H_3=\begin{bmatrix}I_2 & \\ & {\widetilde H}_3 \end{bmatrix}$ такую, что
    $$
    {\widetilde H}_3 \begin{bmatrix} \boldsymbol{\times}\\ \boldsymbol{\times} \\ \boldsymbol{\times}  \end{bmatrix} = \begin{bmatrix} \times \\ 0 \\ 0  \end{bmatrix}.
    $$

:::

::: {.column width="50%"}
* Получаем
    $$
    H_3 H_2 H_1 A =  \begin{bmatrix} \times & \times & \times & \times \\  0 & \times & \times & \times  \\  0 & 0 & {\times} & \times\\  0 &0 & 0 & \times  \\  0 &0 & 0 & \times  \end{bmatrix}
    $$

* Аналогично находим $H_4$ и получаем верхнюю треугольную матрицу.
* Так как произведение ортогональных и обратная к ортогональной матрицы являются ортогональными матрицами, мы получаем **следствие:** (QR-разложение) Любая $A\in \mathbb{R}^{n\times m}$ может быть представлена как
    $$
    A = QR,
    $$
    где $Q$ - ортогональная и $R$ - верхняя треугольная. 

    См. [постер](decompositions.pdf), каковы размеры $Q$ и $R$ для $n>m$ и $n<m$.
:::
::::




## Матрица вращения Гивенса (Якоби)

* Матрица вращения Гивенса (Якоби) - это ортогональная матрица, которая используется для вращения вектора в плоскости на угол $\alpha$. Она имеет следующий вид:
    $$
    G = \begin{bmatrix} \cos \alpha & -\sin \alpha \\ \sin \alpha & \cos \alpha \end{bmatrix}
    $$
    Легко проверить, что $G^T G = GG^T = I$, то есть матрица является ортогональной.

* Для общего случая размерности $n$ мы выбираем две координаты $(i, j)$ и выполняем вращение вектора $x$ только в этой плоскости:
    $$
    x' = G x,
    $$
    где изменяются только $i$-я и $j$-я координаты:
    $$
    x'_i =  x_i\cos \alpha - x_j\sin \alpha , \quad x'_j = x_i \sin \alpha  + x_j\cos\alpha,
    $$
    при этом остальные $x_k$ остаются неизменными.    

* Чтобы обнулить $j$-ю координату вектора, выбираем угол $\alpha$ так, что:
    $$
    \cos \alpha = \frac{x_i}{\sqrt{x_i^2 + x_j^2}}, \quad \sin \alpha = -\frac{x_j}{\sqrt{x_i^2 + x_j^2}}
    $$

* Применяя последовательно матрицы Гивенса, можно привести матрицу к верхнетреугольному виду: для этого нужно $n-1$ вращений, чтобы обнулить элементы под главной диагональю в каждом столбце.

## QR через вращения Гивенса

Также мы можем сделать матрицу верхнетреугольной с помощью вращений Гивенса:

$$
\begin{bmatrix} \times & \times & \times \\ \bf{*} & \times & \times \\ \bf{*} & \times & \times \end{bmatrix} \to \begin{bmatrix} * & \times & \times \\ * & \times & \times \\ 0 & \times & \times \end{bmatrix} \to \begin{bmatrix} \times & \times & \times \\ 0 & * & \times \\ 0 & * & \times \end{bmatrix} \to \begin{bmatrix} \times & \times & \times \\ 0 & \times & \times \\ 0 & 0 & \times \end{bmatrix}
$$

## Вращения Гивенса vs. отражения Хаусхолдера

- Отражения Хаусхолдера полезны для плотных матриц (сложность приблизительно в два раза меньше, чем для Якоби) и мы должны обнулить большое количество элементов.
- Вращения Гивенса более подходят для разреженных матриц или параллельных вычислений, так как они действуют локально на элементах.

## Упражнение: QR-разложение вектора $(1,2)$

Рассмотрим вектор 
$$
a = \begin{bmatrix} 1 \\ 2 \end{bmatrix}.
$$

. . .

**Решение:**

1. Вычисляем норму вектора:
   $$
   \|a\| = \sqrt{1^2 + 2^2} = \sqrt{5}.
   $$

2. Получаем ортонормированный вектор:
   $$
   q_1 = \frac{a}{\|a\|} = \frac{1}{\sqrt{5}}\begin{bmatrix} 1 \\ 2 \end{bmatrix}.
   $$

3. Поскольку вектор один, то QR-разложение имеет вид:
   $$
   a = Q \cdot R,
   $$
   где 
   $$
   Q = \frac{1}{\sqrt{5}}\begin{bmatrix} 1 \\ 2 \end{bmatrix},\quad R = \sqrt{5}.
   $$

## Упражнение: QR-разложение вектора $(1,2,3)$ {.t}

Рассмотрим вектор 
$$
a = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}.
$$

. . .

**Решение:**

1. Вычисляем норму вектора:
   $$
   \|a\| = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14}.
   $$

2. Получаем ортонормированный вектор:
   $$
   q_1 = \frac{a}{\|a\|} = \frac{1}{\sqrt{14}}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}.
   $$

3. Так как вектор один, то имеем:
   $$
   a = Q \cdot R,
   $$
   где 
   $$
   Q = \frac{1}{\sqrt{14}}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix},\quad R = \sqrt{14}.
   $$

# Разложение Шура

## Разложение Шура

Для произвольной квадратной матрицы $A \in \mathbb{R}^{n \times n}$ существует разложение:
$$
A = U T U^*,
$$
где $U$ - унитарная матрица, $T$ - верхняя треугольная матрица c собственными числами матрицы $A$ на главной диагонали.

![Разложение Шура матрицы A](Schur.pdf)

## QR-алгоритм

QR-алгоритм выглядит следующим образом:

1. Начинаем с матрицы $A_0 = A$
2. Для $k = 0, 1, 2, \ldots$ :
   - Вычисляем QR-разложение: $A_k = Q_kR_k$
   - Формируем следующую итерацию: $A_{k+1} = R_kQ_k$

. . .

Для симметричных матриц этот процесс сходится к диагональной матрице, содержащей собственные числа. Для произвольных матриц он сходится к верхнетреугольной матрице, где диагональные элементы являются собственными числами исходной матрицы.

$$
A_{k+1}=R_{k}Q_{k}=Q_{k}^{-1}Q_{k}R_{k}Q_{k}=Q_{k}^{-1}A_{k}Q_{k}=Q_{k}^{\mathsf {T}}A_{k}Q_{k},
$$

* Алгоритм сходится последовательно от старших собственных чисел к младшим, 2-3 итерации на одно собственное число.
* 1 итерация = подсчёт QR-разложения $\mathcal{O}(n^3)$ + умножение матриц $\mathcal{O}(n^3)$. Наивная сложность QR-алгоритма составляет $\mathcal{O}(n^4)$.
* На практике используются различные стратегии ускорения сходимости до $\mathcal{O}(n^3)$. Например, приведение матрицы к форме Гессенберга ($\mathcal{O}(n^3)$), QR разложение которой строится за $\mathcal{O}(n^2)$ итераций. Кроме того, используются сдвиги, которые позволяют ускорить сходимость.



## QR-алгоритм

[![](qr_alg.png)](https://fmin.xyz/docs/visualizations/qr_algorithm.mp4)