---
title: Градиентный спуск и как его можно ускорить
author: Даня Меркулов
institute: МФТИ. AI360
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back10.jpeg}
---

# Градиентный спуск

## Направление локального наискорейшего спуска

:::: {.columns}
::: {.column width="40%"}

Рассмотрим линейное приближение дифференцируемой функции $f$ вдоль некоторого направления $h, \|h\|_2 = 1$:

. . .

$$
f(x + \alpha h) = f(x) + \alpha \langle f'(x), h \rangle + o(\alpha)
$$

. . .

Мы хотим, чтобы направление $h$ было направлением убывания функции:

$$
f(x + \alpha h) < f(x)
$$

$$
f(x) + \alpha \langle f'(x), h \rangle + o(\alpha) < f(x)
$$

. . .

перейдя к пределу при $\alpha \rightarrow 0$:

$$
\langle f'(x), h \rangle \leq 0
$$

:::

. . .

::: {.column width="60%"}

Также из неравенства Коши-Буняковского-Шварца:

$$
\begin{split}
|\langle f'(x), h \rangle | &\leq \| f'(x) \|_2 \| h \|_2 \\
\langle f'(x), h \rangle &\geq -\| f'(x) \|_2 \| h \|_2 = -\| f'(x) \|_2
\end{split}
$$

. . .

Таким образом, направление антиградиента

$$
h = -\dfrac{f'(x)}{\|f'(x)\|_2}
$$

даёт направление **наискорейшего локального** убывания функции $f$.

. . .

Результатом этого является метод градиентного спуска:

$$
x_{k+1} = x_k - \alpha f'(x_k)
$$

:::
::::

. . .

## Сходимость градиентного спуска

Сходимость градиентного спуска сильно зависит от выбора шага $\alpha$:

[![](gd_2d.pdf)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD_2d_visualization.ipynb)

## Наискорейший спуск

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_{k+1}) = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$
Более теоретический, чем практический подход. Он также позволяет анализировать сходимость, но часто точный поиск вдоль направления может быть сложным, если вычисление функции занимает слишком много времени или стоит дорого.

Интересное теоретическое свойство этого метода заключается в том, что каждая следующая итерация ортогональна предыдущей:
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

. . .

Условие оптимальности:

. . .

$$
\nabla f(x_{k+1})^\top \nabla f(x_k) = 0
$$
:::
::: {.column width="20%"}

![Наискорейший спуск](GD_vs_Steepest.pdf)

[Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

# Сходимость для сильно выпуклых квадратичных функций

## Сдвиг координат

:::: {.columns}

::: {.column width="70%"}

Рассмотрим следующую квадратичную оптимизационную задачу:
$$
\label{problem}
\min\limits_{x \in \mathbb{R}^d} f(x) =  \min\limits_{x \in \mathbb{R}^d} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ where }A \in \mathbb{S}^d_{++}.
$$

. . .

* Без ограничения общности можно положить $c = 0$, что не повлияет на процесс оптимизации.
* Второй шаг: представим матрицу $A$ в виде $A = Q \Lambda Q^T$.
* Покажем, что мы можем сделать замену координат, чтобы сделать анализ немного проще. Пусть $\hat{x} = Q^T(x - x^*)$, где $x^*$ - точка минимума исходной функции, определяемая как $Ax^* = b$. При этом $x = Q\hat{x} + x^*$.
    $$
    \begin{split}
    \uncover<+->{ f(\hat{x}) &= \frac12  (Q\hat{x} + x^*)^\top  A (Q\hat{x} + x^*) - b^\top  (Q\hat{x} + x^*) \\}
    \uncover<+->{ &= \frac12 \hat{x}^T Q^TAQ\hat{x} + \frac12 (x^*)^T A (x^*) + (x^*)^TAQ\hat{x} - b^T Q\hat{x} - b^T x^*\\}
    \uncover<+->{ &= \frac12 \hat{x}^T \Lambda \hat{x} + \frac12 (x^*)^T A (x^*) + (x^*)^TAQ\hat{x} - (x^*)^T A^TQ\hat{x} - (x^*)^T A x^*\\}
    \uncover<+->{ &= \frac12 \hat{x}^T \Lambda \hat{x} - \frac12 (x^*)^T A x^*} \uncover<+->{\simeq \frac12 \hat{x}^T \Lambda \hat{x} }
    \end{split}
    $$

:::
::: {.column width="30%"}
![](coordinate_shift.pdf)
:::
::::

## Сходимость

Теперь мы можем работать с функцией $f(x) = \frac12 x^T \Lambda x$ с $x^* = 0$ без потери общности (убрав $\hat{x}$)

:::: {.columns}
::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{x^{k+1} &= x^k - \alpha^k \nabla f(x^k)} 
\uncover<+->{= x^k - \alpha^k \Lambda x^k \\ } 
\uncover<+->{&= (I - \alpha^k \Lambda) x^k \\ }
\uncover<+->{ x^{k}_{(i)} &= (1 - \alpha^k \lambda_{(i)}) x^k_{(i)} \text{ Для $i$-й координаты} \\ }
\uncover<+->{  x^{k}_{(i)} &= (1 - \alpha^k \lambda_{(i)})^k x^0_{(i)}}
\end{split}
$$
\uncover<+->{
Используем постоянный шаг $\alpha^k = \alpha$. Условие сходимости:
$$
\rho(\alpha) = \max_{i} |1 - \alpha \lambda_{(i)}| < 1
$$
Помним, что $\lambda_{\text{min}} = \mu > 0, \lambda_{\text{max}} = L \geq \mu$.}

:::: {.columns}
::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{ |1 - \alpha \mu| &< 1 \\ }
\uncover<+->{ -1 < 1 &- \alpha \mu < 1 \\ }
\uncover<+->{ \alpha < \frac{2}{\mu} \quad & \quad \alpha\mu > 0}
\end{split}
$$
:::
::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{ |1 - \alpha L| &< 1 \\ }
\uncover<+->{ -1 < 1 &- \alpha L < 1 \\ }
\uncover<+->{ \alpha < \frac{2}{L} \quad & \quad \alpha L > 0}
\end{split}
$$
:::
::::

. . .

$\alpha < \frac{2}{L}$ необходимо для сходимости.

:::

. . .

::: {.column width="50%"}
Теперь мы хотели бы настроить $\alpha$ для выбора лучшего (наименьшего) коэффициента сходимости

$$
\begin{split}
\uncover<+->{ \rho^* &=  \min_{\alpha} \rho(\alpha) } \uncover<+->{  = \min_{\alpha} \max_{i} |1 - \alpha \lambda_{(i)}| \\ }
\uncover<+->{ &=  \min_{\alpha} \left\{|1 - \alpha \mu|, |1 - \alpha L| \right\} \\ }
\uncover<+->{ \alpha^* &: \quad  1 - \alpha^* \mu = \alpha^* L - 1 \\ }
\uncover<+->{ & \alpha^* = \frac{2}{\mu + L} } \uncover<+->{ \quad \rho^* = \frac{L - \mu}{L + \mu} \\ }
\uncover<+->{ x^{k}_{(i)} &= \left( \frac{L - \mu}{L + \mu} \right)^k x^0_{(i)} \\}
\uncover<+->{ \|x^{k}\|_2 &\leq \left( \frac{L - \mu}{L + \mu} \right)^k \|x^0\|_2 } \uncover<+->{ \quad f(x^{k}) \leq \left( \frac{L - \mu}{L + \mu} \right)^{2k} f(x^0)}
\end{split}
$$
:::
::::

## Сходимость

Таким образом, мы имеем линейную сходимость в домене с коэффициентом $\frac{\varkappa - 1}{\varkappa + 1} = 1 - \frac{2}{\varkappa + 1}$, где $\varkappa = \frac{L}{\mu}$ называется *числом обусловленности* квадратичной задачи.

| $\varkappa$ | $\rho$ | Итерации для уменьшения ошибки в 10 раз | Итерации для уменьшения невязки в 10 раз |
|:-:|:-:|:-----------:|:-----------:|
| $1.1$ | $0.05$ | $1$ | $1$ |
| $2$ | $0.33$ | $3$ | $2$ |
| $5$ | $0.67$ | $6$ | $3$ |
| $10$ | $0.82$ | $12$ | $6$ |
| $50$ | $0.96$ | $58$ | $29$ |
| $100$ | $0.98$ | $116$ | $58$ |
| $500$ | $0.996$ | $576$ | $288$ |
| $1000$ | $0.998$ | $1152$ | $576$ |

## Число обусловленности $\varkappa$

[![](condition_number_gd.pdf)](https://fmin.xyz/docs/visualizations/condition_number_gd.mp4)

