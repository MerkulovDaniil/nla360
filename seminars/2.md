---
title: Матрицы. Матричные нормы. Матричное умножение. Унитарные матрицы
author: Даня Меркулов
institute: МФТИ. AI360
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back2.jpeg}
---

# Базовые понятия линейной алгебры

## Векторы и матрицы

Мы будем считать, что все векторы являются столбцами по умолчанию. Пространство векторов длины $n$ обозначается $\mathbb{R}^n$, а пространство вещественных $m \times n$ матриц обозначается $\mathbb{R}^{m \times n}$.

[^1]: Подробный вводный курс по прикладной линейной алгебре можно найти в книге [Introduction to Applied Linear Algebra -- Vectors, Matrices, and Least Squares](https://web.stanford.edu/~boyd/vmls/) - книга Стивена Бойда и Ливена Ванденбергена, которая указана в источнике. Также полезной книгой по линейной алгебре является приложение A в книге Numerical Optimization Джорджа Носедаля и Стивена Райта.

$$
x = \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix} \quad x^T = \begin{bmatrix}
x_1 & x_2 & \dots & x_n
\end{bmatrix} \quad x \in \mathbb{R}^n, x_i \in \mathbb{R}
$$ {#eq-vector}

. . .

Аналогично, если $A \in \mathbb{R}^{m \times n}$ мы обозначаем транспонирование как $A^T \in \mathbb{R}^{n \times m}$:
$$
A = \begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix} \quad A^T = \begin{bmatrix}
a_{11} & a_{21} & \dots & a_{m1} \\
a_{12} & a_{22} & \dots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \dots & a_{mn}
\end{bmatrix} \quad A \in \mathbb{R}^{m \times n}, a_{ij} \in \mathbb{R}
$$
Мы будем писать $x \geq 0$ и $x \neq 0$ для обозначения покомпонентного неравенства.

---

![Эквивалентные представления вектора](vector.pdf){#fig-vector}

---

Матрица $A$ называется симметричной, если $A = A^T$. Обозначается как $A \in \mathbb{S}^n$ (множество квадратных симметричных матриц размерности $n$). Заметим, что только квадратная матрица может быть симметричной по определению.

. . .

Матрица $A \in \mathbb{S}^n$ называется **положительно (отрицательно) определенной**, если для всех $x \neq 0 : x^T Ax > (<) 0$. Обозначается как $A \succ (\prec) 0$. Множество таких матриц обозначается как $\mathbb{S}^n_{++} (\mathbb{S}^n_{- -})$.

. . .

Матрица $A \in \mathbb{S}^n$ называется **положительно (отрицательно) полуопределенной**, если для всех $x : x^T Ax \geq (\leq) 0$. Обозначается как $A \succeq (\preceq) 0$. Множество таких матриц обозначается как $\mathbb{S}^n_{+} (\mathbb{S}^n_{-})$.

:::{.callout-question}
Верно ли, что положительно определенная матрица имеет все положительные элементы?
:::

. . .

:::{.callout-question}
Верно ли, что если матрица симметрична, то она положительно определена?
:::

. . .

:::{.callout-question}
Верно ли, что если матрица положительно определена, то она симметрична?
:::


---

## Умножение матриц

Пусть $A$ — матрица размера $m \times n$, и $B$ — матрица размера $n \times p$, и пусть произведение $AB$ задается как:
$$
C = AB
$$
тогда $C$ — матрица размера $m \times p$, с элементом $(i, j)$ задаваемым как:
$$
c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}.
$$

Эта операция в наивной реализации требует $\mathcal{O}(n^3)$ арифметических операций, где $n$ — наибольший размер матриц.

. . .

:::{.callout-question}
Можно ли умножить две матрицы быстрее, чем за $\mathcal{O}(n^3)$? Как насчет $\mathcal{O}(n^2)$, $\mathcal{O}(n)$?
:::

---

## Умножение матрицы на вектор

Пусть $A$ — матрица размера $m \times n$, и $x$ — вектор длины $n$, тогда $i$-й компонент произведения:
$$
z = Ax
$$
определяется как:
$$
z_i = \sum_{k=1}^n a_{ik}x_k
$$

Эта операция в наивной реализации требует $\mathcal{O}(n^2)$ арифметических операций, где $n$ — наибольший размер матриц.

Заметим, что:

* $C = AB \quad C^T = B^T A^T$
* $AB \neq BA$
* $e^{A} =\sum\limits_{k=0}^{\infty }{1 \over k!}A^{k}$
* $e^{A+B} \neq e^{A} e^{B}$ (но если $A$ и $B$ — коммутирующие матрицы, то есть $AB = BA$, то $e^{A+B} = e^{A} e^{B}$)
* $\langle x, Ay\rangle = \langle A^T x, y\rangle$

## Нормы

Норма — это **количественная мера маленькости вектора** и обычно обозначается как $\Vert x \Vert$.

Норма должна удовлетворять определенным свойствам:

1.  $\Vert \alpha x \Vert = \vert \alpha\vert \Vert x \Vert$, $\alpha \in \mathbb{R}$
2.  $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert$ (неравенство треугольника)
3.  Если $\Vert x \Vert = 0$ то $x = 0$

. . .

Расстояние между двумя векторами определяется как:
$$ 
d(x, y) = \Vert x - y \Vert. 
$$
Наиболее широко используемой нормой является **евклидова норма**:
$$
\Vert x \Vert_2 = \sqrt{\sum_{i=1}^n |x_i|^2},
$$
которая соответствует расстоянию в нашей повседневной жизни. Если векторы имеют комплексные элементы, мы используем их модуль. Евклидова норма, или $2$-норма, является подклассом важного класса $p$-норм:

$$
\Vert x \Vert_p = \Big(\sum_{i=1}^n |x_i|^p\Big)^{1/p}. 
$$

---

## $p$-норма вектора

Существуют два очень важных частных случая. Бесконечность норма, или норма Чебышёва, определяется как максимальный абсолютный элемент:
$$
\Vert x \Vert_{\infty} = \max_i | x_i| 
$$

. . .

$L_1$ норма (или **манхэттенское расстояние**) определяется как сумма модулей элементов вектора:
$$
\Vert x \Vert_1 = \sum_i |x_i| 
$$

. . .

$L_1$ норма играет очень важную роль: она связана с методами **compressed sensing**, которые стали одной из популярных тем исследований в середине 00-х. Код для изображения ниже доступен [*здесь:*](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Balls_p_norm.ipynb). Также можно посмотреть [*этот*](https://fmin.xyz/docs/theory/balls_norm.mp4) видео.

![Шары в разных нормах на плоскости](p_balls.pdf)

# Матричные нормы

## Матричные нормы

$\Vert \cdot \Vert$ называется **матричной нормой**, если она является векторной нормой на векторном пространстве $n \times m$ матриц:

1. $\|A\| \geq 0$ и если $\|A\| = 0$, то $A = O$
3. $\|\alpha A\| = |\alpha| \|A\|$
4. $\|A+B\| \leq \|A\| + \|B\|$ (неравенство треугольника)

* Дополнительно, некоторые нормы могут удовлетворять **свойству субмультипликативности**:
    $$
    \Vert A B \Vert \leq \Vert A \Vert \Vert B \Vert
    $$
    Эти нормы называются **субмультипликативными нормами**.

* Пример несубмультипликативной нормы — норма Чебышёва:
    $$
    \|A\|_C = \displaystyle{\max_{i,j}}\, |a_{ij}|
    $$

## Пример {.t}

:::{.callout-question}

### Придумайте контрпример

Покажите, что норма Чебышёва не является субмультипликативной.
:::


## Нормы матриц

В некотором смысле нет большой разницы между матрицами и векторами (можно векторизовать матрицу), таким образом вводится простейшая норма матрицы **Фробениуса**:
$$
\Vert A \Vert_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}
$$

. . .

Спектральная норма, $\Vert A \Vert_2$ является одной из наиболее широко используемых норм матриц (вместе с нормой Фробениуса).
$$
\Vert A \Vert_2 = \sup_{x \ne 0} \frac{\Vert A x \Vert_2}{\Vert x \Vert_{2}},
$$
Её нельзя вычислить непосредственно из элементов, используя простую формулу, как норму Фробениуса, однако существуют эффективные алгоритмы для ее вычисления. Она непосредственно связана с **сингулярным разложением** (SVD) матрицы. Она удовлетворяет следующему свойству:
$$
\Vert A \Vert_2 = \sigma_1(A) = \sqrt{\lambda_{\max}(A^TA)}
$$
где $\sigma_1(A)$ является наибольшим сингулярным значением матрицы $A$.

## Операторные нормы

* Наиболее важный класс матричных норм — это класс **операторных норм**. Они определяются как
    $$
    \Vert A \Vert_{\alpha\to\beta} = \sup_{x \ne 0} \frac{\Vert A x \Vert_\alpha}{\Vert x \Vert_\beta},
    $$
    где $\Vert \cdot \Vert_\alpha$ и $\Vert \cdot \Vert_\beta$ являются **векторными нормами**.

* Легко проверить, что они субмультипликативны, если $\Vert \cdot \Vert_\alpha = \Vert \cdot \Vert_\beta$. Иначе они не субмультипликативны, придумайте пример.

* **Норма Фробениуса** является матричной нормой, но не является операторной нормой, т.е. нельзя найти $\Vert \cdot \Vert_\alpha$ и $\| \cdot \|_{\beta}$ такие, чтобы они её порождали. 
* Это нетривиальный факт и общий критерий матричной нормы для того, чтобы быть операторной нормой можно найти в [Теореме 6 и следствии 4](http://www.sciencedirect.com/science/article/pii/S0024379515004346).

## Пример {.t}

:::{.callout-question}

Покажите, что операторная норма $\Vert \cdot \Vert_{\alpha \to \alpha}$ является субмультипликативной.
:::


## Матричные $p$-нормы

Важный частный случай операторных норм — это матричные $p$-нормы, которые определяются для $\|\cdot\|_\alpha = \|\cdot\|_\beta = \|\cdot\|_p$. <br>

Среди всех $p$-норм три наиболее часто используемые нормы:  

* $p = 1, \quad \Vert A \Vert_{1} = \displaystyle{\max_j \sum_{i=1}^n} |a_{ij}|$.

* $p = 2, \quad$ спектральная норма, обозначаемая как $\Vert A \Vert_2$.

* $p = \infty, \quad \Vert A \Vert_{\infty} = \displaystyle{\max_i \sum_{j=1}^m} |a_{ij}|$.

## Пример {.t}

Покажите, что $\Vert A \Vert_1 = \max_j \sum_{i=1}^n |a_{ij}|$.

## Пример {.t}

Покажите, что $\Vert A \Vert_{\infty} = \max_i \sum_{j=1}^m |a_{ij}|$.

## Скалярное произведение

Стандартное **скалярное (внутреннее) произведение** между векторами $x$ и $y$ из $\mathbb{R}^n$ определяется как:
$$
\langle x, y \rangle = x^T y = \sum\limits_{i=1}^n x_i y_i = y^T x =  \langle y, x \rangle
$$
Здесь $x_i$ и $y_i$ являются $i$-ми компонентами соответствующих векторов.

::: {.callout-example}
Докажите, что можно перемещать матрицу внутри скалярного произведения с транспонированием: $\langle x, Ay\rangle = \langle A^Tx, y\rangle$ и $\langle x, yB\rangle = \langle xB^T, y\rangle$
:::

## Скалярное произведение матриц

Стандартное **скалярное (внутреннее) произведение** между матрицами $X$ и $Y$ из $\mathbb{R}^{m \times n}$ определяется как:

$$
\langle X, Y \rangle = \text{tr}(X^T Y) = \sum\limits_{i=1}^m\sum\limits_{j=1}^n X_{ij} Y_{ij} =  \text{tr}(Y^T X) =  \langle Y, X \rangle
$$

::: {.callout-question} 
Существует ли связь между нормой Фробениуса $\Vert \cdot \Vert_F$ и скалярным произведением между матрицами $\langle \cdot, \cdot \rangle$?
:::

## Пример {.t}

Упростите следующее выражение:

$$
\sum\limits_{i=1}^n \langle S^{-1} a_i, a_i \rangle,
$$
где $S = \sum\limits_{i=1}^n a_ia_i^T, a_i \in \mathbb{R}^n, \det(S) \neq 0$

# Ортогональные матрицы

## Ортогональные (унитарные) матрицы

* Пусть $U$ — матрица размера $n \times n$, и $\Vert U z \Vert_2 = \Vert z \Vert_2$ для всех $z$. 

* Это может произойти **тогда и только тогда**, когда
    $$
    U^T U = I_n,
    $$
    где $I_n$ — единичная матрица $n \times n$.

* Квадратная матрица размера $n \times n$ называется **ортогональной**, если
    $$
    U^T U = UU^T = I_n, 
    $$
    что означает, что столбцы и строки ортогональной матрицы образуют ортонормированный базис в $\mathbb{R}^{n}$.

* Для прямоугольных матриц размера $m\times n$ ($n\not= m$) только одно из равенств может выполняться
    * $U^T U = I_n$ –– матрица $U$ называется **матрицей с ортогональными столбцами** для $m>n$
    * $UU^T = I_m$  –– матрица $U$ называется **матрицей с ортогональными строками** для $m<n$

<!-- ## Скелетное разложение матрицы

:::: {.columns}

::: {.column width="70%"}
Простое, но очень интересное разложение — скелетное, которое может быть записано в двух формах:

$$
A = U V^T \quad A = \hat{C}\hat{A}^{-1}\hat{R}
$$

. . .

Последнее выражение отсылает к любопытному факту: вы можете случайным образом выбрать $r$ линейно независимых столбцов матрицы и любых $r$ линейно независимых строк матрицы и хранить только их, при этом имея возможность точно восстановить всю матрицу.

. . .

Применения скелетного разложения:

* Упрощение модели, сжатие данных и ускорение вычислений в численных методах: для матрицы ранга $r$ с $r \ll n, m$ достаточно хранить $\mathcal{O}((n + m)r) \ll nm$ элементов.
* Извлечение признаков в машинном обучении, где также известно как матричное разложение.
:::

::: {.column width="30%"}
![Иллюстрация скелетного разложения](skeleton.pdf){#fig-skeleton}
:::

::::


## LoRA: Low-Rank Adaptation of Large Language Models ([arXiv:2106.09685](https://arxiv.org/pdf/2106.09685))

Так как современные LLM слишком большие, чтобы поместиться в память обычной GPU, мы используем некоторые трюки, чтобы сделать потребление памяти меньше. Один из самых популярных трюков — LoRA (Low-Rank Adaptation of Large Language Models).

:::: {.columns}
::: {.column width="55%"}
Предположим, что у нас есть матрица $W \in \mathbb{R}^{d \times k}$ и мы хотим выполнить следующее обновление:
$$
W = W_0 + \Delta W.
$$
Основная идея LoRA — разложить обновление $\Delta W$ на две низкоранговые матрицы:

\begin{gather*}
W = W_0 + \Delta W = W_0 + BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, \\
rank(A) = rank(B) = r \ll \min\{d, k\}.
\end{gather*}

Проверьте [\faPython\ ноутбук](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s1_lora_trump.ipynb) для примера реализации LoRA.

:::

::: {.column width="45%"}
![Иллюстрация LoRA](lora.pdf){width=100%}
:::
:::: -->