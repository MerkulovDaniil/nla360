---
title: Рандомизированная линейная алгебра
author: Даня Меркулов
institute: МФТИ. AI360
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back8.jpeg}
---

# Рандомизированная линейная алгебра

## Проверка матричного равенства: Алгоритм Фрейвалдса

Пусть нам даны три матрицы $A, B, C \in \mathbb{R}^{n \times n}$, и мы хотим проверить, равенство $AB = C$. Наивный способ - просто перемножить $A$ и $B$ за $\mathcal{O}(n^3)$ операций и сравнить с $C$. 

. . .

Алгоритм Фрейвалдса показывает, что это можно сделать гораздо быстрее - за $\mathcal{O}(kn^2)$ операций при вероятности ошибки не более $1/2^k$.

. . .

**Идея** алгоритма:

1. Сгенерировать случайный вектор $r \in \mathbb{R}^n$.
2. Вычислить произведения $Br$ и $Cr$.
3. Вычислить $A(Br)$ и сравнить полученный вектор с $Cr$.
4. Если $A(Br) \neq Cr$, то точно $AB \neq C$.
5. Если $A(Br) = Cr$, то с вероятностью **не менее** $1 - 1/2$ мы угадали правильно. Для снижения вероятности ошибки до $1/2^k$ повторяем процедуру $k$ раз с разными случайными векторами.

. . .

**Сложность** каждого шага: $\mathcal{O}(n^2)$ (домножение на вектор), поэтому общее время $\mathcal{O}(kn^2)$.

## Рандомизированное решение линейной системы

Рассмотрим СЛАУ
$$
Ax = b,
$$
где $A \in \mathbb{R}^{m \times n}$. Метод Качмарца (Kaczmarz) обновляет приближение $x_k$ путём выборки случайной строки $i$ (обычно с вероятностью пропорциональной $\|a_i\|^2$):

$$
x_{k+1} = x_k - \frac{a_i^T x_k - b_i}{\|a_i\|^2} a_i.
$$

. . .

- Если система совместна, метод сходится к точному решению.  
- Если система переопределена или шумна, то можно показать сходимость к решению наилучшего приближения.  
- С точки зрения SGD это шаг стохастического градиента для задачи минимизации $\|Ax - b\|^2$.

. . .

**Скорость сходимости**:
$$
\mathbb{E}[\|x_{k+1}-x^*\|^2] \leq \left(1 - \frac{1}{\kappa_F^2(A)}\right)
\mathbb{E}[\|x_k - x^*\|^2],
$$
где $\kappa_F(A)=\frac{\|A\|_F}{\sigma_{\min}(A)}$.

## Рандомизированное матричное умножение

Хотим приблизить произведение $AB$, где $A \in \mathbb{R}^{m \times p}$, $B \in \mathbb{R}^{p \times n}$. Стоимость точного умножения: $\mathcal{O}(mnp)$. Рандомизированный подход даёт приближение:
$$
AB \approx \sum_{t=1}^k \frac{1}{k \, p_{i_t}}\, A^{(i_t)}\, B_{(i_t)},
$$

. . .

где $A^{(i_t)}$ - $i_t$-й столбец $A$, а $B_{(i_t)}$ - $i_t$-я строка $B$, а $p_{i_t}$ - вероятность выбора $i_t$-го столбца/строки. Обычно $p_i$ пропорциональны $\|A^{(i)}\|\|B_{(i)}\|$ (или другой норме).

. . .

**Идея**:  

1. По нормам столбцов $A$ (и строк $B$) выбираем $k$ столбцов-строк.  
2. Усреднённая сумма полученных ранга-$k$ матриц приближает всё произведение.

. . .

Сложность: $\mathcal{O}(mnk)$, если $k \ll p$, экономим по сравнению с $\mathcal{O}(mnp)$.

## Оценка следа матрицы методом Хатчинсона ^[[A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines - M.F. Hutchinson, 1990](https://www.tandfonline.com/doi/abs/10.1080/03610919008812866)]

Пусть $X \in \mathbb{R}^{d \times d}$ и $v \in \mathbb{R}^d$ - случайный вектор такой, что $\mathbb{E}[vv^T] = I$. Тогда,

:::: {.columns}
::: {.column width="30%"}

$$
\mathrm{Tr}(X) = \mathbb{E}[v^TXv] = \frac{1}{V}\sum_{i=1}^{V}v_i^TXv_i.
$$

Интересно, что для оценки следа матрицы иногда можно не знать самой матрицы. Например, в современных моделях машинного обучения можно вычислять произведение гессиана функции на произвольный вектор, не вычисляя сам гессиан с помощью автоматического дифференцирования. [\faPython \ Пример](https://colab.research.google.com/drive/1aLx_-Sv2tTTKz0NCEFcedqQyopBUczJH#scrollTo=DZTgqcHoa8O3)

:::
::: {.column width="70%"}
![[Source](https://docs.backpack.pt/en/master/use_cases/example_trace_estimation.html)](Hutchinson_trace_est.pdf)
:::
::::

## Оценка следа матрицы методом Жирара

**Метод Жирара**:  
- Предшественник метода Хатчинсона, где вектор $w$ берём из $\mathcal{N}(0, I)$ (гауссовский).  
- Дисперсия получается немного больше, чем у Хатчинсона (у хатчинсона минимальная дисперсия).  

. . .

**Intrinsic dimension (intdim)**:  
Для симметричной положительно определённой матрицы $A$ вводится понятие  
$$
\mathrm{intdim}(A) = \frac{\mathrm{Tr}(A)}{\|A\|_F}.
$$

. . .

- Минимальное значение равно 1.
- Максимальное - при всех сингулярных (или собственных) значениях равных, тогда $\mathrm{intdim}(A)$ может достичь $\sqrt{\mathrm{rank}(A)}$.  

. . .
С помощью этой величины можно оценивать вероятность больших отклонений случайной оценки следа.

## Рандомизированный SVD

Напомним, что SVD матрицы $A \in \mathbb{R}^{m \times n}$:  
$$
A = U \Sigma V^T.
$$
Для больших $m$ и $n$ полное вычисление SVD занимает $\mathcal{O}(\min\{mn^2, m^2n\})$. 

**Рандомизированный подход** (Halko et al., 2011):

1. Выбираем $G \in \mathbb{R}^{n \times (k+p)}$ со случайными элементами.
2. Считаем $Y = A \cdot G$ и делаем QR-разложение $Y = Q R$.
3. Утверждается, что $Q Q^T A \approx A$ при хорошей выборке и $k + p$ надёжно покрывают ведущие сингулярные компоненты.
4. Строим $B = Q^T A$, у которого размер $(k+p) \times n$.
5. Вычисляем точное SVD для $B = \hat{U} \hat{\Sigma} \hat{V}^T$.
6. Тогда $U = Q \hat{U}$, и получаем искомое разложение (приближённое).

Параметр $p$ - **oversampling**, чтобы уменьшить ошибку.