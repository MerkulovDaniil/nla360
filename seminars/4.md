---
title: SVD. Eigenfaces. PCA. Линейные системы
author: Даня Меркулов
institute: МФТИ. AI360
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back4.jpeg}
---


# Сингулярное разложение матрицы

## Сингулярное разложение матрицы

:::: {.columns}
::: {.column width="60%"}
![](SVD.pdf)
:::

::: {.column width="40%"}
Для любой матрицы $A \in \mathbb{R}^{m \times n}$ существует разложение:
$$
A = U\Sigma V^*,
$$
где

* $U \in \mathbb{R}^{m \times m}$ - унитарная матрица левых сингулярных векторов
* $\Sigma \in \mathbb{R}^{m \times n}$ - диагональная матрица сингулярных чисел $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_{\min(m,n)} \geq 0$
* $V \in \mathbb{R}^{n \times n}$ - унитарная матрица правых сингулярных векторов
* Сингулярные числа единственны. Если все сингулярные числа различны, то разложение единственно с точностью до унитарной диагональной матрицы $D$: $U \Sigma V^* = U D \Sigma (VD)^* = U \Sigma V^*$.
:::
::::

## Теорема Экарта-Янга

Наилучшее приближение низкого ранга может быть вычислено с помощью SVD.

:::{.callout-tip appearance="simple"}
Пусть $r < \text{rank}(A)$, $A_r = U_r \Sigma_r V_r^*$. Тогда

$$ \min_{\text{rank}(B)=r} \|A - B\|_2 = \|A - A_r\|_2 = \sigma_{r+1}. $$

То же самое верно для $\|\cdot\|_F$, но $\|A - A_r\|_F = \sqrt{\sigma_{r+1}^2 + \dots + \sigma_{\min (n,m)}^2}$.
:::

**Следствие:** вычисление наилучшего приближения ранга $r$ эквивалентно установке $\sigma_{r+1}= 0, \ldots, \sigma_K = 0$. Ошибка 

$$ \min_{A_r} \Vert A - A_r \Vert_2 = \sigma_{r+1}, \quad \min_{A_r} \Vert A - A_r \Vert_F = \sqrt{\sigma_{r+1}^2 + \dots + \sigma_{K}^2} $$

вот почему важно смотреть на скорость убывания сингулярных значений.

## Пример 1{.t}

Найдите SVD следующей матрицы:

$$A = \begin{bmatrix} 1\\2\\3 \end{bmatrix}$$ 

. . .

:::: {.columns}

::: {.column width="50%"}
**Решение**

1. Простейшая форма SVD выглядит так:
  $$
  A = U \Sigma V^T = \begin{bmatrix} \frac{1}{\sqrt{14}} \\ \frac{2}{\sqrt{14}} \\ \frac{3}{\sqrt{14}} \end{bmatrix} \begin{bmatrix} \sqrt{14} \end{bmatrix}  \begin{bmatrix} 1 \end{bmatrix} 
  $$
2. Однако, если вы хотите использовать полную форму с квадратными сингулярными матрицами:
  $$
  A = U \Sigma V^T = \begin{bmatrix}
  \frac{1}{\sqrt{14}} & \frac{1}{\sqrt{3}} & \frac{-5}{\sqrt{42}} \\
  \frac{2}{\sqrt{14}} & \frac{1}{\sqrt{3}} & \frac{4}{\sqrt{42}} \\
  \frac{3}{\sqrt{14}} & \frac{-1}{\sqrt{3}} & \frac{-1}{\sqrt{42}}
  \end{bmatrix} \begin{bmatrix} \sqrt{14} \\ 0 \\ 0 \end{bmatrix}  \begin{bmatrix} 1 \end{bmatrix} 
  $$

:::

::: {.column width="50%"}
3. Вычислим $A^T A$:
  $$
  A^T A = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = 1^2 + 2^2 + 3^2 = 14.
  $$
  Сингулярные значения $\sigma_i$ являются квадратными корнями из собственных значений $A^T A$. Поскольку $A^T A$ является $1 \times 1$ матрицей со значением 14, сингулярное значение равно $\sigma = \sqrt{14}.$
4. Поскольку $V$ является $n \times n$ ортогональной матрицей ($1 \times 1$ в этом случае), она может быть $V = [1]$ (или $V = [-1]$). Мы выбираем $V = [1]$.
:::


::::

## Пример 2{.t}

Решите упражнение [Eigenfaces](https://nla360.fmin.xyz/files/Eigenfaces_exercise.html)

# Метод главных компонент

## Общая идея уменьшения размерности


## Метод главных компонент как задача оптимизации

:::: {.columns}

::: {.column width="40%"}

[![](PCA_anim.png)](http://ids.skoltech.ru/PCA_animation.mp4)

:::
::: {.column width="60%"}

Первая компонента должна быть определена так, чтобы максимизировать дисперсию (вариабельность) проекции. Предположим, что мы уже нормализовали данные, т.е. $\sum_i a_i = 0$, тогда дисперсия выборки станет суммой всех квадратов проекций точек данных на наш вектор ${\mathbf{w}}_{(1)}$, что приводит к следующей задаче оптимизации:

. . .

$$
\mathbf{w}_{(1)}={\underset  {\Vert {\mathbf{w}}\Vert =1}{\operatorname{\arg \,max}}}\,\left\{\sum _{i}\left({\mathbf{a}}^{\top}_{(i)}\cdot {\mathbf{w}}\right)^{2}\right\}
$$

. . .

$$
\mathbf{w}_{(1)}={\underset {\Vert \mathbf{w} \Vert =1}{\operatorname{\arg \,max} }}\,\{\Vert \mathbf{Aw} \Vert ^{2}\}={\underset {\Vert \mathbf{w} \Vert =1}{\operatorname{\arg \,max} }}\,\left\{\mathbf{w}^{\top}\mathbf{A^{\top}} \mathbf{Aw} \right\}
$$

. . .

так как мы ищем единичный вектор, мы можем переформулировать задачу:
$$
\mathbf{w} _{(1)}={\operatorname{\arg \,max} }\,\left\{ \frac{\mathbf{w}^{\top}\mathbf{A^{\top}} \mathbf{Aw} }{\mathbf{w}^{\top}\mathbf{w} }\right\}
$$

. . .

[Известно](https://en.wikipedia.org/wiki/Rayleigh_quotient), что для положительно полуопределенной матрицы $A^\top A$ такой вектор это **собственный вектор** $A^\top A$, соответствующий наибольшему собственному значению.
:::
::::

## Вывод метода

:::: {.columns}

::: {.column width="50%"}
Таким образом, мы можем заключить, что следующее отображение:
$$
\underset{n \times k}{\Pi} = \underset{n \times d}{A} \cdot \underset{d \times k}{W} 
$$

. . .

описывает проекцию данных на $k$ главных компонент, где $W$ содержит первые (по величине собственных значений) $k$ собственных векторов $A^\top A$.

. . .

Теперь мы кратко выведем, как SVD может привести нас к PCA.

Сначала мы запишем SVD нашей матрицы:
$$
A = U \Sigma W^\top
$$

. . .

и транспонируем его:
$$
\begin{aligned}
A^\top
&= (U \Sigma W^\top)^\top \\
&= (W^\top)^\top \Sigma^\top U^\top \\
&= W \Sigma^\top U^\top \\
&= W \Sigma U^\top
\end{aligned}
$$
:::

. . .

::: {.column width="50%"}
Теперь рассмотрим матрицу $A A^\top$:
$$
\begin{aligned}
A^\top A
&= (W \Sigma U^\top)(U \Sigma V^\top)  \\
&= W \Sigma I \Sigma W^\top \\
&= W \Sigma \Sigma W^\top \\
&= W \Sigma^2 W^\top
\end{aligned}
$$
которая соответствует разложению матрицы $A^\top A$, где $W$ - матрица собственных векторов $A^\top A$, а $\Sigma^2$ содержит собственные значения $A^\top A$.

. . .

В итоге:
$$
\begin{aligned}
\Pi &= A \cdot W =\\
 &= U \Sigma W^\top W = U \Sigma
\end{aligned}
$$
Последняя формула дает нам простой способ вычислить PCA через SVD с любым количеством главных компонент:
$$
\Pi_r = U_r \Sigma_r
$$
:::

::::

## PCA. Упражнение 1

:::: {.columns}

::: {.column width="40%"}

[![](pca_ex1.png)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Dimensionality_reduction.ipynb#scrollTo=PzL3k9iZJLc2)

:::
::: {.column width="60%"}
Что могло пойти не так с этим PCA?
:::
::::


## PCA. Упражнение 2

:::: {.columns}

::: {.column width="40%"}

[![](pca_ex2.png)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Dimensionality_reduction.ipynb#scrollTo=PzL3k9iZJLc2)

:::
::: {.column width="60%"}
Что могло пойти не так с этим PCA?
:::
::::

## PCA. Упражнение 3

:::: {.columns}

::: {.column width="40%"}

[![](pca_ex3.png)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Dimensionality_reduction.ipynb#scrollTo=PzL3k9iZJLc2)

:::
::: {.column width="60%"}
Что могло пойти не так с этим PCA?
:::
::::

## Iris dataset variance

![[source](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)](iris.png)

## Iris dataset variance

![Iris dataset explained variance](iris_vars.pdf){width=70%}

## Wine dataset variance

![Wine dataset explained variance](wine_vars.pdf){width=70%}

## PCA on MNIST

![PCA on MNIST](mnist_pca.png)

## t-SNE on MNIST

![t-SNE on MNIST](mnist_tsne.png)

## UMAP on MNIST

![UMAP on MNIST](mnist_umap.png)

# Линейные системы


## Матричные разложения и линейные системы

В задаче наименьших квадратов (aka линейной регрессии) мы имеем измерения $X \in \mathbb{R}^{m \times n}$ и $y \in \mathbb{R}^{m}$ и ищем вектор $\theta \in \mathbb{R}^{n}$ такой, что $X \theta$ близок к $y$. Близость определяется как сумма квадратов разностей: 
$$ 
\sum\limits_{i=1}^m (x_i^\top \theta - y_i)^2 \qquad \|X \theta - y\|^2_2 \to \min_{\theta \in \mathbb{R}^{n}} \qquad X \theta^* = y
$$

![Illustration of linear system aka least squares](lls_idea.pdf)

## Матричные разложения и линейные системы

### Moore–Penrose inverse
Если матрица $X$ относительно мала, мы можем записать и вычислить точное решение:

$$
\theta^* = (X^\top X)^{-1} X^\top y = X^\dagger y, 
$$

. . .

где $X^\dagger$ называется [псевдо-обратной](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) матрицей. Однако, этот подход возводит в квадрат число обусловленности задачи, что может быть проблемой для больших и плохо обусловленных задач. 

. . .

### QR разложение
Для любой матрицы $X \in \mathbb{R}^{m \times n}$ существует QR разложение:

$$
X = Q \cdot R,
$$

. . .

где  $Q$ - ортогональная матрица (ее столбцы ортогональные единичные векторы) и $R$ - верхняя треугольная матрица. Важно отметить, что поскольку $Q^{-1} = Q^\top$, мы имеем:

$$
QR\theta = y \quad \longrightarrow \quad R \theta = Q^\top y
$$

Теперь процесс нахождения $\theta$ состоит из двух шагов:

1. Найдите QR разложение $X$.
1. Решите треугольную систему $R \theta = Q^\top y$, которая треугольная и, следовательно, легко решаемая.

## Матричные разложения и линейные системы

### Разложение Холецкого
Для любой положительно определенной матрицы $A \in \mathbb{R}^{n \times n}$ существует разложение Холецкого:

$$
X^\top X = A = L^\top \cdot L,
$$

где  $L$ - нижняя треугольная матрица. Мы имеем:

$$
L^\top L\theta = y \quad \longrightarrow \quad L^\top z_\theta = y
$$

Теперь процесс нахождения $\theta$ состоит из двух шагов:

1. Найдите разложение Холецкого $X^\top X$.
1. Найдите $z_\theta = L\theta$ путем решения треугольной системы $L^\top z_\theta = y$
1. Найдите $\theta$ путем решения треугольной системы $L\theta = z_\theta$

Обратите внимание, что в этом случае ошибка пропорциональна квадрату числа обусловленности.

## Матричные разложения и линейные системы

![Illustration](lls_times.pdf){width=78%}

## Число обусловленности $\varkappa$

[![](condition_number_gd.pdf)](https://fmin.xyz/docs/visualizations/condition_number_gd.mp4)

## Матричные разложения и линейные системы

![Illustration](non_linear_fit.pdf){width=78%}