---
title: Введение в продвинутые итерационные методы на примере метода сопряженных градиентов. Предобуславливатели
author: Даня Меркулов
institute: МФТИ. AI360
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back11.jpeg}
---

# Квадратичная задача оптимизации

## Сильно выпуклая квадратичная функция

:::: {.columns}

::: {.column width="60%"}
Рассмотрим следующую квадратичную задачу оптимизации:
$$
\min\limits_{x \in \mathbb{R}^n} f(x) =  \min\limits_{x \in \mathbb{R}^n} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ где }A \in \mathbb{S}^n_{++}.
$$ {#eq-main_problem}
:::
::: {.column width="40%"}
Условия оптимальности
$$
Ax^* = b
$$
:::
::::
![](SD_vs_CG.pdf)

## Наискорейший спуск aka точный линейный поиск

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_{k+1}) = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$
Более теоретический, чем практический подход к выбору шага. Он также позволяет анализировать сходимость, но точный линейный поиск может быть численно сложным, если вычисление функции занимает слишком много времени или требует слишком много ресурсов.

Интересное теоретическое свойство этого метода заключается в том, что каждая следующая итерация метода ортогональна предыдущей:
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

. . .

Условия оптимальности:

. . .

$$
\nabla f(x_k)^T\nabla f(x_{k+1})  = 0
$$

:::{.callout-caution}

### Оптимальное значение для квадратичных функций

$$
\nabla f(x_k)^\top A (x_k - \alpha \nabla f(x_k)) - \nabla f(x_k)^\top b = 0 \qquad \alpha_k = \frac{\nabla f(x_k)^T \nabla f(x_k)}{\nabla f(x_k)^T A \nabla f(x_k)}
$$
:::
:::
::: {.column width="20%"}

![Наискорейший спуск](GD_vs_Steepest.pdf)

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

# Ортогональность

## Сопряженные направления. $A$-ортогональность.

[![](A_orthogonality.pdf){#fig-aorth}](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/CG.ipynb)

## Сопряженные направления. $A$-ортогональность.

Предположим, у нас есть две системы координат и квадратичная функция $f(x) = \frac12 x^T I x$ выглядит так, как на левой части [изображения @fig-aorth], в то время как в других координатах она выглядит как $f(\hat{x}) = \frac12 \hat{x}^T A \hat{x}$, где $A \in \mathbb{S}^n_{++}$.

:::: {.columns}

::: {.column width="50%"}
$$
\frac12 x^T I x
$$
:::
::: {.column width="50%"}
$$
\frac12 \hat{x}^T A \hat{x}
$$
:::
::::
Поскольку $A = Q \Lambda Q^T$:
$$
\uncover<+->{ \frac12 \hat{x}^T A \hat{x} }\uncover<+->{ = \frac12 \hat{x}^T Q \Lambda Q^T \hat{x} }\uncover<+->{  = \frac12 \hat{x}^T Q \Lambda^{\frac12}\Lambda^{\frac12} Q^T \hat{x} }\uncover<+->{ = \frac12 x^T I x} \uncover<+->{\text{ и }  \hat{x} = Q \Lambda^{-\frac12} x}
$$

. . .

:::{.callout-caution}

### $A$-ортогональные векторы

Векторы $x \in \mathbb{R}^n$ и $y \in \mathbb{R}^n$ называются $A$-ортогональными (или $A$-сопряженными), если
$$
x^T A y = 0 \qquad \Leftrightarrow \qquad x \perp_A y 
$$
Когда $A = I$, $A$-ортогональность превращается в ортогональность.
:::

## Процесс Грама-Шмидта

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS1.pdf)

## Процесс Грама-Шмидта {.noframenumbering} 

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS2.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS3.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS4.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS5.pdf)

## Процесс Грама-Шмидта

:::: {.columns}
::: {.column width="20%"}

![](GS5.pdf)

![](Projection.pdf)

:::

::: {.column width="80%"}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

. . .

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.
$$
\begin{aligned}
\uncover<+->{ d_0 &= u_0 \\ }
\uncover<+->{ d_1 &= u_1 - \pi_{d_0}(u_1) \\ }
\uncover<+->{ d_2 &= u_2 - \pi_{d_0}(u_2) - \pi_{d_1}(u_2) \\ }
\uncover<+->{ &\vdots \\ }
\uncover<+->{ d_k &= u_k - \sum\limits_{i=0}^{k-1}\pi_{d_i}(u_k) }
\end{aligned}
$$

. . .

$$
d_k = u_k + \sum\limits_{i=0}^{k-1}\beta_{ik} d_i \qquad \beta_{ik} = - \dfrac{\langle d_i, u_k \rangle}{\langle d_i, d_i \rangle}
$$ {#eq-GS}
:::
::::


# Метод сопряженных направлений (CD)

## Общая идея

* В изотропном случае $A=I$ метод наискорейшего спуска, запущенный из произвольной точки в $n$ ортогональных линейно независимых направлениях, сойдется за $n$ шагов в точных арифметических вычислениях. Мы пытаемся построить аналогичную процедуру в случае $A \neq I$ с использованием концепции $A$-ортогональности.
* Предположим, у нас есть набор из $n$ линейно независимых $A$-ортогональных направлений $d_0, \ldots, d_{n-1}$ (которые будут вычислены с помощью процесса Грама-Шмидта). 
* Мы хотим построить метод, который идет из $x_0$ в $x^*$ для квадратичной задачи с шагами $\alpha_i$, который, фактически, является разложением $x^* - x_0$ в некотором базисе:
    $$
    x^* = x_0 + \sum\limits_{i=0}^{n-1} \alpha_i d_i \qquad x^* - x_0 = \sum\limits_{i=0}^{n-1} \alpha_i d_i
    $$
* Мы докажем, что $\alpha_i$ и $d_i$ могут быть построены очень эффективно с вычислительной точки зрения (метод сопряженных градиентов).

## Идея метода сопряженных направлений (CD)

Таким образом, мы формулируем алгоритм:

1. Пусть $k = 0$ и $x_k = x_0$, посчитаем $d_k = d_0 = -\nabla f(x_0)$.
2. С помощью процедуры точного линейного поиска находим оптимальную длину шага. Вычисляем $\alpha$ минимизируя $f(x_k + \alpha_k d_k)$ по формуле
    $$
    \alpha_k = -\frac{d_k^\top (A x_k - b)}{d_k^\top A d_k}
    $$ {#eq-line_search}
3. Выполняем шаг алгоритма:
    $$
    x_{k+1} = x_k + \alpha_k d_k
    $$
4. Обновляем направление: $d_{k+1} = -\nabla f(x_{k+1}) + \beta_k d_k$ в целях сохранения $d_{k+1} \perp_A d_k$, где $\beta_k$ вычисляется по формуле:
    $$
    \beta_k = \frac{\nabla f(x_{k+1})^\top A d_k}{d_k^\top A d_k}.
    $$
5. Повторяем шаги 2-4, пока не построим $n$ направлений, где $n$ - размерность пространства ($x$).

## Метод сопряженных направлений (CD)

::: {.callout-theorem}

## Лемма 1. Линейная независимость $A$-ортогональных векторов.

Если множество векторов $d_1, \ldots, d_n$ - попарно $A$-ортогональны (каждая пара векторов $A$-ортогональна), то эти векторы линейно независимы. $A \in \mathbb{S}^n_{++}$.

:::

. . .

**Доказательство**

Покажем, что если $\sum\limits_{i=1}^n\alpha_i d_i = 0$, то все коэффициенты должны быть равны нулю:

. . .

$$
\begin{aligned}
\uncover<+->{ 0 &= \sum\limits_{i=1}^n\alpha_i d_i \\ }
\uncover<+->{ \text{Умножаем на } d_j^T A \cdot \qquad &= d_j^\top A \left( \sum\limits_{i=1}^n\alpha_i d_i \right) }
\uncover<+->{ =  \sum\limits_{i=1}^n \alpha_i d_j^\top A d_i  \\ }
\uncover<+->{ &=  \alpha_j d_j^\top A d_j  + 0 + \ldots + 0 }
\end{aligned}
$$

. . .

Таким образом, $\alpha_j = 0$, для всех остальных индексов нужно проделать тот же процесс

## Доказательство сходимости

Введем следующие обозначения:

* $r_k = b - Ax_k$ - невязка
* $e_k = x_k - x^*$ - ошибка
* Поскольку $Ax^* = b$, имеем $r_k = b - Ax_k = Ax^* - Ax_k = -A (x_k - x^*)$
    $$
    r_k = -Ae_k.
    $$ {#eq-res_error}
* Также заметим, что поскольку $x_{k+1} = x_0 + \sum\limits_{i=1}^k\alpha_i d_i$, имеем 
    $$
    e_{k+1} = e_0 + \sum\limits_{i=1}^k\alpha_i d_i.
    $$ {#eq-err_decomposition}

## Доказательство сходимости

::: {.callout-theorem}

## Лемма 2. Сходимость метода сопряженных направлений.

Предположим, мы решаем $n$-мерную квадратичную сильно выпуклую задачу оптимизации ([-@eq-main_problem]). Метод сопряженных направлений
$$
x_{k+1} = x_0 + \sum\limits_{i=0}^k\alpha_i d_i
$$
с $\alpha_i = \frac{\langle d_i, r_i \rangle}{\langle d_i, Ad_i \rangle}$ взятым из точного линейного поиска, сходится за не более $n$ шагов алгоритма.
:::

. . .

:::: {.columns}
::: {.column width="33%"}
**Доказательство**

1. Нужно доказать, что $\delta_i = - \alpha_i$:
    $$
    e_0 = x_0 - x^* =  \sum\limits_{i=0}^{n-1}\delta_i d_i
    $$
:::

. . .

::: {.column width="66%"}
2. Умножаем обе части слева на $d_k^T A$:
    $$
    \begin{aligned}
    \uncover<+->{ d_k^T Ae_0 &= \sum\limits_{i=0}^{n-1}\delta_i d_k^T A d_i}\uncover<+->{  = \delta_k d_k^T A d_k \\}
    \uncover<+->{ d_k^T A\left(e_0 + \sum\limits_{i=0}^{k-1}\alpha_i d_i \right)}\uncover<+->{ = d_k^T A e_k }\uncover<+->{  &= \delta_k d_k^T A d_k \quad \left(A-\text{ ортогональность}\right)\\}
    \uncover<+->{ \delta_k = \frac{ d_k^T A e_k}{d_k^T A d_k }}\uncover<+->{ = -\frac{ d_k^T r_k}{d_k^T A d_k } }\uncover<+->{  &\Leftrightarrow \delta_k = - \alpha_k }
    \end{aligned}
    $$
:::
::::

## Леммы для сходимости

::: {.callout-theorem}

## Лемма 3. Разложение ошибки.

$$
e_i = \sum\limits_{j=i}^{n-1}-\alpha_j d_j 
$$ {#eq-err_decomposition}

:::

. . .

**Доказательство**

По определению
$$
\uncover<+->{ e_{i} = e_0 + \sum\limits_{j=0}^{i-1}\alpha_j d_j }\uncover<+->{ = x_0 - x^* + \sum\limits_{j=0}^{i-1}\alpha_j d_j }\uncover<+->{  = -\sum\limits_{j=0}^{n-1}\alpha_j d_j + \sum\limits_{j=0}^{i-1}\alpha_j d_j}\uncover<+->{  = \sum\limits_{j=i}^{n-1}-\alpha_j d_j }
$$

## Леммы для сходимости

::: {.callout-theorem}

## Лемма 4. Невязка ортогональна всем предыдущим направлениям для CD.

Рассмотрим невязку метода сопряженных направлений на $k$ итерации $r_k$, тогда для любого $i < k$:

$$
d_i^T r_k = 0
$$ {#eq-res_orth_dir}

:::

. . .

**Доказательство**

:::: {.columns}
::: {.column width="40%"}
Запишем ([-@eq-err_decomposition]) для некоторого фиксированного индекса $k$:

. . .

$$
e_k = \sum\limits_{j=k}^{n-1}-\alpha_j d_j 
$$

. . .

Умножаем обе части на $-d_i^TA \cdot$
$$
-d_i^TA e_k = \sum\limits_{j=k}^{n-1}\alpha_j d_i^TA d_j  = 0
$$
:::

::: {.column width="60%"}
![](CG_lem1.pdf)
Таким образом, $d_i^T r_k = 0$ и невязка $r_k$ ортогональна всем предыдущим направлениям $d_i$ для метода CD.
:::
::::

# Метод сопряженных градиентов (CG)

## Идея метода сопряженных градиентов (CG)

* Это буквально метод сопряженных направлений, в котором мы выбираем специальный набор $d_0, \ldots, d_{n-1}$, позволяющий значительно ускорить процесс Грама-Шмидта.
* Используется процесс Грама-Шмидта с $A$-ортогональностью вместо Евклидовой ортогональности, чтобы получить их из набора начальных векторов.
* На каждой итерации $r_0, \ldots, r_{n-1}$ используются в качестве начальных векторов для процесса Грама-Шмидта.
* Основная идея заключается в том, что для произвольного метода CD процесс Грама-Шмидта вычислительно дорогой и требует квадратичного числа операций сложения векторов и скалярных произведений $\mathcal{O}\left( n^2\right)$, в то время как в случае CG мы покажем, что сложность этой процедуры может быть уменьшена до линейной $\mathcal{O}\left( n\right)$.

. . .

:::{.callout-caution appearance="simple"}
$$
\text{CG} = \text{CD} + r_0, \ldots, r_{n-1} \text{ как начальные векторы для процесса Грама-Шмидта} + A\text{-ортогональность.}
$$
:::

# Метод сопряженных градиентов (CG)

## Леммы для сходимости

::: {.callout-theorem}

## Лемма 5. Невязки ортогональны друг другу в методе CG

Все невязки в методе CG ортогональны друг другу:
$$
r_i^T r_k = 0 \qquad \forall i \neq k
$$ {#eq-res_orth_cg}

:::

. . .

:::: {.columns}
::: {.column width="40%"}

**Доказательство**

Запишем процесс Грама-Шмидта ([-@eq-GS]) с $\langle \cdot, \cdot \rangle$ замененным на $\langle \cdot, \cdot \rangle_A = x^T A y$

. . .

$$
d_i = u_i + \sum\limits_{j=0}^{i-1}\beta_{ji} d_j \;\; \beta_{ji} = - \dfrac{\langle d_j, u_i \rangle_A}{\langle d_j, d_j \rangle_A}
$$ {#eq-gs_cg1}

. . .

Тогда, мы используем невязки в качестве начальных векторов для процесса и $u_i = r_i$.

. . .

$$ 
d_i = r_i + \sum\limits_{j=0}^{i-1}\beta_{ji} d_j \;\; \beta_{ji} = - \dfrac{\langle d_j, r_i \rangle_A}{\langle d_j, d_j \rangle_A}
$$ {#eq-gs_cg2}
:::

::: {.column width="60%"}
![](CG_lem1.pdf)
Умножаем обе части ([-@eq-gs_cg1]) на $r_k^T \cdot$ для некоторого индекса $k$:
$$
r_k^Td_i = r_k^Tu_i + \sum\limits_{j=0}^{i-1}\beta_{ji} r_k^Td_j 
$$

. . .

Если $j < i < k$, то имеем лемму 4 с $d_i^T r_k = 0$ и $d_j^T r_k = 0$. Имеем:
$$
r_k^Tu_i= 0 \;\text{ для CD} \;\; r_k^Tr_i = 0 \;\text{ для CG}
$$
:::
::::

## Леммы для сходимости

Более того, если $k=i$:
$$
\uncover<+->{ r_k^Td_k = r_k^Tu_k + \sum\limits_{j=0}^{k-1}\beta_{jk} r_k^Td_j}\uncover<+->{  = r_k^Tu_k + 0,}
$$

. . .

и мы имеем для любого $k$ (из-за произвольного выбора $i$):
$$
r_k^Td_k = r_k^Tu_k.
$$ {#eq-lemma5}

. . .

::: {.callout-theorem}

## Лемма 6. Пересчет невязки

$$
r_{k+1} = r_k - \alpha_k A d_k 
$$ {#eq-res_recalculation}

:::

. . .

$$
r_{k+1} = -A e_{k+1} = -A \left( e_{k} + \alpha_k d_k \right) = -A e_{k} - \alpha_k A d_k = r_k - \alpha_k A d_k 
$$

Наконец, все эти вышеуказанные леммы достаточны для доказательства, что $\beta_{ji} = 0$ для всех $i,j$, кроме соседних.

## Грам-Шмидт в методе CG

Рассмотрим процесс Грам-Шмидта в методе CG
$$
\uncover<+->{ \beta_{ji} = - \dfrac{\langle d_j, u_i \rangle_A}{\langle d_j, d_j \rangle_A} }\uncover<+->{  = - \dfrac{ d_j^T A u_i }{ d_j^T A d_j }}\uncover<+->{  = - \dfrac{ d_j^T A r_i }{ d_j^T A d_j }}\uncover<+->{  = - \dfrac{r_i^T A d_j}{ d_j^T A d_j }.}
$$

. . .

Рассмотрим скалярное произведение $\langle r_i, r_{j+1} \rangle$ используя ([-@eq-res_recalculation]):
$$
\begin{aligned}
\uncover<+->{ \langle r_i, r_{j+1} \rangle}\uncover<+->{  &= \langle r_i, r_j - \alpha_j A d_j  \rangle }\uncover<+->{ = \langle r_i, r_j \rangle - \alpha_j\langle r_i, A d_j  \rangle \\}
\uncover<+->{ \alpha_j\langle r_i, A d_j  \rangle }\uncover<+->{  &= \langle r_i, r_j \rangle - \langle r_i, r_{j+1} \rangle }
\end{aligned}
$$

1. Если $i=j$: $\alpha_i\langle r_i, A d_i  \rangle = \langle r_i, r_i \rangle - \langle r_i, r_{i+1} \rangle = \langle r_i, r_i \rangle$. Этот случай не интересен по построению процесса Грам-Шмидта.
2. Соседний случай $i=j + 1$: $\alpha_j\langle r_i, A d_j \rangle = \langle r_i, r_{i-1} \rangle - \langle r_i, r_{i} \rangle = - \langle r_i, r_i \rangle$
3. Для любого другого случая: $\alpha_j\langle r_i, A d_j \rangle = 0$, потому что все невязки ортогональны друг другу.

. . .

Наконец, мы имеем формулу для $i=j + 1$:
$$
\uncover<+->{ \beta_{ji} = - \dfrac{r_i^T A d_j}{ d_j^T A d_j}}\uncover<+->{  = \dfrac{1}{\alpha_j}\dfrac{\langle r_i, r_i \rangle}{ d_j^T A d_j} }\uncover<+->{  =  \dfrac{d_j^T A d_j}{d_j^T r_j}\dfrac{\langle r_i, r_i \rangle}{ d_j^T A d_j} }\uncover<+->{ = \dfrac{\langle r_i, r_i \rangle}{\langle r_j, r_j \rangle} }\uncover<+->{ = \dfrac{\langle r_i, r_i \rangle}{\langle r_{i-1}, r_{i-1} \rangle}}
$$

. . .

И для направления $d_{k+1} = r_{k+1} + \beta_{k,k+1} d_k, \qquad  \beta_{k,k+1} = \beta_k = \dfrac{\langle r_{k+1}, r_{k+1} \rangle}{\langle r_{k}, r_{k} \rangle}.$

## Метод сопряженных градиентов (CG)

$$
\begin{aligned}
& \mathbf{r}_0 := \mathbf{b} - \mathbf{A x}_0 \\
& \hbox{if } \mathbf{r}_{0} \text{ is sufficiently small, then return } \mathbf{x}_{0} \text{ as the result}\\
& \mathbf{d}_0 := \mathbf{r}_0 \\
& k := 0 \\
& \text{repeat} \\
& \qquad \alpha_k := \frac{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k}{\mathbf{d}_k^\mathsf{T} \mathbf{A d}_k}  \\
& \qquad \mathbf{x}_{k+1} := \mathbf{x}_k + \alpha_k \mathbf{d}_k \\
& \qquad \mathbf{r}_{k+1} := \mathbf{r}_k - \alpha_k \mathbf{A d}_k \\
& \qquad \hbox{if } \mathbf{r}_{k+1} \text{ is sufficiently small, then exit loop} \\
& \qquad \beta_k := \frac{\mathbf{r}_{k+1}^\mathsf{T} \mathbf{r}_{k+1}}{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k} \\
& \qquad \mathbf{d}_{k+1} := \mathbf{r}_{k+1} + \beta_k \mathbf{d}_k \\
& \qquad k := k + 1 \\
& \text{end repeat} \\
& \text{return } \mathbf{x}_{k+1} \text{ as the result}
\end{aligned}
$$

## Закрываем квадратичный вопрос

[![](cg_cgd_gd.pdf)](https://fmin.xyz/docs/visualizations/cg_gd.mp4)

## Сходимость

**Теорема 1.** Если матрица $A$ имеет только $r$ различных собственных значений, то метод сопряженных градиентов сходится за $r$ итераций.

**Теорема 2.** Следующая оценка сходимости выполняется для метода сопряженных градиентов, как для итерационного метода в сильно выпуклой задаче:

$$
\| x_{k} - x^* \|_A \leq 2\left( \dfrac{\sqrt{\kappa(A)} - 1}{\sqrt{\kappa(A)} + 1} \right)^k \|x_0 - x^*\|_A,
$$

где $\|x\|^2_A = x^{\top}Ax$ и $\varkappa(A) = \frac{\lambda_1(A)}{\lambda_n(A)}$ - это число обусловленности матрицы $A$, $\lambda_1(A) \geq ... \geq \lambda_n(A)$ - собственные значения матрицы $A$

**Примечание:** Сравните коэффициент геометрической прогрессии с его аналогом в методе градиентного спуска.

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_random_0.001_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_random_10_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_random_10_1000_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_clustered_10_1000_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_clustered_10_1000_600.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_uniform spectrum_1_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_Hilbert_1_10_60.pdf)

# Предобуславливатели

## Предобуславливатели: общая концепция

Общая концепция предобуславливателя проста:

Дана линейная система 
$$
A x = f,
$$
мы хотим найти матрицу $P_R$ и/или $P_L$ такую, что 

1. Число обусловленности $AP_R^{-1}$ (правый предобуславливатель) или $P^{-1}_LA$ (левый предобуславливатель) или $P^{-1}_L A P_R^{-1}$ лучше, чем для $A$
2. Мы можем легко решить $P_Ly = g$ или $P_Ry = g$ для любого $g$ (иначе мы могли бы выбрать $P_L = A$)

::::{.columns}
::: {.column width="34%"}

Тогда мы решаем для (правый предобуславливатель)
$$
AP_R^{-1} y = f \quad \Rightarrow \quad P_R x = y
$$ 
:::

::: {.column width="33%"}

или (левый предобуславливатель)
$$
P_L^{-1} A x = P_L^{-1}f,
$$
:::

::: {.column width="33%"}

или даже оба
$$
P_L^{-1} A P_R^{-1} y = P_L^{-1}f \quad \Rightarrow \quad P_R x = y.
$$ 
:::
::::

Лучший выбор, конечно, $P = A,$ но это не упрощает жизнь.

## Простейший предобуславливатель: Якоби

:::: {.columns}
::: {.column width="60%"}

Рассмотрим $i$-ое уравнение системы:
$$
\begin{aligned}
\sum\limits_{j=1}^n a_{ij} x_j = f_i,\\
a_{ii} x_i = f_i - \sum\limits_{j=1, j\neq i}^n a_{ij} x_j,\\
\end{aligned}
$$

и используем это для итеративного обновления $x_i$:
$$
x_i^{(k+1)} = -\frac{1}{a_{ii}}\left( \sum_{i \ne j} a_{ij} x_j^{(k)} - f_i \right),
$$

. . .

или в матричной форме
$$
x^{(k+1)} = D^{-1}\left((D-A)x^{(k)} + f\right) 
$$
где $D = \mathrm{diag}(A)$ и в итоге
$$
x^{(k+1)} = x^{(k)} - D^{-1}(Ax^{(k)} - f).
$$
:::

::: {.column width="40%"}

Таким образом, метод Якоби это метод Ричардсона (градиентный спуск) с шагом $\tau=1$ и левым предобуславливателем $P = D = \text{diag}(A)$ - диагональ матрицы. Поэтому мы будем называть $P = \mathrm{diag}(A)$ **предобуславливателем Якоби**. Обратите внимание, что он может быть использован для любого другого метода, например, для метода сопряженных градиентов.
:::
::::