---
title: Отражения Хаусхолдера. Вращения Гивенса. Ранг матрицы. Скелетон. SVD
author: Даня Меркулов
institute: МФТИ. AI360
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back3.jpeg}
---

# Ортогональные матрицы

## Ортогональные (унитарные) матрицы

* Пусть $U$ - матрица размера $n \times n$, и $\Vert U z \Vert_2 = \Vert z \Vert_2$ для всех $z$. 

* Это может произойти **тогда и только тогда**, когда
    $$
    U^T U = I_n,
    $$
    где $I_n$ - единичная матрица $n \times n$.

* Квадратная матрица размера $n \times n$ называется **ортогональной**, если
    $$
    U^T U = UU^T = I_n, 
    $$
    что означает, что столбцы и строки ортогональной матрицы образуют ортонормированный базис в $\mathbb{R}^{n}$.
* Для матрицы из комплексных чисел вводится аналогичное определение **унитарной матрицы**:
    $$
    U^* U = UU^* = I_n,
    $$
    где $U^*$ - эрмитово сопряжение матрицы $U$.

* Для прямоугольных матриц размера $m\times n$ ($n\not= m$) только одно из равенств может выполняться
    * $U^T U = I_n$ - матрица $U$ называется **матрицей с ортогональными столбцами** для $m>n$
    * $UU^T = I_m$  - матрица $U$ называется **матрицей с ортогональными строками** для $m<n$

## Ортогональные матрицы

Важное свойство: **произведение двух ортогональных матриц является ортогональной матрицей:**  

$$(UV)^T UV = V^T U^T UV = V^T V = I,$$

- Позже мы покажем, что существуют типы матриц (**отражения Хаусхолдера** и **вращения Гивенса**) композиция которых способна произвести любую унитарную матрицу
- Эта идея является основой некоторых алгоритмов, например, QR-разложения

## Ортогональная инвариантность $\|\cdot\|_2$ и $\|\cdot\|_F$ норм

* Для векторной 2-нормы мы уже видели, что $\Vert U z \Vert_2 = \Vert z \Vert_2$ для любой ортогональной $U$.

* Можно показать, что ортогональные матрицы не изменяют матричные нормы $\|\cdot\|_2$ и $\|\cdot\|_F$, т.е. для любой квадратной $A$ и ортогональной $U$,$V$: 
    $$
    \| UAV\|_2 = \| A \|_2 \qquad \| UAV\|_F = \| A \|_F.
    $$

* Для $\|\cdot\|_2$ это следует из определения операторной нормы и того факта, что векторная 2-норма является ортогонально инвариантной.

* Для $\|\cdot\|_F$ это следует из $\|A\|_F^2 = \mathrm{trace}(A^TA)$ и того факта, что $\mathrm{trace}(BC) = \mathrm{trace}(CB)$.

## Примеры ортогональных матриц

* Матрица вращения
* Матрица перестановки
* Матрица отражения Хаусхолдера
* Матрица вращения Гивенса

## Матрица отражения Хаусхолдера

::::{.columns}
:::{.column width="50%"}
Матрица отражения Хаусхолдера - это ортогональная матрица, которая используется для отражения вектора относительно гиперплоскости. Она имеет следующий вид:

$$
H \equiv H(v) = I - 2 vv^T,
$$

где $v$ - вектор длины $n$ и $v^T v = 1$. 

- Покажите, что $H$ - ортогональная матрица и $H^T = H$.
- Покажите, что $H$ - отражение:
    $$
    Hx = x - 2(v^T x) v
    $$
:::

:::{.column width="50%"}
![](householder.jpeg){width=100% fig-align="center"}
:::
::::

## Важное свойство матрицы отражения Хаусхолдера

Хорошее свойство матрицы отражения Хаусхолдера состоит в том, что она может обнулить все элементы вектора, кроме первого:
$$
H \begin{bmatrix} \times \\ \times \\ \times  \end{bmatrix} =  \begin{bmatrix} \times \\ 0 \\ 0  \end{bmatrix}.
$$

::::{.columns}

:::{.column width="50%"}

* Доказательство. Пусть $e_1 = (1,0,\dots, 0)^T$, тогда мы хотим найти $v$ такой, что
    $$
    H x = x - 2(v^T x) v = \alpha e_1,
    $$
    где $\alpha$ - неизвестная константа.

* Так как $\|\cdot\|_2$ является унитарно инвариантной, мы получаем
    $$
    \|x\|_2 = \|Hx\|_2 = \|\alpha e_1\|_2 = |\alpha|.
    $$
    и $\alpha = \pm \|x\|_2$

* Также мы можем выразить $v$ из $x - 2(v^T x) v = \alpha e_1$:
    $$
    v = \dfrac{x-\alpha e_1}{2 v^T x}
    $$

:::

:::{.column width="50%"}

* Умножая последнее выражение на $x^T$ мы получаем
    $$
    x^T x - 2 (v^T x) x^T v = \alpha x_1; 
    $$
    или 
    $$
    \|x\|_2^2 - 2 (v^T x)^2 = \alpha x_1.
    $$

* Следовательно,
    $$
    (v^T x)^2 = \frac{\|x\|_2^2 - \alpha x_1}{2}.
    $$

* Таким образом, $v$ существует и равно
    $$
    v = \dfrac{x \mp \|x\|_2 e_1}{2v^T x} = \dfrac{x \mp \|x\|_2 e_1}{\pm\sqrt{2(\|x\|_2^2 \mp \|x\|_2 x_1)}}. 
    $$

:::

::::

## Алгоритм Хаусхолдера для построения QR-разложения

:::: {.columns}
::: {.column width="50%"}
* Используя полученное свойство мы можем сделать произвольную матрицу $A$ нижней треугольной:
    $$
    H_2 H_1 A = \begin{bmatrix} \times & \times & \times & \times \\ 0 & \times & \times & \times  \\  0 & 0 & \boldsymbol{\times} & \times\\ 0 &0 & \boldsymbol{\times} & \times  \\ 0 &0 & \boldsymbol{\times} & \times \end{bmatrix}
    $$

* Затем находим $H_3=\begin{bmatrix}I_2 & \\ & {\widetilde H}_3 \end{bmatrix}$ такую, что
    $$
    {\widetilde H}_3 \begin{bmatrix} \boldsymbol{\times}\\ \boldsymbol{\times} \\ \boldsymbol{\times}  \end{bmatrix} = \begin{bmatrix} \times \\ 0 \\ 0  \end{bmatrix}.
    $$

:::

::: {.column width="50%"}
* Получаем
    $$
    H_3 H_2 H_1 A =  \begin{bmatrix} \times & \times & \times & \times \\  0 & \times & \times & \times  \\  0 & 0 & {\times} & \times\\  0 &0 & 0 & \times  \\  0 &0 & 0 & \times  \end{bmatrix}
    $$

* Аналогично находим $H_4$ и получаем верхнюю треугольную матрицу.
* Так как произведение ортогональных и обратная к ортогональной матрицы являются ортогональными матрицами, мы получаем **следствие:** (QR-разложение) Любая $A\in \mathbb{R}^{n\times m}$ может быть представлена как
    $$
    A = QR,
    $$
    где $Q$ - ортогональная и $R$ - верхняя треугольная. 

    См. [постер](decompositions.pdf), каковы размеры $Q$ и $R$ для $n>m$ и $n<m$.
:::
::::

## QR-разложение матрицы

:::: {.columns}
::: {.column width="60%"}
![](QR.pdf)
:::

::: {.column width="40%"}
Для произвольной матрицы $A \in \mathbb{R}^{m \times n}$ существует разложение вида:
$$
A = QR,
$$
где $Q \in \mathbb{R}^{m \times m}$ - ортогональная матрица ($Q^TQ = QQ^T = I$), а $R \in \mathbb{R}^{m \times n}$ - верхнетреугольная матрица. В случае, когда $m > n$, матрица $Q$ может быть усечена до размера $m \times n$ с сохранением ортогональности столбцов. Если зафиксировать все диагональные элементы матрицы $R$, то разложение становится единственным.
:::
::::


## Матрица вращения Гивенса (Якоби)

* Матрица вращения Гивенса (Якоби) - это ортогональная матрица, которая используется для вращения вектора в плоскости на угол $\alpha$. Она имеет следующий вид:
    $$
    G = \begin{bmatrix} \cos \alpha & -\sin \alpha \\ \sin \alpha & \cos \alpha \end{bmatrix}
    $$
    Легко проверить, что $G^T G = GG^T = I$, то есть матрица является ортогональной.

* Для общего случая размерности $n$ мы выбираем две координаты $(i, j)$ и выполняем вращение вектора $x$ только в этой плоскости:
    $$
    x' = G x,
    $$
    где изменяются только $i$-я и $j$-я координаты:
    $$
    x'_i =  x_i\cos \alpha - x_j\sin \alpha , \quad x'_j = x_i \sin \alpha  + x_j\cos\alpha,
    $$
    при этом остальные $x_k$ остаются неизменными.    

* Чтобы обнулить $j$-ю координату вектора, выбираем угол $\alpha$ так, что:
    $$
    \cos \alpha = \frac{x_i}{\sqrt{x_i^2 + x_j^2}}, \quad \sin \alpha = -\frac{x_j}{\sqrt{x_i^2 + x_j^2}}
    $$

* Применяя последовательно матрицы Гивенса, можно привести матрицу к верхнетреугольному виду: для этого нужно $n-1$ вращений, чтобы обнулить элементы под главной диагональю в каждом столбце.

## QR через вращения Гивенса

Также мы можем сделать матрицу верхнетреугольной с помощью вращений Гивенса:

$$
\begin{bmatrix} \times & \times & \times \\ \bf{*} & \times & \times \\ \bf{*} & \times & \times \end{bmatrix} \to \begin{bmatrix} * & \times & \times \\ * & \times & \times \\ 0 & \times & \times \end{bmatrix} \to \begin{bmatrix} \times & \times & \times \\ 0 & * & \times \\ 0 & * & \times \end{bmatrix} \to \begin{bmatrix} \times & \times & \times \\ 0 & \times & \times \\ 0 & 0 & \times \end{bmatrix}
$$

## Вращения Гивенса vs. отражения Хаусхолдера

- Отражения Хаусхолдера полезны для плотных матриц (сложность приблизительно в два раза меньше, чем для Якоби) и мы должны обнулить большое количество элементов.
- Вращения Гивенса более подходят для разреженных матриц или параллельных вычислений, так как они действуют локально на элементах.

# Ранг матрицы

## Скелетное разложение матрицы

:::: {.columns}

::: {.column width="60%"}
Простое, но очень интересное разложение - скелетное, которое может быть записано в двух формах:

$$
A = U V^T \quad A = \hat{C}\hat{A}^{-1}\hat{R}
$$

. . .

Последнее выражение отсылает к любопытному факту: вы можете случайным образом выбрать $r$ линейно независимых столбцов матрицы и любых $r$ линейно независимых строк матрицы и хранить только их, при этом имея возможность точно восстановить всю матрицу.

. . .

Применения скелетного разложения:

* Упрощение модели, сжатие данных и ускорение вычислений в численных методах: для матрицы ранга $r$ с $r \ll n, m$ достаточно хранить $\mathcal{O}((n + m)r) \ll nm$ элементов.
* Извлечение признаков в машинном обучении, где также известно как матричное разложение.
:::

::: {.column width="40%"}
![Иллюстрация скелетного разложения](skeleton.pdf){#fig-skeleton}
:::

::::


## LoRA: Low-Rank Adaptation of Large Language Models ([arXiv:2106.09685](https://arxiv.org/pdf/2106.09685))

Так как современные LLM слишком большие, чтобы поместиться в память обычной GPU, мы используем некоторые трюки, чтобы сделать потребление памяти меньше. Один из самых популярных трюков - LoRA (Low-Rank Adaptation of Large Language Models).

:::: {.columns}
::: {.column width="55%"}
Предположим, что у нас есть матрица $W \in \mathbb{R}^{d \times k}$ и мы хотим выполнить следующее обновление:
$$
W = W_0 + \Delta W.
$$
Основная идея LoRA - разложить обновление $\Delta W$ на две низкоранговые матрицы:

\begin{gather*}
W = W_0 + \Delta W = W_0 + BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, \\
rank(A) = rank(B) = r \ll \min\{d, k\}.
\end{gather*}

Проверьте [\faPython\ ноутбук](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s1_lora_trump.ipynb) для примера реализации LoRA.

:::

::: {.column width="45%"}
![Иллюстрация LoRA](lora.pdf){width=100%}
:::
::::

## Сингулярное разложение матрицы

:::: {.columns}
::: {.column width="60%"}
![](SVD.pdf)
:::

::: {.column width="40%"}
Для любой матрицы $A \in \mathbb{R}^{m \times n}$ существует разложение:
$$
A = U\Sigma V^*,
$$
где

* $U \in \mathbb{R}^{m \times m}$ - унитарная матрица левых сингулярных векторов
* $\Sigma \in \mathbb{R}^{m \times n}$ - диагональная матрица сингулярных чисел $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_{\min(m,n)} \geq 0$
* $V \in \mathbb{R}^{n \times n}$ - унитарная матрица правых сингулярных векторов
* Сингулярные числа единственны. Если все сингулярные числа различны, то разложение единственно с точностью до унитарной диагональной матрицы $D$: $U \Sigma V^* = U D \Sigma (VD)^* = U \Sigma V^*$.
:::
::::