---
title: Отражения Хаусхолдера. Вращения Гивенса. Ранг матрицы. Скелетон. SVD
author: Даня Меркулов
institute: МФТИ. AI360
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back2.jpeg}
---

# Ортогональные матрицы

## Ортогональные (унитарные) матрицы

* Пусть $U$ - матрица размера $n \times n$, и $\Vert U z \Vert_2 = \Vert z \Vert_2$ для всех $z$. 

* Это может произойти **тогда и только тогда**, когда
    $$
    U^T U = I_n,
    $$
    где $I_n$ - единичная матрица $n \times n$.

* Квадратная матрица размера $n \times n$ называется **ортогональной**, если
    $$
    U^T U = UU^T = I_n, 
    $$
    что означает, что столбцы и строки ортогональной матрицы образуют ортонормированный базис в $\mathbb{R}^{n}$.
* Для матрицы из комплексных чисел вводится аналогичное определение **унитарной матрицы**:
    $$
    U^* U = UU^* = I_n,
    $$
    где $U^*$ - эрмитово сопряжение матрицы $U$.

* Для прямоугольных матриц размера $m\times n$ ($n\not= m$) только одно из равенств может выполняться
    * $U^T U = I_n$ - матрица $U$ называется **матрицей с ортогональными столбцами** для $m>n$
    * $UU^T = I_m$  - матрица $U$ называется **матрицей с ортогональными строками** для $m<n$

## Ортогональные матрицы

Важное свойство: **произведение двух ортогональных матриц является ортогональной матрицей:**  

$$(UV)^T UV = V^T U^T UV = V^T V = I,$$

- Позже мы покажем, что существуют типы матриц (**отражения Хаусхолдера** и **вращения Гивенса**) композиция которых способна произвести любую унитарную матрицу
- Эта идея является основой некоторых алгоритмов, например, QR-разложения

## Ортогональная инвариантность $\|\cdot\|_2$ и $\|\cdot\|_F$ норм

* Для векторной 2-нормы мы уже видели, что $\Vert U z \Vert_2 = \Vert z \Vert_2$ для любой ортогональной $U$.

* Можно показать, что ортогональные матрицы не изменяют матричные нормы $\|\cdot\|_2$ и $\|\cdot\|_F$, т.е. для любой квадратной $A$ и ортогональной $U$,$V$: 
    $$
    \| UAV\|_2 = \| A \|_2 \qquad \| UAV\|_F = \| A \|_F.
    $$

* Для $\|\cdot\|_2$ это следует из определения операторной нормы и того факта, что векторная 2-норма является ортогонально инвариантной.

* Для $\|\cdot\|_F$ это следует из $\|A\|_F^2 = \mathrm{trace}(A^TA)$ и того факта, что $\mathrm{trace}(BC) = \mathrm{trace}(CB)$.

## Примеры ортогональных матриц

* Матрица вращения
* Матрица перестановки
* Матрица отражения Хаусхолдера
* Матрица вращения Гивенса

## Матрица отражения Хаусхолдера

::::{.columns}
:::{.column width="50%"}
Матрица отражения Хаусхолдера - это ортогональная матрица, которая используется для отражения вектора относительно гиперплоскости. Она имеет следующий вид:

$$
H \equiv H(v) = I - 2 vv^T,
$$

где $v$ - вектор длины $n$ и $v^T v = 1$. 

- Покажите, что $H$ - ортогональная матрица и $H^T = H$.
- Покажите, что $H$ - отражение:
    $$
    Hx = x - 2(v^T x) v
    $$
:::

:::{.column width="50%"}
![](householder.jpeg){width=100% fig-align="center"}
:::
::::

## Важное свойство матрицы отражения Хаусхолдера

Хорошее свойство матрицы отражения Хаусхолдера состоит в том, что она может обнулить все элементы вектора, кроме первого:
$$
H \begin{bmatrix} \times \\ \times \\ \times  \end{bmatrix} =  \begin{bmatrix} \times \\ 0 \\ 0  \end{bmatrix}.
$$

::::{.columns}

:::{.column width="50%"}

* Доказательство. Пусть $e_1 = (1,0,\dots, 0)^T$, тогда мы хотим найти $v$ такой, что
    $$
    H x = x - 2(v^T x) v = \alpha e_1,
    $$
    где $\alpha$ - неизвестная константа.

* Так как $\|\cdot\|_2$ является унитарно инвариантной, мы получаем
    $$
    \|x\|_2 = \|Hx\|_2 = \|\alpha e_1\|_2 = |\alpha|.
    $$
    и $\alpha = \pm \|x\|_2$

* Также мы можем выразить $v$ из $x - 2(v^T x) v = \alpha e_1$:
    $$
    v = \dfrac{x-\alpha e_1}{2 v^T x}
    $$

:::

:::{.column width="50%"}

* Умножая последнее выражение на $x^T$ мы получаем
    $$
    x^T x - 2 (v^T x) x^T v = \alpha x_1; 
    $$
    или 
    $$
    \|x\|_2^2 - 2 (v^T x)^2 = \alpha x_1.
    $$

* Следовательно,
    $$
    (v^T x)^2 = \frac{\|x\|_2^2 - \alpha x_1}{2}.
    $$

* Таким образом, $v$ существует и равно
    $$
    v = \dfrac{x \mp \|x\|_2 e_1}{2v^T x} = \dfrac{x \mp \|x\|_2 e_1}{\pm\sqrt{2(\|x\|_2^2 \mp \|x\|_2 x_1)}}. 
    $$

:::

::::

## Алгоритм Хаусхолдера для построения QR-разложения

:::: {.columns}
::: {.column width="50%"}
* Используя полученное свойство мы можем сделать произвольную матрицу $A$ нижней треугольной:
    $$
    H_2 H_1 A = \begin{bmatrix} \times & \times & \times & \times \\ 0 & \times & \times & \times  \\  0 & 0 & \boldsymbol{\times} & \times\\ 0 &0 & \boldsymbol{\times} & \times  \\ 0 &0 & \boldsymbol{\times} & \times \end{bmatrix}
    $$

* Затем находим $H_3=\begin{bmatrix}I_2 & \\ & {\widetilde H}_3 \end{bmatrix}$ такую, что
    $$
    {\widetilde H}_3 \begin{bmatrix} \boldsymbol{\times}\\ \boldsymbol{\times} \\ \boldsymbol{\times}  \end{bmatrix} = \begin{bmatrix} \times \\ 0 \\ 0  \end{bmatrix}.
    $$

:::

::: {.column width="50%"}
* Получаем
    $$
    H_3 H_2 H_1 A =  \begin{bmatrix} \times & \times & \times & \times \\  0 & \times & \times & \times  \\  0 & 0 & {\times} & \times\\  0 &0 & 0 & \times  \\  0 &0 & 0 & \times  \end{bmatrix}
    $$

* Аналогично находим $H_4$ и получаем верхнюю треугольную матрицу.
* Так как произведение ортогональных и обратная к ортогональной матрицы являются ортогональными матрицами, мы получаем **следствие:** (QR-разложение) Любая $A\in \mathbb{R}^{n\times m}$ может быть представлена как
    $$
    A = QR,
    $$
    где $Q$ - ортогональная и $R$ - верхняя треугольная. 

    См. [постер](decompositions.pdf), каковы размеры $Q$ и $R$ для $n>m$ и $n<m$.
:::
::::

## Матрица вращения Гивенса (Якоби)

* Матрица вращения Гивенса (Якоби) - это ортогональная матрица, которая используется для вращения вектора в плоскости на угол $\alpha$. Она имеет следующий вид:
    $$
    G = \begin{bmatrix} \cos \alpha & -\sin \alpha \\ \sin \alpha & \cos \alpha \end{bmatrix}
    $$
    Легко проверить, что $G^T G = GG^T = I$, то есть матрица является ортогональной.

* Для общего случая размерности $n$ мы выбираем две координаты $(i, j)$ и выполняем вращение вектора $x$ только в этой плоскости:
    $$
    x' = G x,
    $$
    где изменяются только $i$-я и $j$-я координаты:
    $$
    x'_i =  x_i\cos \alpha - x_j\sin \alpha , \quad x'_j = x_i \sin \alpha  + x_j\cos\alpha,
    $$
    при этом остальные $x_k$ остаются неизменными.    

* Чтобы обнулить $j$-ю координату вектора, выбираем угол $\alpha$ так, что:
    $$
    \cos \alpha = \frac{x_i}{\sqrt{x_i^2 + x_j^2}}, \quad \sin \alpha = -\frac{x_j}{\sqrt{x_i^2 + x_j^2}}
    $$

* Применяя последовательно матрицы Гивенса, можно привести матрицу к верхнетреугольному виду: для этого нужно $n-1$ вращений, чтобы обнулить элементы под главной диагональю в каждом столбце.

## QR через вращения Гивенса

Также мы можем сделать матрицу верхнетреугольной с помощью вращений Гивенса:

$$
\begin{bmatrix} \times & \times & \times \\ \bf{*} & \times & \times \\ \bf{*} & \times & \times \end{bmatrix} \to \begin{bmatrix} * & \times & \times \\ * & \times & \times \\ 0 & \times & \times \end{bmatrix} \to \begin{bmatrix} \times & \times & \times \\ 0 & * & \times \\ 0 & * & \times \end{bmatrix} \to \begin{bmatrix} \times & \times & \times \\ 0 & \times & \times \\ 0 & 0 & \times \end{bmatrix}
$$

## Вращения Гивенса vs. отражения Хаусхолдера

- Отражения Хаусхолдера полезны для плотных матриц (сложность приблизительно в два раза меньше, чем для Якоби) и мы должны обнулить большое количество элементов.
- Вращения Гивенса более подходят для разреженных матриц или параллельных вычислений, так как они действуют локально на элементах.

# Ранг матрицы

# Скелетное разложение

# Сингулярное разложение

# Приложения

## Image compression

## Метод главных компонент

## LoRa